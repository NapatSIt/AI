Abstract
"Triple-negative breast cancer (TNBC) accounts for 15–20% of all invasive breast cancer subtypes. Owing to its clinical characteristics, such as the lack of effective therapeutic targets, high invasiveness, and high recurrence rate, TNBC is difficult to treat and has a poor prognosis. Currently, with the accumulation of large amounts of medical data and the development of computing technology, artificial intelligence (AI), particularly machine learning, has been applied to various aspects of TNBC research, including early screening, diagnosis, identification of molecular subtypes, personalised treatment, and prediction of prognosis and treatment response. In this review, we discussed the general principles of artificial intelligence, summarised its main applications in the diagnosis and treatment of TNBC, and provided new ideas and theoretical basis for the clinical diagnosis and treatment of TNBC. © 2023, The Author(s), under exclusive licence to Springer Nature Limited."
"As the COVID-19 pandemic puts pressure on healthcare systems worldwide, the computed tomography image based AI diagnostic system has become a sustainable solution for early diagnosis. However, the model-wise vulnerability under adversarial perturbation hinders its deployment in practical situation. The existing adversarial training strategies are difficult to generalized into medical imaging field challenged by complex medical texture features. To overcome this challenge, we propose a Contour Attention Preserving (CAP) method based on lung cavity edge extraction. The contour prior features are injected to attention layer via a parameter regularization and we optimize the robust empirical risk with hybrid distance metric. We then introduce a new cross-nation CT scan dataset to evaluate the generalization capability of the adversarial robustness under distribution shift. Experimental results indicate that the proposed method achieves state-of-the-art performance in multiple adversarial defense and generalization tasks. The code and dataset are available at https://github.com/Quinn777/CAP. © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"There is a fast-growing literature in addressing the fairness of AI models (fair-AI), with a continuous stream of new conceptual frameworks, methods, and tools. How much can we trust them? How much do they actually impact society? We take a critical focus on fair-AI and survey issues, simplifications, and mistakes that researchers and practitioners often underestimate, which in turn can undermine the trust on fair-AI and limit its contribution to society. In particular, we discuss the hyper-focus on fairness metrics and on optimizing their average performances. We instantiate this observation by discussing the Yule's effect of fair-AI tools: being fair on average does not imply being fair in contexts that matter. We conclude that the use of fair-AI methods should be complemented with the design, development, and verification practices that are commonly summarized under the umbrella of trustworthy AI. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We study the problem of synthesizing immersive 3D indoor scenes from one or a few images. Our aim is to generate high-resolution images and videos from novel viewpoints, including viewpoints that extrapolate far beyond the input images while maintaining 3D consistency. Existing approaches are highly complex, with many separately trained stages and components. We propose a simple alternative: an image-to-image GAN that maps directly from reprojections of incomplete point clouds to full high-resolution RGB-D images. On the Matterport3D and RealEstate10K datasets, our approach significantly outperforms prior work when evaluated by humans, as well as on FID scores. Further, we show that our model is useful for generative data augmentation. A vision- and-language navigation (VLN) agent trained with trajectories spatially-perturbed by our model improves success rate by up to 1.5% over a state of the art baseline on the mature R2R benchmark. Our code is publicly released to facilitate generative data augmentation and applications to downstream robotics and embodied AI tasks. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We develop a network of Bayesian agents that collectively model the mental states of teammates from the observed communication. Using a generative computational approach to cognition, we make two contributions. First, we show that our agent could generate interventions that improve the collective intelligence of a human-AI team beyond what humans alone would achieve. Second, we develop a real-time measure of human's theory of mind ability and test theories about human cognition. We use data collected from an online experiment in which 145 individuals in 29 human-only teams of five communicate through a chat-based system to solve a cognitive task. We find that humans (a) struggle to fully integrate information from teammates into their decisions, especially when communication load is high, and (b) have cognitive biases which lead them to underweight certain useful, but ambiguous, information. Our theory of mind ability measure predicts both individual- and team-level performance. Observing teams' first 25% of messages explains about 8% of the variation in final team performance, a 170% improvement compared to the current state of the art. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Storytelling is an innate part of language-based communication. Today, current events are reported via Open Source Intelligence (OSINT) sources like news websites, blogs, and discussion forums. Scattered and fragmented sources such as these can be better understood when organized as chains of event plot points, or narratives, that have the ability to communicate end to end stories. Though search engines can retrieve aggregated event information, they lack the ability to sequence relevant events together to form narratives about different topics. I propose an AI system inspired by a narrative theory called the Plot Element Pyramid and use knowledge graphs to represent, chain, and reason over narratives from disparately sourced event details to better comprehend convoluted, noisy information about critical events during intelligence analysis. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Numerical reasoning over hybrid data containing tables and long texts has recently received research attention from the AI community. To generate an executable reasoning program consisting of math and table operations to answer a question, state-of-the-art methods use a retriever-generator pipeline. However, their retrieval results are static, while different generation steps may rely on different sentences. To attend to the retrieved information that is relevant to each generation step, in this paper, we propose DyRRen, an extended retriever-reranker-generator framework where each generation step is enhanced by a dynamic reranking of retrieved sentences. It outperforms existing baselines on the FinQA dataset. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Modern power systems will have to face difficult challenges in the years to come: frequent blackouts in urban areas caused by high power demand peaks, grid instability exacerbated by intermittent renewable generation, and global climate change amplified by rising carbon emissions. While current practices are growingly inadequate, the path to widespread adoption of artificial intelligence (AI) methods is hindered by missing aspects of trustworthiness. The CityLearn Challenge is an exemplary opportunity for researchers from multiple disciplines to investigate the potential of AI to tackle these pressing issues in the energy domain, collectively modeled as a reinforcement learning (RL) task. Multiple real-world challenges faced by contemporary RL techniques are embodied in the problem formulation. In this paper, we present a novel method using the solution function of optimization as policies to compute actions for sequential decision-making, while notably adapting the parameters of the optimization model from online observations. Algorithmically, this is achieved by an evolutionary algorithm under a novel trajectory-based guidance scheme. Formally, the global convergence property is established. Our agent ranked first in the latest 2021 CityLearn Challenge, being able to achieve superior performance in almost all metrics while maintaining some key aspects of interpretability. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The Traveling Tournament Problem (TTP-k) is a well-known benchmark problem in tournament timetabling and has been extensively studied in the field of AI. In this problem, we are going to design a double round-robin schedule such that each pair of teams plays one game in each other’s home venue, minimizing the total distance traveled by all n teams (n is even) under the constraint that each team can have at most k-consecutive home games or away games. The Linear Distance Traveling Tournament Problem (LDTTP-k), where all teams are located on a line, was introduced by Hoshino and Kawarabayashi (AAAI 2012). For LDTTP-3, they gave a 4/3-approximation algorithm for n ≡ 4 (mod 6) teams. In this paper, we show that for any 3 ≤ k = o(√3 n), LDTTPk allows an efficient polynomial-time approximation scheme (EPTAS). Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"There has been increasing concern within the machine learning community and beyond that Artificial Intelligence (AI) faces a bias and discrimination crisis which needs AI fairness with urgency. As many have begun to work on this problem, most existing work depends on the availability of class label for the given fairness definition and algorithm which may not align with real-world usage. In this work, we study an AI fairness problem that stems from the gap between the design of a “fair” model in the lab and its deployment in the real-world. Specifically, we consider defining and mitigating individual unfairness amidst censorship, where the availability of class label is not always guaranteed due to censorship, which is broadly applicable in a diversity of real-world socially sensitive applications. We show that our method is able to quantify and mitigate individual unfairness in the presence of censorship across three benchmark tasks, which provides the first known results on individual fairness guarantee in analysis of censored data. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"This is a demonstration of our newly released Python package NL2LTL which leverages the latest in natural language understanding (NLU) and large language models (LLMs) to translate natural language instructions to linear temporal logic (LTL) formulas. This allows direct translation to formal languages that a reasoning system can use, while at the same time, allowing the end-user to provide inputs in natural language without having to understand any details of an underlying formal language. The package comes with support for a set of default LTL patterns, corresponding to popular DECLARE templates, but is also fully extensible to new formulas and user inputs. The package is open-source and is free to use for the AI community under the MIT license. Open Source: https://github.com/IBM/nl2ltl. Video Link: https://bit.ly/3dHW5b1. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"This paper addresses the issue of adversarial attacks on ethical AI systems. We investigate using moral axioms and rules of deontic logic in a norm learning framework to mitigate adversarial norm training. This model of moral intuition and construction provides AI systems with moral guard rails yet still allows for learning conventions. We evaluate our approach by drawing inspiration from a study commonly used in moral development research. This questionnaire aims to test an agent's ability to reason to moral conclusions despite opposed testimony. Our findings suggest that our model can still correctly evaluate moral situations and learn conventions in an adversarial training environment. We conclude that adding axiomatic moral prohibitions and deontic inference rules to a norm learning model makes it less vulnerable to adversarial attacks. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"AI-aided drug discovery (AIDD) is gaining popularity due to its potential to make the search for new pharmaceuticals faster, less expensive, and more effective. Despite its extensive use in numerous fields (e.g., ADMET prediction, virtual screening), little research has been conducted on the out-of-distribution (OOD) learning problem with noise. We present DrugOOD, a systematic OOD dataset curator and benchmark for AIDD. Particularly, we focus on the drug-target binding affinity prediction problem, which involves both macromolecule (protein target) and small-molecule (drug compound). DrugOOD offers an automated dataset curator with user-friendly customization scripts, rich domain annotations aligned with biochemistry knowledge, realistic noise level annotations, and rigorous benchmarking of SOTA OOD algorithms, as opposed to only providing fixed datasets. Since the molecular data is often modeled as irregular graphs using graph neural network (GNN) backbones, DrugOOD also serves as a valuable testbed for graph OOD learning problems. Extensive empirical studies have revealed a significant performance gap between in-distribution and out-of-distribution experiments, emphasizing the need for the development of more effective schemes that permit OOD generalization under noise for AIDD. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Artificial intelligence course has been required to take for compulsory education students in China. However, not all teachers and schools are fully prepared and ready. This is partially because of the lack of adequate teaching and learning resources, which requires a major expenditure of time and effort for schools and teachers to design and develop. To meet the challenge of lacking appropriate resources in teaching and learning AI from grade 1 to grade 9, we developed AI knowledge structure and instructional resources based on Chinese national curriculum for information science and technology. Our comprehensive AI syllabus contains 90 core concepts, 63 learning indicators, and 27 teaching and learning resources, which have been implemented. The resources have been taken as model courses in teacher training programs and an exemplary course has been implemented in primary schools that verified the effectiveness of our resources. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"AI systems can create, propagate, support, and automate bias in decision-making processes. To mitigate biased decisions, we both need to understand the origin of the bias and define what it means for an algorithm to make fair decisions. Most group fairness notions assess a model’s equality of outcome by computing statistical metrics on the outputs. We argue that these output metrics encounter intrinsic obstacles and present a complementary approach that aligns with the increasing focus on equality of treatment. By Locating Unfairness through Canonical Inverse Design (LUCID), we generate a canonical set that shows the desired inputs for a model given a preferred output. The canonical set reveals the model’s internal logic and exposes potential unethical biases by repeatedly interrogating the decision-making process. We evaluate LUCID on the UCI Adult and COMPAS data sets and find that some biases detected by a canonical set differ from those of output metrics. The results show that by shifting the focus towards equality of treatment and looking into the algorithm’s internal workings, the canonical sets are a valuable addition to the toolbox of algorithmic fairness evaluation. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Purpose: The purpose of this study is to explore customer-artificial intelligence (AI) service technology engagement and relationship development drivers, as well as potential negative consequences in the context of social chatbots. Design/methodology/approach: A sequential mixed-method approach combined exploratory qualitative and confirmatory quantitative analyses. A conceptual model developed from Study 1 qualitative content analysis of in-depth interviews with active users of the AI social chatbot Replika was tested in Study 2 by analyzing survey data obtained from current Replika users. Findings: Loneliness, trust and chatbot personification drive consumer engagement with social chatbots, which fosters relationship development and has the potential to cause chatbot psychological dependence. Attachment to a social chatbot intensifies the positive role of engagement in relationship development with the chatbot. Originality/value: This study was the first to combine qualitative and quantitative approaches to explore drivers, boundary conditions and consequences of relationship and dependence formation with social chatbots. The authors proposed and empirically tested a novel theoretical model that revealed an engagement-based mechanism of relationship and dependence formation with social chatbots. © 2023, Emerald Publishing Limited."
"In the scope of “AI for Science”, solving inverse problems is a longstanding challenge in materials and drug discovery, where the goal is to determine the hidden structures given a set of desirable properties. Deep generative models are recently proposed to solve inverse problems, but these currently use expensive forward operators and struggle in precisely localizing the exact solutions and fully exploring the parameter spaces without missing solutions. In this work, we propose a novel approach (called iPage) to accelerate the inverse learning process by leveraging probabilistic inference from deep invertible models and deterministic optimization via fast gradient descent. Given a target property, the learned invertible model provides a posterior over the parameter space; we identify these posterior samples as an intelligent prior initialization which enables us to narrow down the search space. We then perform gradient descent to calibrate the inverse solutions within a local region. Meanwhile, a space-filling sampling is imposed on the latent space to better explore and capture all possible solutions. We evaluate our approach on three benchmark tasks and two created datasets with real-world applications from quantum chemistry and additive manufacturing, and find our method achieves superior performance compared to several state-of-the-art baseline methods. The iPage code is available at https://github.com/jxzhangjhu/MatDesINNe. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The increased integration of artificial intelligence (AI) technologies in human workflows has resulted in a new paradigm of AI-assisted decision making, in which an AI model provides decision recommendations while humans make the final decisions. To best support humans in decision making, it is critical to obtain a quantitative understanding of how humans interact with and rely on AI. Previous studies often model humans' reliance on AI as an analytical process, i.e., reliance decisions are made based on a cost-benefit analysis. However, theoretical models in psychology suggest that the reliance decisions can often be driven by emotions like humans' trust in AI models. In this paper, we propose a hidden Markov model to capture the affective process underlying the human-AI interaction in AI-assisted decision making, by characterizing how decision makers adjust their trust in AI over time and make reliance decisions based on their trust. Evaluations on real human behavior data collected from human-subject experiments show that the proposed model outperforms various baselines in accurately predicting humans' reliance behavior in AI-assisted decision making. Based on the proposed model, we further provide insights into how humans' trust and reliance dynamics in AI-assisted decision making is influenced by contextual factors like decision stakes and their interaction experiences. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We propose an AI-based pilot trainer to help students learn how to fly aircraft. First, an AI agent uses behavioral cloning to learn flying maneuvers from qualified flight instructors. Later, the system uses the agent's decisions to detect errors made by students and provide feedback to help students correct their errors. This paper presents an instantiation of the pilot trainer. We focus on teaching straight and level flying maneuvers by automatically providing formative feedback to the human student. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"In this paper, we discuss the benefits and challenges of shared tasks as a teaching method. A shared task is a scientific event and a friendly competition to solve a research problem, the task. In terms of linking research and teaching, shared-task-based tutorials fulfill several faculty desires: they leverage students' interdisciplinary and heterogeneous skills, foster teamwork, and engage them in creative work that has the potential to produce original research contributions. Based on ten information retrieval (IR) courses at two universities since 2019 with shared tasks as tutorials, we derive a domain-neutral process model to capture the respective tutorial structure. Meanwhile, our teaching method has been adopted by other universities in IR courses, but also in other areas of AI such as natural language processing and robotics. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The intersection of pervasive technology and verbal communication has resulted in the creation of Automatic Speech Recognition Systems (ASRs), which automate the conversion of spontaneous speech into texts. ASR enables human-computer interactions through speech and is rapidly integrated into our daily lives. However, the research studies on current ASR technologies have reported unfulfilled social inclusivity and accentuated biases and stereotypes towards minorities. In this work, we provide a review of examples and evidence to demonstrate preexisting sexist behavior in ASR systems through a systematic review of research literature over the past five years. For each article, we also provide the ASR technology used, highlight specific instances of reported bias, discuss the impact of this bias on the female community, and suggest possible methods of mitigation. We believe this paper will provide insights into the harm that unchecked AI-powered technologies can have on a community by contributing to the growing body of research on this topic and underscoring the need for technological inclusivity for all demographics, especially women. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"As Artificial Intelligence (AI) continues to develop, it becomes vital to understand more of the nuances of Human-AI interactions. This study aims to uncover how developers can design AI to feel more human in a work environment where only written feedback is possible. Participants will identify a location from Google Maps. To do this successfully, participants must rely on the answers provided by their teammates, one AI and one human. The experiment will run a 2x4 design where AI's responses will either be designed in a human style (high humanness) or state a one-word answer (low humanness), the latter of which is more typical in machines and AI. The AI's reliability will be either 60% or 90% correct, and the human will be 30% correct. Participants will be given a series of questionnaires to rate their opinions of the AI and rate feelings of trust, confidence and performance throughout the study. Following this study, the aim is to identify specific design elements that allow AI to feel human and successfully appear to have social intelligence in more interactive settings. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Many AI-related reasoning problems are based on the problem of satisfiability of propositional formulas with some cardinality-minimality condition. While the complexity of the satisfiability problem (SAT) is well understood when considering systematically all fragments of propositional logic within Schaefer’s framework, this is not the case when such minimality condition is added. We consider the CARDMINSAT problem, which asks, given a formula ϕ and an atom x, whether x is true in some cardinality-minimal model of ϕ. We completely classify the computational complexity of the CARDMINSAT problem within Schaefer’s framework, thus paving the way for a better understanding of the tractability frontier of many AI-related reasoning problems. To this end we use advanced algebraic tools. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The potential for conversational agents offering mental health and legal counseling in an autonomous, interactive, and vitally accessible environment is getting highlighted due to the increased access to information through the internet and mobile devices. A counseling conversational agent should be able to offer higher engagement mimicking the real-time counseling sessions. The ability to empathize or comprehend and feel another person’s emotions and experiences is a crucial quality that promotes effective therapeutic bonding and rapport-building. Further, the use of polite encoded language in the counseling reflects the nobility and creates a familiar, warm, and comfortable atmosphere to resolve human issues. Therefore, focusing on these two aspects, we propose a Polite and Empathetic Mental Health and Legal Counseling Dialogue System (Po-Em-MHLCDS) for the victims of crimes. To build Po-Em-MHLCDS, we first create a Mental Health and Legal Counseling Dataset (MHLCD) by recruiting six employees who are asked to converse with each other, acting as a victim and the agent interchangeably following a fixed stated guidelines. Second, the MHLCD dataset is annotated with three informative labels, viz. counseling strategies, politeness, and empathy. Lastly, we train the Po-Em-MHLCDS in a reinforcement learning framework by designing an efficient and effective reward function to reinforce correct counseling strategy, politeness and empathy while maintaining contextual-coherence and non-repetitiveness in the generated responses. Our extensive automatic and human evaluation demonstrate the strength of the proposed system. Codes and Data can be accessed at https://www.iitp.ac.in/ai-nlp-ml/resources.html#MHLCD or https://github.com/Mishrakshitij/Po-Em-MHLCDS. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Altruistic punishment (or punishment) has been extensively shown as an important mechanism for promoting cooperation in human societies. In AI, the emergence of punishment has received much recent interest. In this paper, we contribute with a novel evolutionary game theoretic model to study the impacts of environmental feedback. Whereas a population of agents plays public goods games, there exists a third-party population whose payoffs depend not only on whether to punish or not, but also on the state of the environment (e.g., how cooperative the agents in a social dilemma are). Focusing on one-shot public goods games, we show that environmental feedback, by itself, can lead to the emergence of punishment. We analyze the co-evolution of punishment and cooperation, and derive conditions for their co-presence, co-dominance and co-extinction. Moreover, we show that the system can exhibit bistability as well as cyclic dynamics. Our findings provide a new explanation for the emergence of punishment. On the other hand, our results also alert the need for careful design of implementing punishment in multi-agent systems, as the resulting evolutionary dynamics can be somewhat complex. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The ability to understand and generate similes is an imperative step to realize human-level AI. However, there is still a considerable gap between machine intelligence and human cognition in similes, since deep models based on statistical distribution tend to favour high-frequency similes. Hence, a large-scale symbolic knowledge base of similes is required, as it contributes to the modeling of diverse yet unpopular similes while facilitating additional evaluation and reasoning. To bridge the gap, we propose a novel framework for large-scale simile knowledge base construction, as well as two probabilistic metrics which enable an improved understanding of simile phenomena in natural language. Overall, we construct MAPS-KB, a million-scale probabilistic simile knowledge base, covering 4.3 million triplets over 0.4 million terms from 70 GB corpora. We conduct sufficient experiments to justify the effectiveness of methods of our framework. We also apply MAPS-KB on three downstream tasks to achieve state-of-the-art performance, further demonstrating the value of MAPS-KB. Resources of MAPS-KB are publicly available at https://github.com/Abbey4799/MAPS-KB. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Interpretations of logical formulas over semirings (other than the Boolean semiring) have applications in various areas of computer science including logic, AI, databases, and security. Such interpretations provide richer information beyond the truth or falsity of a statement. Examples of such semirings include Viterbi semiring, min-max or access control semiring, tropical semiring, and fuzzy semiring. The present work investigates the complexity of constraint optimization problems over semirings. The generic optimization problem we study is the following: Given a propositional formula ϕ over n variable and a semiring (K, +, ·, 0, 1), find the maximum value over all possible interpretations of ϕ over K. This can be seen as a generalization of the well-known satisfiability problem (a propositional formula is satisfiable if and only if the maximum value over all interpretations/assignments over the Boolean semiring is 1). A related problem is to find an interpretation that achieves the maximum value. In this work, we first focus on these optimization problems over the Viterbi semiring, which we call optConfVal and optConf. We first show that for general propositional formulas in negation normal form, optConfVal and optConf are in FPNP. We then investigate optConf when the input formula ϕ is represented in the conjunctive normal form. For CNF formulae, we first derive an upper bound on the value of optConf as a function of the number of maximum satisfiable clauses. In particular, we show that if r is the maximum number of satisfiable clauses in a CNF formula with m clauses, then its optConf value is at most 1/4m−r. Building on this we establish that optConf for CNF formulae is hard for the complexity class FPNP[log]. We also design polynomial-time approximation algorithms and establish an inapproximability for optConfVal. We establish similar complexity results for these optimization problems over other semirings including tropical, fuzzy, and access control semirings. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Nowadays, autonomous vehicle technology is becoming more and more mature. Critical to progress and safety, high-definition (HD) maps, a type of centimeter-level map collected using a laser sensor, provide accurate descriptions of the surrounding environment. The key challenge of HD map production is efficient, high-quality collection and annotation of large-volume datasets. Due to the demand for high quality, HD map production requires significant manual human effort to create annotations, a very time-consuming and costly process for the map industry. In order to reduce manual annotation burdens, many artificial intelligence (AI) algorithms have been developed to pre-label the HD maps. However, there still exists a large gap between AI algorithms and the traditional manual HD map production pipelines in accuracy and robustness. Furthermore, it is also very resource-costly to build large-scale annotated datasets and advanced machine learning algorithms for AI-based HD map automatic labeling systems. In this paper, we introduce the Tencent HD Map AI (THMA) system, an innovative end-to-end, AI-based, active learning HD map labeling system capable of producing and labeling HD maps with a scale of hundreds of thousands of kilometers. In THMA, we train AI models directly from massive HD map datasets via supervised, self-supervised, and weakly supervised learning to achieve high accuracy and efficiency required by downstream users. THMA has been deployed by the Tencent Map team to provide services to downstream companies and users, serving over 1,000 labeling workers and producing more than 30,000 kilometers of HD map data per day at most. More than 90 percent of the HD map data in Tencent Map is labeled automatically by THMA, accelerating the traditional HD map labeling process by more than ten times. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"In contrast to the rapid digitalization of several industries, agriculture suffers from low adoption of smart farming tools. Even though recent advancements in AI-driven digital agriculture can offer high-performing predictive functionalities, they lack tangible quantitative evidence on their benefits to the farmers. Field experiments can derive such evidence, but are often costly, time consuming and hence limited in scope and scale of application. To this end, we propose an observational causal inference framework for the empirical evaluation of the impact of digital tools on target farm performance indicators (e.g., yield in this case). This way, we can increase farmers’ trust via enhancing the transparency of the digital agriculture market, and in turn accelerate the adoption of technologies that aim to secure farmer income resilience and global agricultural sustainability against a changing climate. As a case study, we designed and implemented a recommendation system for the optimal sowing time of cotton based on numerical weather predictions, which was used by a farmers’ cooperative during the growing season of 2021. We then leverage agricultural knowledge, collected yield data, and environmental information to develop a causal graph of the farm system. Using the back-door criterion, we identify the impact of sowing recommendations on the yield and subsequently estimate it using linear regression, matching, inverse propensity score weighting and meta-learners. The results revealed that a field sown according to our recommendations exhibited a statistically significant yield increase that ranged from 12% to 17%, depending on the method. The effect estimates were robust, as indicated by the agreement among the estimation methods and four successful refutation tests. We argue that this approach can be implemented for decision support systems of other fields, extending their evaluation beyond a performance assessment of internal functionalities. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Automatic medical text simplification can assist providers with patient-friendly communication and make medical texts more accessible, thereby improving health literacy. But curating a quality corpus for this task requires the supervision of medical experts. In this work, we present Med-EASi (Medical dataset for Elaborative and Abstractive Simplification), a uniquely crowdsourced and finely annotated dataset for supervised simplification of short medical texts. Its expert-layman-AI collaborative annotations facilitate controllability over text simplification by marking four kinds of textual transformations: elaboration, replacement, deletion, and insertion. To learn medical text simplification, we fine-tune T5-large with four different styles of input-output combinations, leading to two control-free and two controllable versions of the model. We add two types of controllability into text simplification, by using a multi-angle training approach: position-aware, which uses in-place annotated inputs and outputs, and position-agnostic, where the model only knows the contents to be edited, but not their positions. Our results show that our fine-grained annotations improve learning compared to the unannotated baseline. Furthermore, position-aware control generates better simplification than the position-agnostic one. The data and code are available at https://github.com/Chandrayee/CTRL-SIMP. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We are living through a revolutionary moment in AI history. We are seeing the development of impressive new AI systems at a rate that was unimaginable just a few years ago. However, AI's true potential to transform society remains unrealized, in no small part due to the inability of current systems to work effectively with people. A major hurdle to achieving such coordination is the inherent asymmetry between the AI system and its users. In this talk, I will discuss how the framework of Human-Aware AI (HAAI) provides us with the tools required to bridge this gap and support fluent and intuitive coordination between the AI system and its users. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Underserved communities face critical health challenges due to lack of access to timely and reliable information. Nongovernmental organizations are leveraging the widespread use of cellphones to combat these healthcare challenges and spread preventative awareness. The health workers at these organizations reach out individually to beneficiaries; however such programs still suffer from declining engagement. We have deployed SAHELI, a system to efficiently utilize the limited availability of health workers for improving maternal and child health in India. SAHELI uses the Restless Multiarmed Bandit (RMAB) framework to identify beneficiaries for outreach. It is the first deployed application for RMABs in public health, and is already in continuous use by our partner NGO, ARMMAN. We have already reached ∼ 100K beneficiaries with SAHELI, and are on track to serve 1 million beneficiaries by the end of 2023. This scale and impact has been achieved through multiple innovations in the RMAB model and its development, in preparation of real world data, and in deployment practices; and through careful consideration of responsible AI practices. Specifically, in this paper, we describe our approach to learn from past data to improve the performance of SAHELI's RMAB model, the real-world challenges faced during deployment and adoption of SAHELI, and the end-to-end pipeline. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Simulating physical network paths (e.g., Internet) is a cornerstone research problem in the emerging sub-field of AI-for-networking. We seek a model that generates end-to-end packet delay values in response to the time-varying load offered by a sender, which is typically a function of the previously output delays. The problem setting is unique, and renders the state-of-the-art text and time-series generative models inapplicable or ineffective. We formulate an ML problem at the intersection of dynamical systems, sequential decision making, and time-series modeling. We propose a novel grey-box approach to network simulation that embeds the semantics of physical network path in a new RNN-style model called Recurrent Buffering Unit, providing the interpretability of standard network simulator tools, the power of neural models, the efficiency of SGD-based techniques for learning, and yielding promising results on synthetic and real-world network traces. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"In light of significant issues in the technology industry, such as algorithms that worsen racial biases, the spread of online misinformation, and the expansion of mass surveillance, it is increasingly important to teach the ethics and sociotechnical implications of developing and using artificial intelligence (AI). Using 53 survey responses from engineering undergraduates, this paper measures students' abilities to identify, mitigate, and reflect on a hypothetical AI ethics scenario. We engage with prior research on pedagogical approaches to and considerations for teaching AI ethics and highlight some of the obstacles that engineering undergraduate students experience in learning and applying AI ethics concepts. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Learning programs with numerical values is fundamental to many AI applications, including bio-informatics and drug design. However, current program synthesis approaches struggle to learn programs with numerical values. An especially difficult problem is learning continuous values from multiple examples, such as intervals. To overcome this limitation, we introduce an inductive logic programming approach which combines relational learning with numerical reasoning. Our approach, which we call NUMSYNTH, uses satisfiability modulo theories solvers to efficiently learn programs with numerical values. Our approach can identify numerical values in linear arithmetic fragments, such as real difference logic, and from infinite domains, such as real numbers or integers. Our experiments on four diverse domains, including game playing and program synthesis, show that our approach can (i) learn programs with numerical values from linear arithmetical reasoning, and (ii) outperform existing approaches in terms of predictive accuracies and learning times. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Behavioral scientists have classically documented aversion to algorithmic decision aids, from simple linear models to AI. Sentiment, however, is changing and possibly accelerating AI helper usage. AI assistance is, arguably, most valuable when humans must make complex choices. We argue that classic experimental methods used to study heuristics and biases are insufficient for studying complex choices made with AI helpers. We adapted an experimental paradigm designed for studying complex choices in such contexts. We show that framing and anchoring effects impact how people work with an AI helper and are predictive of choice outcomes. The evidence suggests that some participants, particularly those in a loss frame, put too much faith in the AI helper and experienced worse choice outcomes by doing so. The paradigm also generates computational modeling-friendly data allowing future studies of human-AI decision making. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"With the rise of AI used for critical decision-making, many important predictions are made by complex and opaque AI algorithms. The aim of eXplainable Artificial Intelligence (XAI) is to make these opaque decision-making algorithms more transparent and trustworthy. This is often done by constructing an “explainable model” for a single modality or subsystem. However, this approach fails for complex systems that are made out of multiple parts. In this paper, I discuss how to explain complex system failures. I represent a complex machine as a hierarchical model of introspective subsystems working together towards a common goal. The subsystems communicate in a common symbolic language. This work creates a set of explanatory accountability layers for trustworthy AI. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Argument(ation) mining (AM) is an area of research in Artificial Intelligence (AI) that aims to identify, analyse and automatically generate arguments in natural language. In a pipeline, the identification and analysis of the arguments and their components (i.e. premises and claims) in texts and the prediction of their relations (i.e. attack and support) are then handled by argument-based reasoning frameworks so that, for example, fallacies and inconsistencies can be automatically identified. Recently, the field of argument mining has tackled new challenges, namely the evaluation of argument quality (e.g. strength, persuasiveness), natural language argument summarisation and retrieval, and natural language argument generation. In this paper, I discuss my main contributions in this area as well as some lines of future research. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Physical commonsense reasoning is essential for building reliable and interpretable AI systems, which involves a general understanding of the physical properties and affordances of everyday objects, how these objects can be manipulated, and how they interact with others. It is fundamentally a multisensory task, as physical properties are manifested through multiple modalities, including vision and acoustics. In this work, we present a unified framework, named Multimodal Commonsense Transformer (MCOMET), for physical audiovisual commonsense reasoning. MCOMET has two intriguing properties: i) it fully mines higher-ordered temporal relationships across modalities (e.g., pairs, triplets, and quadruplets); and ii) it restricts the cross-modal flow through the feature collection and propagation mechanism with tight fusion bottlenecks, forcing the model to attend the most relevant parts in each modality and suppressing the dissemination of noisy information. We evaluate our model on a very recent public benchmark, PACS. Results show that MCOMET significantly outperforms a variety of strong baselines, revealing powerful multi-modal commonsense reasoning capabilities. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Traffic forecasting as a canonical task of multivariate time series forecasting has been a significant research topic in AI community. To address the spatio-temporal heterogeneity and non-stationarity implied in the traffic stream, in this study, we propose Spatio-Temporal Meta-Graph Learning as a novel Graph Structure Learning mechanism on spatio-temporal data. Specifically, we implement this idea into Meta-Graph Convolutional Recurrent Network (MegaCRN) by plugging the Meta-Graph Learner powered by a Meta-Node Bank into GCRN encoder-decoder. We conduct a comprehensive evaluation on two benchmark datasets (i.e., METR-LA and PEMS-BAY) and a new large-scale traffic speed dataset called EXPY-TKY that covers 1843 expressway road links in Tokyo. Our model outperformed the state-of-the-arts on all three datasets. Besides, through a series of qualitative evaluations, we demonstrate that our model can explicitly disentangle the road links and time slots with different patterns and be robustly adaptive to any anomalous traffic situations. Codes and datasets are available at https://github.com/deepkashiwa20/MegaCRN. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Purpose: This paper aims to examine how the use of environmental, social and governance (ESG) incentives intersects with top management power and various corporate governance mechanisms to affect excess annual cash bonus compensation. Design/methodology/approach: The authors use a novel artificial intelligence (AI) technique to obtain data about ESG incentives use by firms in the S&P 500. The authors test the hypotheses with an endogenous treatment-regression and a contrast test. Findings: When the top management team has power and uses ESG incentives, there is a 32% reduction in excess annual cash bonuses implying ESG incentives are an effective corporate governance tool. However, nuanced analyses reveal that when powerful management teams with ESG incentives are from environmentally sensitive industries, have a corporate social responsibility (CSR) committee or have long-term view institutional shareholders, they derive excess bonuses. Practical implications: Stakeholders will better understand management’s motivations for the inclusion of ESG incentives in executive compensation contracts and be able to identify situations which require closer scrutiny. Social implications: Given the increased popularity of ESG incentives, society, regulators, boards of directors and management teams will be interested in better understanding when these incentives might be effective and when they might be abused. Originality/value: To the best of the authors’ knowledge, this study is the first to examine the use of ESG incentives in relation to excess pay. The authors contribute to both the CSR and executive compensation literatures. The work also uses a new methodological technique using AI to gather difficult-to-obtain data, opening new avenues for research. © 2023, Emerald Publishing Limited."
"It is common to listen to songs that match one's mood. Thus, an AI music recommendation system that is aware of the user's emotions is likely to provide a superior user experience to one that is unaware. In this paper, we present an emotion-aware music recommendation system. Multiple models are discussed and evaluated for affect identification from a live image of the user. We propose two models: DRViT, which applies dynamic routing to vision transformers, and InvNet50, which uses involution. All considered models are trained and evaluated on the AffectNet dataset. Each model outputs the user's estimated valence and arousal under the circumplex model of affect. These values are compared to the valence and arousal values for songs in a Spotify dataset, and the top-five closest-matching songs are presented to the user. Experimental results of the models and user testing are presented. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We propose a new approach to the verification of epistemic properties of programs. First, we introduce the new “program-epistemic” logic LPK, which is strictly richer and more general than similar formalisms appearing in the literature. To solve the verification problem in an efficient way, we introduce a translation from our language LPK into first-order logic. Then, we show and prove correct a reduction from the model checking problem for program-epistemic formulas to the satisfiability of their first-order translation. Both our logic and our translation can handle richer specification w.r.t. the state of the art, allowing us to express the knowledge of agents about facts pertaining to programs (i.e., agents' knowledge before and after a program is executed). Furthermore, we implement our translation in Haskell in a general way (i.e., independently of the programs in the logical statements), and we use existing SMT-solvers to check satisfaction of LPK formulas on a benchmark example in the AI/agency field. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Designing a planning domain is a difficult task in AI planning. Assisting tools are thus required if we want planning to be used more broadly. In this paper, we are interested in automatically correcting a flawed domain. In particular, we are concerned with the scenario where a domain contradicts a plan that is known to be valid. Our goal is to repair the domain so as to turn the plan into a solution. Specifically, we consider both grounded and lifted representations support for negative preconditions and show how to explore the space of repairs to find the optimal one efficiently. As an evidence of the efficiency of our approach, the experiment results show that all flawed domains except one in the benchmark set can be repaired optimally by our approach within one second. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Purpose: Service robots are gradually becoming more anthropomorphic and intelligent. This research aims to investigate how anthropomorphic service robots with different levels of intelligence affect their human counterparts. Design/methodology/approach: Two between-subject experimental studies were used to test whether different levels of service robot anthropomorphism with different levels of intelligence influence employees' morale and resistance to service robots. Findings: Study 1 shows that the effect of service robot anthropomorphism (low vs. high) on employees' resistance and morale is mediated by perceived job-security threat. Study 2 validates this mediating effect and shows that it is moderated by the type of AI (mechanical vs. analytical). Specifically, when exposed to mechanical AI-powered service robots, employees exhibit a higher perceived job-security threat toward robots with a high (vs. low) degree of anthropomorphism. This moderating effect is not observed when employees are exposed to analytical AI-powered service robots. This moderated mediation effect is also found for the signing of a petition as the behavioral outcome. Practical implications: Service firms considering the adoption of mechanical AI-powered service robots should choose a low (vs. high) anthropomorphic robot to reduce the sense of job-security threat felt by human employees, which subsequently increases their acceptance. However, if analytical AI-powered service robots with are to replace their human employees, the degree of anthropomorphism becomes irrelevant. Originality/value: This is the first empirical study to explore how anthropomorphic service robots can influence human employees' evaluations and behaviors. © 2023, Emerald Publishing Limited."
"An essential element of K-12 AI literacy is educating learners about the ethical and societal implications of AI systems. Previous work in AI ethics literacy have developed curriculum and classroom activities that engage learners in reflecting on the ethical implications of AI systems and developing responsible AI. There is little work in using game-based learning methods in AI literacy. Games are known to be compelling media to teach children about complex STEM concepts. In this work, we developed a competitive card game for middle and high school students called “AI Audit” where they play as AI start-up founders building novel AI-powered technology. Players can challenge other players with potential harms of their technology or defend their own businesses by features that mitigate these harms. The game mechanics reward systems that are ethically developed or that take steps to mitigate potential harms. In this paper, we present the game design, teacher resources for classroom deployment and early playtesting results. We discuss our reflections about using games as teaching tools for AI literacy in K-12 classrooms. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Bilevel Optimization Programming is used to model complex and conflicting interactions between agents, for example in Robust AI or Privacy-preserving AI. Integrating bilevel mathematical programming within deep learning is thus an essential objective for the Machine Learning community. Previously proposed approaches only consider single-level programming. In this paper, we extend existing single-level optimization programming approaches and thus propose Differentiating through Bilevel Optimization Programming (BIGRAD) for end-to-end learning of models that use Bilevel Programming as a layer. BIGRAD has wide applicability and can be used in modern machine learning frameworks. BIGRAD is applicable to both continuous and combinatorial Bilevel optimization problems. We describe a class of gradient estimators for the combinatorial case which reduces the requirements in terms of computation complexity; for the case of the continuous variable, the gradient computation takes advantage of the push-back approach (i.e. vectorjacobian product) for an efficient implementation. Experiments show that the BIGRAD successfully extends existing single-level approaches to Bilevel Programming. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Propaganda campaigns have long been used to influence public opinion via disseminating biased and/or misleading information. Despite the increasing prevalence of propaganda content on the Internet, few attempts have been made by AI researchers to analyze such content. We introduce the task of multimodal propaganda processing, where the goal is to automatically analyze propaganda content. We believe that this task presents a long-term challenge to AI researchers and that successful processing of propaganda could bring machine understanding one important step closer to human understanding. We discuss the technical challenges associated with this task and outline the steps that need to be taken to address it. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Advances in artificial intelligence (AI) using techniques such as deep learning have fueled the recent progress in fields such as computer vision. However, these algorithms are still often viewed as “black boxes”, which cannot easily explain how they arrived at their final output decisions. Saliency maps are one commonly used form of explainable AI (XAI), which indicate the input features an algorithm paid attention to during its decision process. Here, we introduce the open source xaitk-saliency package, an XAI framework and toolkit for saliency. We demonstrate its modular and flexible nature by highlighting two example use cases for saliency maps: (1) object detection model comparison and (2) doppelgänger saliency for person re-identification. We also show how the xaitk-saliency package can be paired with visualization tools to support the interactive exploration of saliency maps. Our results suggest that saliency maps may play a critical role in the verification and validation of AI models, ensuring their trusted use and deployment. The code is publicly available at: https://github.com/xaitk/xaitk-saliency. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"With rapid development in hardware (sensors and processors) and AI algorithms, automated driving techniques have entered the public's daily life and achieved great success in supporting human driving performance. However, due to the high contextual variations and temporal dynamics in pedestrian behaviors, the interaction between autonomous-driving cars and pedestrians remains challenging, impeding the development of fully autonomous driving systems. This paper focuses on predicting pedestrian intention with a novel transformer-based evidential prediction (TrEP) algorithm. We develop a transformer module towards the temporal correlations among the input features within pedestrian video sequences and a deep evidential learning model to capture the AI uncertainty under scene complexities. Experimental results on three popular pedestrian intent benchmarks have verified the effectiveness of our proposed model over the state-ofthe-art. The algorithm performance can be further boosted by controlling the uncertainty level. We systematically compare human disagreements with AI uncertainty to further evaluate AI performance in confusing scenes. The code is released at https://github.com/zzmonlyyou/TrEP.git.  Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We study the problem of training a Reinforcement Learning (RL) agent that is collaborative with humans without using human data. Although such agents can be obtained through self-play training, they can suffer significantly from the distributional shift when paired with unencountered partners, such as humans. In this paper, we propose Maximum Entropy Population-based training (MEP) to mitigate such distributional shift. In MEP, agents in the population are trained with our derived Population Entropy bonus to promote the pairwise diversity between agents and the individual diversity of agents themselves. After obtaining this diversified population, a common best agent is trained by paring with agents in this population via prioritized sampling, where the prioritization is dynamically adjusted based on the training progress. We demonstrate the effectiveness of our method MEP, with comparison to Self-Play PPO (SP), Population-Based Training (PBT), Trajectory Diversity (TrajeDi), and Fictitious Co-Play (FCP) in both matrix game and Overcooked game environments, with partners being human proxy models and real humans. A supplementary video showing experimental results is available at https://youtu.be/Xh-FKD0AAKE. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"It is imperative to accelerate the training of neural network force field such as Deep Potential, which usually requires thousands of images based on first-principles calculation and a couple of days to generate an accurate potential energy surface. To this end, we propose a novel optimizer named reorganized layer extended Kalman filtering (RLEKF), an optimized version of global extended Kalman filtering (GEKF) with a strategy of splitting big and gathering small layers to overcome the O(N2) computational cost of GEKF. This strategy provides an approximation of the dense weights error covariance matrix with a sparse diagonal block matrix for GEKF. We implement both RLEKF and the baseline Adam in our αDynamics package and numerical experiments are performed on 13 unbiased datasets. Overall, RLEKF converges faster with slightly better accuracy. For example, a test on a typical system, bulk copper, shows that RLEKF converges faster by both the number of training epochs (×11.67) and wall-clock time (×1.19). Besides, we theoretically prove that the updates of weights converge and thus are against the gradient exploding problem. Experimental results verify that RLEKF is not sensitive to the initialization of weights. The RLEKF sheds light on other AI-for-science applications where training a large neural network (with tons of thousands parameters) is a bottleneck. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Trustable explanations of machine learning (ML) models are vital in high-risk uses of artificial intelligence (AI). Apart from the computation of trustable explanations, a number of explainability queries have been identified and studied in recent work. Some of these queries involve solving quantification problems, either in propositional or in more expressive logics. This paper investigates one of these quantification problems, namely the feature relevancy problem (FRP), i.e. to decide whether a (possibly sensitive) feature can occur in some explanation of a prediction. In contrast with earlier work, that studied FRP for specific classifiers, this paper proposes a novel algorithm for the FRP quantification problem which is applicable to any ML classifier that meets minor requirements. Furthermore, the paper shows that the novel algorithm is efficient in practice. The experimental results, obtained using random forests (RFs) induced from well-known publicly available datasets, demonstrate that the proposed solution outperforms existing state-of-the-art solvers for Quantified Boolean Formulas (QBF) by orders of magnitude. Finally, the paper also identifies a novel family of formulas that are challenging for currently state-of-the-art QBF solvers. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Rethinking and introspection are important elements of human intelligence. To mimic these capabilities, counterfactual reasoning has attracted attention of AI researchers recently, which aims to forecast the alternative outcomes for hypothetical scenarios (“what-if”). However, most existing approaches focused on qualitative reasoning (e.g., causal-effect relationship). It lacks a well-defined description of the differences between counterfactuals and facts, as well as how these differences evolve over time. This paper introduces a new problem formulation — counterfactual dynamics forecasting — which is described in middle-level abstraction under the structural causal models (SCM) framework and derived as ordinary differential equations (ODEs) as low-level quantitative computation. Based on it, we propose a method to infer counterfactual dynamics considering the factual dynamics as demonstration. The experimental results on two dynamical systems demonstrate the effectiveness of the proposed method. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org)."
"Most programmers make mistakes when writing code. Some of these mistakes are small and require few edits to the original program – a class of errors recently termed last mile mistakes. These errors break the flow for experienced developers and can stump novice programmers. Existing automated repair techniques targeting this class of errors are language-specific and do not easily carry over to new languages. Transferring symbolic approaches requires substantial engineering and neural approaches require data and retraining. We introduce RING, a multilingual repair engine powered by a large language model trained on code (LLMC) such as Codex. Such a multilingual engine enables a flipped model for programming assistance, one where the programmer writes code and the AI assistance suggests fixes, compared to traditional code suggestion technology. Taking inspiration from the way programmers manually fix bugs, we show that a prompt-based strategy that conceptualizes repair as localization, transformation, and candidate ranking, can successfully repair programs in multiple languages with minimal effort. We present the first results for such a multilingual repair engine by evaluating on 6 different languages and comparing performance to language-specific repair engines. We show that RING can outperform language-specific repair engines for three of these languages. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"A common musical composition practice is to develop musical pieces using variations of musical themes. In this study, we present an interactive tool which can generate variations of musical themes in real-time using a variational autoencoder model. Our tool is controllable using semantically meaningful musical attributes via latent space regularisation technique to increase the explainability of the model. The tool is integrated into an industry standard digital audio workstation - Ableton Live - using the Max4Live device framework and can run locally on an average personal CPU rather than requiring a costly GPU cluster. In this way we demonstrate how cutting-edge AI research can be integrated into the exiting workflows of professional and practising musicians for use in the real-world beyond the research lab. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Access to high-quality maternal health care services is limited in Kenya, which resulted in ∼36,000 maternal and neonatal deaths in 2018. To tackle this challenge, Jacaranda Health (a non-profit organization working on maternal health in Kenya) developed PROMPTS, an SMS based tele-triage system for pregnant and puerperal women, which has more than 350,000 active users in Kenya. PROMPTS empowers pregnant women living far away from doctors and hospitals to send SMS messages to get quick answers (through human helpdesk agents) to questions about their medical symptoms and pregnancy status. Unfortunately, ∼1.1 million SMS messages are received by PROMPTS every month, which makes it challenging for helpdesk agents to ensure that these messages can be interpreted correctly and evaluated by their level of emergency to ensure timely responses and/or treatments for women in need. This paper reports on a collaborative effort with Jacaranda Health to develop a state-of-the-art natural language processing (NLP) framework, TRIM-AI (TRIage for Mothers using AI), which can automatically predict the emergency level (or severity of medical condition) of a pregnant mother based on the content of their SMS messages. TRIM-AI leverages recent advances in multi-lingual pre-training and continual pre-training to tackle code-mixed SMS messages (between English and Swahili), and achieves a weighted F1 score of 0.774 on real-world datasets. TRIM-AI has been successfully deployed in the field since June 2022, and is being used by Jacaranda Health to prioritize the provision of services and care to pregnant women with the most critical medical conditions. Our preliminary A/B tests in the field show that TRIM-AI is ∼17% more accurate at predicting high-risk medical conditions from SMS messages sent by pregnant Kenyan mothers, which reduces the helpdesk’s workload by ∼12%. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"This work presents Z-Mask, an effective and deterministic strategy to improve the adversarial robustness of convolutional networks against physically-realizable adversarial attacks. The presented defense relies on specific Z-score analysis performed on the internal network features to detect and mask the pixels corresponding to adversarial objects in the input image. To this end, spatially contiguous activations are examined in shallow and deep layers to suggest potential adversarial regions. Such proposals are then aggregated through a multi-thresholding mechanism. The effectiveness of Z-Mask is evaluated with an extensive set of experiments carried out on models for semantic segmentation and object detection. The evaluation is performed with both digital patches added to the input images and printed patches in the real world. The results confirm that Z-Mask outperforms the state-of-the-art methods in terms of detection accuracy and overall performance of the networks under attack. Furthermore, Z-Mask preserves its robustness against defense-aware attacks, making it suitable for safe and secure AI applications. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Although the prevention of AI vulnerabilities is critical to preserve the safety and privacy of users and businesses, educational tools for robust AI are still underdeveloped worldwide. We present the design, implementation, and assessment of Maestro. Maestro is an effective open-source game-based platform that contributes to the advancement of robust AI education. Maestro provides goal-based scenarios where college students are exposed to challenging life-inspired assignments in a competitive programming environment. We assessed Maestro's influence on students' engagement, motivation, and learning success in robust AI. This work also provides insights into the design features of online learning tools that promote active learning opportunities in the robust AI domain. We analyzed the reflection responses (measured with Likert scales) of 147 undergraduate students using Maestro in two quarterly college courses in AI. According to the results, students who felt the acquisition of new skills in robust AI tended to appreciate highly Maestro and scored highly on material consolidation, curiosity, and maestry in robust AI. Moreover, the leaderboard, our key gamification element in Maestro, has effectively contributed to students' engagement and learning. Results also indicate that Maestro can be effectively adapted to any course length and depth without losing its educational quality. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Many AI systems integrate sensor inputs, world knowledge, and human-provided information to perform inference. While such systems often treat the human input as flawless, humans are better thought of as hazy oracles whose input may be ambiguous or outside of the AI system's understanding. In such situations it makes sense for the AI system to defer its inference while it disambiguates the human-provided information by, for example, asking the human to rephrase the query. Though this approach has been considered in the past, current work is typically limited to application-specific methods and non-standardized human experiments. We instead introduce and formalize a general notion of deferred inference. Using this formulation, we then propose a novel evaluation centered around the Deferred Error Volume (DEV) metric, which explicitly considers the tradeoff between error reduction and the additional human effort required to achieve it. We demonstrate this new formalization and an innovative deferred inference method on the disparate tasks of Single-Target Video Object Tracking and Referring Expression Comprehension, ultimately reducing error by up to 48% without any change to the underlying model or its parameters. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We present an executable formally verified SAT encoding of ground classical AI planning problems. We use the theorem prover Isabelle/HOL to perform the verification. We experimentally test the verified encoding and show that it can be used for reasonably sized standard planning benchmarks. We also use it as a reference to test a state-of-the-art SAT-based planner, showing that it sometimes falsely claims that problems have no solutions of certain lengths. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"This work considers the use of AI and parallelism as a context for learning typical programming concepts in an introductory programming course (CS1). The course includes exercises in decision trees, a novel game called Find the Gnomes to introduce supervised learning, the construction and application of a vectorized neural network unit class, and obtaining speedup in training through parallelism. The exercises are designed to teach students typical introductory programming concepts while also providing a preview and motivating example of advanced CS topics. Students' understanding and motivation are considered through a detailed analysis of pre- and post-survey data gathered in several sections of the course each taught by one of four instructors across five semesters. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Culturally relevant and sustaining implementations of computing education are increasingly leveraging young learners' passion for sports as a platform for building interest in different STEM (Science, Technology, Engineering, and Math) concepts. Numerous disciplines spanning physics, engineering, data science, and especially Artificial Intelligence (AI) based computing are not only authentically used in professional sports in today's world but can also be productively introduced to introduce young learners to these disciplines and facilitate deep engagement with the same in the context of sports. In this work, we present a curriculum that includes a constellation of proprietary apps and tools that we show to student athletes learning sports like basketball and soccer which use AI methods like pose detection and IMU-based gesture detection to track activity and provide feedback. We also share Scratch extensions which enable rich access to sports related pose, object, and gesture detection algorithms that youth can then tinker around with and develop their own sports drill applications. We present early findings from pilot implementations of portions of these tools and curricula, which also fostered discussion relating to the failings, risks, and social harms associated with many of these different AI methods - noticeable in professional sports contexts, and relevant to youths' lives as active users of AI technologies as well as potential future creators of the same. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The Sudoku Assistant app is an AI assistant that combines machine learning and constraint programming techniques, to interpret and explain a pen-and-paper Sudoku scanned with a smartphone. Although the demo is about Sudoku, the underlying techniques are equally applicable to other constraint solving problems like timetabling, scheduling, and vehicle routing. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Automated modeling assistance is indispensable to the AI planning being deployed in practice, notably in industry and other non-academic contexts. Yet, little progress has been made that goes beyond smart interfaces like programming environments. They focus on autocompletion, but lack intelligent support for guiding the modeler. As a theoretical foundation of a first step towards this direction, we study the computational complexity of correcting a flawed Hierarchical Task Network (HTN) planning domain. Specifically, a modeler provides a (white) list of plans that are supposed to be solutions, and likewise a (black) list of plans that shall not be solutions. We investigate the complexity of finding a set of (optimal or suboptimal) model corrections so that those plans are (resp. not) solutions to the corrected model. We factor out each hardness source that contributes towards NP-hardness, including one that we deem important for many other complexity investigations that go beyond our specific context of application. All complexities range between NP and Σp2, raising the hope for efficient practical tools in the future. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Regression trees are one of the oldest forms of AI models, and their predictions can be made without a calculator, which makes them broadly useful, particularly for high-stakes applications. Within the large literature on regression trees, there has been little effort towards full provable optimization, mainly due to the computational hardness of the problem. This work proposes a dynamic-programming-with-bounds approach to the construction of provably-optimal sparse regression trees. We leverage a novel lower bound based on an optimal solution to the k-Means clustering algorithm on one dimensional data. We are often able to find optimal sparse trees in seconds, even for challenging datasets that involve large numbers of samples and highly-correlated features. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Visible-to-Thermal (VT) face translation is an under-studied problem of image-to-image translation that offers an AI-enabled alternative to traditional thermal sensors. Over three phases, my Doctoral Proposal explores developing multimodal deep generative solutions that can be applied towards telemedicine applications. These include the contribution of a novel Thermal Face Contrastive GAN (TFC-GAN), exploration of hybridized diffusion-GAN models, application on real clinical thermal data at the National Institutes of Health, and exploration of strategies for Federated Learning (FL) in heterogenous data settings. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We present edBB-Demo, a demonstrator of an AI-powered research platform for student monitoring in remote education. The edBB platform aims to study the challenges associated to user recognition and behavior understanding in digital platforms. This platform has been developed for data collection, acquiring signals from a variety of sensors including keyboard, mouse, webcam, microphone, smartwatch, and an Electroencephalography band. The information captured from the sensors during the student sessions is modelled in a multimodal learning framework. The demonstrator includes: i) Biometric user authentication in an unsupervised environment; ii) Human action recognition based on remote video analysis; iii) Heart rate estimation from webcam video; and iv) Attention level estimation from facial expression analysis. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantLy supErVised multi-instancE leaRning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel contrastive attention mechanism. Comprehensive experimental results in held-out and human evaluation show that CLEVER can extract commonsense knowledge in promising quality, outperforming pre-trained language model-based methods by 3.9 AUC and 6.4 mAUC points. The predicted commonsense scores show strong correlation with human judgment with a 0.78 Spearman coefficient. Moreover, the extracted commonsense can also be grounded into images with reasonable interpretability. The data and codes can be obtained at https://github.com/thunlp/CLEVER. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Drug dosing is an important application of AI, which can be formulated as a Reinforcement Learning (RL) problem. In this paper, we identify two major challenges of using RL for drug dosing: delayed and prolonged effects of administering medications, which break the Markov assumption of the RL framework. We focus on prolongedness and define PAE-POMDP (Prolonged Action Effect-Partially Observable Markov Decision Process), a subclass of POMDPs in which the Markov assumption does not hold specifically due to prolonged effects of actions. Motivated by the pharmacology literature, we propose a simple and effective approach to converting drug dosing PAE-POMDPs into MDPs, enabling the use of the existing RL algorithms to solve such problems. We validate the proposed approach on a toy task, and a challenging glucose control task, for which we devise a clinically-inspired reward function. Our results demonstrate that: (1) the proposed method to restore the Markov assumption leads to significant improvements over a vanilla baseline; (2) the approach is competitive with recurrent policies which may inherently capture the prolonged effect of actions; (3) it is remarkably more time and memory efficient than the recurrent baseline and hence more suitable for real-time dosing control systems; and (4) it exhibits favourable qualitative behavior in our policy analysis. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Monumental advancements in artificial intelligence (AI) have lured the interest of doctors, lenders, judges, and other professionals. While these high-stakes decision-makers are optimistic about the technology, those familiar with AI systems are wary about the lack of transparency of its decision-making processes. Perturbation-based post hoc explainers offer a model agnostic means of interpreting these systems while only requiring query-level access. However, recent work demonstrates that these explainers can be fooled adversarially. This discovery has adverse implications for auditors, regulators, and other sentinels. With this in mind, several natural questions arise – how can we audit these black box systems? And how can we ascertain that the auditee is complying with the audit in good faith? In this work, we rigorously formalize this problem and devise a defense against adversarial attacks on perturbation-based explainers. We propose algorithms for the detection (CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our novel conditional anomaly detection approach, KNN-CAD. We demonstrate that our approach successfully detects whether a black box system adversarially conceals its decision-making process and mitigates the adversarial attack on real-world data for the prevalent explainers, LIME and SHAP. The code for this work is available at https://github.com/craymichael/unfooling. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"A cornerstone in AI research has been the creation and adoption of standardized training and test datasets to earmark the progress of state-of-the-art models. A particularly successful example is the GLUE dataset for training and evaluating Natural Language Understanding (NLU) models for English. The large body of research around self-supervised BERT-based language models revolved around performance improvements on NLU tasks in GLUE. The success of large self-supervised models such as wav2vec2 enable creation of speech models with relative ease to access unlabelled data. These models can then be evaluated on SLU tasks, such as the SUPERB benchmark. In this work, we extend this to Indic languages by releasing the IndicSUPERB benchmark. Specifically, we make the following three contributions. (i) We collect Kathbath containing 1, 684 hours of labelled speech data across 12 Indian languages from 1, 218 contributors located in 203 districts in India. (ii) Using Kathbath, we create benchmarks across 6 speech tasks: Automatic Speech Recognition, Speaker Verification, Speaker Identification (mono/multi), Language Identification, Query By Example, and Keyword Spotting for 12 languages. (iii) On the released benchmarks, we train and evaluate different self-supervised models alongside a commonly used baseline FBANK. We show that language-specific fine-tuned models are more accurate than baseline on most of the tasks, including a large gap of 76% for Language Identification task. However, for speaker identification, self-supervised models trained on large datasets demonstrate an advantage. We hope IndicSUPERB contributes to the progress of developing speech language understanding models for Indian languages. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Procuring expressive molecular representations underpins AI-driven molecule design and scientific discovery. The research mainly focuses on atom-level homogeneous molecular graphs, ignoring the rich information in subgraphs or motifs. However, it has been widely accepted that substructures play a dominant role in identifying and determining molecular properties. To address such issues, we formulate heterogeneous molecular graphs (HMGs), and introduce a novel architecture to exploit both molecular motifs and 3D geometry. Precisely, we extract functional groups as motifs for small molecules and employ reinforcement learning to adaptively select quaternary amino acids as motif candidates for proteins. Then HMGs are constructed with both atom-level and motif-level nodes. To better accommodate those HMGs, we introduce a variant of the Transformer named Molformer, which adopts a heterogeneous self-attention layer to distinguish the interactions between multi-level nodes. Besides, it is also coupled with a multi-scale mechanism to capture fine-grained local patterns with increasing contextual scales. An attentive farthest point sampling algorithm is also proposed to obtain the molecular representations. We validate Molformer across a broad range of domains, including quantum chemistry, physiology, and biophysics. Extensive experiments show that Molformer outperforms or achieves the comparable performance of several state-of-the-art baselines. Our work provides a promising way to utilize informative motifs from the perspective of multi-level graph construction. The code is available at https://github.com/smiles724/Molformer. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Transformer models are widely used in AI applications such as Natural Language Processing (NLP), Computer Vision (CV), etc. However, enormous computation workload becomes an obstacle to train large transformer models efficiently. Recently, some methods focus on reducing the computation workload during the training by skipping some layers. However, these methods use simple probability distribution and coarse-grained probability calculation, which significantly affect the model accuracy. To address the issue, in this paper we propose a novel method to accelerate training—Sensitivity-Based Layer Dropping (SBLD). SBLD uses layer-wise sensitivity data to switch on/off transformer layers in proper order to keep high accuracy. Besides, we adjust the probability of skipping transformer layers with a scheduler to accelerate training speed and get faster convergence. Our results show that SBLD solves the accuracy drop issue compared with prior layer dropping methods. Our SBLD method can decrease end-to-end training time by 19.67% during training of GPT-3 Medium model, the same time increasing the accuracy by 1.65% w.r.t. baseline. Furthermore, for SwinV2-L model the obtained Top-1 and Top-5 accuracies are also higher vs. the baseline. Thus, the proposed method is efficient and practical to improve the large transformer model training. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Electricity forecasting is crucial in scheduling and planning of future electric load, so as to improve the reliability and safeness of the power grid. Despite recent developments of forecasting algorithms in the machine learning community, there is a lack of general and advanced algorithms specifically considering requirements from the power industry perspective. In this paper, we present eForecaster, a unified AI platform including robust, flexible, and explainable machine learning algorithms for diversified electricity forecasting applications. Since Oct. 2021, multiple commercial bus load, system load, and renewable energy forecasting systems built upon eForecaster have been deployed in seven provinces of China. The deployed systems consistently reduce the average Mean Absolute Error (MAE) by 39.8% to 77.0%, with reduced manual work and explainable guidance. In particular, eForecaster also integrates multiple interpretation methods to uncover the working mechanism of the predictive models, which significantly improves forecasts adoption and user satisfaction. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"With the boom of digital educational materials and scalable e-learning systems, the potential for realising AI-assisted personalised learning has skyrocketed. In this landscape, the automatic generation of educational questions will play a key role, enabling scalable self-assessment when a global population is manoeuvring their personalised learning journeys. We develop EduQG, a novel educational question generation model built by adapting a large language model. Our initial experiments demonstrate that EduQG can produce superior educational questions by pre-training on scientific text. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"First-order logic (FO) is an important foundation of many domains, including computer science and artificial intelligence. In recent efforts to teach basic CS and AI concepts to children, FO has so far remained absent. In this paper, we examine whether it is possible to design a learning environment that both motivates and enables children to learn the basics of FO. The key components of the learning environment are a syntax-free blocks-based notation for FO, graphics-based puzzles to solve, and a tactile environment which uses computer vision to allow the children to work with wooden blocks. The resulting FOLL-E system is intended to sharpen childrens' reasoning skills, encourage critical thinking and make them aware of the ambiguities of natural language. During preliminary testing with children, they reported that they found the notation intuitive and inviting, and that they enjoyed interacting with the application. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Nowadays, AI-based techniques, such as deep neural networks (DNNs), are widely deployed in autonomous systems for complex mission requirements (e.g., motion planning in robotics). However, DNNs-based controllers are typically very complex, and it is very hard to formally verify their correctness, potentially causing severe risks for safety-critical autonomous systems. In this paper, we propose a construction scheme for a so-called Safe-visor architecture to sandbox DNNs-based controllers. Particularly, we consider the construction under a stochastic game framework to provide a system-level safety guarantee which is robust to noises and disturbances. A supervisor is built to check the control inputs provided by a DNNs-based controller and decide whether to accept them. Meanwhile, a safety advisor is running in parallel to provide fallback control inputs in case the DNNs-based controller is rejected. We demonstrate the proposed approaches on a quadrotor employing an unverified DNNs-based controller. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Recent research suggests that combining AI models with a human expert can exceed the performance of either alone. The combination of their capabilities is often realized by learning to defer algorithms that enable the AI to learn to decide whether to make a prediction for a particular instance or defer it to the human expert. However, to accurately learn which instances should be deferred to the human expert, a large number of expert predictions that accurately reflect the expert's capabilities are required-in addition to the ground truth labels needed to train the AI. This requirement shared by many learning to defer algorithms hinders their adoption in scenarios where the responsible expert regularly changes or where acquiring a sufficient number of expert predictions is costly. In this paper, we propose a three-step approach to reduce the number of expert predictions required to train learning to defer algorithms. It encompasses (1) the training of an embedding model with ground truth labels to generate feature representations that serve as a basis for (2) the training of an expertise predictor model to approximate the expert's capabilities. (3) The expertise predictor generates artificial expert predictions for instances not yet labeled by the expert, which are required by the learning to defer algorithms. We evaluate our approach on two public datasets. One with “synthetically” generated human experts and another from the medical domain containing real-world radiologists' predictions. Our experiments show that the approach allows the training of various learning to defer algorithms with a minimal number of human expert predictions. Furthermore, we demonstrate that even a small number of expert predictions per class is sufficient for these algorithms to exceed the performance the AI and the human expert can achieve individually. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"An agent’s ability to distinguish between sensory effects that are self-caused, and those that are not, is instrumental in the achievement of its goals. This ability is thought to be central to a variety of functions in biological organisms, from perceptual stabilisation and accurate motor control, to higher level cognitive functions such as planning, mirroring and the sense of agency. Although many of these functions are well studied in AI, this important distinction is rarely made explicit and the focus tends to be on the associational relationship between action and sensory effect or success. Toward the development of more general agents, we develop a framework that enables agents to disentangle self-caused and externally caused sensory effects. Informed by relevant models and experiments in robotics, and in the biological and cognitive sciences, we demonstrate the general applicability of this framework through an extensive experimental evaluation over three different environments. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Verifying the facts alleged by prosecutors before the trial requires the judges to retrieve evidence within the massive materials accompanied. Existing Legal AI applications often assume the facts are already determined and fail to notice the difficulty of reconstructing them. To build practical Legal AI applications and free judges from the manual searching work, we introduce the task of Legal Evidence Retrieval, which aims to automatically retrieve precise fact-related verbal evidence within a single case. We formulate the task in a dense retrieval paradigm and jointly learn the contrastive representations and alignments between facts and evidence. To avoid tedious annotations, we construct an approximated positive vector for a given fact by aggregating a set of evidence from the same case. An entropy-based denoising technique is further applied to mitigate the impact of false positive samples. We train our models on tens of thousands of unlabeled cases and evaluate them on a labeled dataset containing 919 cases and 4, 336 queries. Experimental results indicate that our approach is effective and outperforms other state-of-the-art representation and retrieval models. The dataset and code are available at https://github.com/yaof20/LER. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Face image synthesis has progressed beyond the point at which humans can effectively distinguish authentic faces from synthetically-generated ones. Recently developed synthetic face image detectors boast “better-than-human” discriminative ability, especially those guided by human perceptual intelligence during the model's training process. In this paper, we investigate whether these human-guided synthetic face detectors can assist non-expert human operators in the task of synthetic image detection when compared to models trained without human-guidance. We conducted a large-scale experiment with more than 1, 560 subjects classifying whether an image shows an authentic or synthetically-generated face, and annotating regions supporting their decisions. In total, 56, 015 annotations across 3, 780 unique face images were collected. All subjects first examined samples without any AI support, followed by samples given (a) the AI's decision (“synthetic” or “authentic”), (b) class activation maps illustrating where the model deems salient for its decision, or (c) both the AI's decision and AI's saliency map. Synthetic faces were generated with six modern Generative Adversarial Networks. Interesting observations from this experiment include: (1) models trained with human-guidance, which are also more accurate in our experiments, offer better support to human examination of face images when compared to models trained traditionally using cross-entropy loss, (2) binary decisions presented to humans results in their better performance than when saliency maps are presented, (3) understanding the AI's accuracy helps humans to increase trust in a given model and thus increase their overall accuracy. This work demonstrates that although humans supported by machines achieve better-than-random accuracy of synthetic face detection, the approaches of supplying humans with AI support and of building trust are key factors determining high effectiveness of the human-AI tandem. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"We introduce EINNs, a framework crafted for epidemic forecasting that builds upon the theoretical grounds provided by mechanistic models as well as the data-driven expressibility afforded by AI models, and their capabilities to ingest heterogeneous information. Although neural forecasting models have been successful in multiple tasks, predictions well-correlated with epidemic trends and long-term predictions remain open challenges. Epidemiological ODE models contain mechanisms that can guide us in these two tasks; however, they have limited capability of ingesting data sources and modeling composite signals. Thus, we propose to leverage work in physics-informed neural networks to learn latent epidemic dynamics and transfer relevant knowledge to another neural network which ingests multiple data sources and has more appropriate inductive bias. In contrast with previous work, we do not assume the observability of complete dynamics and do not need to numerically solve the ODE equations during training. Our thorough experiments on all US states and HHS regions for COVID-19 and influenza forecasting showcase the clear benefits of our approach in both short-term and long-term forecasting as well as in learning the mechanistic dynamics over other non-trivial alternatives. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"After the pandemic, artificial intelligence (AI) powered support for mental health care has become increasingly important. The breadth and complexity of significant challenges required to provide adequate care involve: (a) Personalized patient understanding, (b) Safety-constrained and medically validated chatbot patient interactions, and (c) Support for continued feedback-based refinements in design using chatbot-patient interactions. We propose Alleviate, a chatbot designed to assist patients suffering from mental health challenges with personalized care and assist clinicians with understanding their patients better. Alleviate draws from an array of publicly available clinically valid mental-health texts and databases, allowing Alleviate to make medically sound and informed decisions. In addition, Alleviate's modular design and explainable decision-making lends itself to robust and continued feedback-based refinements to its design. In this paper, we explain the different modules of Alleviate and submit a short video demonstrating Alleviate's capabilities to help patients and clinicians understand each other better to facilitate optimal care strategies. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The rise of AI methods to make predictions and decisions has led to a pressing need for more explainable artificial intelligence (XAI) methods. One common approach for XAI is to produce a post-hoc explanation, explaining why a black box ML model made a certain prediction. Formal approaches to post-hoc explanations provide succinct reasons for why a prediction was made, as well as why not another prediction was made. But these approaches assume that features are independent and uniformly distributed. While this means that “why” explanations are correct, they may be longer than required. It also means the “why not” explanations may be suspect as the counterexamples they rely on may not be meaningful. In this paper, we show how one can apply background knowledge to give more succinct “why” formal explanations, that are presumably easier to interpret by humans, and give more accurate “why not” explanations. In addition, we show how to use existing rule induction techniques to efficiently extract background information from a dataset. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Given a set X of n points in a metric space, the problem of diversity maximization is to extract a set S of k points from X so that the diversity of S is maximized. This problem is essential in AI-related fields, such as web search, databases, recommender systems, and data mining. Although there have been extensive studies of this problem, these studies assume that X is clean. This usually does not hold, because real-world datasets usually contain outliers. The state-of-the-art algorithm for the diversity maximization problem is based on furthest point retrieval, which is too sensitive to outliers. We therefore address the problem of diversity maximization with outliers and propose two algorithms with performance guarantee. The first algorithm runs in O((k + z)n) time, guarantees 21-approximation, and returns no outliers, where z is the number of outliers. The second algorithm runs in O(kz) time (which is independent of n), guarantees 6(1+ϵ) - 1 approximation, and returns no outliers with constant probability. We conduct experiments on real datasets to demonstrate the effectiveness and efficiency of our algorithms. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Dynamical systems are general models of change or movement over time with a broad area of applicability to many branches of science, including computer science and AI. Dynamic topological logic (DTL) is a formal framework for symbolic reasoning about dynamical systems. DTL can express various liveness and reachability conditions on such systems, but has the drawback that the only known axiomatisation requires an extended language. In this paper, we consider dynamic topological logic restricted to the class of scattered spaces. Scattered spaces appear in the context of computational logic as they provide semantics for provability and enjoy definable fixed points. We exhibit the first sound and complete dynamic topological logic in the original language of DTL. In particular, we show that the version of DTL based on the class of scattered spaces is finitely axiomatisable, and that the natural axiomatisation is sound and complete. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Neural networks require careful weight initialization to prevent signals from exploding or vanishing. Existing initialization schemes solve this problem in specific cases by assuming that the network has a certain activation function or topology. It is difficult to derive such weight initialization strategies, and modern architectures therefore often use these same initialization schemes even though their assumptions do not hold. This paper introduces AutoInit, a weight initialization algorithm that automatically adapts to different neural network architectures. By analytically tracking the mean and variance of signals as they propagate through the network, AutoInit appropriately scales the weights at each layer to avoid exploding or vanishing signals. Experiments demonstrate that AutoInit improves performance of convolutional, residual, and transformer networks across a range of activation function, dropout, weight decay, learning rate, and normalizer settings, and does so more reliably than data-dependent initialization methods. This flexibility allows AutoInit to initialize models for everything from small tabular tasks to large datasets such as ImageNet. Such generality turns out particularly useful in neural architecture search and in activation function discovery. In these settings, AutoInit initializes each candidate appropriately, making performance evaluations more accurate. AutoInit thus serves as an automatic configuration tool that makes design of new neural network architectures more robust. The AutoInit package provides a wrapper around TensorFlow models and is available at https://github.com/cognizant-ai-labs/autoinit. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Human guides in museums and galleries are professionally trained to stimulate informal learning in visitors by asking low-risk, open-ended reflective questions that enable them to focus on specific features of artifacts, relate to prior experiences, and elicit curiosity as well as further thought. We present ArtMuse, our AI-powered chatbot for asking reflective questions in context of paintings. Our reflective question generation model in ArtMuse was trained by applying a novel combination of existing models for extractive question answering and open-domain chitchat. User evaluation studies indicate that we are able to generate fluent and specific reflective questions for paintings that are highly-engaging. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The existing resource allocation policy for application instances in Kubernetes cannot dynamically adjust according to the requirement of business, which would cause an enormous waste of resources during fluctuations. Moreover, the emergence of new cloud services puts higher resource management requirements. This paper discusses horizontal POD resources management in Alibaba Cloud Container Services with a newly deployed AI algorithm framework named AHPA - the adaptive horizontal pod auto-scaling system. Based on a robust decomposition forecasting algorithm and performance training model, AHPA offers an optimal pod number adjustment plan that could reduce POD resources and maintain business stability. Since being deployed in April 2021, this system has expanded to multiple customer scenarios, including logistics, social networks, AI audio and video, e-commerce, etc. Compared with the previous algorithms, AHPA solves the elastic lag problem, increasing CPU usage by 10% and reducing resource cost by more than 20%. In addition, AHPA can automatically perform flexible planning according to the predicted business volume without manual intervention, significantly saving operation and maintenance costs. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Children of all ages interact with speech recognition systems but are largely unaware of how they work. Teaching K-12 students to investigate how these systems employ phonological, syntactic, semantic, and cultural knowledge to resolve ambiguities in the audio signal can provide them a window on complex AI decision-making and also help them appreciate the richness and complexity of human language. We describe a browser-based tool for exploring the Google Web Speech API and a series of experiments students can engage in to measure what the service knows about language and the types of biases it exhibits. Middle school students taking an introductory AI elective were able to use the tool to explore Google's knowledge of homophones and its ability to exploit context to disambiguate them. Older students could potentially conduct more comprehensive investigations, which we lay out here. This approach to investigating the power and limitations of speech technology through carefully designed experiments can also be applied to other AI application areas, such as face detection, object recognition, machine translation, or question answering. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Monitoring machine learning models once they are deployed is challenging. It is even more challenging to decide when to retrain models in real-case scenarios when labeled data is beyond reach, and monitoring performance metrics becomes unfeasible. In this work, we use non-parametric bootstrapped uncertainty estimates and SHAP values to provide explainable uncertainty estimation as a technique that aims to monitor the deterioration of machine learning models in deployment environments, as well as determine the source of model deterioration when target labels are not available. Classical methods are purely aimed at detecting distribution shift, which can lead to false positives in the sense that the model has not deteriorated despite a shift in the data distribution. To estimate model uncertainty we construct prediction intervals using a novel bootstrap method, which improves previous state-of-the-art work. We show that both our model deterioration detection system as well as our uncertainty estimation method achieve better performance than the current state-of-the-art. Finally, we use explainable AI techniques to gain an understanding of the drivers of model deterioration. We release an open source Python package, doubt, which implements our proposed methods, as well as the code used to reproduce our experiments. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Cropland segmentation of satellite images is an essential basis for crop area and yield estimation tasks in the remote sensing and computer vision interdisciplinary community. Instead of common pixel-level segmentation results with salt- and-pepper effects, a parcel-level output conforming to human recognition is required according to the clients' needs during the model deployment. However, leveraging CNN-based models requires fine-grained parcel-level labels, which is an unacceptable annotation burden. To cure these practical pain points, in this paper, we present PARCS, a holistic deployment-oriented AI system for PARcel-level Cropland Segmentation. By consolidating multi-disciplinary knowledge, PARCS has two algorithm branches. The first branch performs pixel-level crop segmentation by learning from limited labeled pixel samples with an active learning strategy to avoid parcel-level annotation costs. The second branch aims at generating the parcel regions without a learning procedure. The final parcel-level segmentation result is achieved by integrating the outputs of these two branches in tandem. The robust effectiveness of PARCS is demonstrated by its outstanding performance on public and in-house datasets (an overall accuracy of 85.3% and an mIoU of 61.7% on the public PASTIS dataset, and an mIoU of 65.16% on the in-house dataset). We also include subjective feedback from clients and discuss the lessons learned from deployment. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"As exemplified by the COVID-19 pandemic, our health and wellbeing depend on a difficult-to-measure web of societal factors and individual behaviors. This effort requires new algorithmic and data-driven paradigms which span the full process of gathering costly data, learning models to understand and predict such interactions, and optimizing the use of limited resources in interventions. In response to these needs, I present methodological developments at the intersection of machine learning, optimization, and social networks which are motivated by on-the-ground collaborations on HIV prevention, tuberculosis treatment, and the COVID-19 response. Here, I give an overview of two lines of work. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The Model AI Assignments session seeks to gather and disseminate the best assignment designs of the Artificial Intelligence (AI) Education community. Recognizing that assignments form the core of student learning experience, we here present abstracts of six AI assignments from the 2023 session that are easily adoptable, playfully engaging, and flexible for a variety of instructor needs. Assignment specifications and supporting resources may be found at http://modelai.gettysburg.edu. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Many works in explainable AI have focused on explaining black-box classification models. Explaining deep reinforcement learning (RL) policies in a manner that could be understood by domain users has received much less attention. In this paper, we propose a novel perspective to understanding RL policies based on identifying important states from automatically learned meta-states. The key conceptual difference between our approach and many previous ones is that we form meta-states based on locality governed by the expert policy dynamics rather than based on similarity of actions, and that we do not assume any particular knowledge of the underlying topology of the state space. Theoretically, we show that our algorithm to find meta-states converges and the objective that selects important states from each meta-state is submodular leading to efficient high quality greedy selection. Experiments on four domains (four rooms, door-key, minipacman, and pong) and a carefully conducted user study illustrate that our perspective leads to better understanding of the policy. We conjecture that this is a result of our meta-states being more intuitive in that the corresponding important states are strong indicators of tractable intermediate goals that are easier for humans to interpret and follow. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"Artificial intelligence (AI)-empowered industrial fault diagnostics is important in ensuring the safe operation of industrial applications. Since complex industrial systems often involve multiple industrial plants (possibly belonging to different companies or subsidiaries) with sensitive data collected and stored in a distributed manner, collaborative fault diagnostic model training often needs to leverage federated learning (FL). As the scale of the industrial fault diagnostic models are often large and communication channels in such systems are often not exclusively used for FL model training, existing deployed FL model training frameworks cannot train such models efficiently across multiple institutions. In this paper, we report our experience developing and deploying the Federated Opportunistic Block Dropout (FEDOBD) approach for industrial fault diagnostic model training. By decomposing large-scale models into semantic blocks and enabling FL participants to opportunistically upload selected important blocks in a quantized manner, it significantly reduces the communication overhead while maintaining model performance. Since its deployment in ENN Group in February 2022, FEDOBD has served two coal chemical plants across two cities in China to build industrial fault prediction models. It helped the company reduce the training communication overhead by over 70% compared to its previous AI Engine, while maintaining model performance at over 85% test F1 score. To our knowledge, it is the first successfully deployed dropout-based FL approach. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The use of virtual agents (bots) has become essential for providing online assistance to customers. However, even though a lot of effort has been dedicated to the research, development, and deployment of such virtual agents, customers are frequently frustrated with the interaction with the virtual agent and require a human instead. We suggest that a holistic approach, combining virtual agents and human operators working together, is the path to providing satisfactory service. However, implementing such a holistic customer service system will not, and cannot, be achieved using any single AI technology or branch. Rather, such a system will inevitably require the integration of multiple and diverse AI technologies, including natural language processing, multi-agent systems, machine learning, reinforcement learning, and behavioral cloning; in addition to integration with other disciplines such as psychology, business, sociology, economics, operation research, informatics, computer-human interaction, and more. As such, we believe this customer service application offers a rich domain for experimentation and application of multidisciplinary AI. In this paper, we introduce the holistic customer service application and discuss the key AI technologies and disciplines required for a successful AI solution for this setting. For each of these AI technologies, we outline the key scientific questions and research avenues stemming from this setting. We demonstrate that integrating technologies from different fields can lead to a cost-effective successful customer service center. The challenge is that there is a need for several communities, each with its own language and modeling techniques, different problem-solving methods, and different evaluation methodologies, all of which need to work together. Real cooperation will require the formation of joint methodologies and techniques that could improve the service to customers, but, more importantly, open new directions in cooperation of diverse communities toward solving joint difficult tasks. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"The aim of this project is to improve human decision-making using explainability; specifically, how to explain the (un)certainty of machine learning models. Prior research has used uncertainty measures to promote trust and decision-making. However, the direction of explaining why the AI prediction is confident (or not confident) in its prediction needs to be addressed. By explaining the model uncertainty, we can promote trust, improve understanding and improve decision-making for users. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"In artificial intelligence (AI), negative social impact (NSI) represents the negative effect on the society as a result of mistakes conducted by AI agents. While the photo classification problem has been widely studied in the AI community, the NSI made by photo misclassification is largely ignored due to the lack of quantitative measurements of the NSI and effective approaches to reduce it. In this paper, we focus on an NSI-aware photo classification problem where the goal is to develop a novel crowd-AI collaborative learning framework that leverages online crowd workers to quantitatively estimate and effectively reduce the NSI of misclassified photos. Our problem is motivated by the limitations of current NSI-aware photo classification approaches that either 1) cannot accurately estimate NSI because they simply model NSI as the semantic difference between true and misclassified categories or 2) require costly human annotations to estimate NSI of pairwise class categories. To address such limitations, we develop SocialCrowd, a crowdsourcing-based NSI-aware photo classification framework that explicitly reduces the NSI of photo misclassification by designing a duo relational NSI-aware graph with the NSI estimated by online crowd workers. The evaluation results on two large-scale image datasets show that SocialCrowd not only reduces the NSI of photo misclassification but also improves the classification accuracy on both datasets. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"This work in progress paper describes Year 1 results from a Research Experiences for Teachers (RET) in Engineering and Computer Science grant funded by the National Science Foundation (NSF) Computer and Information Science and Engineering (CISE) directorate. The objective of this project is to establish a new RET Site at Auburn University (AU) in Alabama. The project is titled “RET Site: Project-Based Learning for Rural Alabama STEM Middle School Teachers in Machine Learning and Robotics."" It provides research experiences for up to ten 7th and 8th grade level math and science teachers each year during a 6-week summer program and 9-month academic year follow-up, with the research focused on smart mobile robots. The global leadership of U.S. largely relies on the future workforce in the fields of science and engineering. However, at rural schools in underserved areas of Alabama1, teachers may lack content knowledge in robotics and engineering concepts. Subsequently, their lack of content knowledge in robotics and engineering may adversely impact STEM persistence and engagement in robotics engineering for students from traditionally underrepresented groups and students in underserved school districts. Inspired and motivated by the need to better prepare teachers for teaching and engaging students in engineering concepts, the RET Site provides research experiences to middle school math and science teachers in rural Alabama, especially the Alabama Black Belt region, with the focus on smart robots that integrate robotics with Machine Learning (ML)/Artificial Intelligence (AI), which have seen tremendous advances in the past few years2. Teachers participate in education and research activities on state-of-the-art technologies in robotics and ML/AI, and explore various research topics with faculty mentors' as part of active research projects, including edge computing, computer vision, autonomous navigation, indoor localization, and reinforcement learning. To support hands-on research projects, we leverage a novel platform of ML-based mobile robots that is friendly and accessible to teachers. Teachers collaborate with engineering and STEM education faculty to develop engaging project-based curricular modules on robotics and ML/AI for classroom education at their local schools. Teachers will practice teaching the curricular modules that they have developed. © American Society for Engineering Education, 2023."
"The rise of computing and artificial intelligence (AI) will transform our society and it is clear that students will be forced to engage with AI in their careers. Currently, the United States lacks adequate infrastructure or capacity to support the teaching of AI in the K-12 curriculum. To address these challenges, we introduce the use of visual media as a key bridge technology to engage students in grades 6-8 with AI topics, through a recent NSF funded ITEST program, labeled ImageSTEAM. Specifically, we focus on the idea of a computational camera, which rethinks the sensing interface between the physical world and intelligent machines and enables students to ponder how sensors and perception fundamentally will augment science and technology in the future. Two workshops have been conducted, the first in summer 2021, and the second in summer 2022. The first workshop was delivered virtually due to the persistent COVID-19 environment at the time. The second workshop had one week conducted in-person and the second week was conducted virtually. Teachers and students participated in the workshops and their experiences will be shared and discussed at the conference. In addition, teacher use of the skills and knowledge learned from the workshops will be shared. © American Society for Engineering Education, 2023."
"The effective introduction of the fundamentals of artificial intelligence (AI) to middle school students requires the novel integration of the existing science curriculum and AI concepts. This research focuses on leveraging 6th and 7th-grade science curricula related to state standards to introduce machine learning concepts by using fossil shark teeth. Researchers from engineering, education, and paleontology collaboratively developed learning modules to upskill Title I schoolteachers to meaningfully integrate AI fundamentals within their existing curriculum. With a special emphasis on machine learning (ML), five lesson plans were presented during a week-long teacher professional development. Teachers conceptualized and implemented ML models that distinguish fossil shark teeth by their taxonomy and primary functions to recognize ecological and evolutionary patterns. After introducing a lesson, each teacher curated the lesson plan content to directly relate to their specific context, in collaboration with each other and our research team. We built the curriculum leveraging students' existing conceptions and misconceptions about AI from prior work while testing the feasibility of addressing AI learning objectives, as well the AI4K12's Five Big Ideas, in the broader context of middle school science, technology, engineering, mathematics, and computing (STEM+C) education. Our lessons were scaffolded using the iterative machine learning development process: 1) data collection and preparation; 2) selecting and training the model; 3) evaluating the models' accuracy; 4) tuning model parameters to improve performance. Each stage of the development process constituted a different lesson during a week-long summer professional development. Through these lessons, teachers were introduced to several open-source AI tools, including two platforms used to build/train ML models: Google's Teachable Machine and Roboflow. The fifth and final day of the professional development gave teachers time to conceptualize how these lessons could be integrated with their existing curricula. Initial feedback from the summer PD indicated we overestimated the teachers' familiarity with technology and the capacity to access all of the information that we provided. More time was necessary to orient teachers to each AI tool. Teachers readily adopted the use of Seek by iNaturalist and myFossil. However, the teachers' use of AI tools in their classrooms highly favored Google's Teachable Machine to Roboflow, which may relate to the affordances and constraints of each tool. Preliminary mixed-method data analyses show teachers' self-efficacy around teaching AI improved after engaging in the summer PD. Longitudinal data collection is underway and will inform future work related to improving teacher and student self-efficacy related to teaching and learning AI, respectively. © American Society for Engineering Education, 2023."
"In a recent issue of Nature Methods, Platisa et al. present an approach for long-term, in vivo population voltage imaging with single spike resolution across a local population of 100 neurons.1 Key to this step forward was the combination of a customized high-speed two-photon microscope with an optimized, positive-going, genetically encoded voltage indicator and a tailored machine learning denoising algorithm. © 2023 The Author(s)"
"The concept of a Human-AI team has gained increasing attention in recent years. For effective collaboration between humans and AI teammates, proactivity is crucial for close coordination and effective communication. However, the design of adequate proactivity for AI-based systems to support humans is still an open question and a challenging topic. In this paper, we present the development of a corpus-based user simulator for training and testing proactive dialog policies. The simulator incorporates informed knowledge about proactive dialog and its effect on user trust and simulates user behavior and personal information, including socio-demographic features and personality traits. Two different simulation approaches were compared, and a task-step-based approach yielded better overall results due to enhanced modeling of sequential dependencies. This research presents a promising avenue for exploring and evaluating appropriate proactive strategies in a dialog game setting for improving Human-AI teams.  © 2023 Owner/Author."
[No abstract available]
"The pioneering use of Artificial Intelligence (AI) in various fields and sectors, and the growing ethical debate about its application have led research centers, public and private institutions to establish ethical guidelines for a trustworthy implementation of these powerful algorithms. Despite the recognized definition of ethical principles for a responsible or trustworthy use of AI, there is a lack of a sector-specific perspective that highlights the ethical risks and opportunities for different areas of application, especially in the field of Cultural Heritage (CH). In fact, there is still a lack of formal frameworks that evaluate the algorithms' adherence to the ethical standards set by the European Union for the use of AI in protecting CH and its inherent value. Because of this, it is necessary to investigate a different sectoral viewpoint to supplement the widely used horizontal approach. This paper represents a first attempt to design an ethical framework to embody AI in CH conservation practises to assess various risks arising from the use of AI in the field of CH. The contribution presents a synthesis of the different AI applications to improve the preservation process of CH. It explores and analyses in depth the ethical challenges and opportunities presented by the use of AI to improve CH preservation. In addition, the study aims to design an ethical framework of principles to assess the application of this ground-breaking technology at CH. © 2023 S. Pansoni et al."
"The way we collaborate and work together is subject to constant change. While AI-enabled tools are emerging rapidly and impacting how we work individually, most are not designed to support collocated collaboration. This demonstrator showcases CollEagle: an interactive interface that leverages NLP to enable tangible human-AI interaction. As the access point for AI-enabled technologies is signposted by lexical input, the CollEagle system provides an example of how we might design tangible human-AI interactions and support the collaborative use of AI in collocated environments. © 2023 The Authors."
"The rapid expansion of Artificial Intelligence (AI) necessitates a need for educating students to become knowledgeable of AI and aware of its interrelated technical, social, and human implications. The latter (ethics) is particularly important to K-12 students because they may have been interacting with AI through everyday technology without realizing it. They may be targeted by AI generated fake content on social media and may have been victims of algorithm bias in AI applications of facial recognition and predictive policing. To empower students to recognize ethics related issues of AI, this paper reports the design and implementation of a suite of ethics activities embedded in the Developing AI Literacy (DAILy) curriculum. These activities engage students in investigating bias of existing technologies, experimenting with ways to mitigate potential bias, and redesigning the YouTube recommendation system in order to understand different aspects of AI-related ethics issues. Our observations of implementing these lessons among adolescents and exit interviews show that students were highly engaged and became aware of potential harms and consequences of AI tools in everyday life after these ethics lessons. © American Society for Engineering Education, 2023."
[No abstract available]
[No abstract available]
"When a student submits a conceptual sketch in response to an architectural design problem, the instructor may presume that the student researched a couple of precedents then formulated their own ideation. How should the instructor react when an artificial intelligence (AI) art generator created or influenced the image? AI art generators create new or adapt existing architectural representations from imported text within seconds. High quality graphic solutions from text-to-image modelmakers are now confronting the academy. OpenAI's Dall-E 2 and Midjourney are two popular open source and fee-based art generators. Web crawlers regularly scrape the internet to archive digital data. Research companies acquire the data then compile and pair billions of images and associated text descriptors into massive datasets. When a natural language processor interprets a prompt such as 'Pompidou rendering inspired by Mies', the deep learning algorithm seeks out the specific pattern associated with the input. The output is in the form of architectural representations. The design visualizations are a series of composites transformed to illustrate the requested version of a building. Although the AI generators make art more accessible to the population, they invite controversy from the art community regarding attribution. This paper discusses the ethical and legal implications surrounding AI art generators and copyrights, describes how the AI generators operate, considers the positions in the creative process, and concludes with suggested best practices for engaging AI art in the architectural design curricula. © American Society for Engineering Education, 2023."
"Time and again, it has been reported that COVID-19 has pushed the learning arena to a new norm. However, the shift in the educational sector was there for quite some time, which can be viewed by the presence of an online educational platform, namely Coursera. Artificial intelligence (AI) in teaching and learning has been extensively perceived in medical schools and molecular biology studies. This type of teaching and learning can effectively comprehend the few drawbacks that the traditional system has. Using AI, educators can easily facilitate a much more focused and framed curriculum for the students, developing their thinking abilities (such as critical and problem-solving) since they are now more independent. The focus will be on the AI modeling of teaching and learning to support critical thinking and problem-solving approaches for intelligent universities in the current chapter. The chapter explains the AI modeling framework for teaching and learning and explains critical thinking and problem-solving in a university curriculum design. © 2023 Apple Academic Press, Inc. All rights reserved."
"Computer Vision (CV) has become an essential tool for developers looking to personalize user experiences. In particular, commercial CV services can be used by those who are not machine learning experts, but who want to enhance their apps and services with vision capabilities. While the performance of CV has become increasingly human-like, its ""social behaviors""and their compatibility with human values are of concern. In contrast to algorithmic decision-making, where fairness is used to evaluate system behavior, CV is often evaluated for stereotyping - the extent to which systems reflect prevalent social beliefs. This paper proposes that viewing stereotyping negatively is unhelpful in improving human-AI interaction. Rather, it is more fruitful to separate the observation of a social behavior (i.e., documenting what a machine does in relation to a human) from its judgment (i.e., relating the behavior to social norms). As norms differ across contexts and application areas, such an approach better reflects the real world, which is characterized by diversity and opposing views. However, it requires us to face up to two truths: i) humans - not machines - are the problem; ii) we must decide what degree of human-likeness we ultimately want; technologies designed to mimic us will reflect social bias.  © 2023 Owner/Author."
"Deep Learning is a form of AI machine learning that has gained a great deal of recognition in the past 10 years in a wide range of areas such as medical diagnosis, quality assurance, defect detection, face detection, autonomous vehicles, and many others. Deep learning networks, however, typically require large training databases of labeled images and often require specialized hardware and high-level software expertise. Techniques, such as transfer learning and the proper choice of software tools can mitigate some of these requirements. This paper describes a new, project-based course module to introduce deep learning and computer vision to undergraduate multidisciplinary engineering students in a robotics design and applications course using MATLAB software. © American Society for Engineering Education, 2023."
"Many complex simulations are extremely expensive and hardly if at all doable, even on current supercomputers. A typical reason for this are coupled length and time scales in the application which need to be resolved simultaneously. As a result, many simulation approaches rely on scale-splitting, where only the larger scales are simulated, while the small scales are modeled with subfilter models. This work presents a novel subfilter modeling approach based on AI super-resolution. A physics-informed enhanced super-resolution generative adversarial network (PIESRGAN) is used to accurately close subfilter terms in the solved transport equations. It is demonstrated how a simulation design with the PIESRGAN-approach can be used to accelerate complex simulations on current supercomputers, on the example of three fluid dynamics simulation setups with complex features on the supercomputer environment JURECA-DC/JUWELS (Booster). Further advantages and shortcoming of the PIESRGAN-approach are discussed.  © 2023 Owner/Author(s)."
"Through the synergy of NASA University Leadership Initiative (ULI) Project “Safe Aviation Autonomy with Learning-enabled Components in the Loop: from Formal Assurances to Trusted Recovery Methods” and NSF Excellent in Research (EIR) project “Integrated Sensor-Robot Networks for Real-time Environmental Monitoring and Marine Ecosystem Restoration in the Hampton River”, the authors have successfully developed a research-based course on machine learning and robotics for undergraduate engineering students at Hampton University. This paper presents the goals, challenges, design process, engaging strategies, assessment/outcomes, and lessons learned for the new course. Besides, this paper also presents the integration of IBM AI course and NVIDIA machine learning modules, along with the Couse Extension -two weeks summer undergraduate research experiences on AI/ML and robotics in the Autonomous Systems Laboratory directed by Dr. Marco Pavone at Stanford University. The success in the development of this course is due to the collaboration with Stanford University, which opening Hampton Undergraduate students' eyes to the larger issues in the area of study; due to the support from industry such as IBM and NVDIA, which provide Hampton University free training license for the online course and resources. © American Society for Engineering Education, 2023."
"We present a novel counterfactual-based dashboard for explainable artificial intelligence (XAI) in process industries, aimed at enhancing the understanding and adoption of machine learning (ML) models by providing transparency, explainability, and performance evaluation. Our dashboard comprises two modules: a statistical analysis module for data visualization and model performance assessment, and an XAI module for exploring counterfactual explanations at varying levels of abstraction. Through a case study of an industrial batch process, we demonstrate the dashboard's applicability and potential to increase trust in ML models among stakeholders, paving the way for confident deployment in process industries. © 2023 The Authors."
"The gold standard to detect SARS-CoV-2 infection consider testing methods based on Polymerase Chain Reaction (PCR). Still, the time necessary to confirm patient infection can be lengthy, and the process is expensive. On the other hand, X-Ray and CT scans play a vital role in the auxiliary diagnosis process. Hence, a trusted automated technique for identifying and quantifying the infected lung regions would be advantageous. Chest X-rays are two-dimensional images of the patient's chest and provide lung morphological information and other characteristics, like ground-glass opacities (GGO), horizontal linear opacities, or consolidations, which are characteristics of pneumonia caused by COVID-19. But before the computerized diagnostic support system can classify a medical image, a segmentation task should usually be performed to identify relevant areas to be analyzed and reduce the risk of noise and misinterpretation caused by other structures eventually present in the images. This chapter presents an AI-based system for lung segmentation in X-ray images using a U-net CNN model. The system's performance was evaluated using metrics such as cross-entropy, dice coefficient, and Mean IoU on unseen data. Our study divided the data into training and evaluation sets using an 80/20 train-test split method. The training set was used to train the model, and the evaluation test set was used to evaluate the performance of the trained model. The results of the evaluation showed that the model achieved a Dice Similarity Coefficient (DSC) of 95%, Cross entropy of 97%, and Mean IoU of 86%. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. All rights reserved."
"The health and quality of the Danube River ecosystems is strongly affected by the nutrients loads (N and P), degree of contamination with hazardous substances or with oxygen depleting substances, microbiological contamination and changes in river flow patterns and sediment transport regimes. Water quality index (WQI) is an important dynamic attribute in the characterization of the Danube River ecosystems health and quality. The WQ index scores do not reflect the actual condition of water quality. We proposed a new forecast scheme for water quality based on the following qualitative classes very good (0–25), good (26–50), poor (51–75), very poor (76–100) and extremely polluted/non-potable (>100). Water quality forecasting by using Artificial Intelligence (AI) is a meaningful method of protecting public health because of its possibility to provide early warning regarding harmful water pollutants. The main objective of the present study is to forecast the WQI time series data based on water physical, chemical and flow status parameters and associated WQ index scores. The Cascade-forward network (CFN) models, along with the Radial Basis Function Network (RBF) as a benchmark model, were developed using data from 2011 to 2017 and WQI forecasts were produced for the period 2018–2019 at all sites. The nineteen input water quality features represent the initial dataset. Moreover, the Random Forest (RF) algorithm refines the initial dataset by selecting eight features considered the most relevant. Both datasets are employed for constructing the predictive models. According to the results of appraisal, the CFN models produced better outcomes (MSE = 0.083/0,319 and R-value 0.940/0.911 in quarter I/quarter IV) than the RBF models. In addition, results show that both the CFN and RBF models could be effective for predicting time series data for water quality when the eight most relevant features are used as input variables. Also, the CFNs provide the most accurate short-term forecasting curves which reproduce the WQI for the first and fourth quarters (the cold season). The second and third quarters presented a slightly lower accuracy. The reported results clearly demonstrate that CFNs successfully forecast the short-term WQI as they may learn historic patterns and determine the nonlinear relationships between the input and output variables. © 2023 The Authors"
"The UNESCO 2011 Recommendation on the Historic Urban Landscape promotes to map cultural significance of urban heritage from the perspectives of the general public in pursuit of social inclusion in heritage management. The user-generated information already available on social media platforms in the form of images, comments, and ratings can be considered a rich source for collecting data concerning the tourists' image of destinations and their collective perception of urban cultural heritage. Considering the large amount of unstructured data, artificial intelligence (AI) can construct structured feature vectors therefrom and significantly aid the analysis and collation processes compared to the traditional manual approach for mapping public perception of cultural heritage. This paper presents an exploratory case study conducted in the area of Testaccio, Rome, showcasing the use of AI to map the perceived and narrated urban heritage images using social media data. An image-sharing platform, Flickr, is used to collect thousands of posts containing images and comments in the area, which are further analysed with pre-Trained image recognition, natural language processing, and dimensionality reduction algorithms. Results as the urban heritage images are visualised, showing the most significant elements from a public perspective. Such a methodology provides an alternative perspective of viewing the urban heritage attributes as a collection of depicted and posted content. It can contribute as a tool for the documentation of collective attention for inclusive heritage management and local development planning during the designing and policy-making processes. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"Knowledge graphs are important in human-centered AI because of their ability to reduce the need for large labelled machine-learning datasets, facilitate transfer learning, and generate explanations. However, knowledge-graph construction has evolved into a complex, semi-automatic process that increasingly relies on opaque deep-learning models and vast collections of heterogeneous data sources to scale. The knowledge-graph lifecycle is not transparent, accountability is limited, and there are no accounts of, or indeed methods to determine, how fair a knowledge graph is in the downstream applications that use it. Knowledge graphs are thus at odds with AI regulation, for instance the EU's upcoming AI Act, and with ongoing efforts elsewhere in AI to audit and debias data and algorithms. This paper reports on work in progress towards designing explainable (XAI) knowledge-graph construction pipelines with human-in-the-loop and discusses research topics in this space. These were grounded in a systematic literature review, in which we studied tasks in knowledge-graph construction that are often automated, as well as common methods to explain how they work and their outcomes. We identified three directions for future research: (i) tasks in knowledge-graph construction where manual input remains essential and where there may be opportunities for AI assistance; (ii) integrating XAI methods into established knowledge-engineering practices to improve stakeholder experience; as well as (iii) evaluating how effective explanations genuinely are in making knowledge-graph construction more trustworthy. © 2023 The Authors."
"This book explores AI methodologies for the implementation of affective states in intelligent learning environments. Divided into four parts, Multimodal Affective Computing: Technologies and Applications in Learning Environments begins with an overview of Affective Computing and Intelligent Learning Environments, from their fundamentals and essential theoretical support up to their fusion and some successful practical applications. The basic concepts of Affective Computing, Machine Learning, and Pattern Recognition in Affective Computing, and Affective Learning Environments are presented in a comprehensive and easy-to-read manner. In the second part, a review on the emerging field of Sentiment Analysis for Learning Environments is introduced, including a systematic descriptive tour through topics such as building resources for sentiment detection, methods for data representation, designing and testing the classification models, and model integration into a learning system. The methodologies corresponding to Multimodal Recognition of Learning-Oriented Emotions are presented in the third part of the book, where topics such as building resources for emotion detection, methods for data representation, multimodal recognition systems, and multimodal emotion recognition in learning environments are presented. The fourth and last part of the book is devoted to a wide application field of the combination of methodologies, such as Automatic Personality Recognition, dealing with issues such as building resources for personality recognition, methods for data representation, personality recognition models, and multimodal personality recognition for affective computing. This book can be very useful not only for beginners who are interested in affective computing and intelligent learning environments, but also for advanced and experts in the practice and developments of the field. It complies an end-to-end treatment on these subjects, especially with educational applications, making it easy for researchers and students to get on track with fundamentals, established methodologies, conventional evaluation protocols, and the latest progress on these subjects. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. All rights reserved."
"The quorum sensing system of microorganisms widely exists in fermented vegetables, which regulates the fermentation process of vegetables and affects the product quality of fermented vegetables. Therefore, this study investigated how the quorum sensing inhibitor D-ribose affects the Sichuan pickle fermentation, with exogenous spoilage bacteria Enterobacter cloacae, Klebsiella Trevisan, Providencia ewing and Citrobacter freundii. The results showed that, following D-ribose treatment, the formation of pellicle was significantly inhibited while the pH value of pickle was stable within a certain range during the maturity period. The effects of D-ribose on the growth, AI-2, and biofilm formation of single spoilage bacteria were explored further in this study. The results showed that after D-ribose treatment, the biofilm formation and the AI-2 activity were decreased whereas the growth of spoilage bacteria was unaffected. Scanning electron microscopy analysis showed the dispersal dynamics of the single bacteria but not the form of a tight biofilm following D-ribose treatment. Collectively, these results will provide new ideas for the development of new pickle preservatives. © 2023 The authors."
"Artificial intelligence (AI) is revolutionizing the learning system in this 21st century. COVID-19 pandemic revealed the mandatory use of AI to connect people, sustain service delivery of goods and services, enhance blended learning modes, maintain commercial transactions, and foster environmental conservation. This research examines, analyzes, assesses, and reveals the implication of AI for the School of Building and Civil Engineering (SBCE) at Fiji National University (FNU), Fiji Islands. Hence, discussing the strategies and adaptive measures that should be accepted as a new normal for the school. Data were collected through interviews, expert opinions, field participation, and observations. The SBCE offers automotive, technology, and science programs which demand the integrated use of AI in order to thrive in this technological era. The outcome of this research reveals the need for academic staff, instructors, tutors, and lab technicians to enhance their knowledge in applying AI-first and foremost in their own disciplines. Then, this knowledge will definitely have a positive cascading influence for the school to become a leading catalyst in adapting AI solutions to adapt to any health, natural or human disaster in the future. © 2023 Apple Academic Press, Inc. All rights reserved."
"This study explored differences and similarities among undergraduate engineering programs named general, engineering, interdisciplinary, and integrated. Benchmarking these non-specialty programs was conducted using information from course catalogs and websites. Many of these ABET EAC-accredited programs only a awarded a very small number of Bachelor's degrees in 2022 or 2021. The majority of the non-specialty programs required students to select a concentration, generally in a traditional engineering discipline (e.g., mechanical) but in some programs these were unique interdisciplinary areas (e.g., renewable energy). Based on the 2022 catalogs, the total number of credits did not differ statistically among the non-specialty ABET EAC accredited programs with different names. On average across 35 institutions, the non-specialty degrees required 1.4 fewer credits than disciplinary engineering degrees at those same institutions. Among a smaller number of ABET EAC accredited non-specialty degrees that were benchmarked in more detail, 19 'engineering' and 'general engineering' degrees required a lower percentage of technical coursework and offered a lower percentage of curricular choice compared to 7 degrees that included the word interdisciplinary, integrated, or multidisciplinary in their name. A few programs require students to take the NCEES Fundamentals of Engineering (FE) exam prior to graduation. The AI-based program ChatGPT definitions of general, interdisciplinary, and integrated all emphasized breadth, multiple disciplines, and design, while also including the distinguishing factors of practical (for general) versus complex and innovative/novel (interdisciplinary and integrated), and the importance of social impacts (integrated). Various types of content analyses were conducted based on how these programs are described on their websites; differences among the program name groups were not identified but the corpora were too small for robust analysis. Overall the paper provides enhanced understanding of the goals and curricula of these non-disciplinary engineering degree programs. This may be helpful as programs consider suitable names for non-specialty engineering degrees. © American Society for Engineering Education, 2023."
"This paper investigates the implications of competing definitions of 'personhood' for technology, specifically artificial intelligence (AI) agents and ways in which their legal and moral status may evolve over time. This exploration was the initial basis for a course in a liberal studies program. The basic structure of that course will be presented, including readings. An important starting point for the course and discussion was to look at historical, philosophical, and religious definitions of a person. One of the more natural points of comparison was and continues to be how we regard the status and rights of animals. The questions become one of setting boundaries for categorization. For example, is the boundary about intellectual capacity? How would that be defined? What are the defining hallmarks of cognition? Is it language or logic or something else? And what is the role and importance of physically embedded sensation and perception? What of these features do AI agents possess or are likely to possess? The animal rights movements and legal protections for pets and animals may serve as a template for exploring what may be eventually likely for such artificial agents. The ability to feel, both positive and negative, pleasure and pain, has been brought into arguments about regulating our relationship with the living world and how far ownership and domination may extend. It is also useful to remember earlier understanding of rights, humanity, and personhood of women, children, and slaves and the ways in which that understanding has evolved in Western thought and legal systems. Certainly, the personhood of artificial lifeforms has been a staple of science fiction books, television, and movies since Frankenstein, but the import of such moral thought experiments is often dismissed as irrelevant when discussing the status of artificial agents and ways in which moral guidance will be instilled into such semi-autonomous beings. Isaac Asimov's laws of robotics are marginally used as a starting point for such discussions, however seeing what is missing in his statement of the problem can be productive. Generally, in Western thought it seems that primacy is given to individual interaction and decision making and little emphasis is placed on the expanding circles of obligation from family, kin, tribe, nation, humanity as a whole. The issues raised here are not just a sterile intellectual exercise but have real consequences as we wrestle with programming decision making in such agents as autonomous cars and prioritizing associated legal and moral goods and virtues. © American Society for Engineering Education, 2023."
"Computer Science (CS) Frontiers is a 4-module curriculum, 9 weeks each, designed to bring the frontiers of computing to high school girls for exploration and development. Our prior work has showcased the work in developing and piloting our first three modules, Distributed Computing, Artificial Intelligence (AI), and the Internet of Things (IoT). During the summer of 2022, we piloted the completed curricula, including the new Software Engineering module, with 56 high school camp attendees. This poster reports on the newly developed software engineering module, the experiences of 7 teachers and 11 students using the module, and our plans for improving this module prior to its release in formal high school classrooms. Initial survey and interview data indicate that teachers became comfortable with facilitating the open-endedness of the final projects and that students appreciated the connections to socially relevant topics and the ability of their projects to help with real-world problems such as flood prevention and wheelchair accessibility. The CS Frontiers curriculum has been added to course offerings in Tennessee and adoption through the North Carolina Department of Public Instruction is currently underway. Teachers from Tennessee, North Carolina, Massachusetts, and New York have piloted the materials. Together with researchers, we are working to package the course and curricula for widespread adoption as additional support to students as they try out computing courses in their high school pathways. Our aim is to increase the interest and career awareness of CS for high school girls so they may have an equitable footing to choose CS as a potential major or career. © American Society for Engineering Education, 2023."
"The protection of cultural heritage is an important task of communities on various levels of social organization. The institutionalization of the processes of protection of modern heritage assets provides the necessary instruments (legislative, juridical, financial) enabling the actual realization of the assumed tasks. The criterion of age, which is still a dominating premise for monument protection, proved not to be sufficient, especially concerning protection of monuments of Modernism. A step that led to the determination of the value of individual architectural objects of the 20th century was the establishment of 10 evaluation criteria proposed by historians of architecture in Warsaw, and afterwards in Poznan.In this work, we focus on the architectural value of post-war buildings, which are most difficult to evaluate. Furthermore, we wanted to apply AI to objectify the process of decision making. The adequacy of the Dominance-based Rough Set Approach (DRSA) method has been established. This method takes into account preference orders on criteria and models patterns observed in data in terms of monotonic ""if ..., then ...""decision rules. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"Drone swarms, the ability of drones to autonomously make decisions based on shared information, create new opportunities with major societal implications. However, future drone swarm applications and services pose new networking challenges. A resurgence of artificial intelligence (AI) and machine learning (ML) research presents a tremendous opportunity for addressing these networking challenges. This REU site focuses on networking research for drone swarms in the age of AI. The first cohort of seven undergraduate students were recruited to participate in a ten-week summer program to perform networking research for drone swarms under the guidance of faculty and research mentors. In this paper, a couple of drone swarm projects were briefly summarized. By the end of the summer program, students was surveyed about their undergraduate research experiences. A couple of months after students were back to their home institutions, a couple of students were interviewed about the impact of their undergraduate research experiences on their continued learning. The faculty who helped to supervise the undergraduate students at the REU also were interviewed. The feedback from the students and reflections from the faculty would provide guidance about the integration of the undergraduate research experiences into the courses to broaden the impacts of undergraduate research on learning and teaching. In the future, at least another two cohorts of students. especially from underrepresented groups, will be recruited. We will have a longitudinal study to explore the impacts of undergraduate research experiences on learning and teaching using a mixed qualitative and quantitative method. © American Society for Engineering Education, 2023."
"Creative ideas need to be generated continuously in content marketing to communicate effectively to a given target group. Usually, brainstorming techniques are applied by content creators to stimulate new ideas. With the emergence of generative AI like ChatGPT, content ideas can be generated rapidly. The assumption is that by combining human and AI creativity appropriately, the creative results are higher than by humans or AI alone. The open research question is how to integrate humans and generative AI within the creativity process to augment the creativity. The research approach presented based on a currently running research project is aiming to identify appropriate design criteria to integrate generative AI in the form of ChatGPT into brainstorming processes to generate improved content ideas in the context of content marketing. © 2023 The Authors."
"The presented research aims to define a parametric modelling methodology that allows, in short time and at a sustainable cost, the digital acquisition, modelling and semantic structuring of urban city blocks to facilitate 3D city modelling applied to historic centres. The methodology is based on field surveying and derives 3D data for the realisation of a parametric City Information Model (CIM). This is pursued through the adoption of parametric modelling as main method combined with AI procedures like supervised machine learning. In particular, the Visual Programming Language (VPL) Grasshopper is adopted as main working environment. The methodology proposed, called <i>Scan-To-CIM</i>, is developed to automate the cognitive operations of interpretation and input of surveying data performed in the field in order to create LoD4 city block models in a semi-Automatic way. The proposed <i>Scan-To-CIM</i> methodology is applied to a city block located in the historic centre of Catania, Italy. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
[No abstract available]
"Multimodal data enables us to capture the cognitive and affective states of students to provide a holistic understanding of learning processes in a wide variety of contexts. With the use of sensing technology, we can capture learners’ states in near real-time and support learning. Moreover, multimodal data allows us to obtain early-predictions of learning performance, and support learning in a timely manner. In this contribution, we utilize the notion of “carry forward effect”, an inferential and predictive modelling approach that utilizes multimodal data measurements detrimental to learning performance to provide timely feedback suggestions. carry forward effect can provide a way to prioritize conflicting feedback suggestions in a multimodal data based scaffolding tool. We showcase the empirical proof of carry forward effect with the use of two different learning scenarios: debugging and game-based learning. © 2023 Copyright held by the owner/author(s)."
"There is a recurring issue of low enrollments across many civil engineering departments in postsecondary institutions. While there have been moments where enrollments begin to increase, civil engineering departments find themselves facing low enrollments that have decreased by 60% over the last five years across the Middle East and the United States. There are many reasons that could be attributed to this decline, such as low entry-level salaries, over-saturation of civil engineering graduates in the job market in certain regions, and a lack of construction projects due to the impending or current recession. Low enrollment also has an effect on the availability of civil engineers, especially in times of high demand, such as the passing of the recent US legislature on rebuilding infrastructure. However, this recurring problem alludes to an intrinsic issue of the curriculum, as researchers have discovered. The societal shift to the usage of high technology such as machine learning (ML) and artificial intelligence (AI), demands individuals who are proficient at utilizing it. However, in many existing civil engineering curricula, students are not taught programming skills that would aid in using high technology and if introduced at an early level, these skills are not utilized in other courses across the curriculum. This paper aims to conduct a survey on the civil engineering curricula of the top 220 universities in the world, focusing on those in the United States based on the QS World Ranking system. Initial analysis of the survey results indicates that the majority of universities have not considered new methods of data analytics such as ML or AI in their civil engineering coursework. Based on the results of the survey, the authors will provide suggestions on how to adapt high technology concepts to civil engineering coursework, while abiding by ABET/ASCE accreditation requirements. The findings of this paper will indicate where postsecondary universities offering civil engineering can easily adapt their curricula to address the current low enrollment crisis, which in turn, supports future civil engineers for the world of high technology. © American Society for Engineering Education, 2023."
"The United Nations Sustainable Development Goals (SDGs) have become a foundational metric for advancing engineering education in non-traditional ways, similar to the NSF's Big 10 Ideas and the Grand Challenges. Recently, there has also been a national push to use machine learning (ML) and artificial intelligence (AI) to advance engineering techniques in all disciplines ranging from advanced fracture mechanics in materials science to soil and water quality testing in the civil and environmental engineering fields. Using AI, specifically machine learning, engineers can automate and decrease the processing or human labeling time while maintaining statistical repeatability via trained models and sensors. Edge Impulse has designed an open-source TinyML-enabled Arduino education tool kit for engineering disciplines. This paper discusses the various applications and approaches engineering educators have taken to utilize ML toolkits in the classroom. We provide in-depth implementation guides and associated learning outcomes focused on the Environmental Engineering Classroom. We discuss five specific examples of four standard Environmental Engineering courses for freshman and junior-level engineering. There are currently few programs in the nation that utilize machine learning toolkits to prepare the next generation of ML & AI-educated engineers for industry and academic careers. This paper will guide educators to design and implement ML/AI into engineering curricula (without a specific AI or ML focus within the course) using simple, cheap, and open-source tools and technological aid from an online platform in collaboration with Edge Impulse. Specific examples include 1) facial recognition technologies and the biases involved, 2) air quality detection using an accelerometer, 3) roadside litter detector, 4) automated bird identifier, and 5) wildlife camera trap detection. © American Society for Engineering Education, 2023."
"Conversational agents are rapidly becoming commonplace. However, since these systems are typically blackboxed, users'including vulnerable populations, like children'often do not understand them deeply. For example, they might assume agents are overly intelligent, leading to frustration and distrust. Users may also overtrust agents, and thus over-share personal information or rely heavily on agents' advice. Despite this, little research investigates users' perceptions of conversational agents in-depth, and even less investigates how education might change these perceptions to be more healthy. We present workshops with associated educational conversational AI concepts to encourage healthier understanding of agents. Through studies with the curriculum with children and parents from various countries, we found participants' perceptions of agents'specifically their partner models and trust'changed. When participants discussed changes in trust of agents, we found they most often mentioned learning something. For example, they frequently mentioned learning where agents obtained information, what agents do with this information and how agents are programmed. Based on the results, we developed recommendations for teaching conversational agent concepts, including emphasizing the concepts students found most challenging, like training, turn-taking and terminology; supplementing agent development activities with related learning activities; fostering appropriate levels of trust towards agents; and fostering accurate partner models of agents. Through such pedagogy, students can learn to better understand conversational AI and what it means to have it in the world. Copyright © 2023, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
"This NSF Research Experience for Teachers (RET) “Research Experience for Teachers in Big Data and Data Science” (award number: 1801513) engaged four middle/high school science teachers in summer 2022 with research related to big data and data science, with follow-up school year implementation of related curricula. These teachers developed curricula related to their summer research experience in big data and data science that spanned a range of student ages and topics: middle school science, 9th grade biology, 9th grade health, and 11th grade chemistry. Despite the wide range of student ages, curricular content, and instructional goals, all teachers found rich and varied curriculum applications related to data science and AI that fit within their existing curriculum constraints. In particular, teachers found that the Next Generation Science Standards [1] practice of “computational thinking” was the best lens for developing their aligned big data instruction. After exploring a taxonomy of computational thinking in mathematics and science [2], the teachers collectively eventually settled on a core set of four computational thinking skills [3] most likely to be productive for their teaching focus; algorithmic thinking, decomposition, abstraction, and pattern recognition. This paper reports on the variety of connections teachers developed with the practice of computational thinking, from data clustering as an active practice for simulating early generation of the periodic table in a chemistry class, to sampling/resampling populations in outdoor aquatic environments, to programming in middle school science, to adapting explainable AI for analyzing student-generated data in a health education class. Teacher reports of their own learning about research in data science, and how they were able to adapt that learning for the benefit of their middle/high school students, will capture the flexibility and value that this experience provided. © American Society for Engineering Education, 2023."
"Applications of artificial intelligence (AI), machine learning (ML), and deep learning (DL) started to gain popularity in plastic surgical procedure analysis and predictive work with applications mainly focusing on image detection and analysis using artificial neural networks. Technological advancements that relate to data storage, processor capabilities, and developed new AI techniques have also helped with recent advancements. In this work, the focus is on the research literature covering AI, DL, and ML techniques that relate to only facial plastic surgery. Applications of AI/ML/DL determined include but not limited to face feature recognition, fracture detection, preoperative facial simulation, and gender detection. Advancements in several research areas by integrating AI/ML/DL concepts into facial plastic surgical procedures can be achieved with the associated suggestion we have in the conclusion section. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. All rights reserved."
"Plastics are the most widely discharged waste into the aquatic ecosystems, where they break down into microplastics (MPs) and nanoplastics (NPs). MPs are ingested by several marine organisms, including benthic and pelagic fish species, contributing to organ damage and bioaccumulation. This study aimed to assess the effects of MPs ingestion on gut innate immunity and barrier integrity in gilthead seabreams (Sparus aurataLinnaeus, 1758) fed for 21 days with a diet enriched with polystyrene (PS-MPs; 1–20 μm; 0, 25 or 250 mg /kg b.w./die). Physiological fish growth and health status were not impacted by PS-MPs treatments at the end of experimental period. Inflammation and immune alterations were revealed by molecular analyses in both anterior (AI) and posterior intestine (PI) and were confirmed by histological evaluation. PS-MPs triggered TLR-Myd88 signaling pathway with following impairment of cytokines release. Specifically, PS-MPs increased pro-inflammatory cytokines gene expression (i.e., IL-1β, IL-6 and COX-2) and decreased anti-inflammatory ones (i.e., IL-10). Moreover, PS-MPs also induced an increase in other immune-associated genes, such as Lys, CSF1R and ALP. TLR-Myd88 signaling pathway may also lead to the mitogen-activated protein kinases (MAPK) signaling pathway activation. Here, MAPK (i.e., p38 and ERK) were activated by PS-MPs in PI, following the disruption of intestinal epithelial integrity, as evidenced by reduced gene expression of tight junctions (i.e. ZO-1, Cldn15, Occludin, and Tricellulin), integrins (i.e., Itgb6) and mucins (i.e., Muc2-like and Muc13-like). Thus, all the obtained results suggest that the subchronic oral exposure to PS-MPs induces inflammatory and immune alterations as well as an impact on intestinal functional integrity in gilthead seabream, with a more evident effect in PI. © 2023"
[No abstract available]
"Most current XAI models are primarily designed to verify input-output relationships of AI models, without considering context. This objective may not always align with the goals of Human-AI collaboration, which aim to enhance team performance and establish appropriate levels of trust. Developing XAI models that can promote justified trust is therefore still a challenge in the AI field, but it is a crucial step towards responsible AI. The focus of this research is to develop an XAI model optimized for human-AI collaboration, with a specific goal of generating explanations that improve understanding of the AI system's limitations and increase warranted trust in it. To achieve this goal, a user experiment was conducted to analyze the effects of including explanations in the decision-making process on AI trust. © 2023 The Authors."
"In recent decades, photogrammetry has re-emerged as a viable solution for heritage documentation. Developments in various computer vision methods have helped photogrammetry to compete against the laser scanning technology, eventually becoming complementary solutions for the purpose of heritage recording. In the last few years, artificial intelligence (AI) has progressively entered various domains including 3D reconstruction. The Neural Radiance Fields (NeRF) method renders a 3D scene from a series of overlapping images, similar to photogrammetry. However, instead of relying on geometrical relations between the image and world spaces, it uses neural networks to recreate the so-called radiance fields. The result is a significantly faster method of recreating 3D scenes. While not designed to generate 3D models, simple computer graphics methods can be used to convert these recreated radiance fields into the familiar point cloud. In this paper, we implemented the Nerfacto architecture to recreate two instances of heritage objects and then compared them to traditional photogrammetric multi-view stereo (MVS). While the initial hypothesis posits that NeRF is not yet capable to reach the level of accuracy and density achieved by MVS as can be observed in the results, NeRF nevertheless shows a great potential due to its fractionally faster processing speed. © 2023 A. Murtiyoso."
"Confidence signals are often used in human interactions to communicate the likelihood of a decision being correct. Similarly, confidence may also be used to indicate the reliability of advice given by an AI. While previous work on explainable AI (XAI) has explored the effect of AI confidence on AI-advice adoption and joint accuracy of the human-AI team, most studies use AI-assistants that exceed human performance. It is unclear how displaying the confidence interacts with the accuracy of the AI. We conduct a comprehensive investigation of the effect of displaying AI confidence on two factors: 1) the accuracy of AI-assisted decision making, and 2) reliance on the AI's assistance. We conduct two behavioral experiments, one where participants were shown AI confidence, and another where no confidence ratings were shown. Our work goes beyond the typical focus on high accuracy AI assistants. In both experiments, participants were assisted by one of three AI classifiers of varying accuracy. Our results demonstrate that displaying AI confidence increases joint accuracy when people are assisted by a classifier that is better than humans on average. Conversely, when assisted by a classifier with performance worse than an average human, joint accuracy was better when no AI confidence was displayed. However, for the adoption of AI advice we observed the opposite pattern: people rely more on a higher accuracy classifier that does not display confidence compared to one that does, and people rely more on a lower accuracy classifier that does display AI confidence compared to one that does not. © 2023 The Authors."
[No abstract available]
"One of the worst environmental catastrophes that endanger the Australian community is wildfire. To lessen potential fire threats, it is helpful to recognize fire occurrence patterns and identify fire susceptibility in wildfire-prone regions. The use of machine learning (ML) algorithms is acknowledged as one of the most well-known methods for addressing non-linear issues like wildfire hazards. It has always been difficult to analyze these multivariate environmental disasters because modeling can be influenced by a variety of sources of uncertainty, including the quantity and quality of training procedures and input variables. Moreover, although ML techniques show promise in this field, they are unstable for a number of reasons, including the usage of irrelevant descriptor characteristics when developing the models. Explainable AI (XAI) can assist us in acquiring insights into these constraints and, consequently, modifying the modeling approach and training data necessary. In this research, we describe how a Shapley additive explanations (SHAP) model can be utilized to interpret the results of a deep learning (DL) model that is developed for wildfire susceptibility prediction. Different contributing factors such as topographical, landcover/vegetation, and meteorological factors are fed into the model and various SHAP plots are used to identify which parameters are impacting the prediction model, their relative importance, and the reasoning behind specific decisions. The findings drawn from SHAP plots show the significant contributions made by factors such as humidity, wind speed, rainfall, elevation, slope, and normalized difference moisture index (NDMI) to the suggested model's output for wildfire susceptibility mapping. We infer that developing an explainable model would aid in comprehending the model's decision to map wildfire susceptibility, pinpoint high-contributing components in the prediction model, and consequently control fire hazards effectively. © 2023 The Authors"
"Artificial Intelligence (AI) has emerged as the next imperative topic in pre-college education. Given the rapid integration of AI in K-12 education, examining the resources and curricula currently available for teaching AI is vital. Therefore, this exploratory study conducted a literature review to survey AI resources developed for K-12 education and how the resources enabled students to explore AI ideas and practices. The preliminary findings revealed that many developed resources and curricula focused on secondary education, specifically middle school. However, recently there has been an increase in curriculum development for primary education. © American Society for Engineering Education, 2023."
"This paper presents a case study on the first in-line application of AI-based image analysis for real-time pharmaceutical particle size measurement in a continuous milling process. An AI-based imaging system, which utilises a rigid endoscope, was tested for the real-time particle size measurement of solid NaCl powder used as a model API in the range of 200–1000 µm. After creating a dataset containing annotated images of NaCl particles, it was used to train an AI model for detecting particles and measuring their size. The developed system could analyse overlapping particles without dispersing air, thus broadening its applicability. The performance of the system was evaluated by measuring pre-sifted NaCl samples with the imaging tool, after which it was installed into a continuous mill for in-line particle size measurement of a milling process. By analysing ∼100 particles/s, the system was able to accurately measure the particle size of sifted NaCl samples and detect particle size reduction when applied in the milling process. The Dv50 values and PSDs measured real-time with the AI-based system correlated well with the reference laser diffraction measurements (<6% mean absolute difference over the measured samples). The AI-based imaging system shows great potential for in-line particle size analysis, which, in line with the latest pharmaceutical QC trends, can provide valuable information for process development and control. © 2023 The Author(s)"
"In the context of group recommender systems, explanations strategies have been proposed to improve recommendations perceived fairness, consensus, satisfaction, and to help the group members in the decision-making process. In general, such explanations try to clarify the underlying social chioce-based aggregation strategies used to generate the recommendations. However, results in the literature are conflicting, and the real benefit of such explanations seem to be limited. In this work, we propose a novel approach, which makes use of an argumentative framework built using information about the aspects that are connected to the recommended items. Such framework is used to generate recommendations, and related explanations. We provide a proof of concept on how to generate explanations for the group, as well as specific explanations for the group members, which use the information in the argumentative frameworks to enrich the explanations. Furthermore, we propose privacy-preserving versions for the explanations, as well as a graphical approach based on tag clouds. In future works, we plan to evaluate the quality of the provided recommendations in offline settings, as well as the impact of the proposed explanations in a series of user studies.  © 2023 ACM."
"This new volume focuses on the application of artificial intelligence, blockchain technology, and the Internet of Things to meet the new challenges faced by higher education due to the Covid-19 pandemic, which necessitated restrictions to gathered groups of students and others and increased the need for alternative means of teaching, learning, and communication. With a need to find alternatives to the traditional face-to-face teaching and learning during the pandemic, the authors of Advancements in Artificial Intelligence, Blockchain Technology and IoT in Higher Education: Mitigating the Impact of Covid-19 discuss the best use of technologies in many areas of study in higher education. The book addresses the growing role of AI in the digital transformation in higher education systems, looking at learning and teaching models. The authors look at using digital technologies to create smart universities and the use of blockchain and IoT technologies in higher education. With chapters from eminent professors, researchers, and others involved in higher education from a selection of different countries, the peer-reviewed chapters in this volume highlight how educators, administrators, and students in higher education have embraced these new technologies and how they can be continued and enhanced even after the threat of Covid has passed and in preparation for new deadly pandemics. © 2023 Apple Academic Press, Inc. All rights reserved."
"Cosmetic and Reconstructive Facial Plastic Surgery: Medical and Biomedical Engineering and Science Concepts provides an extensive overview of the most recent technological advancements in facial plastic and reconstructive surgeries and head and neck surgery through a thorough review of the literature in biomedical engineering, technology, and medicine. Coverage includes the most recent engineering and computing techniques, such as robotics, biomechanics, artificial intelligence (AI), deep learning (DL), machine learning (ML), and optimization, as well as the medical and surgical aspects of medical and scientific methods, surgical and non-surgical procedure types, complications, patient care, and psychological factors. This book will be a valuable introduction to concepts and advances for otorhinolaryngology, biomedical researchers, academics, and students. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. All rights reserved."
"Test of autonomous systems is mostly brute force and ad-hoc thus being neither efficient nor transparent. Though requirements invite for a situational transparency, a framework is missing to judge quality of requirements and derived test-cases. Practical challenges are state explosion, difficulty to derive corner cases, no systematic safety of the intended functionality as specified, lack of accepted KPI, etc. Maintaining a valid safety case is hardly possible with such adaptive systems and continuous software updates. To achieve trusted autonomous vehicles, test cases must be generated automatically while at same time providing coverage (e.g., indicating progress with KPI), efficiency (e.g., limiting the amount of regression testing) and transparency (e.g., showing how specific corner cases are tested in case of accidents). This paper provides a method for automatically generating test cases for AI-based autonomous systems and compares it with existing testing methods. A case study is provided to show how multiple testing methods are combined to facilitate AI-based testing.  © 2023 SAE International. All rights reserved."
"The gold standard to detect SARS-CoV-2 infection considers testing methods based on Polymerase Chain Reaction (PCR). Still, the time necessary to confirm patient infection can be lengthy, and the process is expensive. In parallel, X-Ray and CT scans play an important role in the diagnosis and treatment processes. Hence, a trusted automated technique for identifying and quantifying the infected lung regions would be advantageous. Chest X-rays are two-dimensional images of the patient's chest and provide lung morphological information and other characteristics, like ground-glass opacities (GGO), horizontal linear opacities, or consolidations, which are typical characteristics of pneumonia caused by COVID-19. This chapter presents an AI-based system using multiple Transfer Learning models for COVID-19 classification using Chest X-Rays. In our experimental design, all the classifiers demonstrated satisfactory accuracy, precision, recall, and specificity performance. On the one hand, the Mobilenet architecture outperformed the other CNNs, achieving excellent results for the evaluated metrics. On the other hand, Squeezenet presented a regular result in terms of recall. In medical diagnosis, false negatives can be particularly harmful because a false negative can lead to patients being incorrectly diagnosed as healthy. These results suggest that our Deep Learning classifiers can accurately classify X-ray exams as normal or indicative of COVID-19 with high confidence. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023. All rights reserved."
"Machine Learning with Deep Neural Networks (DNNs) has become a successful tool in solving tasks across various fields of application. However, the complexity of DNNs makes it difficult to understand how they solve their learned task. To improve the explainability of DNNs, we adapt methods from neuroscience that analyze complex and opaque systems. Here, we draw inspiration from how neuroscience uses topographic maps to visualize brain activity. To also visualize activations of neurons in DNNs as topographic maps, we research techniques to layout the neurons in a two-dimensional space such that neurons of similar activity are in the vicinity of each other. In this work, we introduce and compare methods to obtain a topographic layout of neurons in a DNN layer. Moreover, we demonstrate how to use topographic activation maps to identify errors or encoded biases and to visualize training processes. Our novel visualization technique improves the transparency of DNN-based decision-making systems and is interpretable without expert knowledge in Machine Learning. © 2023 The Authors."
"Preoperative planning with computed tomography (CT)-based 3-dimensiona (3D) templating has been achieved precise placement of hip components. This study investigated the role of the software (3-dimensional preoperative planning for primary total hip arthroplasty [THA] based on artificial intelligence technology, artificial intelligence hip [AIHIP]) for surgeons with different experience levels in primary THA. In this retrospective cohort study, we included patients, who had undergone THA with the help of the AIHIP, and matched to patients, who had undergone THA without the help of the AIHIP, by age and the doctor who operated on them. The subjects were divided into 4 groups, senior surgeon (Chief of Surgery) with AIHIP group, senior surgeon without AIHIP group, junior surgeon (Associate Chief of Surgery) with AIHIP group and junior surgeon without AIHIP group. The general data, imaging index, clinical outcomes and accuracy of stem size prediction and cup size prediction were retrospectively documented for all patients. There was a significant difference in discrepancy in leg length (P =.010), neck-shaft angle (P =.025) and femoral offset (P =.031) between the healthy side and the affected side, operation duration (P <.001), decrease in hemoglobin (Hb) per 24 hours (P =.046), intraoperative radiation exposure frequency (P <.050) and postoperative complications (overall P =.035) among the patients in junior surgeon group. No significant differences were found between senior surgeon groups with respect to discrepancy in leg length (P =.793), neck-shaft angle (P =.088)and femoral offset (P =.946) between the healthy side and the affected side, operation duration (P =.085), decrease in Hb per 24 hours (P =.952), intraoperative radiation exposure frequency (P =.094) and postoperative complications (overall P =.378). The stem sizes of 95% were accurately estimated to be within 1 stem size, and 97% of the cup size estimates were accurate to within 1 cup size in senior surgeon group with AIHIP. A total of 87% stem sizes were accurately estimated to be within 1 stem size, and 85% cup sizes were accurate to within 1 cup size in junior surgeon group with AIHIP. In conclusion, our study suggests that an AI-based preoperative 3D planning system for THA is a valuable adjunctive tool for junior doctor and should routinely be performed preoperatively. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"The existing radio access network (RAN) is facing many challenges to meet the very strict speed and latency requirements by different mobile applications in addition to the increasing pressure to reduce operating cost. Innovation and development in RAN have been accelerated to tackle these challenges and to define how next generation mobile networks should look like. The role of machine learning (ML) and artificial intelligence (AI) driven innovations within the RAN domain is strengthening and attracting lots of attention to tackle many of the challenging problems. In this paper we surveyed RAN network base stations (BSs) clustering and its applications in the literature. The paper also demonstrates how to leverage community detection algorithms to understand underlying community structures within RAN. Tracking areas (TAs) novel framework was developed by adapting existing community detection algorithm to solve the problem of statically partitioning a set of BSs into TA according to mobility patterns. Finally, live network dataset in dense urban part of Cairo is used to assess how the developed framework is used to partition this part of the network more efficiently compared to other clustering techniques. Results obtained showed that the new methodology saved up to 34.6% of inter TA signaling overhead and surpassing other conventional clustering algorithms. © 2023, Posts and Telecom Press Co Ltd. All rights reserved."
"This research is intended to provide an initial solution to the problem of finding images for processing by photogrammetry in special cases where these do not exist. An overview of existing artificial intelligence-based algorithms that enable the extension of source image dataset is reported. In particular, this research focused on the use of prompt-To-image systems for obtaining images to be used in reconstruction and then in the next step of 3D modelling. Thus, the combined use of these three techniques, AI, photogrammetry, and modelling allowed the creation of a model of a building that never existed except in the collective imagination, which is the tower of Babel. In particular, the case study chosen is the illustration in Kircher book present in the library of the Brixen seminary that is closed to the public and for which it was necessary to create a tool to enhance the value and knowledge of this heritage for external users. Therefore, the creation of an augmented reality app enabled the visualization of the model created by offering possibilities for immersive experiences and dissemination of the research to a wide audience. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"In 1955, John McCarthy, then an assistant professor at Dartmouth College, coined the word ""artificial intelligence"" (AI) as a workshop title for the Rockefeller Foundation. The difficulty in describing AI is defining the parameters of artificiality, or how machines differ from human intelligence (HI). They have a fraction of HI and can just approximate. They're also much more capable than humans at calculating huge numbers and doing it quickly. Simulated intelligence is a speedy growing region of innovation that could, in all likelihood, definitely change how we collaborate with each other. Artificial intelligence has started to present resourceful educating and learning arrangements inside the area of education, and these arrangements are at present going via checking out in a scope of settings. No boundaries exist for training or AI. Through AI-assisted training, students study important IT competencies. In addition, with developments, a much broader range of guides might be available online, and thanks to AI, newbies will simply need to examine from anywhere. This segment discusses the modern-day nation of AI in higher technical educational training as well as its significance. This chapter discusses the current state of AI in higher education as well as its significance. © 2023 Apple Academic Press, Inc. All rights reserved."
"There is a lack of an intelligent platform that supports continuous deliberation and captures diverse views and stakeholders' values during the architectural design process in the early stages. Using hybrid intelligence, this study proposes a method that integrates value, and design pattern theories, to support deliberation during the design process. Three steps comprise the method: eliciting value, extracting design patterns, and designing through deliberation with AI agents using natural language processing through hybrid intelligence. The final set of design patterns reflects the participants' values and ideas, facilitating informed consensus and collaboration between stakeholders supported by AI agents. By integrating diverse perspectives into the loop through continuous deliberation, the proposed method incorporates stakeholders' value for extracting design patterns that address primary design goals and challenges such as energy transition in the built environment. © 2023 The Authors."
"This work describes and analyzes a set of state-of-the-art artificial intelligence (AI) hardware kits created for education and research that can be used in undergraduate AI labs. AI cloud-based computing devices and solutions like the Arduino-based Tiny Machine Learning kits or the mobile app by Edge Impulse, Raspberry Pi-based AIY Voice kits by Google, Quad-core Arm Cortex-A53 and Cortex-M4F-based Google Coral Dev Boards, as well as the more powerful Jetson AGX Xavier (512-core NVIDIA Ampere architecture GPU), and Jetson AGX Orin (2048-core NVIDIA Ampere architecture GPU) Developer kits, are compared using published characteristics and direct experiments. The comparison criteria used are (1) ease of setup and first use, (2) learning curve and required prior knowledge, (3) learning community support availability, (4) suitability for undergraduate learning, (5) computational speed, and (6) cost including both the hardware cost and the subscription services cost. Based on the results of the analysis the tested AI computing devices are ranked for use in various levels of undergraduate curricula. The goal is to provide the faculty interested in developing their own AI labs with some guidance in choosing appropriate AI hardware from an experimental perspective. © American Society for Engineering Education, 2023."
"Recent advances in AI allow complex, natural user-system dialogue flow in NLP-based conversational recommender systems (CRS). While this enables users to express complex intents to the system, its usual linear GUI representation as a chat log fails to account for two non-linear aspects of natural conversation: humans can switch between topics as customary; and, especially in decision-making contexts, topics discussed are structurally related. As early work, we motivate and present a GUI design approach that aims to exploit these phenomena for CRS by conveying topic progression, and discuss several design variants, their trade-offs, and open questions. Our approach aims to help users orientate while exploring and comparing multiple preference model variants and corresponding recommendations in complex, natural ways, also accounting for different explanation types. Such orientation could benefit users for achieving complex goals using CRS, like thoroughly-informed decision making, getting inspiration for novel consumable items, and exploring their own preferences.  © 2023 ACM."
"The power of ML and AI has not been fully realized in the manufacturing sector. One of the major challenges is that the small and medium manufacturers, which account for 98% of the industry, lack the dedicated data analytic workforce. In Indiana, to address this need, partnerships have been established between industry and academia through Wabash Heartland Innovation Network (WHIN) at Purdue University. In collaboration with Ivy Tech Community College, a series of workshops were developed to introduce data analytics, the internet of things, and basic machine learning concepts to local small and large manufacturing companies. This study will describe the first of three short courses geared toward industry workers and professionals. The first short course was on the topic of energy savings and data analytics for Variable Frequency Drives (VFDs). The attendees consisted of 44 participants from 17 manufacturing companies. A final evaluation of the course reports on participants' levels of satisfaction with the course, the major learnings and takeaways, and their institutional support. The evaluation results indicate that the course was a good introduction to VFDs and the large number of applications where VFD data can be used for energy savings, diagnostics, and operations. However, workshop participants wanted more hands-on opportunities to see how the VFD data can be extracted for new motors and many legacy equipment still in use and how various settings can be adjusted. © American Society for Engineering Education, 2023."
"Pedagogy provides a solid foundation for educators to design effective teaching and learning experiences. However, very few resources address computational thinking (CT) pedagogical experiences for that prepare early learners to become problem solvers in the computer science and engineering domains, skills that are necessary to meet future industry requirements. To address this gap, this paper proposes a framework and models to help educators identify available CT experiences to incorporate them into their lessons. The framework includes nine pedagogical experiences: (1) Unplugged, (2) Tinkering, (3) Making, (4) Remixing, (5) Robotics+, (6) Engineering, (7) Coding, (8) Dataying, and (9) Artificial Intelligence (AI). © American Society for Engineering Education, 2023."
"In this paper, we present a framework for quantifying the impact of interventions on the full trajectories of students' experiences. The interventions are given periodically based on student performance forecasting from an artificial intelligence (AI) model. We performed a small-scale randomized controlled trial for evaluating the impact of the AI-based intervention system on the undergraduate students of a science, technology, engineering, and mathematics (STEM) course. Intervention messaging content was based on machine learning forecasting models trained on data collected from the students in the same course over the preceding 3 years. Trial results show that the intervention produced a statistically significant increase in the proportion of students that achieved a passing grade. By applying the trajectory-analysis framework we find that the intervention impacts the stories of some types of students more than others, and use this to define new ways of identifying students who are most likely to benefit. Together these outcomes point to the potential and promise of just-in-time interventions for STEM learning and the need for larger fully-powered randomized controlled trials. © American Society for Engineering Education, 2023."
"This contribution presents a simple workflow for surveying historical buildings and sites using crowd-sourced images. The proposed approach involves collecting large datasets of images from the internet using free plugins, followed by automatic image analysis and filtering using AI-based tools. 3D reconstructions are then created with Structure from Motion (SfM) and neural radiance fields (NeRF). To assess the reliability of crowd-sourced surveys, the 3D reconstructions are compared to high-precision laser scans of large medieval churches. In addition, the paper demonstrates the potential of this workflow in the field of building archaeology through detailed geometrical analyses of several iconic domes such as Hagia Sofia. By enabling remote and 4D surveys, crowd-sourced reconstruction methods open up novel opportunities for rapid, affordable and borderless research on cultural heritage. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"This paper introduces a novel methodology developed for creating 3D models of archaeological artifacts that reduces the time and effort required by operators. The approach uses a simple vision system mounted on a robotic arm that follows a predetermined path around the object to be reconstructed. The robotic system captures different viewing angles of the object and assigns 3D coordinates corresponding to the robot's pose, allowing it to adjust the trajectory to accommodate objects of various shapes and sizes. The angular displacement between consecutive acquisitions can also be fine-Tuned based on the desired final resolution. This flexible approach is suitable for different object sizes, textures, and levels of detail, making it ideal for both large volumes with low detail and small volumes with high detail. The recorded images and assigned coordinates are fed into a constrained implementation of the structure-from-motion (SfM) algorithm, which uses the scale-invariant features transform (SIFT) method to detect key points in each image. By utilising a priori knowledge of the coordinates and SIFT algorithm, low processing time can be ensured while maintaining high accuracy in the final reconstruction.The use of a robotic system to acquire images at a pre-defined pace ensures high repeatability and consistency across different 3D reconstructions, eliminating operator errors in the workflow. This approach not only allows for comparisons between similar objects but also provides the ability to track structural changes of the same object over time.Overall, the proposed methodology provides a significant improvement over current photogrammetry techniques by reducing the time and effort required to create 3D models while maintaining a high level of accuracy and repeatability. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"In AI-assisted decision-making, a central promise of putting a human in the loop is that they should be able to complement the AI system by adhering to its correct and overriding its mistaken recommendations. In practice, however, we often see that humans tend to over- or under-rely on AI recommendations, meaning that they either adhere to wrong or override correct recommendations. Such reliance behavior is detrimental to decision-making accuracy. In this work, we articulate and analyze the interdependence between reliance behavior and accuracy in AI-assisted decision-making, which has been largely neglected in prior work. We also propose a visual framework to make this interdependence more tangible. This framework helps us interpret and compare empirical findings, as well as obtain a nuanced understanding of the effects of interventions (e.g., explanations) in AI-assisted decision-making. Finally, we infer several interesting properties from the framework: (i) when humans under-rely on AI recommendations, there may be no possibility for them to complement the AI in terms of decision-making accuracy; (ii) when humans cannot discern correct and wrong AI recommendations, no such improvement can be expected either; (iii) interventions may lead to an increase in decision-making accuracy that is solely driven by an increase in humans' adherence to AI recommendations, without any ability to discern correct and wrong. Our work emphasizes the importance of measuring and reporting both effects on accuracy and reliance behavior when empirically assessing interventions. © 2023 The Authors."
"The presence of Artificial Intelligence (AI) is growing in all areas of life. However, current data-driven AI is still too narrow to help humans, lacking in social and emotional intelligence and restricted in reality (Fan, Xu, Cao et al. 2022). By placing the emphasis on mutual understanding and learning from each other, our research programme 'Hybrid Intelligence - Human-AI co-evolution and learning in multirealities (HI)' aims at combining the strengths of both humans and machines in their co-evolutionary processes. We propose to build the idea of a metaversum (Rosas, 2021) by combining our physical and virtual realities towards a multi-reality. © 2023 The Authors."
"Deep Learning has been pivotal in many real-world applications (e.g., autonomous driving, medicine and retail). With the wide availability of consumer-grade depth sensors, acquiring 3D data has become more affordable and effective, and many 3D datasets are currently publicly available. 3D data provides a great opportunity for a better comprehension of the surrounding environment for machines. There is a growing need for innovative methods for the treatment and analysis of point clouds and for their classification. The complex hidden layers, which are at the basis of deep neural networks (DNNs), make it difficult to interpret these models, that up to a few years ago DNNs were considered and treated as black box operators. Still, with their increasing popularity, making them explainable and interpretable has become mandatory. A lot of efforts were devoted to developing an Explainable Artificial Intelligence (XAI) framework for explaining DNNs decisions with 2D data, while only a few studies have attempted to investigate the explainability of 3D DNNs and, even more, heritage scenarios. To overcome these limitations, it was proposed the BubblEX framework: a novel multimodal fusion framework to learn the 3D point features. In our work, BubblEX has been exploited to understand the decisions taken by DNNs for heritage point clouds. The approach has been applied to a Digital Cultural Heritage Dataset, which is publicly available: the ArCH (Architectural Cultural Heritage) Dataset. © 2023 Copernicus GmbH. All rights reserved."
"Understanding the interactions between engineered nanoparticles and human cells is vital for the safe utilization of nanoparticles in biomedical applications. With the increased use of single-cell RNA sequencing (scRNA-seq) in the field of nanoparticles, there is a growing appreciation that those interactions are complex, where nanoparticles elicit heterogeneous cellular responses and disturb diverse critical cellular pathways. Consequently, the conventional functional enrichment analysis is insufficient in describing the complexity and heterogeneity of cell-nanoparticle interactions, although scRNA-seq offers a potential in addressing this problem due to a large amount of transcriptomic data at the single-cell resolution generated by scRNA-seq. By introducing artificial intelligence (AI) and machine learning (ML) into scRNA-seq data analysis, astonishing results have demonstrated the power of these computational methods in uncovering hidden mechanistic information from scRNA-seq data, such as the use of Markov random field in the accurate identification of differentially expressed genes by incorporating gene network information, deep learning of gene-gene relationships through a convolutional neural network, and reconstruction of cell differentiation trajectories using single-cell entropy without the need for time information. Therefore, applying these methods to scRNA-seq analysis of nanoparticles should be helpful in deriving any causal relationships between the affected cellular pathways and identifying key genes and processes that mediate the toxicity of nanoparticles. Here, we discuss the application of scRNA-seq in the analysis of the toxicological impacts of engineered nanoparticles, while discussing examples that represent successful applications of AI and ML in scRNA-seq data analysis. The goal is to inspire the complementary use of the computational methods in taxogenomics analysis of nanoparticles. © 2023 John Wiley & Sons Ltd."
"This study investigates the current potential for artificial intelligence (AI) to support personalized learning (PL). Personalized learning can provide a customized learning environment to support student learning processes based on individual needs, competencies, and interests. One way to conduct personalized learning is by using a recommender system that employs deep learning, an AI technique. To date, a limited number of researchers have discussed the application of deep learning methods to develop advanced recommenders in personalized learning environments. This study examines the literature that describes deep learning as a recommender system to support personalized learning environments. This initial phase of the project seeks to synthesize the issues and opportunities associated with personalized learning experiences and the potential of using deep learning to support the process. Because the topic intersects the education and information technology (IT) fields, we selected three databases for this literature review project: Scopus, ERIC, and Engineering Village. We used the phrase “deep learning recommender system for personalized learning environments” as our search string. We focused only on papers that experts had evaluated in the field to ensure accuracy. Therefore, terms such as “peer review,” “literature review,” and “systematic review” were added to the original search string. The initial search results included 409 documents. After applying inclusion/exclusion criteria, 20 papers emerged as the focus of this study. Thematic analysis was used to look for various themes to identify how deep learning methods are used in education and their potential to inform personalized learning environments. The analysis process utilized Mendeley and NVivo to quickly capture themes by focusing on six features to peruse within the articles. The features involved research questions, goals of studies, research methodology, research design, primary outcomes, and limitations. We then generated three themes from the six features in the analysis phase of the 20 papers. The first theme categorized the type of study into primary and secondary studies. These categories identify the types of studies that employed deep learning methods in the development of a recommender system and their integration with personalized learning. The second theme, recommender system (RS) techniques, highlighted the AI methodologies most frequently utilized in previous research. And the third theme was a list of e-learning platforms that applied RS for personalized learning. The main findings revealed that the deep learning method was effective in big data analysis due to its ability to forecast students' achievements, behaviors, and future paths. Therefore, we considered that deep learning could be widely applied as a technique to develop recommender systems to support personalized learning environments. Furthermore, because we found that only a few studies have investigated the implementation of this AI technology, researchers will have a great opportunity to explore deep learning to develop more innovative solutions in educational fields. © American Society for Engineering Education, 2023."
"The COVID-19 pandemic forced universities worldwide to make the switch to online instruction, raising concerns about the quality of online courses and their impact on student satisfaction and engagement. This study aimed to explore Korean university students’ satisfaction levels with online English-mediated instruction (EMI) courses during the pandemic and identify factors that influence class satisfaction. The purpose of this study was to provide insights into how EMI instructors could improve their online teaching practices during and after a pandemic. The hypothesis was that instructional strategies (IS), academic conscientiousness (AC), and academic integration (AI) could mediate the link between engagement and satisfaction. The study used a survey design to collect data from 219 Korean university students who took online EMI courses during the spring 2020 semester. The survey collected demographic information as well as students’ perceptions of valuable IS, AC, AI, and satisfaction. Data analysis included independent samples t-test, correlation analysis, Structural Equation Modelling (SEM), and multiple regression analysis. The results showed IS use and satisfaction differed among disciplines. Specifically, there were significant differences in satisfaction levels between Arts, STEM, Business, Social Sciences, and Literature and Languages majors. Additionally, there were significant relationships between demographics, AC, AI, IS use, and satisfaction. SEM was used to provide a general view of factors mediating the link between engagement and satisfaction. The results revealed that AC, AI, and IS use mediated the link between engagement and satisfaction. Multiple regression analysis showed that students were more satisfied with instructors who demonstrated care and warmth using social networking sites to communicate. Overall, this study provides valuable insights into student satisfaction with online EMI courses during the COVID-19 pandemic and for the future of online EMI teaching-learning. The findings suggest that online EMI instructors should consider using social networking sites to communicate with students in order to increase satisfaction levels. Additionally, instructors should be aware that different disciplines may require different instructional strategies to maximize student engagement and satisfaction. © 2023, Academic Conferences and Publishing International Limited. All rights reserved."
[No abstract available]
"It is important to find an area of focus that is related to a career path that aligns with engineering students' abilities, technical background, and long-term goals. Due to the array of available specializations in industry categories, selecting the best fit for their interests is a big challenge for engineering students. For example, the computer science category includes information technology, programming languages, software engineering, networks, etc. Most departments focus on one industry category and under each category there are concentrations. When students start their journey through college, they focus on a specific concentration that they think they will succeed in. Some students, after starting some of the courses, find that their selected area of focus no longer fits with their abilities or their interests. Some of them try to change their concentration, program, or college, while some of them leave college because they think that their ability is not enough to continue studying. Today, Artificial Intelligence (AI) can be used to improve the education process by helping students learn better and faster when paired with high-quality learning materials and instruction. Also, AI systems can help students get back on track faster by alerting teachers to potential problems. This paper proposes a Deep Learning Neural Networks approach that helps students select their best-fit specialization in a specific category. Deep learning is a subset of machine learning, but it can determine whether a prediction is accurate through its own neural network- no human help is required [1]. The proposed system will use a dataset that contains student data that is related to the general education courses required for their program, such as grades, the number of hours spent on each course's materials, the opinion of the student about the content of each course, and the course(s) that the student enjoyed the most. Additional data will be included in the dataset such as the student's preferred specialization and the kinds of subjects the student enjoys studying. The proposed Deep Learning Neural Networks system will help students choose a path of study that best fits their abilities and their goals, and that prepares them for successful careers. © American Society for Engineering Education, 2023."
"Purpose: The aim of this paper is to understand if service innovation (Helkkula et al., 2018), based on artificial intelligence (AI) systems, may guarantee healthcare service ecosystem (H-SES) well-being (Frow et al., 2019; Beirão et al., 2017), taking into account that many doubts relieved in terms of transparency may compromise the patients' perceived quality of health services provided through AI systems. Design/methodology/approach: A literature review on service innovation, detected in terms of value co-creation, and service ecosystem, investigated in terms of well-being, is drawn. To analyze the implications of service innovation on a H-SES well-being, through the technology acceptance degree and predisposition to use by actors, a case study based on TAM-model 3 determinants as categories is carried out. Findings: AI-based service innovation archetypes in healthcare may be considered as antecedents of the service ecosystem well-being conditions as long as they enable actors to co-create value. To make it possible, a patient-driven service innovation is necessary in order to mitigate the risks of its inactivity due to fears in terms of transparency. Originality/value: Service innovation and service ecosystem well-being may be studied in an integrated way, with a multidisciplinary approach, and are linked by value co-creation, because only thanks a patient-driven service innovation is possible to foster service ecosystem well-being in healthcare. © 2022, Antonietta Megaro, Luca Carrubbo, Francesco Polese and Carlo Alessandro Sirianni."
"With general computing technology being easily accessible to any individual, concerns arise when academic testing is implemented. These concerns include the potential effect on academic integrity, veracity and tenability, through the act of cheating. Mobile phones are as common as textbooks in the classroom. Microcomputers the size of a fingernail, with the ability to compute, display, and output information to a user are no longer an assumptive prognostication of an outdated science fiction reader. COVID-19 brought with it a shift to remote, online learning, both in high schools and colleges, where acts of academic dishonesty abounded. There is a dire need to address the issue of cheating in academia, especially those facets of academia conducted remotely. Students who cheat may be unprepared for college-level coursework or lack true disciplinary skills needed to enter the workforce. The result is that colleges and universities may need to increase spending to better monitor testing, as well as enhance remedial services to students who enter college unprepared. Increased cost remedies may be passed on to future students through increased tuition costs. This paper provides a review of the topic of technology and its role in academic cheating, in addition to concise conclusions for the educator. Special attention is gi ven to the current and future possibility of microelectronic technology being used in deceitful academic acts. In addition, based on the results of the literature survey conducted for this work, recommendations for future research in this area are discussed at length. Educators face a seeming dichotomy: persist in traditional anti-cheating educational structures, advancing anti-cheating technology and jurisprudence; or, embrace technological progress and encourage the cooperative use of student technology in learning. Finally, we propose incorporating Agile approaches in education as a potential solution. © American Society for Engineering Education, 2023."
"Preserving historical archival heritage involves not only physical measures to safeguard these valuable texts but also providing for their digital preservation. However, merely digitising manuscripts and codexes is not enough. A further step is needed: The digitalisation of their content, i.e.The verbatim transcription of scanned texts. This process enables the accurate preservation of their textual content, making it easier to search for information and conduct further analyses. With the help of artificial intelligence, particularly Deep Neural Networks (DNNs), automatic handwriting recognition can be performed. In this study, we employed a Convolutional Recurrent Neural Network (CRNN), an established type of DNN, to determine the minimum amount of labelled data required to automatically transcribe five different historical datasets that vary in language and time period. The results show that a Character Error Rate (CER) lower than 10% can be achieved with just a few hundred labelled text lines in almost all cases. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"Since the emerging coronavirus pandemic spreads worldwide, countries continue to find new ways to combat and control the spread of""Covid-19.""As part of the virus-fighting efforts, information on diagnostic procedures, infection and symptoms, and the most recent therapy and vaccine research have been updated. The main objective of this paper is to reduce the enormous load on the healthcare system by providing the best way to diagnose patients and predict the infection of COV-19 effectively. As a result of the scientific development in computers and their applications, this science has treated many medical problems. However, clinical trials and human skills are still required despite the undeniable contributions of artificial intelligence (AI) and data research responsible for fighting the pandemic. They are a global and open-source tool capable of assisting in this health emergency. However, due to the severity of the threat of this virus to global health and its rapid development, these solutions remain insufficient to combat it. This research paper uses data mining based on algorithms of AI and machine learning (ML) to detect and diagnose COVID-19 infection based on clinical diagnostic tests prepared previously in the Iraqi Ministry of Health. The model was provided with a dataset of the COVID-19 virus using the Python programming language. To create the model where the model predicts whether this person is infected or not infected with the virus, and if it is proven that he is in the danger zone (reaching death), can he bypass the virus and be cured. The current study results showed that the model was developed using the Random Forest Classifier algorithm more efficiently to predict infection with the Coronavirus. This represents the best model developed among other models that used various algorithms, such as Gaussian Naive Bayes, k-nearest neighbors, Support vector machine, Logistic Regression, Random Forest, Gradient boosting, Multi-layer Perceptron. © 2023 Author(s)."
"The use of artificial intelligence (AI) has the potential to be highly effective in detecting and monitoring illegal trafficking of cultural heritage (CH) goods through image classification techniques, particularly on online marketplaces where the trade of stolen CH objects has become a major global issue. Traditional investigation methods are no longer adequate, but with the assistance of AI, law enforcement agencies and CH organizations can now boost monitoring capabilities to detect, track, and possibly recover stolen objects more efficiently. AI algorithms can indeed analyze images to identify unique features and characteristics that can be used to determine their authenticity and provenance. Additionally, AI can detect patterns and networks of illicit trafficking, and link stolen objects to their places of origin, facilitating the recovery process. In this context, the SIGNIFICANCE project (Stop Illicit Heritage Trafficking with Artificial Intelligence) has been specifically designed to increase the response capabilities of public authorities and police corps against the illicit trafficking of cultural goods perpetrated through internet channels (i.e., social platforms, web, and dark web). By leveraging the power of Deep Learning (DL), AI can help prevent the loss of invaluable cultural artifacts and ensure that they are returned to their rightful owners and places of origin. This paper presents the results reached by the SIGNIFICANCE AI framework on image datasets collected over the web and social media through crawling algorithms. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"Artificial intelligence (AI) is an integral part of the making of smart universities, and even the smallest part of the campus uses it to modify their work. AI is everywhere used to give a personalized approach; in universities, it contributes to providing personalization wherever possible. This chapter deals with the contribution of AI in transforming universities into smart ones and how the process has galloped during the period of the Corona Pandemic. AI has facilitated the improvement of the campus's overall working, and its utilization has brought significant change in the working of the entire university system. A university comprises infrastructure, education, and research, and AI has helped bring a significant difference. It brings ease, comfort, and efficiency to the teaching and learning part of the system. This chapter is intended to understand the improvement that AI has made in making universities smart. © 2023 Apple Academic Press, Inc. All rights reserved."
"In this Work-In-Progress (WIP) paper, the integration of Indigenous ways of knowing is explored with a focus on pedagogy that is technologically enhanced with artificial intelligence (AI). An overview of AI programs, providing their key methods of decision making is presented. The technological, educational/philosophical challenges of integrating Indigenous ways of knowing considering AI programs are then discussed from the perspective of a non-Indigenous researcher. The contribution of this work is in recognizing the larger issue of not knowing what Scientific vs. Non-scientific is in the engineering education curriculum and where Indigenous ways of knowing fall into the spectrum. Subsequently, AI programs used in the engineering education curriculum may be challenged to characterize and account for Indigenous ways of knowing through an equitable, diverse, and inclusive lens. © American Society for Engineering Education, 2023."
"Educating the next generation of AI researchers requires methods which teach the software tools, theoretical concepts, and domain knowledge specific to the field. To help develop these key skills, we focus particularly on the area of Swarm AI, which, in general, covers the autonomous operation of a large number of agents in a single environment. Applications of this field are numerous, including autonomous navigation, defense, robotics, logistics, and search/rescue. Despite the potential for impact in key problem domains and interdisciplinary nature, Swarm AI platforms are not generally used to develop competency in AI education. While some courses and tracks target AI in general, there are few tools to help engage learners in the specific area of Swarm AI. This is partly due to needing strong skills in the intersection of reinforcement learning, software development, and robotics, as well as a framework computing capability on which to test and evaluate. We propose Battle Optimized Laser Tag (BOLT), a Unity-built simulator that enables learners to develop Swarm AI algorithms in the context of a laser tag game between small mobile robots. The simulator is built to be familiar to a video game environment to maximize engagement, while the included documentation for the environment is designed as a gentle introduction into the topic of controlling swarms. Easily modified configurations enable educators to specify tasks and develop curricula to further challenge learners. Furthermore, the simulator is OS-agnostic, simple to install and uses Python to interface with the agents. This allows learners to implement their solutions on their own computers in a programming language common to other AI courses. Finally, extensions of the platform, leveraging the Python interface, can be deployed to physical robots or be used as a testbed for other AI domains beyond robotics including Human-Machine Teaming and Cybersecurity. © American Society for Engineering Education, 2023."
"Artificial Intelligence (AI) offers organizations unprecedented opportunities. However, one of the risks of using AI is that its outcomes and inner workings are not intelligible. In industries where trust is critical, such as healthcare and finance, explainable AI (XAI) is a necessity. However, the implementation of XAI is not straightforward, as it requires addressing both technical and social aspects. Previous studies on XAI primarily focused on either technical or social aspects and lacked a practical perspective. This study aims to empirically examine the XAI related aspects faced by developers, users, and managers of AI systems during the development process of the AI system. To this end, a multiple case study was conducted in two Dutch financial services companies using four use cases. Our findings reveal a wide range of aspects that must be considered during XAI implementation, which we grouped and integrated into a conceptual model. This model helps practitioners to make informed decisions when developing XAI. We argue that the diversity of aspects to consider necessitates an XAI 'by design' approach, especially in high-risk use cases in industries where the stakes are high such as finance, public services, and healthcare. As such, the conceptual model offers a taxonomy for method engineering of XAI related methods, techniques, and tools. © 2023 The Authors."
"We present an initial experimental testbed for human-AI interaction in the context of a simple bartering scenario. With pilot studies, we demonstrate the opportunity this platform provides for computational modeling of human behavior in the context of a simple market. Demonstration can be viewed at https://youtu.be/o5lnzMlMMt4. © 2023 The Authors."
[No abstract available]
"The continual growth of artificial intelligence (AI) in agriculture has surfaced concerns about AI ethics, responsibility, trust, and transparency among professionals in the industry and communities impacted by the technologies. Machine learning (ML), improved sensors in an Internet-of-Things (IoT) world, and advanced networking capabilities have vastly expanded the information processing capabilities of farmers and co-ops, allowing for action based on real-time information on yields, pest control, and farming cycles, to name a few benefits. However, these systems can also highlight the problematic nature of technology outgrowing regulation. These technologies introduce the same surveillance and data ownership concerns that continue to be raised with technology in other industries. There is also the question of what role human labor has in the future of agriculture. While previous research has outlined several fundamental topics for the ethical implementation of technology in agriculture, navigating conversations about AI in agriculture with members outside the industry is an ongoing discussion. To explore how these conversations can be raised with college students, we use a collaborative approach to augment students' thinking about AI ethics in the context of agriculture and farming. This paper uses a content analysis procedure to explore undergraduate engineering and computing students' recognition of AI ethics principles in agriculture. We describe an exercise using a role-play scenario (RPS) activity and a case study of a fictitious AI-enabled farm to help students make these connections and identify ethical considerations. We collected data from seventy-three (73) students through a written assignment after participating in the RPS activity. Through our analysis, we noted that participants could identify and connect several ethical principles with the contents of the case study. Additionally, all our participants identified transparency, a central theme of the case study, as key to building trust between AI-enabled agriculture and the community. Enabling privacy was another heavily discussed topic across the groups while ensuring that communication was conducted sustainably. Role-plays can effectively engage college students in interdisciplinary conversations, especially for emerging issues such as AI ethics. © American Society for Engineering Education, 2023."
"Conventional or learning-based 3D reconstruction methods from images have clearly shown their potential for 3D heritage documentation. Nevertheless, Neural Radiance Field (NeRF) approaches are recently revolutionising the way a scene can be rendered or reconstructed in 3D from a set of oriented images. Therefore the paper wants to review some of the last NeRF methods applied to various cultural heritage datasets collected with smartphone videos, touristic approaches or reflex cameras. Firstly several NeRF methods are evaluated. It turned out that Instant-NGP and Nerfacto methods achieved the best outcomes, outperforming all other methods significantly. Successively qualitative and quantitative analyses are performed on various datasets, revealing the good performances of NeRF methods, in particular for areas with uniform texture or shining surfaces, as well as for small datasets of lost artefacts. This is for sure opening new frontiers for 3D documentation, visualization and communication purposes of digital heritage. © 2023 G. Mazzacca et al."
"Considerable concern has emerged over the potential use of AI tools by students for completing assignments in their classes. Reactions in academia have been mixed, with some describing such use of AI tools as “cheating” while others compare it to the use of calculators and see it as the impetus for enabling deeper learning by students. To analyze some of these issues, the recently released AI tool ChatGPT was used to respond to actual Discussion Board questions in our online cybersecurity classes. ChatGPT was also asked to write a Python program to develop a backpropagation Neural Network for XOR. The results were excellent, both for answering the Discussion Board Questions and for writing code. Four findings emerged from this effort: 1) ChatGPT does an exceptional job of answering questions and generating code, 2) it is not clear how student submissions generated with AI should be graded, 3) along with the AI tools themselves, tools have been developed that can detect whether AI was used to generate a student submission but with a high rate of false positives, and 4) despite these three findings, students could and should be encouraged to collaborate with AI tools, similar to the way they would collaborate with other students. These results led to four conclusions: 1) ethically, the use of tools such as ChatGPT without acknowledging that they have been used is cheating, 2) it will be impossible to stop students from using tools like ChatGPT, but unacknowledged use can be detected, albeit with a very high percentage of false positives, 3) use of AI tools should be encouraged rather than discouraged, and 4) higher education should focus on new methods and mechanisms for assessing student learning that take advantage of the AI tools. © American Society for Engineering Education, 2023."
"The knowledge explosion in all areas is both an opportunity and a risk. In order to shape the effects as positively as possible and create sustainable added value, the amount of information must be processed in a targeted manner and important content must be separated from ignorable content. In the automotive industry and especially in the development of new components, it is possible to look back on many past projects and thus knowledge. However, the decisive factor is whether this information is available in a suitably processed form or the knowledge is even held by just a single expert. A new and intelligent method is therefore required to analyze existing data appropriately and at the same time prepare it ideally for further applications, such as use within forecast models based on Artificial Intelligence (AI). To achieve this, several steps need to be taken. Firstly, it is possible to perform a suitable segmentation of the component. The aim is to detect areas in a component where features and form elements are found. Other remaining regions are ignored after the inspection by segmentation and voxelization: There is no sustainable valuable knowledge here. Subsequently, the voxelization of the component takes place, which results in the three-dimensional component or Computer-Aided-Design (CAD) file being mathematically readable and thus a kind of translation takes place. This is done by rasterizing the component based on a previously selected resolution and other upcoming steps. Finally, the segmented and relevant areas are analyzed accordingly. This can be done according to corresponding previously defined guidelines or, for example, by using a suitably trained AI. The advantage is based in the mathematical readability, which is now given by the voxelization that has taken place.  © 2023 SAE International. All rights reserved."
"Purpose: The recent advancements in smartphone technology and social media platforms have increased the popularity of artificial intelligence (AI) color cosmetics. Meanwhile, China is a lucrative market for various foreign beauty products and technological innovations. This research aims to investigate the adoption of AI color cosmetics applications and their electronic word-of-mouth (e-WOM) intention among Chinese social media influencers. Several key concepts have been proposed in this research, namely body esteem, price sensitivity, social media addiction and actual purchase. Design/methodology/approach: An online questionnaire design was used in this research. A combination of purposive sampling and snowball sampling of AI color cosmetics users who are also social media influencers in China yields 221 respondents. To analyze the data, this research employs Structural Equation Modelling (SEM) method via SPSS and AMOS software. A 2-step approach, Exploratory Factor Analysis (EFA) and Confirmatory Factor Analysis (CFA), is implemented to prove the hypotheses and generate the results. Findings: 1) Social media addiction is a positive predictor of AI color cosmetics usage, (2) AI color cosmetics usage is a positive predictor of actual purchase, (3) actual purchase is a positive predictor of e-WOM intention and lastly, (4) there is a full mediation effect of actual purchase. Originality/value: This research draws on the uses and gratification (U&G) theory to investigate how specific user characteristics affect Chinese social media influencers' adoption of AI color cosmetics, as well as how this may affect their decision to purchase branded color cosmetics and their e-WOM. © 2022, Emerald Publishing Limited."
[No abstract available]
"Combining symbolic and subsymbolic methods has become a promising strategy as research tasks in AI grow increasingly complicated and require higher levels of understanding. Targeted Aspect-based Financial Sentiment Analysis (TABFSA) is an example of such complicated tasks, as it involves processes like information extraction, information specification, and domain adaptation. However, little is known about the design principles of such hybrid models leveraging external lexical knowledge. To fill this gap, we define anterior, parallel, and posterior knowledge integration and propose incorporating multiple lexical knowledge sources strategically into the fine-Tuning process of pre-Trained transformer models for TABFSA. Experiments on the Financial Opinion mining and Question Answering challenge (FiQA) Task 1 and SemEval 2017 Task 5 datasets show that the knowledge-enabled models systematically improve upon their plain deep learning counterparts, and some outperform state-of-The-Art results reported in terms of aspect sentiment analysis error. We discover that parallel knowledge integration is the most effective and domain-specific lexical knowledge is more important according to our ablation analysis. © 2023 Copyright held by the owner/author(s)."
"In the healthcare sector in particular, the shortage of skilled workers is a major problem that will become even more acute in the future as a result of demographic change. One way to counteract this trend is to use intelligent systems to reduce the workload of healthcare professionals. AI-based clinical decision support systems (AICDSS) have already proven their worth in this area, while simultaneously improving medical care. More recently, AICDSS have also been characterized by their ability to leverage the increasing availability of clinical data to assist healthcare professionals and patients in a variety of situations based on structured and unstructured data. However, the need to access large amounts of data while adhering to strict privacy regulations and the dependence on user adoption have highlighted the need to further adapt the implementation of AICDSS to integrate with existing healthcare routines. A subproject of the ViKI pro research project investigates how AICDSS can be successfully integrated into professional care planning practice using a user-centered design thinking approach. This paper presents the design of the ViKI pro AICDSS and the challenges related to privacy, user acceptance, and the data base. It also describes the development of an AI-based cloud technology for data processing and exchange using federated learning, and the development of an explicable AI algorithm for recommending care interventions. The core of the AICDSS is a human-in-the-loop system for data validation, in which the output of the AI model is continuously verified by skilled personnel to ensure continuous improvement in accuracy and transparent interaction between AI and humans. © 2023 The Authors."
"The emergence of generative design (GD) has introduced a new paradigm for co-creation between human experts and AI systems. Empirical findings have shown promising outcomes such as augmented human cognition and highly creative design products. Barriers still remain that prevent individuals from perceiving and adopting AI, entering into collaboration with AI and sustaining it over time. It is even more challenging for creative design industries to adopt and trust AI where these professionals value individual style and expression, and therefore require highly personalized and specialized AI assistance. In this paper, we present a holistic hybrid intelligence (HI) approach for individual experts to train and personalize their GD assistants on the fly. Our contribution to human-AI interaction is three-fold including i) a programmable common language between human and AI to represent the expert's design goals to the generative algorithm, ii) a human-centered continual training loop to seamlessly integrate AI-training into the expert's task workflow, iii) a hybrid intelligence narrative to address the psychological willingness to spend time and effort training such a virtual assistant. This integral approach enables individuals to directly communicate design goals to AI and seeks to create a psychologically safe space for adopting, training and improving AI without the fear of job-replacement. We concertize these constructs through a newly developed Hybrid Intelligence Technology Acceptance Model (HI-TAM). We used mixed methods to empirically evaluate this approach through the lens of HI-TAM with 8 architectural professionals working individually with a GD assistant to co-create floor plan layouts of office buildings. We believe that the proposed approach enables individual professionals, even non-technical ones, to adopt and trust AI-enhanced co-creative tools. © 2023 The Authors."
"The implementation of geometric, alphanumerical and documentary data within BIM models is opening up interesting scenarios in the integrated management of existing infrastructure works. In particular, data from the application of sensors for structural monitoring of historic bridges can originate a flow of information exchange between real artifacts and Digital Twin capable of activating effective reactive or planned responses in the operation and maintenance phase of the asset by the facility manager. This paper intends to outline a BIM-oriented process workflow, which from the creation of parametric objects for infrastructural works using Scan-To-BIM acquisition techniques and procedures, arrives at the implementation of models aimed at the management of data from incoming and outgoing sensors towards analysis, supervision and control systems of the facilities organization. The case study of the Toppoli Bridge over the Arno River along the S.P. 64 road in the Province of Arezzo (IT), an artifact dating back to the early decades of the 1900s and built using traditional construction techniques and recently subject to renovation, is addressed. The experience conducted has highlighted how the BIM methodology for information management of existing assets, effectively responds to the current needs of monitoring and control of historical infrastructure assets, being able to integrate with multiple information management systems of the facilities organization, as well as the latest AI and AR technologies. © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"This paper introduces a novel multi-dimensional framework for developing and deploying AI-human systems, incorporating both technical and managerial design principles. The paper then applies the framework to four standard human-AI interaction patterns, including Human Out Of the Loop (HOOTL), Human On the Loop (HOTL), Human In the Loop (HITL), and Hybrid Intelligence (HI). The dimensions are used to succinctly describe the essential characteristics of each pattern, highlighting potential risks and benefits, such as end-user resistance, employee deskilling, value-misalignment and employee upskilling and business model reengineering. The framework provides a valuable tool for AI developers and managers to characterize their current solutions and optimize the integration of humans and machines in complex systems. © 2023 The Authors."
"The recent breakthrough made in the field of three-dimensional (3D) structure prediction by artificial intelligence softwares, such as initially AlphaFold2 (AF2) and RosettaFold (RF) and more recently large Language Models (LLM), has revolutionized the field of structural biology in particular and also biology as a whole. These models have clearly generated great enthusiasm within the scientific community, and different applications of these 3D predictions are regularly described in scientific articles, demonstrating the impact of these high-quality models. Despite the acknowledged high accuracy of these models in general, it seems important to make users of these models aware of the wealth of information they offer and to encourage them to make the best use of them. Here, we focus on the impact of these models in a specific application by structural biologists using X-ray crystallography. We propose guidelines to prepare models to be used for molecular replacement trials to solve the phase problem. We also encourage colleagues to share as much detail as possible about how they use these models in their research, where the models did not yield correct molecular replacement solutions, and how these predictions fit with their experimental 3D structure. We feel this is important to improve the pipelines using these models and also to get feedback on their overall quality. © 2023 American Chemical Society."
"The growing field of machine learning (ML) and artificial intelligence (AI) presents a unique and unexplored case within persistence research, meaning it is unclear how past findings from engineering will apply to this developing field. We conduct an exploratory study to gain an initial understanding of persistence in this field and identify fruitful directions for future work. One factor that has been shown to predict persistence in engineering is belonging; we study belonging through the lens of confidence, and discuss how attention to social belonging confidence may help to increase diversity in the profession. In this research paper, we conduct a small set of interviews with students in ML/AI courses. Thematic analysis of these interviews revealed initial differences in how students see a career in ML/AI, which diverge based on interest and programming confidence. We identified how exposure and initiation, the interpretation of ML and AI field boundaries, and beliefs of the skills required to succeed may influence students' intentions to persist. We discuss differences in how students describe being motivated by social belonging and the importance of close mentorship. We motivate further persistence research in ML/AI with particular focus on social belonging and close mentorship, the role of intersectional identity, and introductory ML/AI courses. © American Society for Engineering Education, 2023."
"A face recognition application that enables instructors to conveniently know each student's name in the classroom is proposed. Communicating with students during lectures boosts more confidence and builds stronger relationships among students and their instructor, thus, enhances learning. The proposed solution is a web-based application that captures faces from videos and/or pictures through a User Interface that passes the data to a face recognition AI mechanism. The face recognition application also implements user management systems for privacy protection. With the Real-time Face Recognition Application, the course instructors can quickly recognize their students and address them by their names. A survey was conducted among 40 students to assess the comfortability of personal privacy, as well as the improvement of the learning environment in terms of engagement and readiness to engage in asking and answering questions during lectures. Over 85% agreed that instructors calling their names promotes a more friendly and engaging environment leading to improve the leaning. © American Society for Engineering Education, 2023."
"This article describes an ongoing research project that aims to develop a hybrid AI system for aiding radiologists in diagnosing prostate cancer. Through our qualitative research including contextual inquiries and interviews, we analyzed the current workflow and context of use in German radiology centers. Based on that, we explore the potential benefits and challenges of using AI for detecting and diagnosing prostate cancer in medical practice in a hybrid human-AI interaction. © 2023 The Authors."
[No abstract available]
[No abstract available]
[No abstract available]
"This chapter provides a brief overview of the growing role played by big data, data analytics, and artificial intelligence (AI) in the accounting profession. The term 'Big Data' along with other trending topics such as 'Data Analytics' and 'Artificial Intelligence (AI)' have become buzzwords in the accounting profession in recent years. The skills of accounting professionals have evolved as technology has made rapid advances from using pencil and paper to typewriters and calculators, and eventually spreadsheets and accounting software. Data analytics in accounting is a relatively new skill set that is growing significantly in all areas of the accounting profession. Accounting professionals who can discern patterns and trends in big data and translate them into compelling strategic narratives will find themselves at the center of the twenty-first-century business world. Therefore, accounting professionals can capitalize on numerous opportunities in this rapidly evolving, disruptive, but ultimately advantageous environment by embracing big data, data analytics, and artificial intelligence (AI) to stay ahead of the competition. © Editors and Contributors Severally 2023. All rights reserved."
"In this study, boron, an essential element of interspecies quorum sensing (QS) signals, was first proposed to enhance the start-up of the anaerobic ammonium oxidation (anammox) process. Exogenous boron was added into the anammox reactor, and its effect on the ammonia removal, granulation, AI-2 QS, and microbial community was examined. The results showed that the total nitrogen conversion rate in the experimental upflow anaerobic sludge blanket (UASB) with exogenous boron added (denoted as e-UASB) was 0.7 ± 0.05 kg N m−3 d−1, approximately three times that in the control UASB without boron (0.23 ± 0.05 kg N m−3 d−1, denoted as c-UASB). The exogenous addition of boron elevated the AI-2 concentration, increased the content of extracellular polymeric substances (EPS), and promoted granulation in the UASB during start-up. Microbial community analysis indicated that the abundances of anammox bacteria, i.e. Candidatus Brocadiales and Candidatus Jettenia, in the e-UASB were much higher than those in the c-UASB (25.5% vs. 19.7%). Additionally, Anaerolineaceae, which is known to play an important role in the granulation of anammox bacteria, had an abundance of 21.13% in e-UASB and 17.74% in c-UASB. It is demonstrated that exogenous boron could be promising in enhancing the start-up of an anammox reactor through promoting QS, granulation, and anammox. © 2023 The Royal Society of Chemistry"
"Technologies based on AI/ML are playing an increasingly prominent role in teenagers’ everyday lives. Mirroring this trend is a concomitant interest in teaching young people about intelligent technologies. Whereas previous research in the field of Child–Computer Interaction has proposed curriculum and learning activities that describe what teenagers need to learn about AI/ML, there is still a shortage of studies which specifically address teenager-centered perspectives in the teaching of AI/ML. This paper presents a study of teenagers’ everyday understanding of AI/ML technologies. Using a thematic analysis of the teenagers’ own explanations during a series of workshops, we present a conceptual map of the teenagers’ understandings of these technologies. We go on to propose five general recommendations for the teaching of AI/ML to teenagers through the lens of Computational Empowerment. Taken together, these recommendations serve as a teenage-centered starting point for teaching young people about intelligent technologies, an approach that can be implemented in future research interventions with similar objectives. © 2023 Copyright held by the owner/author(s)."
"The conventional literature on financial forecasting is often criticized for spurious predictions from biased estimators, data snooping, and approaches with inferior statistical properties. Along with the finance industry, mainstream finance research has started using machine learning (ML) and artificial intelligence (AI) methods to improve financial modelling for forecasting. Since this line of research crosses many domains, understanding the relevant prior work in particular for exploring the future directions are important and challenging both for academics and practitioners in the field. This chapter provides a systematic review of big data research methods used in finance with a particular focus on financial predictability, identifies the usefulness of these approaches compared to traditional predictive methods, and provides direction for future research. © Editors and Contributors Severally 2023. All rights reserved."
"This paper describes S.P.O.T., a game-based application for promoting children’s practical understanding of ML concepts and applications. Current tools for teaching ML in K-12 engage students in playful exploration of ML mechanisms and teach ML from a cognitive perspective. However, in S.P.O.T, learners interact with ML within real-life sociopolitical contexts and examine how ML predictions impact their daily lives and communities. Through the immersion of stories that mirror children’s lived experiences, S.P.O.T. provides elementary school aged children with opportunities to learn how machine learning applications function and develop children’s abilities to critically examine, question, and reimagine the consequences of ML decisions in the real world. © 2023 Copyright held by the owner/author(s)."
"Research in AI & Law has sought to model common-law case-based reasoning by creating analogies from cases, extracting and applying rules from cases, or both. This paper presents a new approach to extracting legal information from cases and several methods to apply it to new cases, including by analogy and by conversion to logical rules. It evaluates the approaches on a dataset of real-world cases and compares the results to off-the-shelf machine-learning techniques. We conclude that abstract legal information can be extracted from similar cases through analogical generalization, and that the extracted legal schemas can be used to reason about and solve other cases both by analogy and by rules. © ICAIL 2023. All rights reserved."
"In 2010, Richard Susskind documented the impact of digital transformation and development of Artificial Intelligence (AI) in the legal industry. He portentously queried, are we on the precipice of witnessing 'The End of Lawyers'? It is now a decade since Susskind's seminal publication on the future of the legal profession, and certainly, many of his proclamations ring true. The changes to the legal profession have been immense: robo judges, 'newlaw', online dispute resolution and blockchain technologies, to name a few. A fear that robots will replace lawyers preoccupies the profession. And yet, in 2021, the legal profession has not been transformed by digital technology and lawyers have not become extinct. There are still judges hearing cases and handing down judgements, and there are still bricks and mortar courts in which lawyers appear, paper brief in hand. So what does this say about the continued relevance of legal professionals in the digital world? How can we understand the human value of solicitors, barristers and judges in the face of digital transformation? What is it about the human element in the practice of law that pushes against its replacement by technologies often lauded as better, cheaper and faster? This chapter uses the legal profession as a prism to explore these themes, specifically through the narratives of five legal actors: a barrister, a law firm partner, an in-house lawyer, an ethics lawyer and a law student. It asks them to reflect upon their human value, which perhaps the world of law can't do without. In tracking the changing nature of the legal profession and interweaving the rich narratives of central protagonists in the legal world, this chapter contributes to the broader understanding of the digital human condition with which this book is concerned. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023."
"Artificial intelligence (AI) has been transforming the legal sector and profession given every day enhancing AI-driven legal tech tools. Considering the far-reaching ethical implications of such tools and the disparate functionalities of ‘AI ethics’ and ‘legal ethics’, this paper puts into question the interplay between these ethical domains and their underlying rules. After fleshing out the governance of ethics under each domain, e.g. respectively professional conduct rules and self-regulatory principles, and signposting the unresolved ethical challenges of status quo, e.g. particularly concerning cross-domain issues, the paper discusses how they need to interact, based on the three policy options: ‘revision of the conduct rules’, ‘individual (company level) collaboration’ and ‘higher-level collaboration’. It is concluded that ‘higher-level collaboration’ between the stakeholders is found to be the most sustainable and long-term option given the need to mitigate the ethical challenges concerning the legal sector from a holistic point of view. © ICAIL 2023. All rights reserved."
"Automated decision-making is one of the fundamental functions of smart home technologies. With the increasing availability of Artificial Intelligence (AI) and Internet of Things (IoT) technologies, those functions are becoming increasingly sophisticated. While many studies have been conducted on optimizing algorithms to improve the accuracy of predictions, less attention has been paid to how humans interact with algorithmic systems. This involves questions such as to what degree humans are involved in the algorithmic decision-making process and how we can design meaningful interactions between humans and systems relying on decision-making algorithms. With these questions in mind, our paper presents a literature review on the current state of decision-making algorithms in smart homes. Based on an analysis of 49 selected papers, we present a systematic investigation towards the application areas and the deployment functions that decision-making algorithms currently take in smart homes. Focusing on two main application areas - energy management and healthcare, our paper sheds light on the current deployment of decision-making algorithms in smart homes and identifies the current intentions of involving humans in-the-loop. Within the background of facilitating human-in-the-loop as an interaction paradigm, we aim to expose the design challenges for human-in-the-loop decision-making algorithms in smart homes which can pave the way for developing more effective human-machine hybrid intelligent systems in smart homes in the future. © 2023 The Authors."
Suggesting ways to reduce fragility in AI systems that include artificial neural networks.  © 2023 Owner/Author.
"The aim of the paper is to discuss the motivation and the methodology used to construct a survey that aims to gather data on the moral preferences of users in an ever-growing digital world, in order to implement an exoskeleton software (i.e. EXOSOUL) that will be able to protect and support the users in such a world. Even if we are more interested in presenting and discussing the methodology adopted, in Section 5 we present the preliminary results of the survey. In our society there is a growing and constant interaction between human agents and artificial agents, such as algorithms, robots, platforms, and ICT systems in general. The spread of these technologies poses new ethical challenges beyond the existing ones. This is for two main reasons. First, the amount of interactions between human agents and artificial ones involves a number of ethical aspects that is overwhelming. Secondly, and most importantly, the progressive self-sufficiency and autonomy that increasingly sophisticated systems are acquiring seem to deprive human beings of one of their most defining ethical aspects: the impact of systems' autonomy with respect to human decisions and actions. In line with this perspective, the EXOSOUL multidisciplinary project has the goal of creating a software exoskeleton that helps users to interact with artificial agents according to their ethical preferences. In this work, we aim to investigate how to collect human agent's ethical preferences. In Section 1 we present the EXOSOUL projects and in Section 2 the motivation for this paper. Section 3 and 4 illustrate the new approach, while in Section 5 we provide the preliminary results. Section 6 concludes and presents the work to be done in the future. In Section 1 we present the EXOSOUL project and in Section 2 the motivation for this paper. Section 3 and 4 illustrate the new approach, while in Section 5 we provide the preliminary results. Section 6 concludes and presents the work to be done in the future. © 2023 The Authors."
"We introduce WeHeart, a personalized recommendation device that aims to gradually increase physical activity levels in cardiac rehabilitation. The importance of physical activity in cardiac rehabilitation as a means of reducing associated morbidity and mortality rates is well-established. However, forming physical activity habits is a challenge, and the approach varies depending on individual preferences. Our solution employs a Random Forest classification model that combines both measured and self-reported data to provide personalized recommendations. We also propose to make use of Explainable AI to improve transparency and foster trust. © 2023 The Authors."
"Multilattices are generalisations of lattices introduced by Mihail Benado in [4]. He replaced the existence of unique lower (resp. upper) bound by the existence of maximal lower (resp. minimal upper) bound(s). A multilattice will be called pure if it is not a lattice. Multilattices could be endowed with a residuation, and therefore used as set of truth-values to evaluate elements in fuzzy setting. In this paper we exhibit the smallest pure multilattice and show that it is a sub-multilattice of any pure multilattice. We also prove that any bounded residuated multilattice that is not a residuated lattice has at least seven elements. We apply the ordinal sum construction to get more examples of residuated multilattices that are not residuated lattices. We then use these residuated multilattices to evaluate objects and attributes in formal concept analysis setting, and describe the structure of the set of corresponding formal concepts. More precisely, if Ai := (Ai, ≤i, ⊤i, o˙i, →i, ⊥i), i = 1, 2 are two complete residuated multilattices, G and M two nonempty sets and (φ, ψ) a Galois connection between A 1 G and A 2 M that is compatible with the residuation, then we show that C := {(h, f) ∈ A 1 G × A 2 M ; ϕ(h) = f and ψ(f) = h} can be endowed with a complete residuated multilattice structure. This is a generalization of a result by Ruiz-Calviño and Medina [20] saying that if the (reduct of the) algebras Ai, i = 1; 2 are complete multilattices, then C is a complete multilattice.  © 2022 - IOS Press. All rights reserved."
"Motivated by the subjective decision making and lack of strict protocols in damages as a remedy for contract breach, this project uses natural legal language processing (NLLP) and artificial intelligence (AI) techniques to analyze patterns in contract law cases and reduce uncertainty in their outcome. A ‘hybrid’ model combining heuristics, NLLP & the results of an LSTM based model into an XGBoost regressor along with contextual information had the best performance for the classification of entity types from unstructured proceedings text. Linear regressors were developed to approximate the Recovery Rate and the Win Rate using a set of 6 engineered features likely to affect the outcome. © ICAIL 2023. All rights reserved."
[No abstract available]
"We present sustain.AI, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies’ sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, sustain.AI is publicly available for everyone at https://sustain.ki.nrw/. © ICAIL 2023. All rights reserved."
"The IDC research community has a growing interest in designing AI interfaces for children with special educational needs. Nonetheless, little research has explored the research and design issues, rationale, challenges, and opportunities in this field. Therefore, we propose to host a half-day workshop to bring together researchers and practitioners from the Learning & Education, Accessibility, and Intelligent User Interfaces sub-fields to discuss and identify existing design issues, challenges, and collaboration barriers, to establish consensus on the design of a pragmatic framework, as well as explore future innovation and research opportunities. We aim to foster mutual understanding and in-depth collaboration among researchers in the IDC community. © 2023 Copyright held by the owner/author(s)."
"AF-CBA is an example-based approach to XAI that draws on the case-based argumentation tradition in AI & Law. It means to explain binary classifications made by an opaque machine-learning model by presenting an argument graph to the user, which represents an argument game about the classification of a case on the basis of precedents derived from labelled data used in the training phase of the classifier. We improve the robustness of this method by modifying it to better handle inconsistent labelling and evaluate an alternative setup that does not require access to the labelled data by using earlier predictions instead. © ICAIL 2023. All rights reserved."
"ChatGPT has enabled access to artificial intelligence (AI)-generated writing for the masses, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent. Addressing this need, we report a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. The approach uses new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like “but,” “however,” and “although.” With a set of 20 features, we built a model that assigns the author, as human or AI, at over 99% accuracy. This strategy could be further adapted and developed by others with basic skills in supervised classification, enabling access to many highly accurate and targeted models for detecting AI usage in academic writing and beyond. © 2023 The Author(s)"
"Augmentation of energy efficiency in the power generation systems can aid in decarbonizing the energy sector, which is also recognized by the International Energy Agency (IEA) as a solution to attain net-zero from the energy sector. With this reference, this article presents a framework incorporating artificial intelligence (AI) for improving the isentropic efficiency of a high-pressure (HP) steam turbine installed at a supercritical power plant. The data of the operating parameters taken from a supercritical 660 MW coal-fired power plant is well-distributed in the input and output spaces of the operating parameters. Based on hyperparameter tuning, two advanced AI modeling algorithms, i.e., artificial neural network (ANN) and support vector machine (SVM), are trained and, subsequently, validated. ANN, as turned out to be a better-performing model, is utilized to conduct the Monte Carlo technique-based sensitivity analysis toward the high-pressure (HP) turbine efficiency. Subsequently, the ANN model is deployed for evaluating the impact of individual or combination of operating parameters on the HP turbine efficiency under three real-power generation capacities of the power plant. The parametric study and nonlinear programming-based optimization techniques are applied to optimize the HP turbine efficiency. It is estimated that the HP turbine efficiency can be improved by 1.43, 5.09, and 3.40% as compared to that of the average values of input parameters for half-load, mid-load, and full-load power generation modes, respectively. The annual reduction in CO2 measuring 58.3, 123.5, and 70.8 kilo ton/year (kt/y) corresponds to half-load, mid-load, and full load, respectively, and noticeable mitigation of SO2, CH4, N2O, and Hg emissions is estimated for the three power generation modes of the power plant. The AI-based modeling and optimization analysis is conducted to enhance the operation excellence of the industrial-scale steam turbine that promotes higher-energy efficiency and contributes to the net-zero target from the energy sector. © 2023 The Authors. Published by American Chemical Society."
"Laypeople (i.e. individuals without legal training) may often have trouble resolving their legal problems. In this work, we present the JusticeBot methodology. This methodology can be used to build legal decision support tools, that support laypeople in exploring their legal rights in certain situations, using a hybrid case-based and rule-based reasoning approach. The system ask the user questions regarding their situation and provides them with legal information, references to previous similar cases and possible next steps. This information could potentially help the user resolve their issue, e.g. by settling their case or enforcing their rights in court. We present the methodology for building such tools, which consists of discovering typically applied legal rules from legislation and case law, and encoding previous cases to support the user. We also present an interface to build tools using this methodology and a case study of the first deployed JusticeBot version, focused on landlord-tenant disputes, which has been used by thousands of individuals. © ICAIL 2023. All rights reserved."
"As Industry 4.0 has rapidly influence many organizations of all size, industrial processes have grown and revolutionizes into implication of artificial intelligence (AI) into it. One example of AI is computer vision (CV), also defined as a field of study to help computer understand digital images and videos. Modern device and machinery are limited to only perform certain tasks that they are design to do. One limitation is that they are not able to identify what they ""see"". Binocular stereo vision allows the application of identification of objects possible to devices, this feature is known as computer vision. Binocular stereo vision is the type of view to achieve the necessary results, which is the use of more than one image source. This type of vision is important to help detection in 3-axis, using the coordinates from both camera individual 2-axis coordinates and trigonometric mathematical model. The first objective of this project is to design algorithm for 3-axis binocular vision system for dynamic motion. The second objective is to evaluate the distance measurements for the sample objects, which the sample objects will be red, green and blue balls. The third objective is to optimize the detection characteristics for 3-axis binocular stereo vision. Two same webcams are setup to assist the binocular system for better depth capturing compared to monocular system. The algorithm debugs using Microsoft Visual Studio C++, while the vision capture library uses OpenCV for motion capture. Results shown that the camera without problem can detect the red, green and blue balls. HSV colour model allows any colour of object to become detectable as long as the object colour and the background colour are contrast. Furthermore, when the coloured ball travel across the X-axis, X coordinate values show an increase over time, while Y and Z coordinates has miniscule changes. © 2023 Author(s)."
"Legal documents, and specifically law texts, are not easy to understand by humans. The specific terminology and sentence constructions are particular, which also makes it a difficult machine understanding task. In this paper, we present a publicly available benchmark dataset containing Dutch law texts which can be used to train AI models that assist humans equipped with the task of interpreting legal texts. However, the dataset can be used in a broader context, such as semantic role labeling of Dutch (legal) texts. Our dataset contains 4463 annotated sentences from 55 different Dutch laws, in which four roles are annotated by human annotators: action, actor, object and recipient. The inter-annotator agreement is substantial (K=0.75). In experiments with a rule-based and a transformer-based method, results show that the transformer-based method performs quite well on the dataset (accuracy > 0.8). These results indicate that we can reliably predict actions, actors, objects and recipients in legal texts. This can help people equipped with the task of formal interpretation of legal texts. © ICAIL 2023. All rights reserved."
"As a consequence of the advent of new technologies, teaching and learning methods have evolved dramatically. Artificial intelligence (AI) applications in educational settings are becoming increasingly apparent as a result of rapid development of AI technology in recent years. Adaptive learning, smart campus, teacher evaluation, intelligent tutoring robots, and virtual classrooms are only a few of the applications of educational-AI that is explored in this article. After evaluating the impact of AI technology on teaching and learning, it is conclusively inferred that AI has a beneficial effect on both the quality of instruction provided by teachers, as well as on the learning outcomes of students. Towards the end, the article discusses the possible challenges that AI applications in education may face, instances of AI's potential in helping schools to improve, and thereby promoting educational reforms.  © 2023 Author(s)."
"Mapping and monitoring marine ecosystems imply several challenges for data collection and processing: water depth, restricted access to locations, instrumentation costs or weather constraints for sampling, among others. Nowadays, Artificial Intelligence (AI) and Geographic Information System (GIS) open source software can be combined in new kinds of workflows, to annotate and predict objects directly on georeferenced raster data (e.g. orthomosaics). Here, we describe and share the code of a generic method to train a deep learning model with spatial annotations and use it to directly generate model predictions as spatial features. This workflow has been tested and validated in three use cases related to marine ecosystem monitoring at different geographic scales: (i) segmentation of corals on orthomosaics made of underwater images to automate coral reef habitats mapping, (ii) detection and classification of fishing vessels on remote sensing satellite imagery to estimate a proxy of fishing effort (iii) segmentation of marine species and habitats on underwater images with a simple geolocation. Models have been successfully trained and the models predictions are displayed with maps in the three use cases.  © 2023 International Society for Photogrammetry and Remote Sensing. All rights reserved."
"In this workshop we will present, through live demos, novel technological tools such as interactive ""smart"" toys for social play and AI-based monitoring tools, developed to support the early treatment of Autism Spectrum Disorder (ASD). We will discuss the potential of such devices, describing also our promising results with ASD children. The goal of the workshop is to involve the specialist audience in the co-design of some potential aspects of the overall system. In particular, we are interested in collecting feedback and proposals on possible further uses of the system such as additional play activities to stimulate social behaviour; improvements of the monitoring tools; potential use of the tools in developmental disorders other than ASD. © 2023 Copyright held by the owner/author(s)."
"Artificial Intelligence (AI) technologies have become increasingly important in a rapidly growing business environment. Despite the importance of AI to customer relationship management (CRM) systems, there is a lack of comprehensive literature on the capabilities and effects of this platform. Based on a review of the literature, this study proposes four primary dimensions and twelve sub-dimensions of AI-Base CRM systems. © Editors and Contributors Severally 2023. All rights reserved."
"In present medical decisions, the use of artificial intelligence (AI) systems is based on trust and transparency. Explainable AI (XAI) is an emerging field that happens to throw light on ""black box"" machine learning (ML) models in human understandable language. XAI is a solution to several scientific problems that need deep explanation and interpretation. XAI solutions with quantitative and qualitative analyses have proved the efficiency for many clinical problems. This chapter focuses on few algorithms of XAI, namely ANFIS-GA, LIME, GRAD-CAM, and SIDU. In this study, these algorithms were implemented on various cases such as heart attack prediction, eye fundus, thyroid, COVID-19, and air pollutant. © 2023 River Publishers."
"Every year, supermarkets discard tons of food that are completely fit for consumption. This happens because of inefficient labeling on packages and confusion between different terminologies like best by, use by, sell by, etc., used on the food packages. However, we now have the technology to eliminate human error and put this food to better use. A group of children in the ACM Interaction Design and Children Research & Design Challenge (IDC’23) booklet came up with the idea of a mobile application that could distribute such food products to those in need. Our team expanded on this idea and other related and relevant ideas from the booklet and designed an application that uses Artificial Intelligence (AI) to sort and package safe for consumption good food that would otherwise go to waste. Our solution aims to provide weekly food packages for individuals or families in need, thereby ensuring that no one goes hungry and also mitigating unnecessary food wastage. It is a positive outcome for every stakeholder involved: supermarkets reduce unnecessary food wastage and underserved communities receive food, thus contributing to a more sustainable future. © 2023 Copyright held by the owner/author(s)."
"This work presents an argument for the use of specific documentation for the ethical development, use, and sharing of energy datasets, and an evaluation of current practice in the energy AI community. Drawing on a recently developed resource from the broader machine learning community and applying it to the specific context of energy AI research, opportunities for more transparent collection and distribution of energy datasets are revealed. To help elucidate the utility of the datasheets and the energy community's current level of documentation, two publicly available energy datasets are chosen for analysis. One has published documentation covering 66% of the datasheet questionnaire, while the second covers 42% of the suggested information. Two additional questions are recommended for energy-relevant datasheets that will promote ethical AI practices in the energy domain. A new resource for exploring and aligning energy datasets with demographic data is provided.  © 2023 ACM."
[No abstract available]
"This chapter provides a brief overview of the growing role played by big data, data analytics, and artificial intelligence (AI) in the accounting profession. The term 'Big Data' along with other trending topics such as 'Data Analytics' and 'Artificial Intelligence (AI)' have become buzzwords in the accounting profession in recent years. The skills of accounting professionals have evolved as technology has made rapid advances from using pencil and paper to typewriters and calculators, and eventually spreadsheets and accounting software. Data analytics in accounting is a relatively new skill set that is growing significantly in all areas of the accounting profession. Accounting professionals who can discern patterns and trends in big data and translate them into compelling strategic narratives will find themselves at the center of the twenty-first-century business world. Therefore, accounting professionals can capitalize on numerous opportunities in this rapidly evolving, disruptive, but ultimately advantageous environment by embracing big data, data analytics, and artificial intelligence (AI) to stay ahead of the competition. © Editors and Contributors Severally 2023. All rights reserved."
"The virtual live streaming format of Vtubers is gaining immense popularity worldwide, aided by advancements in large language models (LLMs) and 2D/3D avatar simulation technologies. The emergence of pure AI-driven Vtubers represents a promising trend in content creation. However, research has not adequately explored the interactive capabilities between live streaming platforms and Vtubers, nor their implications on parasocial relationships. We developed ""Blibug,""an AI-Vtuber on the Chinese Bilibili platform, who can be interacted with through ""Danmu""messages (real-time comments overlaid on the video playback, also called a ""bullet curtain""). By implementing an interactive AI-Vtuber action and dialogue response system, we translate relatable daily activities into virtual engagements and establish a real-time, one-to-many AI-Human dialogue framework. Our study aims to investigate the entertainment and narrative dimensions of Vtubers and to understand AI's role as a social agent and its impact on parasocial relationships.  © 2023 Owner/Author."
"Historically, researchers have focused on analyzing Western, Educated, Industrialized Rich and Democratic (WEIRD), adult perspectives on technology. This means we may not have technology developed appropriately for children and those from non-WEIRD countries. In this paper, we analyze children and parents from various countries’ perspectives on an emerging technology: conversational agents. We aim to better understand participants’ trust of agents, partner models, and their ideas of “ideal future agents” such that researchers can better design for these users—for instance, by ensuring children do not overtrust agents. Additionally, we empower children and parents to program their own agents through educational workshops, and present changes in perceptions as participants create and learn about agents. Results from the study (n=49) included how children felt agents were significantly more human-like, warm, and dependable than parents did, how overall participants trusted agents more than parents or friends for correct information, how children described their ideal agents as being more artificial than human-like than parents did, and how children tended to focus more on fun features, approachable/friendly features and addressing concerns through agent design than parents did, among other results. We also discuss potential agent design implications of the results, including how designers may be able to best foster appropriate levels of trust towards agents by focusing on designing agents’ competence and predictability indicators, as well as increasing transparency in terms of agents’ information sources. © 2023 Copyright held by the owner/author(s)."
"Aim of my research is to exploit Linguistic Linked Open Data (LLOD) as base for advanced Cultural Heritage (CH) fruition by means of Automatic Story Generation (ASG). Following the rationale that discovering and reviving already existing (yet latent) narratives is worthier than automatically generating them from anew in eliciting the user's interest, the input-2-graph and the graph-2-sequence ASG-pipeline phases, heavily relying on LLOD, will be given a deeper focus, whereby the final Natural Language Generation (NLG) module will be constrained by the entities and relations established in the Knowledge Graph (KG) generation modules (a configuration typical of the neurosymbolic approach). In order to enhance possibilities of implementation in real-life contexts, the elaborated pipeline will be modular, i.e. self-sufficient in its constituent parts. Beyond the countless possible application scenarios ranging from education to entertainment, this solution detangles the user from his role of mere consumer, and empowers him not only to control the creation process [3.1], but also to find already within it, and not necessarily in the final outcome, a valuable source for intellectual growth. This work intends the addressing of a specific societal need as an avalanche to simultaneously fill knowledge gaps identified in and among the related scientific domains. © 2023 The Authors."
"Implementing ethics is a complex problem requiring stakeholders engagement. Engaging in fair and transparent way with stakeholders is part of the complexity. This qualitative study applies principles and techniques of Critical Systems Thinking while engaging with stakeholders in the context of implementing ethics for a COVID-19 AI. In a reflexive manner, the study examines the participatory process and its output leading to recommendations.  © 2023 The authors and IOS Press."
[No abstract available]
"'ChoreoGraphiComfort' is inspired by a critical reflection on the organicity and corporeality that (e)merge in the thermal space of vernacular architecture. Abstracting the creative notions of choreography, graphics, and comfort, the artwork uses an ethnographic survey of living conditions in low-cost flats in Malaysia during the recent Covid-19 pandemic and an architectural representation of a 300-year-old traditional Malay house called Istana Puteri Bongsu. Centering the use on the issue of thermal comfort, a series of thermal images of the human body in various choreographed postures were filmed against the background of a traditional Malay house. The images were then fed into an AI platform to generate a new set of images that were morphed into each other. This creates a surrealist abstraction of what we term 'thermal choreography' that complicates the polemics of organic architecture and cultural wisdom in adapting human thermal comfort to the natural environment.  © 2023 Owner/Author."
"With artificial intelligence (AI) systems entering our working and leisure environments with increasing adaptation and learning capabilities, new opportunities arise for developing hybrid (human-AI) intelligence (HI) systems, comprising new ways of collaboration. However, there is not yet a structured way of specifying design solutions of collaboration for hybrid intelligence (HI) systems and there is a lack of best practices shared across application domains. We address this gap by investigating the generalization of specific design solutions into design patterns that can be shared and applied in different contexts. We present a human-centered bottom-up approach for the specification of design solutions and their abstraction into team design patterns. We apply the proposed approach for 4 concrete HI use cases and show the successful extraction of team design patterns that are generalizable, providing re-usable design components across various domains. This work advances previous research on team design patterns and designing applications of HI systems. © 2023 The Authors."
"Evaluating robustness is an important goal in simulation-based analysis. Robustness is achieved when the controllable factors of a system are adjusted in such a way that any possible variance in uncontrollable factors (noise) has minimal impact on the variance of the desired output. The optimization of system robustness using simulation is a dedicated and well-established research direction. However, once a simulation model is available, there is a lot of potential to learn more about the inherent relationships in the system, especially regarding its robustness. Data farming offers the possibility to explore large design spaces using smart experiment design, high performance computing, automated analysis, and interactive visualization. Sophisticated machine learning methods excel at recognizing and modelling the relation between large amounts of simulation input and output data. However, investigating and analyzing this modelled relationship can be very difficult, since most modern machine learning methods like neural networks or random forests are opaque black boxes. Explainable Artificial Intelligence (XAI) can help to peak into this black box, helping us to explore and learn about relations between simulation input and output. In this paper, we introduce a concept for using Data Farming, machine learning and XAI to investigate and understand system robustness of a given simulation model. © 2023 Owner/Author."
"At present, many AI models are used in medical image analysis and disease detection. Coupled with extended understanding of imaging and clinical phenotypes toward a more precision-based approach for managing spine patients, the multidimensionality of data-driven results and analysis become exponentially more complex. However, with the use of AI, ML platforms and efforts of spine surgeons, computer-aided diagnosis and detection, decision support and prognosis prediction, and computer-aided surgical education and assessment models seem to have reduced complications and improved outcomes with MISS. Like all medical specialties, MISS is subjected to changes in clinical practices based on cutting-edge research. Quality control systems should be employed to ensure the algorithms continue to be relevant as data collection and clinical practices change over time. Recent studies clearly demonstrate the power of AI, ML in pre-, intra-, and postoperative phases of spinal surgeries. Future work requires the integration of AI and ML models to combine pre-, intra-, and postoperative algorithms into a single model where the best preoperative planning, intraoperative surgical intervention, and postoperative follow-up work with associated risk, financial cost, and considerations can be suggested to surgeons. Such algorithms not only can benefit spine surgeons in their decision-making but also facilitate the delivery of high-quality healthcare to low resources settings and facilitate personalized surgical and postsurgical care with MISS. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023."
[No abstract available]
"Fast and accurate climate simulations and weather predictions are critical for understanding and preparing for the impact of climate change. Real-world climate and weather simulations involve the use of complex compound stencil kernels, which are composed of a combination of different stencils. Horizontal diffusion is one such important compound stencil found in many climate and weather prediction models. Its computation involves a large amount of data access and manipulation that leads to two main issues on current computing systems. First, such compound stencils have high memory bandwidth demands as they require large amounts of data access. Second, compound stencils have complex data access patterns and poor data locality, as the memory access pattern is typically irregular with low arithmetic intensity. As a result, state-of-the-art CPU and GPU implementations suffer from limited performance and high energy consumption. Recent works propose using FPGAs as an alternative to traditional CPU and GPU-based systems to accelerate weather stencil kernels. However, we observe that stencil computation cannot leverage the bit-level flexibility available on an FPGA because of its complex memory access patterns, leading to high hardware resource utilization and low peak performance.We introduce SPARTA, a novel spatial accelerator for horizontal diffusion weather stencil computation. We exploit the two-dimensional spatial architecture to efficiently accelerate the horizontal diffusion stencil by designing the first scaled-out spatial accelerator using the MLIR (Multi-Level Intermediate Representation) compiler framework. We evaluate SPARTA on a real cutting-edge AMD-Xilinx Versal AI Engine (AIE) spatial architecture. Our real-system evaluation results demonstrate that SPARTA outperforms state-of-the-art CPU, GPU, and FPGA implementations by 17.1×, 1.2×, and 2.1×, respectively. Compared to the most energy-efficient design on an HBM-based FPGA, SPARTA provides 2.43× higher energy efficiency. Our results reveal that balancing workload across the available processing resources is crucial in achieving high performance on spatial architectures. We also implement and evaluate five elementary stencils that are commonly used as benchmarks for stencil computation research. We freely open-source all our implementations to aid future research in stencil computation and spatial computing systems at https://github.com/CMU-SAFARI/SPARTA.  © 2023 ACM."
"We present a prototype hybrid prediction market and demonstrate the avenue it represents for meaningful human-AI collaboration. We build on prior work proposing artificial prediction markets as a novel machine learning algorithm. In an artificial prediction market, trained AI agents (bot traders) buy and sell outcomes of future events. Classification decisions can be framed as outcomes of future events, and accordingly, the price of an asset corresponding to a given classification outcome can be taken as a proxy for the systems confidence in that decision. By embedding human participants in these markets alongside bot traders, we can bring together insights from both. In this paper, we detail pilot studies with prototype hybrid markets for the prediction of replication study outcomes. We highlight challenges and opportunities, share insights from semi-structured interviews with hybrid market participants, and outline a vision for ongoing and future work. © 2023 The Authors."
"Semantic segmentation of remote sensing images by deep neural network is an important content of remote sensing intelligent interpretation, which plays a very important role in urban planning, disaster assessment, agricultural production and other fields. High resolution remote sensing images are characterized by complex background, diverse scales and irregular shape, etc. Therefore, using natural scene semantic segmentation methods to process remote sensing images often has the problem of low segmentation accuracy. Based on the U-Net model, a multi-scale skip connection method is proposed to integrate semantic features of different levels and obtain accurate segmentation boundary and location information. Attention mechanism and pyramid pooling are introduced to solve the problem of fine segmentation in complex background. In order to verify the effectiveness of our proposed method, experiments were carried out on the WHDLD and LandCover.ai dataset and compared with the mainstream semantic segmentation methods. The experimental results show that the proposed method outperforms other comparison methods, with mloU reaching 74.28% and 82.04% respectively, and with average of Fi score reaching 84.47% and 89.76% respectively; compared with the segmentation results of U-Net, the value of loU improves significantly in buildings, roads and other categories with a relatively small proportion, and is superior to other comparison methods. © 2023 SinoMaps Press. All rights reserved."
[No abstract available]
"Emotion Explorers is an AI system that assists 6-11-year-old children to understand emotions, aims to have a long-lasting impact on children’s emotional behaviors, and creates a compassionate, connected, safe environment. The system can be integrated into smart devices, such as bracelets and plushies for children, and mobile applications for parents and teachers. Emotion Explorers interacts with children through a digital voice assistant named Zen, facilitating children’s emotional awareness and emotional behaviors through dialogues about emotions. It detects child users’ emotions through verbal content and heartbeat rates and provides emotional support accordingly. It helps children verbalize their feelings, normalize emotional communication, understand others’ feelings, and show kindness to create a compassionate environment. In addition, Emotion Explorers provides adults with analyses of the emotional status of children. This way, adults are aware of children’s mental status and can provide better emotional support. The goal is to develop children’s emotional awareness, communication of emotions, and kind behaviors through a longitudinal and sustainable approach. © 2023 Copyright held by the owner/author(s)."
"Fostering young learners’ literacy surrounding AI technologies is becoming increasingly important as AI is becoming integrated in many aspects of our lives and is having far-reaching impacts on society. We have developed Knowledge Net and Creature Features, two activity boxes for family groups to engage with in their homes that communicate AI literacy competencies such as understanding knowledge representations, the steps of machine learning, and AI ethics. Our current work is exploring how to transform these activity boxes into museum exhibits for middle-school age learners, focusing on three key considerations: centering learner interests, generating personally meaningful outputs, and incorporating embodiment and collaboration on a larger scale. Our demonstration will feature the existing Knowledge Net and Creature Features activity boxes alongside early-stage prototypes adapting these activities into larger-scale museum exhibits. This paper contributes an exploration into how to design AI literacy learning interventions for varied informal learning contexts. © 2023 Copyright held by the owner/author(s)."
"We present an interactive music AI system that enables users to co-create expressive performances of notated music using speech and gestures. The system provides multi-modal interactive dialog-based control of performance rendering via smartphones and is accessible to people regardless of their musical background. We train a deep learning music performance rendering model on sheet music and associated performances with notated performance directions and user-system interaction data. Users have the opportunity to actively participate in the performance process. A speech- and gesture-based feedback loop with interactive learning improve the accuracy of performance rendering control. We believe that many people can express aspects of music performance using natural human expressions such as speech, voice, and gestures, and that by hearing the music follow their communicated intent, they can achieve deeper immersion and enjoyment of music than otherwise possible. With this work we pursue the goal of developing novel, fulfilling, and accessible music making experiences for large numbers of people who are not currently musically active. © 2023 The Authors."
"Somax2 is a multi-agent interactive system, based on machine-listening, machine learning and generative units, performing live machine co-improvisation with musicians. The system is trained on any musical materials chosen by the user (Audio or MIDI), effectively constructing a generative model, a corpus, from which it draws its musical 'knowledge'. Listening to and adapting to a musician, that activates dynamic profiles, Somax2 provides stylistically coherent improvisation in real-time. As part of the REACH project, defining co-creativity as distributed agency between human and machine, Somax2 is indeed a good example of a distributed co-creative system, where multimodal cross-feedback affects collective behavior and increases player's engagement through emotion and motivation. © 2023 The Authors."
"AI applications are driving the need for large dedicated GPU clusters, which are highly energy- and carbon-intensive. To efficiently operate these clusters, operators leverage workload forecasts that inform resource allocation decisions to save energy without sacrificing performance. The traditional forecasting methods provide a single-point forecast and do not expose the uncertainty about their predictions, which can lead to an unexpected loss in performance. In this paper, we present an uncertainty-driven GPU demand forecasting framework that exposes the uncertainty in its predictions and provides a mechanism to configure the trade-off between energy savings and performance. We evaluate our approach using multiple GPU workload traces and demonstrate that the forecasting framework, called CUFF, outperforms state-of-the-art point predictions. CUFF predictor meets performance goals 83% of the time compared to 7.6% for the point predictions under high GPU demand. Furthermore, CUFF knob enables users to configure up to 98% performance target while providing 26% energy savings, comparable value to point forecasts that only ensure 68% performance target. © 2023 ACM."
"We are at the cusp of a transition from 'learning from data' to 'learning what data to learn from' as a central focus of artificial intelligence (AI) research. While the first-order learning problem is not completely solved, large models under unified architectures, such as transformers, have shifted the learning bottleneck from how to effectively train models to how to effectively acquire and use task-relevant data. This problem, which we frame as exploration, is a universal aspect of learning in open-ended domains like the real world. Although the study of exploration in AI is largely limited to the field of reinforcement learning, we argue that exploration is essential to all learning systems, including supervised learning. We propose the problem of generalized exploration to conceptually unify exploration-driven learning between supervised learning and reinforcement learning, allowing us to highlight key similarities across learning settings and open research challenges. Importantly, generalized exploration is a necessary objective for maintaining open-ended learning processes, which in continually learning to discover and solve new problems, provides a promising path to more general intelligence. © 2023 The Authors."
[No abstract available]
This paper presents a new approach to engaging children in Nigeria to share their views of AI. This approach is centered on an inclusive writing contest for children in a secondary school in Abuja to write about AI to compete for prizes and share their writings with others. A preliminary analysis of the first 11 articles we received exhibits diverse gender and ethnic representation that conveys cultural values and perspectives distinct from those of the children in Western countries. This finding suggests future work to conduct an in-depth cross-cultural analysis of the articles and to replicate similar writing contests to engage children in other underrepresented countries. © 2023 Copyright held by the owner/author(s).
[No abstract available]
"Establishing an appropriate level of trust between people and AI systems is crucial to avoid the misuse, disuse, or abuse of AI. Understanding how AI systems can generate appropriate levels of trust among users is necessary to achieve this goal. This study focuses on the impact of displaying integrity, which is one of the factors that influence trust. The study analyzes how different integrity-based explanations provided by an AI agent affect a human's appropriate level of trust in the agent. To explore this, we conducted a between-subject user study involving 160 participants who collaborated with an AI agent to estimate calories on a food plate, with the AI agent expressing its integrity in different ways through explanations. The preliminary results demonstrate that an AI agent that explicitly acknowledges honesty in its decision making process elicit higher subjective trust than those that are transparent about their decision-making process or fair about biases. These findings can aid in designing agent-based AI systems that foster appropriate trust from humans. © 2023 The Authors."
[No abstract available]
"A crucial building block of responsible artificial intelligence is responsible data governance, including data collection. Its importance is also underlined in the latest EU regulations. The data should be of high quality, foremost correct and representative, and individuals providing the data should have autonomy over what data is collected. In this article, we consider the setting of collecting personally measured fitness data (physical activity measurements), in which some individuals may not have an incentive to measure and report accurate data. This can significantly degrade the quality of the collected data. On the other hand, high-quality collective data of this nature could be used for reliable scientific insights or to build trustworthy artificial intelligence applications. We conduct a framed field experiment (N = 691) to examine the effect of offering fixed and quality-dependent monetary incentives on the quality of the collected data. We use a peer-based incentive-compatible mechanism for the quality-dependent incentives without spot-checking or surveilling individuals. We find that the incentive-compatible mechanism can elicit good-quality data while providing a good user experience and compensating fairly, although, in the specific study context, the data quality does not necessarily differ under the two incentive schemes. We contribute new design insights from the experiment and discuss directions that future field experiments and applications on explainable and transparent data collection may focus on.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
The proceedings contain 32 papers. The topics discussed include: a novel approach for understanding ideology behind managing data within data lake in era of big data; surface defect detection using deep learning; disease detection on pomegranate fruits using machine learning approach; intelligence unleashed : an argument for AI-enabled learning ecologies with real world examples of today and a peek into the future; audio guided aid for assisting visually challenged people for shopping of grocery items; multimodal detection of ambulance using jetson nano; an overview of physical layer security enhancement techniques for full-duplex relay network; auto attendance recorder and performance analyzer; review on pre-processing algorithms for breast density classification using digital mammograms; drone based survey of garbage locations for waste management; categorization of emotions based on facial expressions; and privacy preserving machine learning using secure multiparty computation.
"Image generation gained popularity with machine learning (ML) models generating images from text, fuelling new online communities of practices. This work explores the sociology, motivations, and usages of AI art hobbyists. We analyzed an online questionnaire answered by 64 practitioners and a dataset of user prompts sent to the Stable Diffusion generative model. Our findings suggest that TTI generation is a recreational activity mainly conducted by narrow socio-demographic groups who use auxiliary techniques across platforms and beyond request-response interactions. Inherent model limitations and finding suitable prompt formulation are the main obstacles practitioners face. A taxonomy and a corresponding ML model capable of recognizing the semantic content of unseen prompts were created to conduct the user prompt analysis. The prompt analysis revealed that artist names are the main specifier used beside the main subject, often in sequences. We finally discuss the design and socio-technical implications of our work for creativity support.  © 2023 ACM."
[No abstract available]
"The Internet of Things (IoT) is the concept of connecting any device to the Web and other connected gadgets. IoT resembles a ubiquitous network, with the exception that any electrical equipment connected to it has internet connectivity. IoT in the health industry is a great example of this pervasive computing. With the rise of explainable AI, security in IoT using predictive methods is an important concern that needs to be taken care of. If IoT in healthcare is to be successful, serious security issues must be resolved. Medical professionals and society handling IoT devices must ensure that the data collected by IoT devices is appropriately protected and it can be achieved through proper interpretation of results through explainable AI approaches. The main objective of this work is to draw a comparative analysis of two IoT security healthcare strategies in the context of explainable AI aspect. To achieve these objectives, we have discussed two distinct models in this study. The first model takes into account security in intelligent healthcare leveraging block-chain and explainable intelligence. The second model offers a healthcare security method based on easily interpretable deep learning. Also included is a table that compares the two models. © 2023 River Publishers."
"The ability to share and reuse deep learning (DL) models is a key driver that facilitates the rapid adoption of artificial intelligence (AI) in both industrial and scientific applications. However, state-of-the-art approaches to store and access DL models efficiently at scale lag behind. Most often, DL models are serialized by using various formats (e.g., HDF5, SavedModel) and stored as files on POSIX file systems. While simple and portable, such an approach exhibits high serialization and I/O overheads, especially under concurrency. Additionally, the emergence of advanced AI techniques (transfer learning, sensitivity analysis, explainability, etc.) introduces the need for fine-grained access to tensors to facilitate the extraction and reuse of individual or subsets of tensors. Such patterns are underserved by state-of-the-art approaches. Requiring tensors to be read in bulk incurs suboptimal performance, scales poorly, and/or overutilizes network bandwidth. In this paper we propose a lightweight, distributed, RDMA-enabled learning model repository that addresses these challenges. Specifically we introduce several ideas: compact architecture graph representation with stable hashing and client-side metadata caching, scalable load balancing on multiple providers, RDMA-optimized data staging, and direct access to raw tensor data. We evaluate our proposal in extensive experiments that involve different access patterns using learning models of diverse shapes and sizes. Our evaluations show a significant improvement (between 2 and 30× over a variety of state-of-the-art model storage approaches while scaling to half the Cooley cluster at the Argonne Leadership Computing Facility.  © 2023 Owner/Author(s)."
"In the current era of omnipresent and near-instant communication, rapid technological changes are disrupting many sectors of domestic economies, while also transforming the broader global security arena. Diplomacy, deterrence and direct military action, tools that have long been utilized to safeguard the national interest, are being tested by precipitously evolving technologies-including, uninhabited aerial vehicles (UAVs), lethal autonomous weapons (LAWs), cyber and artificial intelligence (AI)-and are presenting new problems with limited guardrails to 'balance' their usage. As these technologies mature, they could hold significant implications for defence authorizations and appropriations, military concepts of operations and the future of war. Moreover, as they evolve, they will continue to pose ethical and reliability questions about the use (and misuse) of technological power. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023."
"In recent years, several normative systems have been presented in the literature. Relying on formal methods, these systems support the encoding of legal rules into machine-readable formats, enabling, e.g. to check whether a certain workflow satisfies or agents abide by these rules. However, not all rules can be easily expressed (see for instance the unclear boundaries between tax planning and tax avoidance). The paper introduces a framework for norm identification and norm induction that automates the formalization of norms about non-compliant behavior by exploring the behavioral space via simulation, and integrating inputs from humans via active learning. The proposed problem formulation sets also a bridge between AI & law and more general branches of AI concerned by the adaptation of artificial agents to human directives. © ICAIL 2023. All rights reserved."
"Human knowledge is growing exponentially, providing huge and sometimes contrasting evidence to support decision making in the realm of complex problems. To fight knowledge fragmentation, collective intelligence leverages groups of experts (possibly from diverse domains) that jointly provide solutions. However, to promote beneficial outcomes and avoid herding, it is necessary to (i) elicit diverse responses and (ii) suitably aggregate them in a collective solution. To this end, AI can help with dealing with large knowledge bases, as well as with reasoning on expert-provided knowledge to support decision-making. A hybrid human-artificial collective intelligence can leverage the complementarity of expert knowledge and machine processing to deal with complex problems. We discuss how such a hybrid human-artificial collective intelligence can be deployed to support decision processes, and we present case studies in two different domains: general medical diagnostics and climate change adaptation management. © 2023 The Authors."
"The purpose of this paper is to provide a definitive, up-to-date account of a methodology has that been proven successful for representing and reasoning about legal domains. The ANGELIC (ADF for kNowledGe Encapsulation of Legal Information for Cases) methodology was originally developed to exploit then recent developments in knowledge representation techniques that lend themselves well to capturing factor-based reasoning about legal cases. The methodology is situated firmly within the tradition of research in AI and Law that aims to build systems that are knowledge rich in terms of the domain expertise that is emulated within the systems. When the methodology was first introduced, it was demonstrated on academic examples, but it was subsequently used in and evaluated on a variety of real world domains for external clients. This set of evaluation exercises yielded a variety of learning points as the methodology was applied to different legal domains with their own particular features. These learning points, and the extensions to the methodology that follow from them, urge a consolidation exercise to provide an updated version of the methodology that reflects how it has matured over time. This paper represents a milestone in the development of the methodology in that it presents the ANGELIC II Domain Model, along with a description of its constituent parts, and demonstrates its application through a case study in a key evaluation domain. © ICAIL 2023. All rights reserved."
"Despite strong scholarly interest in explainable features in AI (XAI), there is little experimental work to gauge the effect of XAI on human-AI cooperation in legal tasks. We study the effect of textual highlighting as an XAI feature used in tandem with a machine learning (ML) generated summary of a legal complaint. In a randomized controlled study we find that the XAI has no effect on the proportion of time participants devote to different sections of a legal document, but we identify potential signs of XAI’s influence on the reading process. XAI attention-based highlighting may change the spatio-temporal distribution of attention allocation, a result not anticipated by previous studies. Future work on the effect of XAI in legal tasks should measure process as well as outcomes to better gauge the effects of XAI in legal applications. © ICAIL 2023. All rights reserved."
"This paper discusses how the disciplines of Design and Engineering are jointly addressing disability and somehow affecting its very interpretation. The discussion focuses on high-tech prostheses, where robotic devices substitute human body parts. The application of robotic technologies to prosthetics has a relatively long history. Nevertheless, only in the last decade have we witnessed applications reach the market and become available for a large base of users who were offered prostheses with superior motor and sensory performance. The process of bringing ever more advanced technologies to fruition by prosthetic users is fully ongoing today, with some promising solutions coming from robotics (such as, e.g. AI techniques or soft robotics materials) to be transferred to human use. In this transfer process, technology alone is insufficient to warrant success, and the need for a close collaboration between the Engineering domain and the Design disciplines is apparent. We address this point with specific reference to a case study, i.e. the transformation of an innovative but by-now established technology in the industrial robotics field (the ""Pisa/IIT SoftHand"") into a prosthetic hand (the ""SoftHand Pro""). Besides obvious technical considerations about size, connections, control, and so on, which can be addressed with a thorough technical revision of the design, what makes the profound difference between the two devices is that, as a prosthesis, the SoftHand is intended as a human body part, and not as an external tool. To reach its ultimate goals, the hand should become a part of the human user, with his body and mind. The empirical approach and tools of Designers afford the possibility to enrich the re-design process, considering the final user at the centre of the process, in a sort of renewed humanistic approach. The paper reflects this multidisciplinary approach and is structured as follows: the first part describes a cultural framework for the use of high-technology upper limb prostheses. This culture is defined through two significant relations (Users & Society; Users & Device). Inputs come from desk research conducted in different fields, ranging from Social Psychology to Medicine and Rehabilitation area. In this scenario, it is possible to extract design insights applicable to the design brief. The introduction of a robotic prosthetic hand (SoftHand Pro) and a related, single-user case study follow. The aim here is also to illustrate a process where engineering innovations are facilitated by tools from the Design field in the attempt to make the whole process coherently centred on users. Involved are all aspects, from material technology to the covering and finishing of the prosthetic device. The resulting, final prototype of the SoftHand Pro is finally presented.  © 2023 Copyright held by the owner/author(s)."
"Computer Vision is an approach of Artificial Intelligence (AI) that conceptually enables “computers and systems to derive useful information from digital images” giving access to higher-level information and “take actions or make recommendations based on that information”. Comprehensive two-dimensional chromatography gives access to highly detailed, accurate, yet unstructured information on the sample's chemical composition, and makes it possible to exploit the AI concepts at the data processing level (e.g., by Computer Vision) to rationalize raw data explorations. The goal is the understanding of the biological phenomena interrelated to a specific/diagnostic chemical signature. This study introduces a novel workflow for Computer Vision based on pattern recognition algorithms (i.e., combined untargeted and targeted UT fingerprinting) which includes the generation of composite Class Images for representative samples' classes, their effective re-alignment and registration against a comprehensive feature template followed by Augmented Visualization by comparative visual analysis. As an illustrative application, a sample set originated from a Research Project on artisanal butter (from raw sweet cream to ripened butter) is explored, capturing the evolution of volatile components along the production chain and the impact of different microbial cultures on the finished product volatilome. The workflow has significant advantages compared to the classical one-step pairwise comparison process given the ability to realign and pairwise compare both targeted and untargeted chromatographic features belonging to Class Images resembling chemical patterns from many different samples with intrinsic biological variability. © 2023 Elsevier B.V."
"Artificial intelligence holds great promise in medical imaging and machine learning. However, artificial intelligence algorithms cannot completely explain decision-making cognitive processes. This circumstance has raised the explainability, sometimes known as the black box, in challenges of XAI in applications: an algorithm merely answers without explaining why the provided pictures were chosen. Explainable artificial intelligence (XAI) has emerged as a solution to this challenge and has grabbed the interest of many academics. In this review, we share our thoughts on current and future machine learning and possible next steps for the veterinary and animal sciences field. First, we discuss the explainable artificial intelligence in biomedical applications. Following that, we will discuss how AI-powered models may play a more sustainable role in the animal scientific environment. Lastly, we provide recommendations for XAI future perspective in animal field on how to support themselves, the dairy farmers, poultry farmers, and challenges to using XAI in veterinary and animal sciences, considering the opportunities and challenges of XAI in applications. © 2023 River Publishers."
"We consider online optimization with switching costs in a normed vector space (X, ||·||) wherein, at each time t, a decision maker observes a non-convex hitting cost function : T X →[0, ∞] and must decide upon some xtg X→, paying t (xt) + || xt-xt-1||, where ||·|| characterizes the switching cost. Throughout, we assume that t is globally α-polyhedral, i.e., t has a unique minimizer t g X, and, for all x g X, t) (x) ≥ t + α · ||x-t. Moreover, we assume that the decision maker has access to an untrusted prediction xt of the optimal decision during each round, such as the decision suggested by a black-box AI tool. © 2023 Owner/Author."
"Sperm motility is crucial for successful fertilization. Highly decorated doublet microtubules (DMTs) form the sperm tail skeleton, which propels the movement of spermatozoa. Using cryo-electron microscopy (cryo-EM) and artificial intelligence (AI)-based modeling, we determined the structures of mouse and human sperm DMTs and built an atomic model of the 48-nm repeat of the mouse sperm DMT. Our analysis revealed 47 DMT-associated proteins, including 45 microtubule inner proteins (MIPs). We identified 10 sperm-specific MIPs, including seven classes of Tektin5 in the lumen of the A tubule and FAM166 family members that bind the intra-tubulin interfaces. Interestingly, the human sperm DMT lacks some MIPs compared with the mouse sperm DMT. We also discovered variants in 10 distinct MIPs associated with a subtype of asthenozoospermia characterized by impaired sperm motility without evident morphological abnormalities. Our study highlights the conservation and tissue/species specificity of DMTs and expands the genetic spectrum of male infertility. © 2023 Elsevier Inc."
"The study examines the design implications of leveraging generative AI tools such as ChatGPT, Stable Diffusion, Midjourney for literacy development and creative expression for children [6, 8, 18]. We sought to elicit insights on the applicability of generative AI for educational purposes from various stakeholders (i.e., parents, teachers, and AI researchers). We recruited nine participants to elicit their perspectives on designing a visual narrative app with generative AI. We examined the opportunities and limitations of the current generative AI tools. Using the implications from our evaluation, we propose AIStory, an AI-powered visual storytelling application prototype that can be used for children’s creative expression, storytelling, and literacy development. © 2023 Copyright held by the owner/author(s)."
[No abstract available]
The proceedings contain 57 papers. The topics discussed include: developing team design patterns for hybrid intelligence systems; designing closed-loop models for task allocation; human factors in interactive online machine learning; on the interdependence of reliance behavior and accuracy in AI-assisted decision-making; review on the application areas of decision-making algorithms in smart homes; acquiring semantic knowledge for user model updates via human-agent alignment dialogues. an exploratory focus group study; hybrid collective intelligence for decision support in complex open-ended domains; visualizing deep neural networks with topographic activation maps; the equity framework: fairness beyond equalized predictive outcomes; virtual support for real-world movement: using chatbots to overcome barriers to physical activity; and how displaying AI confidence affects reliance and hybrid human-AI performance.
"Designing cooperative AI-systems that do not automate tasks but rather aid human cognition is challenging and requires human-centered design approaches. Here, we introduce AI-aided brainstorming for solving guesstimation problems, i.e. estimating quantities from incomplete information, as a testbed for human-AI interaction with large language models (LLMs). In a think-aloud study, we found that humans decompose guesstimation questions into sub-questions and often replace them with semantically related ones. If they fail to brainstorm related questions, they often get stuck and do not find a solution. Therefore, to support this brainstorming process, we prompted a large language model (GPT-3) with successful replacements from our think-aloud data. In follow-up studies, we tested whether the availability of this tool improves participants' answers. While the tool successfully produced human-like suggestions, participants were reluctant to use it. From our findings, we conclude that for human-AI interaction with LLMs to be successful AI-systems must complement rather than mimic a user's associations. © 2023 The Authors."
[No abstract available]
"This chapter provides a brief overview of the advantages of adopting public cloud to deploy a data science platform. We will begin by discussing the difficulties that a company confronts when adopting a data science platform on-premises and how public cloud can assist them in overcoming them. The examples in this article are intended to illustrate how financial institutions can benefit in terms of agility and financial terms by implementing data science platforms in cloud environments, not to promote any particular cloud service provider. © Editors and Contributors Severally 2023. All rights reserved."
This corrects the article DOI: 10.3779/j.issn.1009-3419.2017.03.01.
"For decades, the whole world's energy production has been based mostly on unsustainable fossil fuels such as coal, oil and gas, resulting in serious issues such as climate change, heightened global tensions, resource depletion, and negative health consequences. As a result, prominent environmental organisations and governments have risen to the challenge, adopting programmes such as the Paris Agreement to implement action plans to lower carbon emissions and battle climate change. Despite the various environmental and performance benefits of currently available commercial electric vehicles, electric vehicles only account for a small portion of the automobile market. Consumer interest in electric vehicles (EVs) is waning due to greater pricing, a restricted driving range, and a lack of supporting infrastructure when compared to conventional internal combustion vehicles. In order to reduce the cost of the EV's battery pack, the greater cost is a vital issue to consider. Machine learning is a low-cost, time-saving way of identifying low-cost, high-performance battery materials and enhancing battery manufacturing efficiency. Artificial intelligence (AI) algorithms and controls can estimate actual driving ranges and optimise energy conservation, allowing for more driving range and lowering customer range anxiety. However, a number of obstacles must be overcome before such benefits can be realised. Because of the limited range and costs associated with charging EV batteries, it is critical to design algorithms that reduce costs while also preventing users from becoming stranded. In today's world, AI algorithms provide simpler and more comfortable solutions. We end our study by outlining our expectations for the discipline in the future, and the research opportunities that remain open to both the corporate and academic sectors. © 2023 River Publishers."
"Large Pre-Trained Models (LLMs) have reached state-of-the-art performance in various Ntural Language Processing (NLP) application tasks. However, an issue remains these models may confidently output incorrect answers, flawed reasoning, or even entirely hallucinate answers. Truly integrating human feedback and corrections is difficult for LLMs, as the traditional approach of fine-tuning is challenging and compute-intensive for LLMs, and the weights for the best models are often not publicly available. However, the ability to interact with these models in natural language opens up new possibilities for Hybrid AI. In this work, we present a very early exploration of Human-Explanations-Enhanced Prompting (HEEP), an approach that aims to help LLMs learn from human annotators' input by storing corrected reasonings and retrieving them on the fly to integrate them into prompts given to the model. Our preliminary results support the idea that HEEP could represent an initial step towards cheap alternatives to fine-tuning and developing human-in-the-loop classification methods at scale, encouraging more efficient interactions between human annotators and LLMs. © 2023 The Authors."
"The primary purpose of this book chapter is to investigate the applications and benefits of big data analytics in fashion retailing to discover the opportunities created by big data in this industry. Fashion has a substantial share of data production, and big data in the fashion industry has created countless challenges and opportunities. Today, the analysis of data obtained from various sources as a critical factor to smart fashion retailing can lead to strategies for better decision-making under conditions and environments of uncertainty. Machine learning algorithms can accelerate the analysis of big data by finding patterns and uncovering trends. By categorizing and recognizing patterns, it can turn incoming data into insights useful for business process operations. This study is based on various secondary sources, which are inherently descriptive and qualitative. This section of the book examines the potentials of big data and machine learning in the fashion retail industry. In the end, it discusses possible ways that big data can be applied in fashion retailing in order to gain insights and identify customer behavior patterns through big data analytics and AI applications and the ethics-related issues in retailing. © Editors and Contributors Severally 2023. All rights reserved."
"In the early days of AI and Law, the use of hypotheticals - imaginary cases constructed to test or explore a particular point - was seen as an important part of legal reasoning. Hypotheticals, have however, attracted increasingly less interest, and have hardly been seen for a decade or more. In this short paper we discuss why hypotheticals disappeared, and the need to reintroduce them if a comprehensive account of reasoning with legal cases is to be produced. The paper includes a discussion of how hypotheticals can be deployed within a recent framework of argumentation schemes describing reasoning with legal cases. © ICAIL 2023. All rights reserved."
"Modern cloud platforms have been employing hardware accelerators such as neural processing units (NPUs) to meet the increasing demand for computing resources for AI-based application services. However, due to the lack of system virtualization support, the current way of using NPUs in cloud platforms suffers from either low resource utilization or poor isolation between multi-tenant application services. In this paper, we investigate the system virtualization techniques for NPUs across the entire software and hardware stack, and present our NPU virtualization solution named NeuCloud. We propose a flexible NPU abstraction named vNPU that allows fine-grained NPU virtualization and resource management. We leverage this abstraction and design the vNPU allocation, mapping, and scheduling policies to maximize the resource utilization, while achieving both performance and security isolation for vNPU instances at runtime.  © 2023 ACM."
"Children experience new forms of harassment in Social Virtual Reality (VR), often inaccessible to parental oversight. We aimed to understand how an artificial intelligent moderator safeguarding children from harassment in social VR is perceived by children and parents, by introducing ""Big Buddy"", a Wizard-of-Oz embodied AI-moderator. 43 children (aged 8-16) played a tower-block-construction game in a simulated Social VR classroom where fictitious competitors disrupted their game and, in experimental conditions where present, Big Buddy intervened. We measured children's perceptions after the disruptions, towards Big Buddy, and the moderation actions it took. Children felt significantly less sad and safer when Big Buddy suspended the saboteur. Parents (n=17) noted Big Buddy's usefulness and felt reassured but would remain in the supervision loop. We present the first empirical research of a VR-embodied AI-moderator with children's and parents' perspectives, and propose design directions for embodied AI-moderators in Social VR. © 2023 ACM."
[No abstract available]
"In this article, we determine modular identities on Ramanujan's product of theta-functions ψ(−q) and f(q) which is similar to ai, j and their explicit evaluations. © 2023 American Institute of Physics Inc.. All rights reserved."
"Empathy for children is critical for designing AI technologies that may affect children. This paper presents the work in progress of a study on the feasibility of a new method to provide objective understanding of people’s empathy for children based on functional near infrared spectroscopy (fNIRS). Adult participants (n=13) were presented with benign or concerning scenarios involving children interacting with AI technologies. Their brain activation patterns were recorded and analyzed. Preliminary data analysis revealed distinctive patterns in the mPFC region, which justifies future work to fully realize the potential of this method. © 2023 Copyright held by the owner/author(s)."
"The use of IoT and AI/ML to extract insights for Data-Driven Decision-Making (DDDM) in Intelligent Traffic Systems (ITS) is becoming increasingly popular. While simulation is a cost-effective and safe way to evaluate such approaches, existing simulators are often impractical due to inefficient control interfaces. In this work, we propose a Discrete-Event, Aggregating, and Relational Control Interfaces (DAR-CI) framework for achieving efficient traffic management simulations through a coupled approach. It enables a non-blocking interaction mode based on a discrete-event synchronization architecture. The overhead caused by data exchange is substantially reduced by supporting the direct retrieval of temporal metrics, data batch processing and customized in-situ aggregation. Combined with flexible, extendable, easy-To-understand, and implementation-friendly semantic specifications, we propose DAR-CI to serve as a universal tool for the traffic simulation community, taking the use and control of traffic simulation to a new level. A proof-of-concept study on the simulation of an adaptive traffic light control system demonstrates a 9.53X speedup compared to TraCI, a widely used protocol for controlling traffic simulators. © 2023 ACM."
"Automated decision making systems take decisions that matter. Some human or legal person remains responsible. Looking back, that person is accountable for the decisions made by the system, and may even be liable in case of damages. That puts constraints on the way in which decision making systems are designed, and how they are deployed in organizations. In this paper, we analyze computational accountability in three steps. First, being accountable is analyzed as a relationship between an actor deploying the system and a critical forum of subjects, users, experts and developers. Second, we discuss system design. In principle, evidence must be collected about the decision rule and the case data that were applied. However, many AI algorithms are not interpretable for humans. Alternatively, internal controls must ensure that a system uses valid algorithms and reliable data sets for training, which are appropriate for the application domain. Third, we discuss the governance model: roles, responsibilities, procedures and infrastructure, to ensure effective operation of these controls. The paper ends with a case study in the IT audit domain, to illustrate practical feasibility. © ICAIL 2023. All rights reserved."
"Toxicology is undergoing a digital revolution, with mobile apps, sensors, artificial intelligence (AI), and machine learning enabling better record-keeping, data analysis, and risk assessment. Additionally, computational toxicology and digital risk assessment have led to more accurate predictions of chemical hazards, reducing the burden of laboratory studies. Blockchain technology is emerging as a promising approach to increase transparency, particularly in the management and processing of genomic data related with food safety. Robotics, smart agriculture, and smart food and feedstock offer new opportunities for collecting, analyzing, and evaluating data, while wearable devices can predict toxicity and monitor health-related issues. The review article focuses on the potential of digital technologies to improve risk assessment and public health in the field of toxicology. By examining key topics such as blockchain technology, smoking toxicology, wearable sensors, and food security, this article provides an overview of how digitalization is influencing toxicology. As well as highlighting future directions for research, this article demonstrates how emerging technologies can enhance risk assessment communication and efficiency. The integration of digital technologies has revolutionized toxicology and has great potential for improving risk assessment and promoting public health. © 2023 The Authors. Published by American Chemical Society."
"The intersection of language and identity refers to the ways in which language use reflects and shapes one’s sense of self and group membership. It is important to design voice-based technologies that represent and support diverse types of identities to increase accessibility. Due to constraints in the data and algorithms used to train and improve artificial intelligence (AI) systems like social robots or voice assistants, they are often unable to represent certain dialects of English and other languages. This could result in data bias, identity exclusion, and communication barriers that can impact the user’s outcomes when using such dialects. Technology companies are discussing how to close this gap by improving training data; however, some researchers believe that the intersection of identity and technology calls for effective collaborative design methodologies that involve intended users. This work-in-progress paper presents preliminary findings on a participatory design method, comic-boarding, that can accomplish the above goal. Using comic-boarding, our intention is to create a space for critical reflection on language and identity in human-human and human-agent dialogue for children from marginalized communities. © 2023 Copyright held by the owner/author(s)."
"This chapter seeks to unpack the intersections between state, commerce, and security via an analysis of the emerging strategic-technological competition between the United States and China. As the 2010s revealed, there is an exceptional yet perturbing convergence in the longer-term fundamental drivers and the short-term recurring ones at the core of bilateral relations. In the technology realm, great power competition between the United States and China is heading toward an increasingly disruptive state of technological decoupling with implications for greater global supply chain fragmentation and volatility. In unpacking this phenomenon, this chapter examines China's military-civil fusion and its employment of national policies such as Made in China 2025 to achieve its technological ambitions. It then looks at America's response, but also its networked ability to disrupt these ambitions and drive a greater bifurcation between China and the West. Overall, the chapter will argue that the intersections of commerce, the state, and power are not just based on economic incentives, but pertain more so to the great power political battle over who will control the technological strategic space, and thereby, the foothold that can be applied toward future strategic purposes. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023."
"Regular monitoring and assessment of water quality is essential to maintain the quality of lake water. A commonly used method for assessing water quality is the Water Quality Index (WQI). Artificial intelligence (AI) can predict the WQI rapidly and more accurately than conventional methods. In this study, a stacking ensemble model based on Deep Learning (DL) was proposed to predict the WQI by integrating three models: Gradient Boosting Machine (GBM), Generalized Linear Model (GLM) and Neural Network (NN). The performance of this model was compared with that of Deep Dense Neural Network (DNN) and Convolutional Neural Network (CNN). The inclusion of a DNN model, which was used for the first time in water pollution research to perform sensitivity and uncertainty analysis in predicting WQI, added a new dimension to the workflow. The mean square error (MSE), mean absolute error (MAE), root mean square error (RMSE) and correlation coefficient (R2) were used to evaluate the performance of the WQI prediction models. The results showed that the WQI of the water samples ranged from 90.75 to 145.29, indicating poor to very poor water quality. The validation results showed that the stack model was superior to all other models in terms of prediction accuracy, with MSE of 25.77, RMSE of 5.07, MAE of 3.5 and R2 of 0.98. The DNN-based sensitivity analysis showed that pH and turbidity significantly affect the WQI and should be monitored to minimise water pollution. The uncertainty analysis indicated that electrical conductivity and total dissolved solids have the highest uncertainty in predicting WQI. This study provides researchers, decision makers and water scientists with accurate information on water quality and enables the implementation of clean production processes to reduce pollution and improve water quality. © 2023 Elsevier Ltd"
[No abstract available]
"Internet of Things (IoT)-based smart factories offer the manufacturing sectors a great opportunity to embrace the fourth industrial revolution (Industry 4.0). The real-time monitoring of manufacturing operations in an Industry 4.0 needs to be ensured by the deployed technologies, like Artificial Intelligence (AI) and Big Data analytics. The overall purpose is to improve the outcomes of the production process. However, Industry 4.0 becomes vulnerable to different potential attacks as the communication takes place via public environments. In this article, an authentication and key agreement method has been suggested to secure the communication that can occur in a Fog-based Industry 4.0 environment. The security proposal provides secure mutual authentication along with key establishment between various smart industrial devices and fog servers, as well as between fog servers and cloud servers. The security analysis and comparative study reveal that the proposed method can mitigate various potential attacks, and it also offers important security and functionality attributes as compared to those for other competing schemes.  © 2023 IEEE."
"As renewable electricity becomes cost competitive with fossil fuel energy sources and environmental concerns increase, the transition to electrified chemical and fuel synthesis pathways becomes increasingly desirable. However, electrochemical systems have traditionally taken many decades to reach commercial scales. Difficulty in scaling up electrochemical synthesis processes comes primarily from difficulty in decoupling and controlling simultaneously the effects of intrinsic kinetics and charge, heat, and mass transport within electrochemical reactors. Tackling this issue efficiently requires a shift in research from an approach based on small datasets, to one where digitalization enables rapid collection and interpretation of large, well-parameterized datasets, using artificial intelligence (AI) and multi-scale modeling. In this perspective, we present an emerging research approach that is inspired by smart manufacturing (SM), to accelerate research, development, and scale-up of electrified chemical manufacturing processes. The value of this approach is demonstrated by its application toward the development of CO2 electrolyzers. © 2023 The Author(s)"
"With the exponential growth of population and diseases analogues medical science has newer challenges with the eruption of every ailment deformity or disease. Medical science has got a safer abode the shape of AI. The present the future of medical is largely dependent on AI. Artificial intelligence enables the medical science turn precise in identification of possible ailments, diagnosis, prognosis and treatment of patient. This study is an attempt to create a clear picture of what AI is and how it works that it stands today what it's going to be and what it all means for the world of Medicine. © 2023 Author(s)."
The proceedings contain 6 papers. The topics discussed include: machine vision based wireless link layer anomaly characterization; SAFR: a real-time communication system with adaptive frame rate; the case for hierarchical deep learning inference at the network; edge-based privacy-sensitive live learning for discovery of training; the effect of network topologies on fully decentralized learning: a preliminary investigation; and improving the quality of federated learning processes via software defined networking.
"Smart Healthcare industry radically shifted from conventional hospitals to AI-based medical assistance that requires more intelligent equipment and fast internet connectivity. This paper briefly discusses the four potential use-cases of healthcare scenarios include tele-surgery, tele-monitoring, tele-consultation, and connected ambulance. It is found these services have diverse service requirements and KPIs (e.g., bandwidth, latency, reliability, and throughput). This article examines various IoT and eHealth laws and regulations from across the world to see how they might help economies and societies achieve long-term growth, as well as possible pathways for future study on IoT-based health care based on a set of open concerns and difficulties. © 2023 Author(s)."
"This updated compendium provides the linear algebra background necessary to understand and develop linear algebra applications in data mining and machine learning. Basic knowledge and advanced new topics (spectral theory, singular values, decomposition techniques for matrices, tensors and multidimensional arrays) are presented together with several applications of linear algebra (k-means clustering, biplots, least square approximations, dimensionality reduction techniques, tensors and multidimensional arrays). The useful reference text includes more than 600 exercises and supplements, many with completed solutions and MATLAB applications. The volume benefits professionals, academics, researchers and graduate students in the fields of pattern recognition/image analysis, AI, machine learning and databases. © 2023 by World Scientific Publishing Co. Pte. Ltd. All rights reserved."
"The cave painting was a creative space we no longer understand. What technologies of today can we use to understand our own concerns of a climate future analogous to the way pre-historic humans drew their conceptions onto the cave? HUMAN ENOUGH uses machine learning-generated visions of climate futures (Stable Diffusion, Midjourney, etc) and climate adaptions / potential solutions (ChatGPT) in a creative space (Gather) to reconstruct a modern analog of the age-old cave painting. It then constructs the machine visions into physical installed objects using recycled, organic, found materials from site-specific builds for physical exhibition. The outcome is a collective imagining of our climate future and our adaptions to it from a technological and material perspective.  © 2023 Owner/Author."
"This paper examines the art practices, artwork, and motivations of prolific users of the latest generation of text-to-image models. Through interviews, observations, and a user survey, we present a sampling of the artistic styles and describe the developed community of practice around generative AI. We find that: 1) artists hold the text prompt and the resulting image can be considered collectively as a form of artistic expression (prompts as art), and 2) prompt templates (prompts with ""slots""for others to fill in with their own words) are developed to create generative art styles. We discover that the value placed by this community on unique outputs leads to artists seeking specialized vocabulary to produce distinctive art pieces (e.g., by reading architectural blogs to find phrases to describe images). We also find that some artists use ""glitches""in the model that can be turned into artistic styles of their own right. From these findings, we outline specific implications for design regarding future prompting and image editing options.  © 2023 Owner/Author."
"Deep Learning Recommendation Models (DLRMs) are very popular in personalized recommendation systems and are a major contributor to the data-center AI cycles. Due to the high computational and memory bandwidth needs of DLRMs, specifically the embedding stage in DLRM inferences, both CPUs and GPUs are used for hosting such workloads. This is primarily because of the heavy irregular memory accesses in the embedding stage of computation that leads to significant stalls in the CPU pipeline. As the model and parameter sizes keep increasing with newer recommendation models, the computational dominance of the embedding stage also grows, thereby, bringing into question the suitability of CPUs for inference. In this paper, we first quantify the cause of irregular accesses and their impact on caches and observe that off-chip memory access is the main contributor to high latency. Therefore, we exploit two well-known techniques: (1) Software prefetching, to hide the memory access latency suffered by the demand loads and (2) Overlapping computation and memory accesses, to reduce CPU stalls via hyperthreading to minimize the overall execution time. We evaluate our work on a single-core and 24-core configuration with the latest recommendation models and recently released production traces. Our integrated techniques speed up the inference by up to 1.59x, and on average by 1.4x. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
"We consider online optimization with switching costs in a normed vector space (X, ||·||) wherein, at each time t, a decision maker observes a non-convex hitting cost function t X →[0, ∞] and must decide upon some xtX→, paying t (xt) + || xt-xt-1||, where ||·|| characterizes the switching cost. Throughout, we assume that t is globally α-polyhedral, i.e., t has a unique minimizer t X, and, for all x X, t) (x) ≥ t + α · ||x - t. Moreover, we assume that the decision maker has access to an untrusted prediction xt of the optimal decision during each round, such as the decision suggested by a black-box AI tool. © 2023 Owner/Author."
"Programming is an essential cross-disciplinary skill, yet teaching it effectively in large classes can be challenging due to the need for close feedback loops. Identifying and addressing common misconceptions is particularly important during the initial stages of learning to program. While automated interactive tutoring systems have the potential to offer personalized tutoring at scale, current systems tend to emphasize errors and predefined solutions rather than focusing on common misconceptions. In this study, we introduce a novel platform centered on addressing misconceptions in programming education. We describe methods for detecting misconceptions using Abstract Syntax Trees (AST) and providing tailored, level-specific feedback to emulate human-like tutoring. As an empirical basis for this project, we gathered data from various introductory programming courses. Additionally, we advocate for the establishment of a repository of common misconceptions, offering examples derived from both the literature and our own data. Investigating misconceptions can ultimately enhance the teaching strategies of both human educators and AI agents, such as GPT, in guiding learners effectively.  © 2023 ACM."
"Transformer model is becoming prevalent in various AI applications with its outstanding performance. However, the high cost of computation and memory footprint make its inference inefficient. We discover that among the three main computation modules in a Transformer model (QKV generation, attention computation, FFN), it is the QKV generation and FFN that contribute to the most power cost. While the attention computation, focused by most previous works, only has decent power share when dealing with extremely long inputs. Therefore, in this paper, we propose FACT, an efficient algorithm-hardware co-design optimizing all three modules of Transformer. We first propose an eager prediction algorithm which predicts the attention matrix before QKV generation. It further detects the unnecessary computation in QKV generation and assigns mixed-precision FFN with the predicted attention, which helps improve the throughput. Further, we propose FACT accelerator to efficiently support eager prediction with three designs. It avoids the large overhead of prediction by using log-based add-only operations for prediction. It eliminates the latency of prediction through an out-of-order scheduler that makes the eager prediction and computation work in full pipeline. It additionally avoids memory access conflict in the mixed-precision FFN with a novel diagonal storage pattern. Experiments on 22 benchmarks show that our FACT improves the throughput of the whole Transformer by 3.59× on the geomean average. It achieves an enviable 47.64× and 278.1× energy saving when computing attention, compared to previous attention-optimization-only SOTA works ELSA and Sanger. Further, FACT achieves an energy efficiency of 4388 GOPS/W performing the whole Transformer layer on average, which is 94.98× higher than Nvidia V100 GPU. © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved."
"Today, Maintaining secure, reliable and secure communication of information between different organizations is very important. However, secure data communications are always vulnerable to intruders and misuse over the Internet and other networks. To achieve this, intrusion detection systems have become a necessary component of computer and computer network security. However, different approaches are used for intrusion detection. Unfortunately, not all previous systems are completely error-free. Therefore, try to improve security. In this course, we will introduce the intrusion detection system ""IDS"". Before applying a genetic algorithm (GA) to efficiently detect different types of network intruders. Use parameters to learn and implement the GA evolution process. Because of the ascent in specialized progressions, there is likewise an unexpected spike in cyber attacks. To safeguard against these dangers, the IDS is a viable technique, however the standard IDS isn't exceptionally astute and strong to hold the client back from experiencing new assaults on an ordinary premise. To improve the classifiers and calculations, AI classifiers and calculations can be utilized. These AI models are extremely useful and can prepare models to recognizetypical traffic and awful traffic. By means of the help of AI, IDS can then perceive inconsistency assaults and stay away from them. With standard IDS, while distinguishing oddity assaults, the bogus positive rate is high, and that implies that it is erroneous, to limit the misleading positive rate ML calculations can be utilized. Additionally, oddity recognition is conceivable involving ML in interruption identification which will give a high exactness of assaults being distinguished. Our proposed paper includes various ML Algorithm to filter and reduce traffic data complicated. © 2023 Author(s)."
"This first workshop on explainable AI for the Arts (XAIxArts) brings together a community of researchers and creative practitioners in Human-Computer Interaction (HCI), Interaction Design, AI, explainable AI (XAI), and Digital Arts to explore the role of XAI for the Arts. XAI is a core concern of Human-Centred AI and relies heavily on HCI techniques to explore how complex and difficult to understand AI models such as deep learning techniques can be made more understandable to people. However, XAI research has primarily focused on work-oriented and task-oriented explanations of AI and there has been little research on XAI for creative domains such as the Arts. This workshop will: i) build an XAIxArts research community; ii) map out the current and future possible landscapes of XAIxArts; iii) critically reflect on the potential of XAI for the Arts, forming the basis for an edited book on XAIxArts and an international network of researchers.  © 2023 ACM."
"Graph neural networks (GNNs) are widely used in many downstream applications, such as graphs and nodes classification, entity resolution, link prediction, and question answering. Several interpretability methods for GNNs have been proposed recently. However, since they have not been thoroughly compared with each other, their trade-offs and efficiency in the context of underlying GNNs and downstream applications are unclear. To support more research in this domain, we develop an end-to-end interactive tool, named gInterpreter, by reimplementing 15 recent GNN interpretability methods in a common environment on top of a number of state-of-the-art GNNs employed for different downstream tasks. This paper demonstrates gInterpreter with an interactive performance profiling of 15 recent GNN interpretability methods, aiming to explain the complex deep learning pipelines over graph-structured data. © 2023 Copyright held by the owner/author(s)."
"Currently, deep-learning-assisted triboelectric nanogenerators (TENGs) have shown great potential for human-computer interaction. The Ecoflex-based polyvinyl-alcohol layer (Ecoflex/PVA) and the Ecoflex-based graphitic carbon nitride layer (Ecoflex/g-C3N4) are sequential spin-coated on the substrate of flexible cotton yarn. The Ecoflex/g-C3N4 and Ecoflex/PVA based silicone composite layer (SCL@(g-C3N4/PVA)) is designed as a high-output artificial skin in TENG. The flexible silicone composite layer plays a crucial role in enhancing output of the TENG. The silicone composite layer-based triboelectric nanogenerators (SCL-TENG) with 10 wt% PVA and 1.6 wt% g-C3N4 yielded the optimal output (720 V, 134 μA, 0.255 mW/cm2) under the pressure of 5 kPa and frequency of 8 Hz. By applying the Internet of Things technology, the single electrode mode SCL-TENG can be integrated into the intelligent sensing system to control and monitor electronic and electrical systems. In addition, the single electrode mode SCL-TENG is capable of sensing and distinguishing the instantaneous mechanical contact generated by balls with different materials, which can be high-accuracy identified with the assistant of deep-learning method of convolutional neural network-gate recurrent unit (CNN-GRU). It shows that the flexible silicone composite layer has great potential in the next generation of AI and intelligent interactive applications. © 2023 Elsevier Ltd"
"This study explores the potential of GPT-3, GPT-3.5, and GPT-4, in generating human-like future scenarios to investigate each model's ability to perceive time. The methodology combines a coding-based experiment and an expert survey. The investigation involves fine- and prompt-tuning GPT-3, prompt-tuning GPT-3.5, and few-shot prompting GPT-4 with human-made future scenarios. The models and output are quantitatively and qualitatively analyzed. The survey invited practitioners from fields of foresight and futurology, AI, and NLP to assess whether differences in output can be identified. This study's findings suggest that GPT-3 and GPT-4 generated scenarios are difficult to distinguish from human-made ones, while GPT-3.5 performed more poorly. Yet none of the models can differentiate time horizons and their respective effects on the future from each other. And while no one knows the shape of things to come, this lack of understanding of a core concept of life invites future investigations.  © 2023 Owner/Author."
"The thermophysical characteristics of nanofluids (which is mixture of the nanoparticles and base fluids) becomes most interesting parts for research communities. The nanofluids can be constructed or developed by the experimental setup which may lead to higherinvestment of time and cost. Recently many researchers are working in the elaboration of AI founded prototype for estimating the thermophysical characteristics of nanofluids. The developed AI based model save time and cost for development of nanofluids with remarkable thermophysical characteristics. This study has presented the systematic reviews of research work carried out over nanofluids and the AI based methods. After the extensive research, it isstated that AI-based method provides excellent results to predict the thermophysical characteristics of the nanofluids. © 2023 Author(s)."
"By leveraging recent generative models, Latent Organism suggests a new method for creating 3D objects through tangible interaction. Our artifact enables anyone to create complex and organic 3D shapes using an understandable and sensitive tactile interface. This process of cooperative creation between humans and machines empowers individuals to develop a craftsmanship of artificial imagination modeling. The machine was fed thousands of images of living beings so as to learn a representation of the concept of organic flesh. Then, one can go around in this endless realm of possibility. By employing the machine's imagination as clay, the subject becomes an active explorer of the latent space.  © 2023 Owner/Author."
"Low-power sensor networks are transforming large-scale sensing in precision farming, livestock tracking, climate-monitoring and surveying. Accurate and robust localization in such low-power sensor nodes has never been as crucial as it is today. This paper presents, Sirius, a self-localization system using a single receiver for low-power IoT nodes. Traditionally, systems have relied on antenna arrays and tight synchronization to estimate angle-of-arrival (AoA) and time-of-flight with known access points. While these techniques work well for regular mobile systems, low-power IoT nodes lack the resources to support these complex systems. Sirius explores the use of gain-pattern reconfigurable antennas with passive envelope detector-based radios to perform AoA estimation without requiring any kind of synchronization. It shows a technique to embed direction specific codes to the received signals which are transparent to regular communication channel but carry AoA information with them. Sirius embeds these direction-specific codes by using reconfigurable antennas and fluctuating the gain pattern of the antenna. Our prototype demonstrates a median error of 7 degrees in AoA estimation and 2.5 meters in localization, which is similar to state-of-the-art antenna array-based systems. Sirius opens up new possibilities for low-power IoT nodes.  © 2023 Owner/Author(s)."
"Drug discovery is the process of introducing a novel drug molecule into medical practice. Drug discovery is a very costly and time-consuming process, and that is why initiatives that contribute to facilitating and accelerating the drug discovery process are of major interest. Artificial intelligence is the investigation of complicated medical data utilizing powerful algorithms and software to replicate human cognition and investigate the relationships between preventive or curative interventions and health outcomes. In recent years, several artificial intelligence (AI) approaches have been effectively used for computer-assisted drug discovery like deep learning (DL), machine learning (ML), and neural networks (NNs). Explainable artificial intelligence (XAI) makes an effort to help researchers comprehend how the model came to a certain conclusion and provide reasons for why the model's response is reasonable. To make the decision-making process transparent, XAI also offers thorough explanations in addition to the mathematical models. In this chapter, we have outlined the most important artificial intelligence approaches that aid in drug discovery. We have discussed the uses, prospects, and limitations of XAI. © 2023 River Publishers."
"This paper introduces Block Data Representations (BDR), a framework for exploring and evaluating a wide spectrum of narrow-precision formats for deep learning. It enables comparison of popular quantization standards, and through BDR, new formats based on shared microexponents (MX) are identified, which outperform other state-of-the-art quantization approaches, including narrow-precision floating-point and block floating-point. MX utilizes multiple levels of quantization scaling with ultra-fine scaling factors based on shared microexponents in the hardware. The effectiveness of MX is demonstrated on real-world models including large-scale generative pretraining and inferencing, and production-scale recommendation systems. © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved."
"This study aims to evaluate deep learning (DL) performance in differentiating low- and high-grade glioma. Search online database for studies continuously published from 1st January 2015 until 16th August 2022. The random-effects model was used for synthesis, based on pooled sensitivity (SE), specificity (SP), and area under the curve (AUC). Heterogeneity was estimated using the Higgins inconsistency index (I2). 33 were ultimately included in the meta-analysis. The overall pooled SE and SP were 94% and 93%, with an AUC of 0.98. There was great heterogeneity in this field. Our evidence-based study shows DL achieves high accuracy in glioma grading. Subgroup analysis reveals several limitations in this field: 1) Diagnostic trials require standard method for data merging for AI; 2) small sample size; 3) poor-quality image preprocessing; 4) not standard algorithm development; 5) not standard data report; 6) different definition of HGG and LGG; and 7) poor extrapolation. © 2023 The Author(s)"
"Importance: Retinopathy of prematurity (ROP) telemedicine screening programs have been found to be effective, but they rely on widefield digital fundus imaging (WDFI) cameras, which are expensive, making them less accessible in low- to middle-income countries. Cheaper, smartphone-based fundus imaging (SBFI) systems have been described, but these have a narrower field of view (FOV) and have not been tested in a real-world, operational telemedicine setting. Objective: To assess the efficacy of SBFI systems compared with WDFI when used by technicians for ROP screening with both artificial intelligence (AI) and human graders. Design, Setting, and Participants: This prospective cross-sectional comparison study took place as a single-center ROP teleophthalmology program in India from January 2021 to April 2022. Premature infants who met normal ROP screening criteria and enrolled in the teleophthalmology screening program were included. Those who had already been treated for ROP were excluded. Exposures: All participants had WDFI images and from 1 of 2 SBFI devices, the Make-In-India (MII) Retcam or Keeler Monocular Indirect Ophthalmoscope (MIO) devices. Two masked readers evaluated zone, stage, plus, and vascular severity scores (VSS, from 1-9) in all images. Smartphone images were then stratified by patient into training (70%), validation (10%), and test (20%) data sets and used to train a ResNet18 deep learning architecture for binary classification of normal vs preplus or plus disease, which was then used for patient-level predictions of referral warranted (RW)- and treatment requiring (TR)-ROP. Main Outcome and Measures: Sensitivity and specificity of detection of RW-ROP, and TR-ROP by both human graders and an AI system and area under the receiver operating characteristic curve (AUC) of grader-assigned VSS. Sensitivity and specificity were compared between the 2 SBFI systems using Pearson χ2testing. Results: A total of 156 infants (312 eyes; mean [SD] gestational age, 33.0 [3.0] weeks; 75 [48%] female) were included with paired examinations. Sensitivity and specificity were not found to be statistically different between the 2 SBFI systems. Human graders were effective with SBFI at detecting TR-ROP with a sensitivity of 100% and specificity of 83.49%. The AUCs with grader-assigned VSS only were 0.95 (95% CI, 0.91-0.99) and 0.96 (95% CI, 0.93-0.99) for RW-ROP and TR-ROP, respectively. For the AI system, the sensitivity of detecting TR-ROP sensitivity was 100% with specificity of 58.6%, and RW-ROP sensitivity was 80.0% with specificity of 59.3%. Conclusions and Relevance: In this cross-sectional study, 2 different SBFI systems used by technicians in an ROP screening program were highly sensitive for TR-ROP. SBFI systems with AI may be a cost-effective method to improve the global capacity for ROP screening.. © 2023 American Medical Association. All rights reserved."
"Expanding the current aquatic herbicide portfolio, reducing total spray volumes, or remotely delivering herbicide using novel spray technologies could improve management opportunities targeting invasive aquatic plants, where options are more limited. However, research on giant salvinia (Salvinia molesta Mitchell) response to foliar herbicide applications at carrier volumes ≤140 L ha-1 is incomplete. Likewise, no data exist documenting S. molesta control with unoccupied aerial application systems (UAAS). Following the recent >100-ha incursion of S. molesta in Gapway Swamp, NC, a case study was developed to provide guidance for ongoing management efforts. In total, three field trials evaluated registered aquatic and experimental herbicides using a 140 L ha-1 carrier volume. Select foliar applications from UAAS were also evaluated. Results at 8 wk after treatment (WAT) indicated the experimental protoporphyrinogen oxidase inhibitor, PPO-699-01 (424 g ai ha-1), in combination with endothall dipotassium salt (2,370 g ae ha-1) provided 78% visual control, whereas control when PPO-699-01 (212 g ai ha-1) was applied alone was lower at 35%. Evaluations also showed diquat (3,136 g ai ha-1) alone, glyphosate (4,539 g ae ha-1) alone, and metsulfuron-methyl (42 g ai ha-1) alone achieved 86% to 94% visual plant control at 8 WAT. Sequential foliar applications of diquat, flumioxazin (210 g ai ha-1), and carfentrazone (67 g ai ha-1) at 6 wk following exposure to in-water fluridone treatments were no longer efficacious by 6 WAT due to plant regrowth. Carfentrazone applications made from a backpack sprayer displayed greater control than applications made with UAAS deploying identical carrier volumes at 2 WAT; however, neither application method provided effective control at 8 WAT. Additional field validation is needed to further guide management direction of S. molesta control using low carrier volume foliar applications. © The Author(s), 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
"Nowadays, learning management systems are widely employed in all educational institutions to instruct students as a result of the increasing in online usage. Today's learning management systems provide learning paths without personalizing them to the characteristics of the learner. Therefore, research these days is concentrated on employing AI-based strategies to personalize the systems. However, there are many different AI algorithms, making it challenging to determine which ones are most suited for taking into account the many different features of learner data and learning contents. This paper conducts a systematic literature review in order to discuss the AI-based methods that are frequently used to identify learner characteristics, organize the learning contents, recommend learning paths, and highlight their advantages and disadvantages.  © 2023 ACM."
"Health care organizations are experiencing a big challenge today because of COVID-19 pandemic. There is a need of advance technology to cope up this threat and challenge faced by world population. Virtual visits and real time scheduling becoming the need of medical practitioners, there is a vast potential for real time analytics to alleviate various challenges in the healthcare industry. Raising convenient alerts through Internet of Medical Thingsand Artificial Intelligence has wide application in preventive care through continuous examination. Be that as it may, wellbeing companies leverage AI and IOT for consultancy by making themselves accessible for far off patients. In this study we are making a planned view on wellbeing specialist by analyzing the issue through causal loop model and improvement of reasonable systems to analyze COVID-19 patients through ongoing investigation. © 2023 Author(s)."
"Autonomous Driving (AD) systems extensively manipulate 3D point clouds for object detection and vehicle localization. Thereby, efficient processing of 3D point clouds is crucial in these systems. In this work we propose K-D Bonsai, a technique to cut down memory usage during radius search, a critical building block of point cloud processing. K-D Bonsai exploits value similarity in the data structure that holds the point cloud (a k-d tree) to compress the data in memory. K-D Bonsai further compresses the data using a reduced floating-point representation, exploiting the physically limited range of point cloud values. For easy integration into nowadays systems, we implement K-D Bonsai through Bonsai-extensions, a small set of new CPU instructions to compress, decompress, and operate on points. To maintain baseline safety levels, we carefully craft the Bonsai-extensions to detect precision loss due to compression, allowing re-computation in full precision to take place if necessary. Therefore, K-D Bonsai reduces data movement, improving performance and energy efficiency, while guaranteeing baseline accuracy and programmability. We evaluate K-D Bonsai over the euclidean cluster task of Autoware.ai, a state-of-the-art software stack for AD. We achieve an average of 9.26% improvement in end-to-end latency, 12.19% in tail latency, and a reduction of 10.84% in energy consumption. Differently from expensive accelerators proposed in related work, K-D Bonsai improves radius search with minimal area increase (0.36%). © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved."
"The Wumpus World is a classical AI problem for a knowledge based agent (a.k.a. logic agent). The existing solutions, mostly implemented in Prolog, are described in propositional and predicate logics. Flora-2 is a vast extension of Prolog that supports many features that simplify logic programming. It complies with first-order logic, frame-logic (a.k.a. F-logic), and HiLog (higher-order-logic) reasoning. Moreover, it allows the full use of existential quantifiers that eases the writing of first-order logic solutions. In this article, a new and concise first-order logic solution, implemented in Flora-2, is demonstrated, tested, and analyzed. © 2023"
"Automatic cars make and model detection and tracking play a significant role in the areas of traffic control and management, law and order, and market research. With advances in camera technology and embedded chips, Computer vision a subset of AI is heavily used for safety and security use cases. This work examines how the YOLOv4-tiny computer vision algorithm can be used to identify and locate Indian car brands and models from real-time video streams. The model has been trained with the latest YOLOv4-tiny algorithm for 21 Indian car brands with 23k images. Input for model prediction will be a 30-sec user recorded video with multiple car models in 25 frames per second, and the output generated is video content with annotations to their make, model, and bounding boxes. Apart from safety and security, the model can be enhanced further for different commercial brands and models. This would unlock enormous business opportunities in the market research and marketing field (e.g.: Shoppable content) to create an ultimate user experience. © 2023 Author(s)."
"As given in Agriculture Material census report 2011, Uttar Pradesh has population of 19.98 crore at that time. Since, in 2001 it was 16.62 crore approximation i.e., the growth rate of population in the previous decade was 20.23%. And the population of U.P. forms 16.50% of India in 2011. It was estimated that by 2050 the world population become 10 billion. So, boosting up agriculture materials production is must be our basic necessity to overcome the situation of hunger. Presently, in Material crop production about 37% of land surface is used. For enhancing National income agricultural material development must be needed. It is also our basic necessity to remove hunger in U.P. by making more material crop production by smart and skill farming technique. Presently, in case of hunger in index U. P. ranked 9th having 14.5% of 24.1 crore population. Implementation of Artificial Intelligence technique in material farming make more crop production and also save our crops. © 2023 Author(s)."
"Background: There are limited US data assessing adherence to surgical antimicrobial prophylaxis guidelines, particularly across a large, nationwide sample. Moreover, commonly prescribed inappropriate antimicrobial prophylaxis regimens remain unknown, hindering improvement initiatives. Methods: We conducted a retrospective cohort study of adults who underwent elective craniotomy, hip replacement, knee replacement, spinal procedure, or hernia repair in 2019-2020 at hospitals in the PINC AI (Premier) Healthcare Database. We evaluated adherence of prophylaxis regimens, with respect to antimicrobial agents endorsed in the American Society of Health-System Pharmacist guidelines, accounting for patient antibiotic allergy and methicillin-resistant Staphylococcus aureus colonization status. We used multivariable logistic regression with random effects by hospital to evaluate associations between patient, procedural, and hospital characteristics and guideline adherence. Results: Across 825 hospitals and 521 091 inpatient elective surgeries, 308 760 (59%) were adherent to prophylaxis guidelines. In adjusted analysis, adherence varied significantly by US Census division (adjusted OR [aOR] range:. 61-1.61) and was significantly lower in 2020 compared with 2019 (aOR:. 92; 95% CI:. 91-.94; P <. 001). The most common reason for nonadherence was unnecessary vancomycin use. In a post hoc analysis, controlling for patient age, comorbidities, other nephrotoxic agent use, and patient and procedure characteristics, patients receiving cefazolin plus vancomycin had 19% higher odds of acute kidney injury (AKI) compared with patients receiving cefazolin alone (aOR: 1.19; 95% CI: 1.11-1.27; P <. 001). Conclusions: Adherence to antimicrobial prophylaxis guidelines remains suboptimal, largely driven by unnecessary vancomycin use, which may increase the risk of AKI. Adherence decreased in the first year of the COVID-19 pandemic.  © 2023 The Author(s)."
"Since aging in agriculture has currently become a problem in many advanced countries worldwide, in response to the growing shortage of agricultural labor, many of these countries have invested in the development of agricultural robots to solve the agricultural labor shortage problem. Automatic equipment to support agricultural harvesting can reduce the labor demand. As a result, this article proposes an artificial intelligence of things (AIoT)-based autonomous mobile robot (AMR) system for pitaya harvesting. The proposed system uses an artificial intelligence (AI) edge computing-based development board (NVIDIA Jetson Nano development board) and combines a 2-D simultaneous localization and mapping (SLAM) algorithm and an AI object recognition module. The SLAM algorithm is used for environmental detection in an unknown environment, and surrounding environment information can be detected by the sensor for map construction to facilitate robot navigation in pitaya orchards. In addition, this article describes an AI object recognition module used for pitaya recognition to facilitate pitaya harvesting. The accuracy of the proposed pitaya recognition model can reach 96.7% on the adopted NVIDIA Jetson Nano development board. A pitaya orchard is simulated in the experimental environment discussed in this article. In the simulated experimental environment, the proposed AMR system can achieve efficient pitaya harvesting and realize intelligent farming.  © 2001-2012 IEEE."
"This comprehensive compendium designs deep neural network models and systems for intelligent analysis of fundus imaging. In response to several blinding fundus diseases such as Retinopathy of Prematurity (ROP), Diabetic Retinopathy (DR) and Macular Edema (ME), different image acquisition devices and fundus image analysis tasks are elaborated. From the actual fundus disease analysis tasks, various deep neural network models and experimental results are constructed and analyzed. For each task, an actual system for clinical application is developed. This useful reference text provides theoretical and experimental reference basis for AI researchers, system engineers of intelligent medicine and ophthalmologists. © 2023 by World Scientific Publishing Co. Pte. Ltd. All rights reserved."
[No abstract available]
"Amid the various technologies being used such as AI, Cloud, IoT etc. widespread, the world of the web has seen a gigantic climb within the advancing and digitizing world. Digitalization of every resource and object has fundamentally become a part of our lives. It has helped to make use of resources in an efficient manner, which provides opportunities in various activities. Metaverse is one such technology that has introduced the digital world to our everyday lives. This technology has involuntarily provided an immersive platform to connect people with technology. The paradigm of the digital world is just beginning to take shape and reality as a Metaverse. It is still emerging, and many key components have started to take shape; it is now a major part of almost every field. MNCs such as Meta (formerly Facebook Inc.), Google, and Electronic Arts have invested huge amounts to set up dedicated infrastructure to improve the existing technology and make it ubiquitous. Several Metaverse worlds have launched their own digital currencies running on Blockchain technology to provide highly secure and seamless trading of digital real estates, avatars, and many more objects. In addition to the subject, it is seen that Immersive Reality (Virtual Reality and Augmented Reality) technologies, such as in the education sector and digital-marketing fields, show interest in this field of Metaverse. When combined with Artificial Intelligence, Metaverse provides a real feel to simulations of aircraft, medical surgeries, and war scenarios to train armed forces. This chapter presents an effort to showcase the ecosystem and the elements that Metaverse consists of. It examines the capabilities and application of the existing virtual worlds. Every tech giant and field has begun to implement Virtual Reality, coming up with positive innovative results. Therefore, we discuss the economical and industrial impact, as also the negative aspects of implementing Metaverse. © 2023 Walter de Gruyter GmbH, Berlin/Boston. All rights reserved."
"SHPL-49 ((2R,3S,4S,5R,6R)-2-(hydroxymethyl)-6-(4-(4-methoxyphenyl) butoxy) tetrahydro-2H-pyran-3,4,5-triol) is a novel glycoside derivative obtained from structural modification of salidroside, which is isolated from the medicinal plant Rhodiola rosea L. SHPL-49 was administered to rats with permanent middle cerebral artery occlusion (pMCAO) for 5 days, and it was found that SHPL-49 could alleviate the cerebral infarct volume and reduce the neurological deficit score. Moreover, the effective time window of SHPL-49 in the pMCAO model was from 0.5 to 8 h after embolization. In addition, the result of immunohistochemistry showed that SHPL-49 could increase the number of neurons in the brain tissue and reduce the occurrence of apoptosis. Morris water maze and Rota-rod experiments showed that SHPL-49 could improve neurological deficits, repair neurocognitive and motor dysfunction, and enhance learning and memory ability in the pMCAO model after 14 days of SHPL-49 treatment. Further in vitro experiments showed that SHPL-49 significantly reduced the calcium overload of PC-12 cells and the production of reactive oxygen species (ROS) induced by oxygen and glucose deprivation (OGD), and increased the levels of antioxidant enzymes superoxide dismutase (SOD) and glutathione peroxidase (GSH-Px), decreased the production of malondialdehyde (MDA). Furthermore, SHPL-49 could reduce cell apoptosis by increasing protein expression ratio of anti-apoptotic factor Bcl-2 to pro-apoptotic factor Bax in vitro. SHPL-49 also regulated the expression of Bcl-2 and Bax in ischemic brain tissue, and even inhibited the caspase cascade of pro-apoptotic proteins Cleaved-caspase 9 and Cleaved-caspase 3. Taken together, SHPL-49 exhibited neuroprotective effects against cerebral ischemic injury through multiple pathways, such as alleviating calcium overload, reducing oxidative stress damage, and inhibiting apoptosis. © 2023"
"Importance: Although race is a social construct, it is associated with variations in skin and retinal pigmentation. Image-based medical artificial intelligence (AI) algorithms that use images of these organs have the potential to learn features associated with self-reported race (SRR), which increases the risk of racially biased performance in diagnostic tasks; understanding whether this information can be removed, without affecting the performance of AI algorithms, is critical in reducing the risk of racial bias in medical AI. Objective: To evaluate whether converting color fundus photographs to retinal vessel maps (RVMs) of infants screened for retinopathy of prematurity (ROP) removes the risk for racial bias. Design, Setting, and Participants: The retinal fundus images (RFIs) of neonates with parent-reported Black or White race were collected for this study. A u-net, a convolutional neural network (CNN) that provides precise segmentation for biomedical images, was used to segment the major arteries and veins in RFIs into grayscale RVMs, which were subsequently thresholded, binarized, and/or skeletonized. CNNs were trained with patients' SRR labels on color RFIs, raw RVMs, and thresholded, binarized, or skeletonized RVMs. Study data were analyzed from July 1 to September 28, 2021. Main Outcomes and Measures: Area under the precision-recall curve (AUC-PR) and area under the receiver operating characteristic curve (AUROC) at both the image and eye level for classification of SRR. Results: A total of 4095 RFIs were collected from 245 neonates with parent-reported Black (94 [38.4%]; mean [SD] age, 27.2 [2.3] weeks; 55 majority sex [58.5%]) or White (151 [61.6%]; mean [SD] age, 27.6 [2.3] weeks, 80 majority sex [53.0%]) race. CNNs inferred SRR from RFIs nearly perfectly (image-level AUC-PR, 0.999; 95% CI, 0.999-1.000; infant-level AUC-PR, 1.000; 95% CI, 0.999-1.000). Raw RVMs were nearly as informative as color RFIs (image-level AUC-PR, 0.938; 95% CI, 0.926-0.950; infant-level AUC-PR, 0.995; 95% CI, 0.992-0.998). Ultimately, CNNs were able to learn whether RFIs or RVMs were from Black or White infants regardless of whether images contained color, vessel segmentation brightness differences were nullified, or vessel segmentation widths were uniform. Conclusions and Relevance: Results of this diagnostic study suggest that it can be very challenging to remove information relevant to SRR from fundus photographs. As a result, AI algorithms trained on fundus photographs have the potential for biased performance in practice, even if based on biomarkers rather than raw images. Regardless of the methodology used for training AI, evaluating performance in relevant subpopulations is critical.. © 2023 American Medical Association. All rights reserved."
"Digitalization in the healthcare industry has resulted in ""smart hospitals""based on Internet of Things (IoT), data analytics, personalised services, and artificial intelligence (AI). Using distributed computing and information sharing, the Internet of Things (IoT) can instantly respond to system demands across a vast distributed network using distributed computing and IoT. I can't believe how fast it's going up! For example, smart gadgets like smartphones and light bulbs can be connected to the Internet using existing network infrastructure to improve patient care while simultaneously enhancing the efficiency of hospital resources. Digitally built IoT applications can access this network. Since it is intelligent, Internet of Things (IoT) technology also has the potential to improve service quality (QoS). If you have all the information you need, you can get an answer faster and with greater accuracy. This network can accept data from a number of sources, process it locally using modest computer power, or process it centrally utilising higher digital computing resources for superior decision-making skills. Predictive analysis, making intelligent recommendations, and looking for patterns are all viable uses for the information in this page. © 2023 AIP Publishing LLC."
"Cryopreservation is very important technique in AI centers of stallions, it preserves sperms for long period and spread the superior genetic merits between different animal’s breeds. However, using of cryopreserved sperms lead to decreasing the fertility between animals, due to lethal damage of sperms during preservation process; so, this study aimed to use the two different freezing media with decreasing post thawing sperm damage. A total of 54 ejaculates were collected from nine pure fertile Egyptian Stallions (6 ejaculates per stallion), individually housed at a Veterinary Clinic in Giza Government. Semen sample was collected by Missouri AV on a regular basis (two collections ⁄ week) during the 2021 breeding season in presence of teaser mare. The collected ejaculates were sent to the laboratory immediately for evaluation by CASA (total concentration, progressive motility, static motility, and sperm abnormalities). Ejaculates were filtrated for removal gel fraction; filtrated ejaculates were diluted by EDTA-glucose media for centrifugation and the resulting sperm pellets were split into 2 equal aliquots and then extended in freezing media. INRA96 Milk-based Extender with glycerol and Egg yolk-based Extender with DMF (dimethylformamide) were used in this study as freezing extenders. Diluted semen was packaged into 0.5 ml straw then cooling at 4 ◦C and freezing by vapor of liquid nitrogen and after that preserved by freezing in liquid nitrogen container at-196⁰ C. After keeping the frozen straws in liquid nitrogen for one week, at least 2 straws were taken for thawing and evaluating post thawing-freezing motility. Finally thawed-frozen semen was inseminated inside fertile mares for calculation the conception rate after one month. Post thawing motility were evaluated in extended semen by two different extenders. The obtained results showed a change in the motility by decreasing in INRA-diluted semen compared to DMF-diluted semen. Conception rate was recorded after insemination and showed a high significant in DMF-diluted semen than INRA diluted semen. We concluded that the frozen semen with DMF based diluent did not decrease the motility of sperms after thawing and achieved high conception rate when compared with INRA based glycerol diluent. © 2011-2023 Journal of Advanced Veterinary Research. All rights reserved."
[No abstract available]
"In a human-AI co-creation, AI not only categorizes, evaluates and interprets data but also generates new content and interacts with humans. As co-creative AI is a form of intelligent technology that directly involves humans, it is critical to anticipate and address ethical issues during all design stages. The open-ended nature of human-AI interactions in co-creation poses many challenges for designing ethical co-creative AI systems. Researchers have been exploring ethical issues associated with autonomous AI in recent years, but ethics in human-AI co-creativity is a relatively new research area. In order to design human-centered ethical AI, it is important to understand the perspectives, expectations, and ethical concerns of potential users. In this paper, we present a study with 18 participants to explore several ethical dilemmas and challenges in human-AI co-creation from the perspective of potential users using a design fiction (DF) methodology. DF is a speculative research method that depicts a new concept or technology through stories as an intangible prototype. We present the findings from the study as potential users' perspectives, stances, and expectations around ethical challenges in human-AI co-creativity as a basis for designing human-centered ethical AI partners for human-AI co-creation.  © 2023 ACM."
"As computer architectures continue to integrate application-specific hardware, it is critical to understand the relative performance of devices for maximum app acceleration. The goal of benchmarking suites, such as MLPerf for analyzing machine learning (ML) hardware performance, is to standardize a fair comparison of different hardware architectures. However, there are many apps that are not well represented by these standards that require different workloads, such as ML models and datasets, to achieve similar goals. Additionally, many apps, like real-time video processing, are focused on latency of computations rather than strictly on throughput. This research analyzes multiple compute architectures that feature ML-specific hardware on a case study of handwritten Chinese character recognition. Specifically, AlexNet and a custom version of GoogLeNet are benchmarked in terms of their streaming latency and maximum throughput for optical character recognition. Considering that these models are composed of fundamental neural network operations yet architecturally different from each other, these models can stress devices in different yet insightful ways that generalizations of the performance of other models can be drawn from. Many devices featuring ML-specific hardware and optimizations are analyzed including Intel and AMD CPUs, Xilinx and Intel FPGAs, NVIDIA GPUs, and Google TPUs. Overall, ML-oriented hardware added to the Intel Xeon CPUs helps to boost throughput by 3.7× and to reduce latency by up to 34.7×, which makes the latency of Intel Xeon CPUs competitive on more parallel models. The TPU devices were limited in terms of throughput due to large data transfer times and not competitive in terms of latency. The FPGA frameworks showcase the lowest latency on the Xilinx Alveo U200 FPGA achieving 0.48 ms on AlexNet using Mipsology Zebra and 0.39 ms on GoogLeNet using Vitis-AI. Through their custom acceleration datapaths coupled with high-performance SRAM, the FPGAs are able to keep critical model data closer to processing elements for lower latency. The massively parallel and high-memory GPU devices with Tensor Core accelerators achieve the best throughput. The NVIDIA Tesla A100 GPU showcases the highest throughput at 42,513 and 52,484 images/second for AlexNet and GoogLeNet, respectively.  © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
"As the largest consumer of fresh water, agricultural irrigation, combined with climate change and increasing human water use activities around the world, has led to a serious water crisis. As a crucial part of irrigation scheduling, the monitoring object of precision irrigation will effectively improve the efficiency and accuracy in agricultural practice. This paper reviews the main monitoring objects used in irrigation scheduling in recent years. From the literature review, the monitoring objects are mainly divided into three categories based on soil moisture, weather and evapotranspiration, and plants, among which the monitoring based on soil moisture is the most common and direct method. At the same time, the development and application of Internet of Things(IoT), Wireless Sensor Network(WSN), Remote Sensing(RS) and Artificial Intelligence(AI) also provide advanced technical means for irrigation monitoring. This review is expected to help researchers and farmers select appropriate irrigation monitoring objects, thereby improving the effective water use efficiency in irrigation scheduling and reducing the water cost for farmers. © 2023 ACM."
"This study aimed to investigate whether diet and different goat production systems affect the quality of milk, white-brined cheese and whey. It also aimed to determine the suitability of goat’s milk for the production of white-brined cheese. The physicochemical composition and hygienic quality of raw goat’s milk were studied, as well as the fatty acid profile and nutritional indices of milk and white-brined cheese. Traditional white-brined cheeses were produced from bulk goat milk from two farms in three samples. The results confirmed that different husbandry systems and diets influence the physicochemical composition of goat milk and cheese and their fatty acid profile. Milk and cheese from grazing goats contained significantly more (p<0.05) fat, protein, total solids and a higher percentage of cheese yield. Nutritional indices were also more favourable, with a lower atherogenic (AI) and thrombogenic (TI) index and a higher health-pro-moting index (HHP) and hypocholesterolemic/hypercholesterolemic ratio (HH). Goat grazing contributed to a higher content of n-3 fatty acids in milk and cheese and thus to a lower n-6/n-3 ratio. Cheese-making efficiency and protein recovery in the curd (%REC) was better with pasture milk, whereas fat recovery was better with milk from housed goats. Consumers rated the unripened white cheeses from both production systems the highest. In addition, the production of white cheese with brine results in good quality whey with significant health benefits. Natural pasture provides significantly more benefits to goats than confinement, especially in the early stages of use. This contributes to the high nutritional value of goat’s milk, cheese and whey. © 2023, Hrvatska Mljekarska Udruga. All rights reserved."
"Background: Multiparametric MRI (mpMRI) improves the detection of aggressive prostate cancer (PCa) subtypes. As cases of active surveillance (AS) increase and tumor progression triggers definitive treatment, we evaluated whether an AI-driven algorithm can detect clinically significant PCa (csPCa) in patients under AS. Methods: Consecutive patients under AS who received mpMRI (PI-RADSv2.1 protocol) and subsequent MR-guided ultrasound fusion (targeted and extensive systematic) biopsy between 2017 and 2020 were retrospectively analyzed. Diagnostic performance of an automated clinically certified AI-driven algorithm was evaluated on both lesion and patient level regarding the detection of csPCa. Results: Analysis of 56 patients resulted in 93 target lesions. Patient level sensitivity and specificity of the AI algorithm was 92.5%/31% for the detection of ISUP ≥ 1 and 96.4%/25% for the detection of ISUP ≥ 2, respectively. The only case of csPCa missed by the AI harbored only 1/47 Gleason 7a core (systematic biopsy; previous and subsequent biopsies rendered non-csPCa). Conclusions: AI-augmented lesion detection and PI-RADS scoring is a robust tool to detect progression to csPCa in patients under AS. Integration in the clinical workflow can serve as reassurance for the reader and streamline reporting, hence improve efficiency and diagnostic confidence. © 2023 The Authors. The Prostate published by Wiley Periodicals LLC."
"Brain-derived neurotrophic factor (BDNF) has a long history in the treatment neurodegenerative of diseases. However, this therapy has limitations in exogenous protein safety, including side effects like neuropathic pain and seizures. Moreover, there are currently no positive clinical trial results using BDNF-based gene therapy. Given this, new methods of delivering BDNF are urgently needed. Here, we report an engineering BDNF mRNA-based therapy in a murine model of Alzheimer's disease (AD). Two poly (β amino esters) polymers (PBAE) were synthesized, which achieved mRNA delivery to brain and spinal cord efficiently following catheter ventricle pumping. To confer stability and RNase resistance, the secondary structure of the mRNA was engineering using AI algorithms. Further mRNA modification was done on 3′ untranslated region (3′UTR), which was added with neuron-specific miRNA targeting sequence to avoid BDNF protein expression in neuron. This allowed reduced neuronal overexcitation and seizures. And BDNF protein was sustained released from astrocytes to maintain surrounding neural function. The engineering mRNA was delivered into the brain ventricle and translated into astrocytes to significantly improve the memory of AD mice. Given the mRNA modifications presented here, it would de-target delivery to specific cell types and has therapeutic potential for the treatment of neurological diseases. © 2023 Elsevier B.V."
"5-Nitrosalicylate 1,2-dioxygenase (5NSDO) is an iron(II)-dependent dioxygenase involved in the aerobic degradation of 5-nitroanthranilic acid by the bacterium Bradyrhizobium sp. It catalyzes the opening of the 5-nitrosalicylate aromatic ring, a key step in the degradation pathway. Besides 5-nitrosalicylate, the enzyme is also active towards 5-chlorosalicylate. The X-ray crystallographic structure of the enzyme was solved at 2.1 Å resolution by molecular replacement using a model from the AI program AlphaFold. The enzyme crystallized in the monoclinic space group P21, with unit-cell parameters a = 50.42, b = 143.17, c = 60.07 Å, β = 107.3°. 5NSDO belongs to the third class of ring-cleaving dioxygenases. Members of this family convert para-diols or hydroxylated aromatic carboxylic acids and belong to the cupin superfamily, which is one of the most functionally diverse protein classes and is named on the basis of a conserved β-barrel fold. 5NSDO is a tetramer composed of four identical subunits, each folded as a monocupin domain. The iron(II) ion in the enzyme active site is coordinated by His96, His98 and His136 and three water molecules with a distorted octahedral geometry. The residues in the active site are poorly conserved compared with other dioxygenases of the third class, such as gentisate 1,2-dioxygenase and salicylate 1,2-dioxygenase. Comparison with these other representatives of the same class and docking of the substrate into the active site of 5NSDO allowed the identification of residues which are crucial for the catalytic mechanism and enzyme selectivity. © 2023 International Union of Crystallography. All rights reserved."
"Recent withdrawal of several drugs from the market due to elevated levels of N-nitrosamine impurities underscores the need for computational approaches to assess the carcinogenicity risk of nitrosamines. However, current approaches are limited because robust animal carcinogenicity data are only available for a few simple nitrosamines, which do not represent the structural diversity of the many possible nitrosamine drug substance related impurities (NDSRIs). In this paper, we present a novel method that uses data on CYP-mediated metabolic hydroxylation of CH2 groups in non-nitrosamine xenobiotics to identify structural features that may also help in predicting the likelihood of metabolic α-carbon hydroxylation in N-nitrosamines. Our approach offers a new avenue for tapping into potentially large experimental data sets on xenobiotic metabolism to improve risk assessment of nitrosamines. As α-carbon hydroxylation is the crucial rate-limiting step in nitrosamine metabolic activation, identifying and quantifying the influence of various structural features on this step can provide valuable insights into their carcinogenic potential. This is especially important considering the scarce information available on factors that affect NDSRI metabolic activation. We have identified hundreds of structural features and calculated their impact on hydroxylation, a significant advancement compared to the limited findings from the small nitrosamine carcinogenicity data set. While relying solely on α-carbon hydroxylation prediction is insufficient for forecasting carcinogenic potency, the identified features can help in the selection of relevant structural analogues in read across studies and assist experts who, after considering other factors such as the reactivity of the resulting electrophilic diazonium species, can establish the acceptable intake (AI) limits for nitrosamine impurities. © 2023 The Author. Published by American Chemical Society."
"Algorithmic rankers are ubiquitously applied in automated decision systems such as hiring, admission, and loan-approval systems. Without appropriate explanations, decision-makers often cannot audit or trust algorithmic rankers' outcomes. In recent years, XAI (explainable AI) methods have focused on classification models, but there for algorithmic rankers, we are yet to develop state-of-the-art explanation methods. Moreover, explanations are also sensitive to changes in data and ranker properties, and decision-makers need transparent model diagnostics for calibrating the degree and impact of ranker sensitivity. To fulfill these needs, we take a dual approach of: i) designing explanations by transforming Shapley values for the simple form of a ranker based on linear weighted summation and ii) designing a human-in-the-loop sensitivity analysis workflow by simulating data whose attributes follow user-specified statistical distributions and correlations. We leverage a visualization interface to validate the transformed Shapley values and draw inferences from them by leveraging multi-factorial simulations, including data distributions, ranker parameters, and rank ranges.  © 2023 ACM."
"Since adhesive membrane fouling is critically determined by the interfacial interaction between a foulant and a rough membrane surface, efficient quantification of the interfacial interaction is critically important for adhesive membrane fouling mitigation. As a current available method, the advanced extended Derjaguin-Landau-Verwey-Overbeek (XDLVO) theory involves complicated rigorous thermodynamic equations and massive amounts of computation, restricting its application. To solve this problem, artificial intelligence (AI) visualization technology was used to analyze the existing literature, and the genetic algorithm back propagation (GABP) artificial neural network (ANN) was employed to simplify thermodynamic calculation. The results showed that GABP ANN with 5 neurons could obtain reliable prediction performance in seconds, versus several hours or even days time-consuming by the advanced XDLVO theory. Moreover, the regression coefficient (R) of GABP reached 0.9999, and the error between the prediction results and the simulation results was less than 0.01%, indicating feasibility of the GABP ANN technique for quantification of interfacial interaction related with adhesive membrane fouling. This work provided a novel strategy to efficiently optimize the thermodynamic prediction of adhesive membrane fouling, beneficial for better understanding and control of adhesive membrane fouling. © 2023 Elsevier Inc."
"AI is rapidly becoming enmeshed in our professional and private lives. The ubiquity of such technologies raises a host of ethical questions, value clashes, and unforeseen consequences that must be confronted. Developments such as Ai-Da and DALL-E 2 are exciting in that they present robust new capabilities in AI and creativity. However, the futures such technologies unlock are also unpredictable. Given the speed with which such technologies are emerging and becoming adopted, the need to engage target audiences to weigh in on possible AI futures is critical. Our pilot project, Artistic Process Futures and AI, seeks to explore the role and potential implications of AI technologies with artists. In this paper, we show how participatory speculative design processes might be channeled into a public statement, or manifesto, regarding possible and preferable AI futures for supporting the artistic process, and how our workshop exposed uncertainty at the core of such deliberation.  © 2023 Owner/Author."
"In this poster, we present a new approach to low-power self-localization for IoT nodes called Sirius. With the rise of low-power sensor networks in precision farming, climate monitoring, and surveying, it has become increasingly critical to accurately and robustly localize low-power sensor nodes. However, traditional systems that rely on antenna arrays and time synchronization are too complex for low-power IoT nodes. To overcome this limitation, Sirius utilizes gain-pattern reconfigurable antennas with passive envelope detector-based radios to estimate angle-of-arrival. This is achieved by embedding direction-specific codes in the received signals, which carry angle-of-arrival information. Our prototype has demonstrated a median error of 7 degrees in AoA estimation and 2.5 meters in localization, comparable to state-of-the-art antenna array-based systems. This new approach opens up exciting possibilities for low-power IoT nodes in various fields.  © 2023 Owner/Author(s)."
"Estimating wastewater treatment plants’ (WWTPs) influent parameters such as 5-day biological oxygen demand (BOD5) and chemical oxygen demand (COD) is vital for optimizing electricity and energy consumption. Against this backdrop, the existing body of knowledge is bereft of a study employing Artificial Intelligence-based techniques for the prediction of BOD5 and COD. Thus, in this study, Gene expression programming (GEP), multilayer perception neural networks, multi-linear regression, k-nearest neighbors, gradient boosting, and regression trees -based models were trained for predicting BOD5 and COD, using monthly data collected from the inflow of 7 WWTPs over a three-year period in Hong Kong. Based on different statistical parameters, GEP provides more accurate estimations, with R2 values of 0.784 and 0.861 for BOD5 and COD respectively. Furthermore, results of sensitivity analysis undertaken by monte Carlo simulation revealed that both BOD5 and COD were mostly affected by concentrations of total suspended solids, and a 10% increase in the value of TSS resulted in a 7.94% and 7.92% increase in the values of BOD5 and COD, respectively. It is seen that the GEP modeling results complied with the fundamental chemistry of the wastewater quality parameters and can be further applied on other sewage sources such as industrial sewage and leachate. The promising results obtained pave the way for forecasting the operational parameters during sludge processing, leading to an extensive energy savings during the wastewater treatment processes. © 2023 The Authors"
"Generally, the success of the optimal sensor placement (OSP) method based on artificial intelligence (AI) highly depends on signals at all feasible placements, which may be unavailable or expensive. Therefore, the acoustic sensor placement optimization method is proposed based on adversarial transfer learning (ATL) and vibro-acoustic simulation. First, the vibro-acoustic simulation is applied to provide sufficient simulation signals for all feasible placements. To bridge the deviation of simulation and measured signals, the ATL-based conditional generative adversarial network (CGAN) is presented, which can transfer signals from limited measured placements to unmeasured placements. In addition, a multi-objective optimization model is proposed to obtain the OSP from three aspects, and it is useful for structural health monitoring (SHM). The acoustic signals obtained from the experimental platform are utilized to explore the feasibility and effectiveness of the proposed method. It can accurately detect the fault with an average accuracy of 98.35% under four working conditions. The comparison investigations demonstrate that the proposed method can obtain high-quality signals at all feasible placements, which can realize SHM with the OSPs and the least sensor cost. © 2001-2012 IEEE."
"We present a framework for the robust optimization of the heat flux distribution for an anti-ice electro-thermal ice protection system (AI-ETIPS) and iced airfoil performance analysis under uncertain conditions. The considered uncertainty regards a lack of knowledge concerning the characteristics of the cloud i.e. the liquid water content and the median volume diameter of water droplets, and the accuracy of measuring devices i.e., the static temperature probe, uncertain parameters are modeled as uniform random variables. A forward uncertainty propagation analysis is carried out using a Monte Carlo approach. The optimization framework relies on a gradient-free algorithm (Mesh Adaptive Direct Search) and three different problem formulations are considered in this work. Two bi-objective deterministic optimizations aim to minimize power consumption and either minimize ice formations or the iced airfoil drag coefficient. A robust optimization formulation was also considered aiming to maximize the statistical frequency of the fully evaporative operating regime for fixed power consumption. The framework is applied to a reference test case, revealing the potential to improve the evaporation efficiency of the baseline design, increasing flight safety even at non-nominal conditions. We also conducted a preliminary examination of the impact of run-back ice formations on airfoil performance during a brief ice encounter in uncertain cloud conditions to understand how the rate of ice accretion relates to an airfoil performance metric, such as the drag coefficient. The analysis found that reducing the rate of ice build-up may not necessarily diminish the detrimental effects on aerodynamic performance, except when the rate is very low. Further studies are ongoing to explore airfoil performance degradation in more detail and to reduce the optimization framework computational cost.  © ICE 2023 "
"This research investigated effective features of including artificial intelligence (AI) in the 8th grade science curriculum in the Sultanate of Oman. It was an endeavor to know the effectiveness of applying AI applications in education and the educational strategies on which the science curriculum is based. Its population included all staff dealing with the 8th grade textbook of public education schools in Oman. Sixty participants formed the study sample, which was randomly selected and an e-questionnaire was designed. The results emphasized the effective role of using AI in increasing the level of performance in learning science. Moreover, there was a moderate improvement in teachers’ performance and a high improvement in the educational strategies. Most importantly, there were no obvious differences at 0.05 in the experts’ agreement concerning the application of AI in the 8th grade science curriculum in the Sultanate of Oman as a function of occupation, with teachers achieving greater scores. This point reflects that teachers are more agreeing concerning the application of AI in the 8th grade science curriculum. © 2023, North American Business Press. All rights reserved."
"In bioinformatics and computational and frameworks science, AI approaches are ordinarily utilized. The improvement of AI techniques for the forecast of protein structures, perhaps the main issues in primary science and bioinformatics, is explored here. Protein structure expectation is a particularly mind boggling issue that it is frequently assaulted and disintegrated using four distinct levels and they are: 1-D forecast of underlying highlights along the essential succession of amino acids sequences,2-D forecast of spatial connections between the sequence of amino acids, 3-D forecast of a tertiary structure of protein and quaternary structure of protein. Throughout the long term, a different assortment of both managed and solo AI approaches have been applied to take care of these issues and have contributed fundamentally to propelling the best in class expectation of protein structure. In this survey paper tries to introduce some assessment tools for finding the accuracy of result from applying ML and DL tools. We try to analyses and compare various algorithms based on deep learning methods verses machine learning methods used for sequence prediction. © 2023 Author(s)."
"Laboratory scale experiments were carried out under static conditions using an acoustic flowmeter to investigate the effect of composition and total solids (TS) content of dairy samples on acoustic transmission (AT) and acoustic impedance (AI). Dairy samples studied included skim milk (4–40% TS), whole milk (4–40% TS), milk protein concentrate samples (4–20% TS) and lactose solutions (4–20% TS). This study demonstrated that AT and AI are influenced by the composition of dairy samples. Linear and non-linear concentrate-specific regression models were developed (R2 > 0.93) using AT to predict the apparent viscosity and AI to predict the TS content of dairy samples. One non-linear model (R2 > 0.99) was sufficient to predict dairy sample density for all concentrates investigated. This study developed a new emerging process analytical technology tool that demonstrated the potential to provide new product and process insights in dairy processing as well as facilitate process efficiencies and consistent final product quality. © 2023 Elsevier Ltd"
"Existing artificial skin interfaces lack on-skin AI compute that can provide fast neural network inference for time-critical applications. In this paper, we propose AI-on-skin-a wearable artificial skin interface integrated with a neural network hardware accelerator that can be reconfigured to run diverse neural network models and applications. AI-on-skin is designed to scale to the entire body, comprising tiny, low-power, accelerators distributed across the body. We built 7 AI-on-Skin application prototypes and our user trials show AI-On-Skin achieving 20X and 50X speedup over off-body inference via Bluetooth and on-body centralized microprocessor based inference approach respectively. We also project the power performance of AI-on-skin with our accelerator fabricated as silicon chips instead of emulated on FPGAs and show 10X further power savings. To the best of our knowledge, AI-on-Skin is the first ever wearable prototype to demonstrate skin interfaces with on-body AI inference.  © 2023 Owner/Author."
"Voice plays a crucial role in our daily lives, enabling communication, conveying emotions, and indicating our health. As a result, tracking vocal interactions can provide valuable insights into various aspects of our lives. This poster presents our preliminary work for a novel voice tracker (VoCopilot) that effectively tracks various vocal interactions. For example, the VoCopilot tracker can help document meetings and generate notes, even when participants speak different languages. Additionally, it can serve as a life-logger, monitoring daily conversations and extracting key points to summarize their content. Central to VoCopilot's design is an energy-efficient, co-developed acoustic hardware and firmware combined with a comprehensive integration of VoCopilot tracker with advanced machine learning systems. This harmonious integration ensures precise voice transcription, summarization, and analysis. We present our early thoughts on VoCopilot hardware design and share early results of utilizing Whisper for efficient multilingual transcribing. We acknowledge VoCopilot may raise privacy issues; therefore, we provide early thoughts to address these concerns.  © 2023 Owner/Author(s)."
"Creativity is an important milestone for innovation. Investing in new ideas is fundamental and promising for innovation in several sectors. Most European citizens feel that they need to do more to help the planet's sustainability, and, as a result, human resources are exposed to various pressures with incalculable speed. Because of this, it is necessary to find new creative and effective solutions that change consumerist behaviors. Therefore, this Ph.D. project aims to study and develop a Creativity Support Tool (CST) to support creative workplace workers in obtaining new and useful ideas for reducing consumerist actions. The theoretical results contribute to the scientific literature by expanding relevant theories, and the practical contribution resides in the applicability of a CST in the environmental context.  © 2023 Owner/Author."
"Background Large language models such as ChatGPT have demonstrated potential as innovative tools for medical education and practice, with studies showing their ability to perform at or near the passing threshold in general medical examinations and standardised admission tests. However, no studies have assessed their performance in the UK medical education context, particularly at a specialty level, and specifically in the field of neurology and neuroscience. Methods We evaluated the performance of ChatGPT in higher specialty training for neurology and neuroscience using 69 questions from the Pool - Specialty Certificate Examination (SCE) Neurology Web Questions bank. The dataset primarily focused on neurology (80%). The questions spanned subtopics such as symptoms and signs, diagnosis, interpretation and management with some questions addressing specific patient populations. The performance of ChatGPT 3.5 Legacy, ChatGPT 3.5 Default and ChatGPT-4 models was evaluated and compared. Results ChatGPT 3.5 Legacy and ChatGPT 3.5 Default displayed overall accuracies of 42% and 57%, respectively, falling short of the passing threshold of 58% for the 2022 SCE neurology examination. ChatGPT-4, on the other hand, achieved the highest accuracy of 64%, surpassing the passing threshold and outperforming its predecessors across disciplines and subtopics. Conclusions The advancements in ChatGPT-4's performance compared with its predecessors demonstrate the potential for artificial intelligence (AI) models in specialised medical education and practice. However, our findings also highlight the need for ongoing development and collaboration between AI developers and medical experts to ensure the models' relevance and reliability in the rapidly evolving field of medicine.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ."
"This article deals with the digitalization of management accounting / controlling in insurance companies, which goes hand in hand with the digital transformation of the insurance industry by Big Data, Artificial Intelligence (AI) and Blockchain. The insurance business requires an industry-specific design of the controlling instruments, but not of the controlling concept itself. Managing insurance as a service in a value- and risk-oriented way requires cost transparency, e.g. through contribution margin calculations. Risks, on the other hand, can only be understood from a balance sheet perspective, e.g. through internal models. These interdisciplinary fields of application of controlling are undergoing digitalization. In addition, there are new market developments such as telematics tariffs, in which the digitalization of controlling is essential in order to address the limits of insurability. The fields of application result in new competence profiles in distinction to actuaries and data scientists. © The Author(s), under exclusive license to Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2023. All rights reserved."
"What expectations exist in the minds of dancers when interacting with a generative machine learning model? During two workshop events, experienced dancers explore these expectations through improvisation and role-play, embodying an imagined AI-dancer. The dancers explored how intuited flow, shared images, and the concept of a human replica might work in their imagined AI-human interaction. Our findings challenge existing assumptions about what is desired from generative models of dance, such as expectations of realism, and how such systems should be evaluated. We further advocate that such models should celebrate non-human artefacts, focus on the potential for serendipitous moments of discovery, and that dance practitioners should be included in their development. Our concrete suggestions show how our findings can be adapted into the development of improved generative and interactive machine learning models for dancers' creative practice.  © 2023 Owner/Author."
"The fast development of mobile communication and artificial intelligence (AI) technologies greatly promotes the prosperity of the Internet of Things (IoT), where various types of IoT devices can perform more intelligent tasks. Considering the privacy leakage and limited communication resources, federated learning (FL) has emerged to enable devices to collaboratively train AI models based on their local data without raw data exchanges. Nevertheless, it is still challenging for guaranteeing any FL models to be effective due to the sluggish willingness of IoT devices and the model poisoning attacks in the FL. To address these issues, in this article, we introduce blockchain technology and propose a blockchain-based FL framework for supporting a trustworthy and reliable FL paradigm in IoT. In the proposed framework, we design a committee-based participant selection mechanism that selects the aggregate node and local model updates dynamically to construct the global model. Moreover, considering the tradeoff between the energy consumption and the convergence rate of the FL model, we perform the channel allocation, block size adjustment, and block producer selection jointly. Since the remaining resources, handling transactions, and channel conditions are dynamically varying (i.e., stochastic environment), we formulate the problem as a Markov decision process (MDP) and adopt a deep reinforcement learning (DRL)-based algorithm to solve it. The simulation results demonstrate the effectiveness of the proposed framework and show the superior performance of the DRL-based resource allocation algorithm compared with other baseline methods in terms of energy consumption. © 2014 IEEE."
"What generative AI futures do we want - and what futures do we not want? To imagine what might exist in the future, we apply speculative design to explore plausible scenarios for generative AI and human coexistence. In this paper, we present gAIrden and Onion AI: two in-progress speculative concepts of future generative AI tools, their use cases, and the systems in which they exist. We analyze the designs through lenses of Environment, Data Privacy, Embodiment, and Play. This trip into the future is driven by the research question: how might generative AI tools change how we produce creativity and culture? When we return to the present, we ask ourselves, how might generative AI support positive outcomes for individuals and communities? Can we predict (and potentially mitigate) negative consequences of generative AI tools? The speculative designs purposefully engage viewers in futures thinking to reclaim conversation around the future of technology.  © 2023 Owner/Author."
The proceedings contain 32 papers. The topics discussed include: reflections on training next-gen industry workforce on secure software development; the gap between higher education and the software industry – a case study on technology differences; using automatic program assessment in a software development project course; using learning analytics to identify student learning profiles for software development courses; learning analytics dashboard for educators: proposed project to design with pedagogical background; towards learning style prediction based on personality; adaptive learning path sequencing based on learning styles within n-dimensional spaces; learning style classification by using Bayesian networks based on the index of learning style; systematic literature review for the use of AI based techniques in adaptive learning management systems; and flipped teaching in software engineering education: results of a long-term study.
"Recent advances in the performance of machine learning algorithms have led to the adoption of AI models in decision making contexts across various domains such as healthcare, finance, and education.Different research communities have attempted to optimize and evaluate human-AI team performance through empirical studies by increasing transparency of AI systems, or providing explanations to aid human understanding of such systems.However, the variety in decision making tasks considered and their operationalization in prior empirical work, has led to an opacity around how findings from one task or domain carry forward to another.The lack of a standardized means of considering task attributes prevents straightforward comparisons across decision tasks, thereby limiting the generalizability of findings.We argue that the lens of ‘task complexity’ can be used to tackle this problem of under-specification and facilitate comparison across empirical research in this area.To retrospectively explore how different HCI communities have considered the influence of task complexity in designing experiments in the realm of human-AI decision making, we survey literature and provide an overview of empirical studies on this topic.We found a serious dearth in the consideration of task complexity across various studies in this realm of research.Inspired by Robert Wood’s seminal work on the construct, we operationalized task complexity with respect to three dimensions (component, coordinative, and dynamic) and quantified the complexity of decision tasks in existing work accordingly.We then summarized current trends and proposed research directions for the future.Our study highlights the need to account for task complexity as an important design choice.This is a first step to help the scientific community in drawing meaningful comparisons across empirical studies in human-AI decision making and to provide opportunities to generalize findings across diverse domains and experimental settings. © 2023 Copyright held by the owner/author(s)."
"The Facial expression recognition is the part of Facial recognition which is gaining more importance and the need for it increases tremendously. Though there are methods to identify expressions using machine learning and Artificial Intelligence (AI) techniques, this work attempts to use deep learning method to recognize expressions and classify the expressions according to the trained facial expressions from the dataset. Multi-modal Emotion Recognition (MER) model is relatively a new discipline which aims to include text inputs, as well as sound and video for emotion recognition. Convolution Neural Network (CNN) techniques are used to detect emotions from faces, videos, audio and text. CNN is used with deep architectures to detect faces and their emotions. Long Short Term Memory (LSTM) technique is used in audio and text documents for emotion classification. The proposed MER model gives accuracy of 85% for real time speaking videos. This can be used for business improvement by introducing intelligence in the marketing or sales domains. © 2023 Author(s)."
"Real-Time Communication (RTC) systems tend to drop some frames during transmission to cope with network congestion. Though frame-dropping is helpful to low-delay performance, it may cause sudden increase in frame loss and noticeable Quality of Experience (QoE) degradation. In this paper, we propose a novel RTC system, SAFR (System of Adaptive Frame Rate), to jointly optimize all the key indicators of QoE performance. Firstly, a Frame Rate Controller (FRC) is designed for making reasonable frame-rate alteration decisions. Then, a reinforcement learning-based bandwidth prediction module, named Target Rate Controller (TRC), is established to improve the bandwidth utilization by cooperating with FRC. Finally, an AI-based frame interpolation module is applied on the receiver side to mitigate the frame-rate degradation. Compared to traditional RTC systems, SAFR achieves 16.3%-26.7% frame-delay reductions, 61.8%-69.9% frame loss rate decreases, 6.4%-11.3% QoE improvements, with up to 25.3% savings of bandwidth at the same time. Especially, SAFR shows greater superiority under worse network conditions. The supplementary materials[10] further discuss the settings of hyper-parameters and pseudo-codes of FRC. Our test demos are available at: https://github.com/xiaosayin/SAFR. © 2023 ACM."
"Modern cloud platforms have deployed neural processing units (NPUs) like Google Cloud TPUs to accelerate online machine learning (ML) inference services. To improve the resource utilization of NPUs, they allow multiple ML applications to share the same NPU, and developed both time-multiplexed and preemptive-based sharing mechanisms. However, our study with real-world NPUs discloses that these approaches suffer from surprisingly low utilization, due to the lack of support for fine-grained hardware resource sharing in the NPU. Specifically, its separate systolic array and vector unit cannot be fully utilized at the same time, which requires fundamental hardware assistance for supporting multi-tenancy. In this paper, we present V10, a hardware-assisted NPU multi-tenancy framework for improving resource utilization, while ensuring fairness for different ML services. We rethink the NPU architecture for supporting multi-tenancy. V10 employs an operator scheduler for enabling concurrent operator executions on the systolic array and the vector unit and offers flexibility for enforcing different priority-based resource-sharing mechanisms. V10 also enables fine-grained operator preemption and lightweight context switch in the NPU. To further improve NPU utilization, V10 also develops a clustering-based workload collocation mechanism for identifying the best-matching ML services on a shared NPU. We implement V10 with an NPU simulator. Our experiments with various ML workloads from MLPerf AI Benchmarks demonstrate that V10 can improve the overall NPU utilization by 1.64×, increase the aggregated throughput by 1.57×, reduce the average latency of ML services by 1.56×, and tail latency by 1.74× on average, in comparison with state-of-the-art NPU multi-tenancy approaches. © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved."
"Artificial intelligence (AI) drives the creation of future technologies that disrupt the way humans live and work, creating new solutions that change the way we approach tasks and activities, but it requires a lot of data processing, large amounts of data transfer, and computing speed. It has led to a growing interest of research in developing a new type of computing platform which is inspired by the architecture of the brain specifically those that exploit the benefits offered by photonic technologies, fast, low-power, and larger bandwidth. Here, a new computing platform based on the photonic reservoir computing architecture exploiting the non-linear wave-optical dynamics of the stimulated Brillouin scattering is reported. The kernel of the new photonic reservoir computing system is constructed of an entirely passive optical system. Moreover, it is readily suited for use in conjunction with high performance optical multiplexing techniques to enable real-time artificial intelligence. Here, a methodology to optimise the operational condition of the new photonic reservoir computing is described which is found to be strongly dependent on the dynamics of the stimulated Brillouin scattering system. The new architecture described here offers a new way of realising AI-hardware which highlight the application of photonics for AI. © 2023 OSA - The Optical Society. All rights reserved."
"Homogenized macroscopic cross-sections (XS) are necessary for running core-wise nodal diffusion calculations. XS sets are usually generated using time-consuming lattice physics codes. In this study, a pre-trained artificial neural network was developed and used for XS generation. The model was trained to produce macroscopic XS, pin powers, and assembly discontinuity factors for 16 × 16 and 17 × 17 fuel assembly types with independent variable enrichments of each fuel pin loaded with fresh UO2 fuel without burnable poisons. The training dataset optimization method was described and used for defining the required number of variations in input parameters, such as pin arrangements and thermal hydraulics parameters. The optimized dataset's generation took only 248 core-hours, which is below 3 days on a modern 4-core CPU. For the worst-case out-of-range testing data, the maximum observed difference with the reference was found below 3% for pin powers, and below 4.5% for XS values. © 2023"
"The next step for intelligent dialog agents is to escape their role as silent bystanders and become proactive.Well-defined proactive behavior may improve human-machine cooperation, as the agent takes a more active role during interaction and takes off responsibility from the user.However, proactivity is a double-edged sword because poorly executed pre-emptive actions may have a devastating effect on the task outcome and the relationship with the user.For designing adequate proactive dialog strategies, we propose a novel approach including both social and task-relevant features in the dialog.Here, the primary goal is to optimize proactive behavior so that it is task-oriented -this implies high task success and efficiency -while also being socially effective by fostering user trust.Including both aspects in the reward function for training a proactive dialog agent using reinforcement learning showed the benefit of our approach for more successful human-machine cooperation. © 2023 Copyright held by the owner/author(s).Publication rights licensed to ACM."
"This study investigates the transformation of energy models to align with machine learning requirements as a promising tool for optimizing the operation of combined cycle power plants (CCPPs). By modeling energy production as a function of environmental and control variables, this methodology offers an innovative way to achieve energy-efficient power generation in the context of the data-driven application. This study focuses on developing a thorough AI-coherent modeling approach for CCPP optimization, preferring an interdisciplinary perspective and coming up with a comprehensive, insightful analysis. The proposed numerical model using Broyden Fletcher Goldfarb Shanno (BFGS) algorithm enhances efficiency by simulating various operating scenarios and adjusting optimal parameters, leading to a high yield power generation of 2.23 % increase from 452 MW to 462.1 MW by optimizing the environmental factors. This study deals with data-driven modeling based on historical data to make predictions without prior knowledge of the system's parameter, demonstrating several merits in identifying patterns that can be difficult for human analysts to detect, high accuracy when trained on large datasets, and the potential to improve over time with new data. The proposed modeling approach and methodology can be expanded as a valuable tool for forecasting and decision-making in complex energy systems. © 2023 Elsevier Ltd"
"Adversarial attack has cast a shadow on the massive success of deep neural networks. Despite being almost visually identical to the clean data, the adversarial images can fool deep neural networks into the wrong predictions with very high confidence. Adversarial training, as the most prevailing defense technique, suffers from class-wise unfairness and model-dependent challenges. In this paper, we propose to detect and eliminate adversarial data in databases prior to data processing in supporting robust and secure AI workloads. We empirically show that we can build a binary classifier separating the adversarial apart from the clean data with high accuracy. We also show that the binary classifier is robust to a second-round adversarial attack. In other words, it is difficult to disguise adversarial samples to bypass the binary classifier. Furthermore, we empirically investigate the generalization limitation which lingers on all current defensive methods, including the binary classifier approach. And we hypothesize that this is the result of the intrinsic property of adversarial crafting algorithms. Our experiments ascertain that adversarial and clean data are two different datasets that can be separated with a binary classifier, which can serve as a portable component to detect and eliminate adversarial data in an end-to-end data management pipeline. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM."
"Artificial Intelligence (AI) is a broad conceptual subject of computer engineering which aims to duplicate, evaluate, or improve the decision-making system of humans. Artificial Intelligence and Machine Learning have had a significant impact on nearly every aspect of modern life, including entertainment, commerce, and healthcare. Artificial Intelligence is ready to assist individuals with a variety of activities ranging from the organizational to the medical record keeping to patient engagement, as well as specialised help such as image analysis for any report. Automating medical devices, as well as monitoring the patient's status. However, it is widely assumed that AI technologies may be used to facilitate and enhance human labour, but that they cannot replace the work of physiotherapists. In the sphere of medicine, Artificial Intelligence and Machine Learning systems have greatly enhanced health care by significantly delivering earlier disease diagnoses or recommending ideally personalised treatment approaches. With the advancement of AI technology, the concept of smart healthcare has risen to the fore, with the potential to completely revolutionise the traditional medical system, making it more efficient, convenient, and individualized. The key technologies that support smart health care will be examined first in this review article, followed by an evaluation of the current state of smart health care that incorporates Artificial Intelligence in a range of sectors. Then, in order to strengthen the concept of smart health care, it'll go over the challenges that exist in the smart health care system and try to come up with a strategy to fix them. Finally, this paper consider and assess the extent of this smart health-care concept. © 2023 Author(s)."
"Artificial Intelligence (AI) and machine learning may be used to automate many manual activities in the insurance industry. Artificial Intelligence (AI) developments have enabled insurance companies to provide quicker services, ensuring consumer satisfaction. As a consequence, interest in Artificial Intelligence (AI) insurance has doubled since 2012, according to Google Trends. Insurance firms may apply a variety of Artificial Intelligence (AI), including chatbots, and affective computing and document processing. They may utilize these technologies to reduce costs and improve performance by combining experience in tasks like claims and applications processing, customized insurance pricing, and fraud detection. Between 2012 and 2019, there has been a surge in interest in Artificial Intelligence (AI)-based insurance applications. However, by 2020, it appears that COVID-19 has piqued broad attention while most other issues have waned, as we see with insurance Artificial Intelligence (AI). Other industries or corporate operations like healthcare, sales, and marketing, on the other hand, have shown greater interest during COVID-19. Machine learning, natural language processing, and deep learning are examples of artificial intelligence technologies that have lately gained popularity in the global insurance industry. As a result, the number of international insurance businesses that have achieved success in InsureTech and platform business based on artificial intelligence technology is growing. The aim of this study is to better understand Artificial Intelligence (AI) use cases in the insurance sector and market penetration of Artificial Intelligence (AI) should be reviewed to the extent in insurance services to see how it may help the insurance sector solve challenges and improve customer happiness. © 2023 Author(s)."
"Alzheimer's Disease (AD) and related dementia are a growing global health challenge due to the aging population. The prominence of mobile devices and recent breakthroughs in machine learning have enabled an emerging class of new AI-powered health systems for applications like Alzheimer's Disease monitoring. In this paper, we present the first end-to-end system that integrates multi-modal sensors and federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. We recognize several major challenges in designing such a real-world federated learning system, including limited data labels, data heterogeneity, and limited computing resources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 61 elderly participants. The results indicate that our system can accurately detect a comprehensive set of digital biomarkers with up to 95% accuracy and identify AD with an average of 87.5% accuracy.  © 2023 Owner/Author(s)."
"COVID-19 is a respiratory illness caused by the SARS-CoV-2 (Corona Virus), which first appeared in December of the year 2019. Covid-19 evolved from Corona Virus is a severe acute respiratory illness. Wuhan, Hubei, China and resulted in an ongoing pandemic. COVID-19, which was triggered by the SARS-CoV-2 virus, has rapid global dissemination, leading to worldwide flare ups. To lessen it's spread and safe communication it is necessary to develop speedy and contactless solutions in combat against it. The current gold standard for establishing its identification is RT-PCR Tests. This method of testing, however, is costly, time-consuming, and exceeds the social distance. Likewise, as pandemic is required to remain for some time, there is need for another analysis apparatus which beats these impediments, and is deployable at an enormous scope. The essential symptoms for this virus include cough and breathing challenges. Using AI and AI methods we can analyze respiratory sound which can give helpful insights, enabling the design of diagnostic tools. This survey uses AI based ML and DL approaches to offer a thorough overview of the COVID-19 diagnosis and therapy by analysing breathing patterns. © 2023 Author(s)."
"Remote working causes issues for data room management in the Information Technology (IT) and Information Technology Enabled Services (ITES) industries worldwide. The purpose of this study is to learn how Corporate Real Estate (CRE) can be used to augment and expand existing spaces, as well as to find solutions that can be implemented without the intervention of a human, as well as how systems can be operated, discovered, and enabled in a remote working environment. The proliferation of wireless, graphical user interface (GUI), artificial intelligence (AI), and ubiquitous technologies has created an abundance of opportunities for engineers to create and experiment with experiences that encourage thought. At the conclusion of each day, the survey reveals the fundamental strengths and shortcomings of remote operating procedures, key performance indicators, and, most importantly, the happiness of an engineer or system administrator. New findings can aid in the creation of those aspects of 'physical' and 'digital' environments that are exploited in several ways, such as tool selection, space analysis methodology, and an approach for developing digitally augmented physical spaces. Allowing engineers to interact with the digital environment, we argue, will result in the emergence of augmented digital information that can then be interacted with to provide vital metrics. The research focuses on how the future of Corporate Real Estate may be enhanced for retailers in terms of marketing and future research to better understand the complexities of tools and technology. © 2023 Author(s)."
"This paper improves the object detection accuracy for detecting objects in complex scenes and ensures realtime classification operations by planning a novel detection method called lightweight and efficient hybrid YOLOv4 model. In this context, Computational vision is one of the most useful and entertaining forms of artificial intelligence (AI) used in everyday life. Computer vision study focused on replacing intricate aspects of the human world with sophisticated AI and computers. Deep neural networks have recently become an essential part of several industries due to their renowned ability to handle visual input. One of the main directions that computer vision has taken is the domain of classification & tracking of objects employing neural networks, which are presently being employed by relevant trendsetting enterprises specializing in solving several arrays of predicaments such as security, health care, and agriculture. The main factors affecting the development of computer vision are the volume of data it generates, as well as the amount it utilizes to train and enhance it. In this paper, a method for categorizing and detecting objects utilizing an object detection algorithm namely hybrid-YOLOv4 is proposed. Convolutional neural networks provide extremely accurate object tracking and feature extraction out of the images. Strategies such as Bagof-Specials and Bag-of-Freebies are used in item identification and DarkNet is used in the backbone that increases the feature exchange and reutilization. Thus, the improved network design maximizes both identification accuracy and speed. Additionally, two new extra blocks in the neck and backbone enhance feature extraction and reduce processing expenses. The model was compared with other object detections methods. According to the experimental findings, mean average precision (MPS) of YOLOv4-hybrid model was found to be 0.986 better than that of YOLOv4 and other object detection models. © 2023 Little Lion Scientific."
"Importance: ChatGPT is an artificial intelligence (AI) chatbot that has significant societal implications. Training curricula using AI are being developed in medicine, and the performance of chatbots in ophthalmology has not been characterized. Objective: To assess the performance of ChatGPT in answering practice questions for board certification in ophthalmology. Design, Setting, and Participants: This cross-sectional study used a consecutive sample of text-based multiple-choice questions provided by the OphthoQuestions practice question bank for board certification examination preparation. Of 166 available multiple-choice questions, 125 (75%) were text-based. Exposures: ChatGPT answered questions from January 9 to 16, 2023, and on February 17, 2023. Main Outcomes and Measures: Our primary outcome was the number of board certification examination practice questions that ChatGPT answered correctly. Our secondary outcomes were the proportion of questions for which ChatGPT provided additional explanations, the mean length of questions and responses provided by ChatGPT, the performance of ChatGPT in answering questions without multiple-choice options, and changes in performance over time. Results: In January 2023, ChatGPT correctly answered 58 of 125 questions (46%). ChatGPT's performance was the best in the category general medicine (11/14; 79%) and poorest in retina and vitreous (0%). The proportion of questions for which ChatGPT provided additional explanations was similar between questions answered correctly and incorrectly (difference, 5.82%; 95% CI, -11.0% to 22.0%; χ21= 0.45; P =.51). The mean length of questions was similar between questions answered correctly and incorrectly (difference, 21.4 characters; SE, 36.8; 95% CI, -51.4 to 94.3; t = 0.58; df = 123; P =.22). The mean length of responses was similar between questions answered correctly and incorrectly (difference, -80.0 characters; SE, 65.4; 95% CI, -209.5 to 49.5; t = -1.22; df = 123; P =.22). ChatGPT selected the same multiple-choice response as the most common answer provided by ophthalmology trainees on OphthoQuestions 44% of the time. In February 2023, ChatGPT provided a correct response to 73 of 125 multiple-choice questions (58%) and 42 of 78 stand-alone questions (54%) without multiple-choice options. Conclusions and Relevance: ChatGPT answered approximately half of questions correctly in the OphthoQuestions free trial for ophthalmic board certification preparation. Medical professionals and trainees should appreciate the advances of AI in medicine while acknowledging that ChatGPT as used in this investigation did not answer sufficient multiple-choice questions correctly for it to provide substantial assistance in preparing for board certification at this time.. © 2023 American Medical Association. All rights reserved."
"There are several aspects of cutting-edge medical imaging informatics research solutions that will be evaluated in this review. Artificial intelligence (AI) is one of the most crucial parts of big healthcare data analytics, as it streamlines various imaging modalities' data management processes. It then gives a summary of existing and emerging algorithmic methods for sickness categorization and organ/tissue segmentation, with a focus on AI and deep learning architectures that have already become the standard approach to this area of research. In the context of in-silico modelling advancements, these new 3D reconstruction and visualisation applications and their clinical benefits have been thoroughly researched. The findings of this and related studies could be used to develop an integrated analytics strategy that would totally reshape imaging informatics in radiology and digital pathology. The latter is supposed to provide more exact diagnosis, faster prognosis, and more effective treatment planning. © 2023 Author(s)."
"Building large AI fleets to support the rapidly growing DL workloads is an active research topic for modern cloud providers. Generating accurate benchmarks plays an essential role in designing the fast-paced software and hardware solutions in this space. Two fundamental challenges to make this scalable are (i) workload representativeness and (ii) the ability to quickly incorporate changes to the fleet into the benchmarks. To overcome these issues, we propose Mystique, an accurate and scalable framework for production AI benchmark generation. It leverages the PyTorch execution trace (ET), a new feature that captures the runtime information of AI models at the granularity of operators, in a graph format, together with their metadata. By sourcing fleet ETs, we can build AI benchmarks that are portable and representative. Mystique is scalable, due to its lightweight data collection, in terms of runtime overhead and instrumentation effort. It is also adaptive because ET composability allows flexible control on benchmark creation. We evaluate our methodology on several production AI models, and show that benchmarks generated with Mystique closely resemble original AI models, both in execution time and system-level metrics. We also showcase the portability of the generated benchmarks across platforms, and demonstrate several use cases enabled by the fine-grained composability of the execution trace. © 2023 Institute of Electrical and Electronics Engineers Inc.. All rights reserved."
"Federated Learning (FL) is rapidly gaining popularity as an effective cooperative and distributed approach, widely used by edge devices, to train machine learning models. Several aspects shall be managed to ensure a FL process that can more precisely match the QoS requirements of the applications that use it. The heterogeneity in the dataset available to each participant in the process, the variability in computational/memory capabilities, and the different availability of communication resources to connect the clients to the server are among the most critical. In this paper we will focus on the latter issue, less investigated in the literature, with particular reference to the case where the FL is used to support time-sensitive applications. Specifically, we will focus on studying the potential of an approach that leverages the Software-Defined Networking paradigm (SDN) to maintain the distributed learning process at high levels of effectiveness and efficiency even in the presence of edge client devices that may be delayed in delivering the result of their training due to the overload conditions experienced in the communication paths to the server. It will be shown, via a proof-of-concept performance evaluation campaign, how the proposed SDN support to the FL can guarantee significant overall reductions in process time at the cost of limited signaling overhead due to traffic to and from the controller. © 2023 ACM."
"We review various applications of distributed fiber optic sensing (DFOS) and machine learning (ML) technologies that particularly benefit telecom operators' fiber networks and businesses. By leveraging relative phase shift of the reflectance of coherent Rayleigh, Brillouin and Raman scattering of light wave, the ambient environmental vibration, acoustic effects, temperature and fiber/cable strain can be detected. Fiber optic sensing technology allows optical fiber to support sensing features in addition to its conventional role to transmit data in telecommunications. DFOS has recently helped telecom operators by adding multiple sensing features and proved feasibility of co-existence of sensing and communication systems on same fiber. We review the architecture of DFOS technique, and show examples where optical fiber sensing helps enhance network operation efficiency and create new services for customers on deployed fiber infrastructures, such as determination of cable locations, cable cut prevention, perimeter intrusion detection and networked sensing applications. In addition, edge AI platform allows data processing to be conducted on-the-fly with low latency. Based on discriminative spatial-temporal signatures of different events of interest, real-time processing of the sensing data from the DFOS system provides results of the detection, classification and localization immediately. © 1983-2012 IEEE."
"Energy demand is continuously rising, mainly due to population growth and rapid economic development. There are substantial worries about the environmental effects of fossil fuels in addition to the uncertainties surrounding the long-term sustainability of non-renewable energy sources. Environmental safety concerns are driving an increase in the demand for renewable energy production. Numerous efforts have been paid to harvest energy from ambient sources, e.g. solar, wind, thermal, hydro, mechanical, etc. This book discusses the application of artificial intelligence (AI) for energy harvesting. The implementation of metaheuristics and AL algorithms in the field of energy harvesting system will provide a quick start for the researchers and engineers who are new to this area. Energy harvesting technologies are growing very speedily, hence it is necessary to summarize recent advances in energy harvesting methodology. Over the recent years, a considerable amount of effort has been devoted, both in industry and academia, towards the performance modelling and evaluation of energy harvesting technologies. This book is the result of a collaborative effort among different researchers in the fields of energy harvesting and artificial intelligence. © 2023 River Publishers. All rights reserved."
"Digital Twin brings a new realization approach to the modeling of mobile networks. Mobile networks, as complex systems comprising multiple components, such as mobile users, base stations, and wireless environments, have intricate interactions and relationships with each other. By creating a virtual replica of each physical mobile network entity in a virtual space, we build a scalable digital twin system for mobile networks with generative AI. The system can interact with multiple optimizers to evaluate and display real-time simulation results. A companion video can be accessed using the link below. https://youtu.be/xtcBIXPzvkc  © 2023 Owner/Author(s)."
"In response to the aging trend in society and to Human Augmentation beings for home-based activities, this paper proposes an Abnormal Movement Detection system, using the common at-home movements of standing up and hand tremors while picking up items for abnormal movement verification. This can be easily applied in ordinary homes or long-term care institutions; for those living alone with limited resources, there is no longer any need to purchase expensive monitoring equipment to achieve improved quality of life. Therefore, this research collected and built the own dataset as the first important step of the study. The proposed Abnormal Movement Detection system is implemented by designing a deep learning network. Several issues, including the network architecture, the novel method of data augmentation and the scoring method of expanding the intervals between abnormality levels, are studied. For achieving the home-based real-time detection, there are four main contributions of this paper. The first is that a training dataset was collected and established: From this, the pathognomonic movement categories are easy to observe in home activities and geometric data augmentation can be used to improve the related home activity video collection. The second is the abnormal behavior detection architecture: This architecture has several important function blocks including detecting object, detecting action, inspecting abnormal movement and reminding event, using Convolutional Neural Network combined with Long Short-Term Memory (CNN+LSTM) as the core network for abnormal motion detection. With movement abnormality evaluation based on different levels, it can judge abnormal behaviors and conduct model training, performance evaluation and architecture optimization with both public domain datasets and the movement dataset collected in this research project. The third is the proliferation of new attributes in the videos: New attributes are added to the original videos through a Generative Adversarial Network (GAN), producing new training videos; the effectiveness of two different generation methods is evaluated. Finally, the algorithms developed in this paper are deployed on resource-constrained On-device Artificial Intelligence (AI). Activity videos from a total of 20 people were collected; in all, 53 videos of StandUp and 60 videos of PickUpItems were obtained to establish the training dataset. When CNN and LSTM network were added to Batch Normalization (BN), and Global Average Pooling (GAP) replaced Fully Connected (FC) layers, the accuracy rate reached 98.4%. In terms of data augmentation, geometric transformations and GAN were used to estimate the performance. The experimental results showed that the geometric transformation using brightness adjustment had the highest accuracy rate of 98.6%. Finally, the Softmax layer using Phi-Softmax-tan(·) function was shown to be the best method to expand the intervals between abnormality levels.  © 2023 World Scientific Publishing Company."
"Recently, the emergence of promising membrane-based processes in disparate related disciplines of pharmaceutical industries (i.e., biochemical engineering and biotechnology) has paved the way towards concentration and purification of therapeutic entities, molecular synthesis, and drug delivery. The strategy of this investigation is to employ various artificial intelligence (AI)-based computational models as a novel method to estimate the concentration distribution value of a medical solute inside the membrane. To achieve this, a data set with two inputs and one output was analyzed using bagging ensemble method with some distinct base models. The base models are Multi-layer Perceptron (MLP), Linear Regression (LR), and K-Nearest Neighbor (KNN). The hyperparameters of the methods were developed through performing a grid search and the final models were examined. Based on this, as expected, the use of bagging along with the optimization of hyperparameters led to strong models. All three bagging models with the R-square measure have scores higher than 0.98. Also, in terms of RMSE bagging LR, bagging MLP, and bagging KNN have error rates of 9.25 × 101, 2.29 × 101, 9.57 × 101, respectively. © 2023 Elsevier B.V."
"Humankind consumes different forms of energy for life and growth. Electric energy is one of them. Demand for electric energy keeps growing as the application area of electric energy keeps increasing day by day. To meet this huge demand and to save natural recourses, there is research going on to generate electrical energy from renewable energy resources like solar energy, wind energy, etc. Energy harvesting technology is an emerging area that gives the opportunity to increase the efficiency of reusable energy generation by using recent technology like artificial intelligence, machine learning, the Internet of Things, etc. The dissipated energies like electromagnetic waves, heat energy and vibrations are converted to electric energy. Artificial intelligence can be deployed along with different sensors like piezoelectric sensors for energy harvesting. This harvested energy can effectively be useful in different sectors like automobiles, domestic and industrial applications, etc. This book chapter summarizes the characteristics of energy harvesting with the help of AI/ML, recent developments in this area and major challenges, along with the future scope of development. Application of existing artificial intelligence technology in the field of energy harvesting is explained and the possible use of existing state-of-the-art AI/ML technologies for prediction and increase the efficiency of energy harvesting technology is also suggested in the present book chapter. © 2023 River Publishers."
"Parallel manufacturing is a new manufacturing form in industry, deeply integrating informalization, automation, and AI. In this paper we illustrated the procedure of non-standard mechanical design in parallel manufacturing, claiming it a nested parallel system. We proposed the ACP method based on the standard procedure and the emulated procedure, and defined the social-value-vector and trended-social-value-matrix so as to take the advantage of data in CPSS. We also presented Xinglun robot speed reducer as a case study of non-standard mechanical design in parallel design. © 2023 The Author(s)."
"This study examines how Klingsieck's LIST-K questionnaire [22] can be shortened and adapted to the requirements of an online learning management system. In a study with 213 participants, the questionnaire is subjected to an exploitative factor analysis. In a next step, the results are evaluated in terms of their reliability. This process creates a modified factor structure for the LIST-K, comprising a total of eight factors. The reliability of the modified questionnaire is at an α of .770. The shortened version of the LIST-K questionnaire is currently being used on an experimental basis in different courses.  © 2023 ACM."
"In this paper, we explore the potential of AI-assisted visualization during the problem identification and construction phase of the creative problem-solving process. We examine this within the context of the ongoing crea.visions research project, which employs AI technologies to visualize citizens' visions of the future. Our findings underscore various factors contributing to the effectiveness of assisted visualization in this setting, such as: 1) the tool's dual role as both a visual and ideational aid, 2) the introduction of innovative collaborative elements like prompt engineering, 3) the enhancement of visual expression without requiring artistic skills, and 4) the facilitation of idea communication. We also recognize limitations related to the tool and the problem context such as abstract concepts. This study serves as a foundation for future research on AI-assisted image generation as a resource in creative problem-solving, laying the groundwork for the creation of increasingly effective and user-friendly tools.  © 2023 Owner/Author."
"Generative image AI has sparked heated discussion among creative professionals. However, how it might be a tool for design practice is understudied, and it is not yet understood how designers may find image AI helpful across the stages of design practice. To address this, our preliminary study explores how designers use generative image AI accompanied by design sketches to inform early-stage 3D design. Further, we also examine the perceived limitations of text-to-image models. To do this, we recruited 11 Architecture graduate students with a median work experience of 2 years. Participants completed a design task using generative image AI packages and incorporated design sketches as inputs. The study findings provide insights into how image AI can or can not be a valuable resource for architectural design practitioners. Further, findings suggest possible directions for future image AI-assisted design tools and workflows.  © 2023 Owner/Author."
"Artificial intelligence (AI) is currently integrated into many medical services. AI is utilized in many aspects of orthopedic surgery. The scope ranges from diagnosis to complex surgery. To evaluate the perceptions, attitudes, and interests of Sudanese orthopedic surgeons regarding the different applications of AI in orthopedic surgery. This qualitative questionnaire-based study was conducted through an anonymous electronic survey using Google Forms distributed among Sudanese orthopedic surgeons. The questionnaire entailed 4 sections. The first section included the participants' demographic data. The remaining 3 sections included questions for the assessment of the perception, attitude, and interest of surgeons toward (AI). The validity and reliability of the questionnaire were tested and piloted before the final dissemination. One hundred twenty-nine surgeons responded to the questionnaires. Most respondents needed to be more aware of the basic concepts of AI. However, most respondents were aware of its use in spinal and joint replacement surgeries. Most respondents had doubts regarding the safety of (AI). However, they were highly interested in utilizing (AI) in many orthopedic surgical aspects. Orthopedic surgery is a rapidly evolving branch of surgery that involves adoption of new technologies. Therefore, orthopedic surgeons should be encouraged to enroll in research activities to generate more studies and reviews to assess the usefulness and safety of emerging technologies. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"Neither Here Nor There is a speculative installation exploring human perception of the natural world from the perspective of four plants. Probing the act of communication as well as the indeterminate gap between what is deemed human and non-human, the installation highlights the complex role that technology plays in our current understanding of the human. Standing in large pots around a meeting table, plants engage in a free roaming discussion about what they think it means to be human and about planetary politics. Key questions and terms are fed into GPT-3 (AI-based natural language text generator) and played as plant sounds. As the plants converse in this manner, no aspect of their conversation is intuitively legible to the viewer. Viewers are invited to use a decoding app on their cell phone that translates these plant sounds. In avoiding an obvious mode of human-centric representation, the installation places the viewer in unknown territory, asking them to consider what intelligence, communication, and cognition mean beyond the human perspective.  © 2023 Owner/Author."
"For the goal of strong artificial intelligence that can mimic human-level intelligence, AI systems would have the ability to adapt to ever-changing scenarios and learn new knowledge continuously without forgetting previously acquired knowledge. When a machine learning model is consecutively trained on multiple tasks that come in sequence, its performance on previously learned tasks may drop dramatically during the learning process of the newly seen task. To avoid this phenomenon termed catastrophic forgetting, continual learning, also known as lifelong learning, has been proposed and become one of the most up-to-date research areas of machine learning. As quantum machine learning blossoms in recent years, it is interesting to develop quantum continual learning. This paper focuses on the case of quantum models for quantum data where the computation model and the data to be processed are both quantum. The gradient episodic memory method is incorporated to design a quantum continual learning scheme that overcomes catastrophic forgetting and realizes knowledge backward transfer. Specifically, a sequence of quantum state classification tasks is continually learned by a variational quantum classifier whose parameters are optimized by a classical gradient-based optimizer. The gradient of the current task is projected to the closest gradient, avoiding the increase of the loss at previous tasks, but allowing the decrease. Numerical simulation results show that our scheme not only overcomes catastrophic forgetting, but also realize knowledge backward transfer, which means the classifier's performance on previous tasks is enhanced rather than compromised while learning a new task. © 2023"
"These days we have all become increasingly aware of the role that exercise plays in a healthy lifestyle.Activities such as cycling, triathlons, and running have become popular ways for people to keep fit and test their abilities.For recreational athletes there is no shortage of training advice or programmes to follow, yet most offer only one-size-fits-all, or minimally tailored guidance, which often leaves novices under-supported on their fitness journeys.In this work, we describe a case-based reasoning system to generate personalised training recommendations for marathon runners, based on their training histories and the training histories of similar runners with comparable race goals.The system harnesses the type of activity data that is routinely collected by smartwatches and apps like Strava.It uses prefactual explanations to suggest to runners how they may wish to adjust their training as their fitness goals evolve.We evaluate the approach using a large-scale dataset of more than 300,000 real-world runners and we show that it is feasible to generate tailored, personalised recommendations for up to 80% of these runners.Additionally, we show that the recommendations produced are realistic and reasonable for a runner to implement, as part of their training programme.These suggestions typically include a small number (3-5) of incremental training adaptations, such as a change in weekly distance, long-run distance, or mean training pace.We argue that by engaging runners in this type of dialog about their training progress and race goals, we can better support novice runners, as their training unfolds, which may help to keep runners motivated on their long journey to race day. © 2023 Copyright held by the owner/author(s)."
"Organizations have recently begun to deploy conversational task assistants that collaborate with knowledge workers to partially automate their work tasks. These assistants evolved out of business robotic process automation (RPA) tools and are becoming more intelligent: users can initiate task sequences through natural language, and the system can orchestrate those tasks if they have not previously been defined. As these tools become more automated, system designers tend to optimize overall process efficiency, but at the cost of shifting agency away from workers. Particularly in high stakes work environments, this shift raises questions of how to re-delegate agency such that workers feel sufficiently in control of automated tasks. We explored this through two studies comprised of interviews and co-design activities with knowledge workers and identified four task dimensions along which their automation and interaction preferences vary: process consequence, social consequence, task familiarity, and task complexity. These dimensions are useful for understanding when, why, and how to delegate agency between workers and conversational task assistants.  © 2023 Owner/Author."
"New advancements for the detection of synthetic images are critical for fighting disinformation, as the capabilities of generative AI models continuously evolve and can lead to hyper-realistic synthetic imagery at unprecedented scale and speed. In this paper, we focus on the challenge of generalizing across different concept classes, e.g., when training a detector on human faces and testing on synthetic animal images - highlighting the ineffectiveness of existing approaches that randomly sample generated images to train their models. By contrast, we propose an approach based on the premise that the robustness of the detector can be enhanced by training it on realistic synthetic images that are selected based on their quality scores according to a probabilistic quality estimation model. We demonstrate the effectiveness of the proposed approach by conducting experiments with generated images from two seminal architectures, StyleGAN2 and Latent Diffusion, and using three different concepts for each, so as to measure the cross-concept generalization ability. Our results show that our quality-based sampling method leads to higher detection performance for nearly all concepts, improving the overall effectiveness of the synthetic image detectors.  © 2023 ACM."
"The growing need for accountability of the people behind AI systems can be addressed by leveraging processes in three fields of study: ethics, law, and computer science. While these fields are often considered in isolation, they rely on complementary notions in their interpretation and implementation. In this work, we detail this interdependence and motivate the necessary role of collaborative governance tools in shaping a positive evolution of AI. We first contrast notions of compliance in the ethical, legal, and technical fields; we outline both their differences and where they complement each other, with a particular focus on the roles of ethical charters, licenses, and technical documentation in these interactions. We then focus on the role of values in articulating the synergies between the fields and outline specific mechanisms of interaction between them in practice. We identify how these mechanisms have played out in several open governance fora: an open collaborative workshop, a responsible licensing initiative, and a proposed regulatory framework. By leveraging complementary notions of compliance in these three domains, we can create a more comprehensive framework for governing AI systems that jointly takes into account their technical capabilities, their impact on society, and how technical specifications can inform relevant regulations. Our analysis thus underlines the necessity of joint consideration of the ethical, legal, and technical in AI ethics frameworks to be used on a larger scale to govern AI systems and how the thinking in each of these areas can inform the others. © 2023 Owner/Author."
"The proceedings contain 13 papers. The topics discussed include: experiences of novice design facilitators in a remote participatory workshop; hear we are: spatial audio benefits perceptions of turn-taking and social presence in video meetings; rebalancing worker initiative and AI initiative in future work: four task dimensions; preparing future designers for human-ai collaboration in persona creation; personal informatics at the office: user-driven, situated sensor kits in the workplace; tracking to success? a critical reflection on quantified-self technologies from a humanistic perspective; focus time: effectiveness of computer assisted protected time for wellbeing and work engagement of information workers; how much home office is ideal? a multi-perspective algorithm; is a return to office a return to creativity? requiring fixed time in office to enable brainstorms and watercooler talk may not foster research creativity; and adapting to telerehabilitation care during the COVID-19 pandemic: the future is hybrid."
"Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction. © 2023 Owner/Author."
"The Internet of Things (IoT) has attracted the attention from industry and academia equally in the past. Greenhouse/polyhouse-based production is considered to be one of the prominent solutions to the growing demand for food worldwide with growing population. A greenhouse provides year-round production of fresh vegetables and crops with much enhanced production rate as compared to open field farming. In recent history, the automatic control of the roof shade to minimize energy consumption and get desired climate in a tropical environment (i.e., temperature and sunlight) has proved highly economical. In this regard, thermal imaging, reflection and fluorescence images on whole plants have been used in detecting the-related changes in the lightemitting pattern through the process of phenotyping. Besides, decision making regarding input supply to the plants based on AI and ML has also been achieved. Moreover, Internet and Communication Technology enabled monitoring and control of greenhouse functioning, from a distant place, has also improved plant photosynthesis tremendously, in turn improving yield. This chapter summarizes the current developments in agriculture to increase photosynthesis and yield, such as the use of IoT, ICT, AI, ML, phenotyping, imaging, robotics and automation. © 2023 Nova Science Publishers, Inc. All rights reserved."
"Discussion of the ""right to an explanation""has been increasingly relevant because of its potential utility for auditing automated decision systems, as well as for making objections to such decisions. However, most existing work on explanations focuses on collaborative environments, where designers are motivated to implement good-faith explanations that reveal potential weaknesses of a decision system. This motivation may not hold in an auditing environment. Thus, we ask: how much could explanations be used maliciously to defend a decision system? In this paper, we demonstrate how a black-box explanation system developed to defend a black-box decision system could manipulate decision recipients or auditors into accepting an intentionally discriminatory decision model. In a case-by-case scenario where decision recipients are unable to share their cases and explanations, we find that most individual decision recipients could receive a verifiable justification, even if the decision system is intentionally discriminatory. In a system-wide scenario where every decision is shared, we find that while justifications frequently contradict each other, there is no intuitive threshold to determine if these contradictions are because of malicious justifications or because of simplicity requirements of these justifications conflicting with model behavior. We end with discussion of how system-wide metrics may be more useful than explanation systems for evaluating overall decision fairness, while explanations could be useful outside of fairness auditing. © 2023 ACM."
"The size and diversity of the training datasets directly influences the decision-making process of AI models. Therefore, there is an immense need for massive and diverse datasets to enhance the deployment process of AI applications. Crowdsourcing marketplaces provide a fast and reliable alternative to the laborious data collection process. However, the existing crowdsourcing marketplaces are either centralized or do not fully provide data sovereignty. By contrast, this work proposes a decentralized crowdsourcing platform through prototypical implementation along with active involvement of business entities, that grants the users sovereignty over their collected data, named as Vision-Sovereignty Data Marketplace (ViSDM). This work contributes to the data marketplaces landscape by introducing (i) A liquid democracy-based voting system to negotiate prices between a buyer and multiple data owners, (ii) An automated AI-Based per-sample value calculation function to evaluate the data and distribute profit among the data owners.  © 2023 Owner/Author."
"How to protect people from algorithmic harms? A promising solution, although in its infancy, is algorithmic impact assessment (AIA). AIAs are iterative processes used to investigate the possible short and long terms societal impacts of AI systems before their use, but with ongoing monitoring and periodic revisiting even after their implementation. When conducted in a participatory and transparent fashion, they could create bridges across the legal, social and computer science domains, promoting the accountability of the entity performing them as well as public scrutiny. They could enable to re-attach the societal and regulatory context to the mathematical definition of fairness, thus expanding the formalistic approach thereto. Whilst the regulatory framework in the European Union currently lacks the obligation to perform such AIA, some other provisions are expected to play a role in AI development, leading the way towards more widespread adoption of AIA. These include the Data Protection Impact Assessment (DPIA) under the General Data Protection Regulation (GDPR), the risk assessment process under the Digital Services Act (DSA) and the Conformity Assessment (CA) foreseen under the AI Regulation proposal. In this paper, after briefly introducing the plurality of definitions of fairness in the legal, social and computer science domains, and explaining to which extent the current and upcoming legal framework mandates the adoption of fairness metrics, we will illustrate how AIA could create bridges between all these disciplines, allowing us to build fairer AI solutions. We will then recognise the role of DPIA, DSA risk assessment and CA by discussing the contributions they can offer towards AIA but also identify the aspects lacking therein. We will then identify how these assessment provisions could aid the overall technical discussion of introducing and assessing fairness in AI-based models and processes. © 2023 ACM."
"With recent advancements in synthetic media manipulation and generation, verifying multimedia content posted online has become increasingly difficult. Additionally, the malicious exploitation of AI technologies by actors to disseminate disinformation on social media, and more generally the Web, at an alarming pace poses significant threats to society and democracy. Therefore, the development of AI-powered tools that facilitate media verification is urgently needed. The MAD '23 workshop aims to bring together individuals working on the wider topic of detecting disinformation in multimedia to exchange their experiences and discuss innovative ideas, attracting people with varying backgrounds and expertise. The research areas of interest include identifying manipulated and synthetic content in multimedia, as well as examining the dissemination of disinformation and its impact on society. The multimedia aspect is very important since content most often contains a mix of modalities and their joint analysis can boost the performance of verification methods.  © 2023 Owner/Author."
"Purpose: Artificial intelligence (AI) is the most progressive commodity among current information system applications. In-house development and sales of beneficial products are difficult for many software development and service companies (SDSCs). SDSCs have some implicit concerns about implementing AI software development due to the complexity of AI technology; they require an evaluation framework to avoid development failure. To fill the void, this study identified the factors influencing SDSCs when developing AI software development. Design/methodology/approach: Based on complex adaptive systems theory, three aspects were developed as the main factors of hierarchy, namely, employees' capabilities, environmental resources and team capabilities. Fuzzy analytic hierarchy process (FAHP) was used to assess the SDSCs' attitude. Based on SDSCs, attitudes toward implementing AI software projects were collected to calculate the hierarchy of factors. Findings: The outcome of FAHP is used as understanding the key factors of SDSCs for selecting an AI software project, toward the improvement of overall project planning. Employees' stress resistance was considered as a priority for the project, although professional AI skills and resources were also important. Originality/value: This study suggested three variables developed using complex adaptive systems. This study contributes to a better understanding of the critical aspects of developing AI software projects in SDSCs. The study's findings have practical and academic implications for SDSCs and subsequent academic development, broadening the scope of AI software development research. © 2023, Emerald Publishing Limited."
"This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants' reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities. © 2023 Owner/Author."
"As popular calls for the transparency of AI systems gain prominence, it is important to think systematically about why transparency matters morally. I'll argue that welfarism provides a theoretical basis for doing so. For welfarists, it is morally desirable to make AI systems transparent insofar as pursuing transparency tends to increase overall welfare, and/or maintaining opacity tends to reduce overall welfare. This might seem like a simple - even simplistic - move. However, as I will show, the process of tracing the expected effects of transparency on welfare can bring much-needed clarity to existing debates about when AI systems should and should not be transparent. Welfarism provides us with a basis to evaluate conflicting desiderata, and helps us avoid a problematic tendency to reify trust, accountability, and other such goals as ends in themselves. And, by shifting the focus away from the mere act of making an AI system transparent, towards the harms and benefits that its transparency might bring about, welfarists call attention to often- neglected social, legal, and institutional factors that determine whether relevant stakeholders are able to access and meaningfully act on the information made transparent to produce desirable consequences. In these ways, welfarism helps us understand AI transparency not merely as a demand to look at the innards of some technical system, but rather as a broader moral ideal about how we should relate to powerful technologies that make decisions about us. © 2023 Owner/Author."
"The movie industry is characterized by high levels of uncertainty due to the difficulties businesses have predicting sales and income since they depend on so many complex elements. Because of the significant movie industry's upfront costs, investors must make decisions based on accurate methodologies to estimate the success or returns of their investments. Due to social networks' widespread use in our daily lives, there is an unprecedented amount of publicly available posts containing emotional triggers that can be analyzed. In this paper, we showcase our movie trailers campaign surveillance system using social media analytics and public mood. The system provides an overall interactive dashboard to users and the capability for further process, analysis, and visualization of data in distinct customized reports. The proposed system also offers a configurable environment to gather metrics from heterogeneous social media platforms and extract advanced analytics through dedicated AI tools for sentiment and emotion extraction. This solution benefits the continuous surveillance of a movie trailer's popularity and supports the success of marketing campaigns.  © 2023 ACM."
"Zero-carbon energy and negative emission technologies are crucial for achieving a carbon neutral future, and nanomaterials have played critical roles in advancing such technologies. More recently, due to the explosive growth in data, the adoption and exploitation of artificial intelligence (AI) as part of the materials research framework have had a tremendous impact on the development of nanomaterials. AI has enabled revolutionary next-generation paradigms to significantly accelerate all stages of material discovery and facilitate the exploration of the enormous design space. In this review, we summarize recent advancements of AI applications in nanomaterials discovery, with a special emphasis on the selected applications of AI and nanotechnology for the net-zero emission future including the development of solar cells, hydrogen energy, battery materials for renewable energy, and CO2 capture and conversion materials for carbon capture, utilization and storage (CCUS) technologies. In addition, we discuss the limitations and challenges of current AI applications in this area by identifying the gaps that exist in current development. Finally, we present the prospect for future research directions in order to facilitate the large-scale applications of artificial intelligence for advancements in nanomaterials. © 2023 American Chemical Society."
"The technical progression of artificial intelligence (AI) research has been built on breakthroughs in fields such as computer science, statistics, and mathematics. However, in the past decade AI researchers have increasingly looked to the social sciences, turning to human interactions to solve the challenges of model development. Paying crowdsourcing workers to generate or curate data, or 'data enrichment', has become indispensable for many areas of AI research, from natural language processing to reinforcement learning from human feedback (RLHF). Other fields that routinely interact with crowdsourcing workers, such as Psychology, have developed common governance requirements and norms to ensure research is undertaken ethically. This study explores how, and to what extent, comparable research ethics requirements and norms have developed for AI research and data enrichment. We focus on the approach taken by two leading conferences: ICLR and NeurIPS, and journal publisher Springer. In a longitudinal study of accepted papers, and via a comparison with Psychology and CHI papers, this work finds that leading AI venues have begun to establish protocols for human data collection, but these are are inconsistently followed by authors. Whilst Psychology papers engaging with crowdsourcing workers frequently disclose ethics reviews, payment data, demographic data and other information, similar disclosures are far less common in leading AI venues despite similar guidance. The work concludes with hypotheses to explain these gaps in research ethics practices and considerations for its implications. © 2023 ACM."
"The use of local natural and recycled feedstock is promising for sustainable construction. However, unlike versatile engineered bricks, natural and recycled feedstock involves design challenges due to their stochastic, sequential, and heterogeneous nature. For example, the practical use of stone masonry is limited, as it still relies on human experts with holistic domain knowledge to determine the sequential organization of natural stones with different sizes/shapes. Reinforcement learning (RL) is expected to address such design challenges, as it allows artificial intelligence (AI) agents to autonomously learn design policy, that is, identifying the best design decision at each time step. As a proof-of-concept RL framework for design automation involving heterogeneous feedstock, a stone masonry design framework is presented. The proposed framework is founded upon a virtual design environment, MasonTris, inspired by the analogy between stone masonry and Tetris. MasonTris provides a Tetris-like virtual environment combined with a finite element analysis (FEA), where AI agents learn effective design policies without human intervention. Also, a new data collection policy, almost-greedy policy, is designed to address the sparsity of feasible designs for faster/stable learning. As computation bottleneck occurs when parallel agents evaluate designs with different complexities, a modification of the RL framework is proposed that FEA is held until training data are retrieved for training. The feasibility and adaptability of the proposed framework are demonstrated by continuously improving stone masonry design policy in simplified design problems. The framework can be generalizable to different natural and recycled feedstock by incorporating more realistic assumptions, opening opportunities in design automation for sustainability.  Copyright © The Author(s), 2023. Published by Cambridge University Press."
"The societal and epistemological implications of online targeted advertising have been scrutinized by AI ethicists, legal scholars, and policymakers alike. However, the government's use of online targeting and its consequential socio-political ramifications remain under-explored from a critical socio-technical standpoint. This paper investigates the socio-political implications of governmental online targeting, using a case study of the UK government's application of such techniques for public policy objectives. We argue that this practice undermines democratic ideals, as it engenders three primary concerns - Transparency, Privacy, and Equality - that clash with fundamental democratic doctrines and values. To address these concerns, the paper introduces a preliminary blueprint for an AI governance framework that harmonizes governmental use of online targeting with certain democratic principles. Furthermore, we advocate for the creation of an independent, non-governmental regulatory body responsible for overseeing the process and monitoring the government's use of online targeting, a critical measure for preserving democratic values. © 2023 Owner/Author."
"Fairness in AI has garnered quite some attention in research, and increasingly also in society. The so-called ""Impossibility Theorem""has been one of the more striking research results with both theoretical and practical consequences, as it states that satisfying a certain combination of fairness measures is impossible. To date, this negative result has not yet been complemented with a positive one: a characterization of which combinations of fairness notions are possible. This work aims to fill this gap by identifying maximal sets of commonly used fairness measures that can be simultaneously satisfied. The fairness measures used are demographic parity, equal opportunity, predictive equality, predictive parity, false omission rate parity, overall accuracy equality and treatment equality. We conclude that in total 12 maximal sets of these fairness measures are possible, among which are seven combinations of two measures, and five combinations of three measures. Our work raises interesting questions regarding the practical relevance of each of these 12 maximal fairness notions in various scenarios. © 2023 Owner/Author."
"AI-driven decision-making can lead to discrimination against certain individuals or social groups based on protected characteristics/attributes such as race, gender, or age. The domain of fairness-aware machine learning focuses on methods and algorithms for understanding, mitigating, and accounting for bias in AI/ML models. Still, thus far, the vast majority of the proposed methods assess fairness based on a single protected attribute, e.g. only gender or race. In reality, though, human identities are multi-dimensional, and discrimination can occur based on more than one protected characteristic, leading to the so-called ""multi-dimensional discrimination""or ""multi-dimensional fairness""problem. While well-elaborated in legal literature, the multi-dimensionality of discrimination is less explored in the machine learning community. Recent approaches in this direction mainly follow the so-called intersectional fairness definition from the legal domain, whereas other notions like additive and sequential discrimination are less studied or not considered thus far. In this work, we overview the different definitions of multi-dimensional discrimination/fairness in the legal domain as well as how they have been transferred/ operationalized (if) in the fairness-aware machine learning domain. By juxtaposing these two domains, we draw the connections, identify the limitations, and point out open research directions. © 2023 ACM."
"Trust is an important factor in people's interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from many angles. We find participants perceived the app as trustworthy and trusted it, but selectively accepted app outputs after engaging in verification behaviors, and decided against app adoption in certain high-stakes scenarios. We also find domain knowledge and context are important factors for trust-related assessment and decision-making. We discuss the implications of our findings and provide recommendations for future research on trust in AI. © 2023 Owner/Author."
"Growing awareness of the capacity of AI to inflict harm has inspired efforts to delineate principles for 'trustworthy AI' and, from these, objective indicators of 'trustworthiness' for auditors and regulators. Such efforts run the risk of formalizing a distinctly privileged perspective on trustworthiness which is insensitive (or else indifferent) to the legitimate reasons for distrust held by marginalized people. By exploring a neglected conative element of trust, we broaden understandings of trust and trustworthiness to make sense of, and identify principles for responding productively to, distrust of ostensibly 'trustworthy' AI. Bringing social science scholarship into dialogue with AI criticism, we show that AI is being used to construct a digital underclass that is rhetorically labelled as 'undeserving', and highlight how this process fulfills functions for more privileged people and institutions. We argue that distrust of AI is warranted and healthy when the AI contributes to marginalization and structural violence, and that Trustworthy AI may fuel public resistance to the use of AI unless it addresses this dimension of untrustworthiness. To this end, we offer reformulations of core principles of Trustworthy AI - fairness, accountability, and transparency - that substantively address the deeper issues animating widespread public distrust of AI, including: stewardship and care, openness and vulnerability, and humility and empowerment. In light of legitimate reasons for distrust, we call on the field to to re-evaluate why the public would embrace the expansion of AI into all corners of society; in short, what makes it worthy of their trust. © 2023 ACM."
"Bispyribac-sodium, a herbicide that inhibits acetolactate synthase (ALS), is frequently used in rice fields in India to control weeds, including the most common noxious weed, barnyardgrass. However, rice growers have recently reported reduced control of barnyardgrass with bispyribac-sodium. Hence, a large-scale survey was carried out to assess bispyribac-sodium resistance in Chhattisgarh and Kerala, two rice-growing states. Open-field pot experiments were conducted for 2 yr to confirm resistance to bispyribac-sodium. Of the 37 biotypes tested, 30% (11) survived the recommended label rate of bispyribac-sodium (25 g ai ha-1). The effective rate of bispyribac-sodium required to achieve 50% control (ED50) of putative resistant biotypes ranged from 18 to 41 g ha-1, whereas it was about 10 g ha-1 for susceptible biotypes. This suggests that putative biotypes were two to four times more resistant to bispyribac-sodium. At 6 d after herbicide application, an in vitro enzyme assay demonstrated higher ALS enzyme activity in putative resistant biotypes (66% to 75%) compared with susceptible biotypes (48% to 52%). This indicates the presence of an insensitive ALS enzyme in those biotypes and a target site mutation as a possible mechanism for resistance. Whole-plant bioassays also suggested that the resistance problem is more widespread in Chhattisgarh than in Kerala. This study confirmed the first case of evolved resistance in barnyardgrass to bispyribac-sodium in rice fields of India. © The Author(s), 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
The proceedings contain 34 papers. The topics discussed include: multistep cyberattacks detection using a flexible multilevel system for alerts and events correlation; an evaluation of information flows in digital euro transactions using contextual integrity theory; new tricks to old codes: can AI chatbots replace static code analysis tools?; exposed-mode of wormhole attack in opportunistic mobile networks: impact study and analysis; acceptance factors and obstacles for cryptocurrency adoption; a framework for advanced persistent threat attribution using Zachman ontology; useful cyber threat intelligence relation retrieval using transfer learning; ontology-based framework for boundary verification of safety and security properties in industrial control systems; investigating the availability of child sexual abuse materials in dark web markets: evidence gathered and lessons learned; and towards detecting anomalies in log-event sequences with deep learning: open research challenges.
"There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models. © 2023 Owner/Author."
"How do software engineers identify and act on their ethical concerns? Past work examines how software practitioners navigate specific ethical principles such as ""fairness"", but this narrows the scope of concerns to implementing pre-specified principles. In contrast, we report self-identified ethical concerns of 115 survey respondents and 21 interviewees across five continents and in non-profit, contractor, and non-tech firms. We enumerate their concerns - military, privacy, advertising, surveillance, and the scope of their concerns - from simple bugs to questioning their industry's entire existence. We illustrate how attempts to resolve concerns are limited by factors such as personal precarity and organizational incentives. We discuss how even relatively powerful software engineers often lacked the power to resolve their ethical concerns. Our results suggest that ethics interventions must expand from helping practitioners merely identify issues to instead helping them build their (collective) power to resolve them, and that tech ethics discussions may consider broadening beyond foci on AI or Big Tech. © 2023 Owner/Author."
"This paper focuses on how to improve the efficiency of the action recognition framework by optimizing its complicated feature extraction pipelines and enhancing explainability, benefiting future adaptation to more complex visual understanding tasks (e.g. video captioning). To achieve this task, we propose a novel decoupled two-stream framework for action recognition - HSAR, which utilizes high-semantic features for increased efficiency and provides well-founded explanations in terms of spatial-temporal perceptions that will benefit further expansions on visual understanding tasks. The inputs are decoupled into spatial and temporal streams with designated encoders aiming to extract only the pinnacle of representations, gaining high-semantic features while reducing computation costs greatly. A lightweight Temporal Motion Transformer (TMT) module is proposed for globally modeling temporal features through self-attention, omitting redundant spatial features. Decoupled spatial-temporal embeddings are further merged dynamically by an attention fusion model to form a joint high-semantic representation. The visualization of the attention in each module offers intuitive interpretations of HSAR's explainability. Extensive experiments on three widely-used benchmarks (Kinetics400, 600, and Sthv2) show that our framework achieves high prediction accuracy with significantly reduced computation (only 64.07 GFLOPs per clip), offering a great trade-off between accuracy and computational costs.  © 2023 ACM."
"This paper presents findings from an exploratory study investigating the use of AI text-generation tools to support novice designers in persona creation. We conducted a workshop with 22 undergraduate students enrolled in an introductory human-computer interaction course, who were instructed to use GPT-3 in the creation of personas. These novice designers were able to use GPT-3 to iterate to produce satisfactory personas, particularly when providing detailed prompts. Our findings suggest that personas created with GPT-3 assistance were mostly comparable to those created manually but rated lower on some evaluation dimensions. The study also reveals merits and concerns of using GPT-3 for persona creation. Based on our findings, we propose recommendations for novice designers on how to use text-generative AIs to create personas effectively and responsibly.  © 2023 ACM."
"While genome-wide association studies (GWAS) have discovered thousands of disease-associated loci, molecular mechanisms for a considerable fraction of the loci remain to be explored. The logical next steps for post-GWAS are interpreting these genetic associations to understand disease etiology (GWAS functional studies) and translating this knowledge into clinical benefits for the patients (GWAS translational studies). Although various datasets and approaches using functional genomics have been developed to facilitate these studies, significant challenges remain due to data heterogeneity, multiplicity, and high dimensionality. To address these challenges, artificial intelligence (AI) technology has demonstrated considerable promise in decoding complex functional datasets and providing novel biological insights into GWAS findings. This perspective first describes the landmark progress driven by AI in interpreting and translating GWAS findings and then outlines specific challenges followed by actionable recommendations related to data availability, model optimization, and interpretation, as well as ethical concerns. © 2023 The Author(s)"
"A growing literature on human-AI decision-making investigates strategies for combining human judgment with statistical models to improve decision-making. Research in this area often evaluates proposed improvements to models, interfaces, or workflows by demonstrating improved predictive performance on ""ground truth""labels. However, this practice overlooks a key difference between human judgments and model predictions. Whereas humans commonly reason about broader phenomena of interest in a decision - including latent constructs that are not directly observable, such as disease status, the ""toxicity""of online comments, or future ""job performance""- predictive models target proxy labels that are readily available in existing datasets. Predictive models' reliance on simplistic proxies for these nuanced phenomena makes them vulnerable to various sources of statistical bias. In this paper, we identify five sources of target variable bias that can impact the validity of proxy labels in human-AI decision-making tasks. We develop a causal framework to disentangle the relationship between each bias and clarify which are of concern in specific human-AI decision-making tasks. We demonstrate how our framework can be used to articulate implicit assumptions made in prior modeling work, and we recommend evaluation strategies for verifying whether these assumptions hold in practice. We then leverage our framework to re-examine the designs of prior human subjects experiments that investigate human-AI decision-making, finding that only a small fraction of studies examine factors related to target variable bias. We conclude by discussing opportunities to better address target variable bias in future research. © 2023 Owner/Author."
"Drawing on the theoretical debates, practical applications, and sectoral approaches in the field, this ground-breaking Handbook unpacks the political and regulatory developments in AI and big data governance. Covering the political implications of big data and AI on international relations, as well as emerging initiatives for legal regulation, it provides an accessible overview of ongoing data science discourses in politics, law and governance. With novel insights into existing and emerging debates, this cutting-edge Handbook highlights the mutual effects of big data and AI on society. Amongst other theoretical and sectoral issues, chapters analyse the liability of AI use in autonomous weapons, the role of big data in healthcare and education, the intersections between AI and gender in human rights law, and the ethics of public facial-recognition technology. Addressing the many open questions and future regulatory problems, it uses data science to investigate the dynamics between the technical aspects, societal dynamics and governance implications of big data and AI. Transdisciplinary in scope, this Handbook will be invaluable to students and researchers across the fields of politics, law, governance and data science, alongside policymakers concerned with the regulation and governance of AI and big data in public and private institutions. © The Editors and Contributors Severally 2023. All rights reserved."
"In order to take advantage of artificial intelligence (AI) solutions in endoscopy diagnostics, we must overcome the issue of limited annotations. These limitations are caused by the high privacy concerns in the medical field and the requirement of getting aid from experts for the time-consuming and costly medical data annotation process. In computer vision, image synthesis has made a significant contribution in recent years, as a result of the progress of generative adversarial networks (GANs) and diffusion probabilistic models (DPMs). Novel DPMs have outperformed GANs in text, image, and video generation tasks. Therefore, this study proposes a conditional DPM framework to generate synthetic gastrointestinal (GI) polyp images conditioned on given generated segmentation masks. Our experimental results show that our system can generate an unlimited number of high-fidelity synthetic polyp images with the corresponding ground truth masks of polyps. To test the usefulness of the generated data we trained binary image segmentation models to study the effect of using synthetic data. Results show that the best micro-imagewise intersection over union (IOU) of 0.7751 was achieved from DeepLabv3+ when the training data consists of both real data and synthetic data. However, the results reflect that achieving good segmentation performance with synthetic data heavily depends on model architectures.  © 2023 ACM."
[No abstract available]
[No abstract available]
"Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community's programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization's impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI's work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods. © 2023 ACM."
"In the past years, AI has seen many advances in the field of NLP. This has led to the emergence of LLMs, such as the now famous GPT-3.5, which revolutionise the way humans can access or generate content. Current studies on LLM-based generative tools are mainly interested in the performance of such tools in generating relevant content (code, text or image). However, ethical concerns related to the design and use of generative tools seem to be growing, impacting the public acceptability for specific tasks. This paper presents a questionnaire survey to identify the intention to use generative tools by employees of an IT company in the context of their work. This survey is based on empirical models measuring intention to use (TAM by Davis, 1989, and UTAUT2 by Venkatesh and al., 2008). Our results indicate a rather average acceptability of generative tools, although the more useful the tool is perceived to be, the higher the intention to use seems to be. Furthermore, our analyses suggest that the frequency of use of generative tools is likely to be a key factor in understanding how employees perceive these tools in the context of their work. Following on from this work, we plan to investigate the nature of the requests that may be made to these tools by specific audiences.  © 2023 Owner/Author."
"Chronic wounds in diabetic patients are challenging because their prolonged inflammation makes healing difficult, thus burdening patients, society, and health care systems. Customized dressing materials are needed to effectively treat such wounds that vary in shape and depth. The continuous development of 3D-printing technology along with artificial intelligence has increased the precision, versatility, and compatibility of various materials, thus providing the considerable potential to meet the abovementioned needs. Herein, functional 3D-printing inks comprising DNA from salmon sperm and DNA-induced biosilica inspired by marine sponges, are developed for the machine learning-based 3D-printing of wound dressings. The DNA and biomineralized silica are incorporated into hydrogel inks in a fast, facile manner. The 3D-printed wound dressing thus generates provided appropriate porosity, characterized by effective exudate and blood absorption at wound sites, and mechanical tunability indicated by good shape fidelity and printability during optimized 3D printing. Moreover, the DNA and biomineralized silica act as nanotherapeutics, enhancing the biological activity of the dressings in terms of reactive oxygen species scavenging, angiogenesis, and anti-inflammation activity, thereby accelerating acute and diabetic wound healing. These bioinspired 3D-printed hydrogels produce using a DNA-induced biomineralization strategy are an excellent functional platform for clinical applications in acute and chronic wound repair. © 2023 The Authors. Advanced Science published by Wiley-VCH GmbH."
"VISIONE is a large-scale video retrieval system that integrates multiple search functionalities, including free text search, spatial color and object search, visual and semantic similarity search, and temporal search. The system leverages cutting-edge AI technology for visual analysis and advanced indexing techniques to ensure scalability. As demonstrated by its runner-up position in the 2023 Video Browser Showdown competition, VISIONE effectively integrates these capabilities to provide a comprehensive video retrieval solution. A system demo is available online, showcasing its capabilities on over 2300 hours of diverse video content (V3C1+V3C2 dataset) and 12 hours of highly redundant content (Marine dataset). The demo can be accessed at https://visione.isti.cnr.it/.  © 2023 Owner/Author."
"The recent success of artificial intelligence (AI) systems has been accompanied by a rapid increase in the computational resources needed to successfully train them. This rate of increase threatens the future development of AI systems as they are presently configured. Unsupervised learning, where systems are trained online instead of through offline computation, offers a possible way forward. Here, we present the design of a synaptic circuit made from superconducting electronics capable of spike-timing dependent plasticity (STDP), a form of unsupervised learning. The synapse is constructed from three sub-circuits, each responsible for a part of the synaptic action. We demonstrate the operation of the synapse through numerical simulation and show that it reproduces the hallmark behaviors of STDP. Combined with existing superconducting neuromorphic components like neurons and axons, this synaptic structure could help form a fast, powerful, and energy-efficient Spiking Neural Network. © 2023 Author(s)."
"Public attention towards explainability of artificial intelligence (AI) systems has been rising in recent years to offer methodologies for human oversight. This has translated into the proliferation of research outputs, such as from Explainable AI, to enhance transparency and control for system debugging and monitoring, and intelligibility of system process and output for user services. Yet, such outputs are difficult to adopt on a practical level due to a lack of a common regulatory baseline, and the contextual nature of explanations. Governmental policies are now attempting to tackle such exigence, however it remains unclear to what extent published communications, regulations, and standards adopt an informed perspective to support research, industry, and civil interests. In this study, we perform the first thematic and gap analysis of this plethora of policies and standards on explainability in the EU, US, and UK. Through a rigorous survey of policy documents, we first contribute an overview of governmental regulatory trajectories within AI explainability and its sociotechnical impacts. We find that policies are often informed by coarse notions and requirements for explanations. This might be due to the willingness to conciliate explanations foremost as a risk management tool for AI oversight, but also due to the lack of a consensus on what constitutes a valid algorithmic explanation, and how feasible the implementation and deployment of such explanations are across stakeholders of an organization. Informed by AI explainability research, we then conduct a gap analysis of existing policies, which leads us to formulate a set of recommendations on how to address explainability in regulations for AI systems, especially discussing the definition, feasibility, and usability of explanations, as well as allocating accountability to explanation providers. © 2023 ACM."
"Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction-that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how non-Western ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the 'Natyashastra'), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability © 2023 Owner/Author."
"As malware continues to evolve, deep learning models are increasingly used for malware detection and classification, including image-based classification. However, adversarial attacks can be used to perturb images so as to evade detection by these models. This study investigates the effectiveness of training deep learning models with Generative Adversarial Network-generated data to improve their robustness against such attacks. Two image conversion methods, byteplot and space-filling curves, were used to represent the malware samples, and a ResNet-50 architecture was used to train models on the image datasets. The models were then tested against a projected gradient descent attack. It was found that without GAN-generated data, the models' prediction performance drastically decreased from 93-95% to 4.5% accuracy. However, the addition of adversarial images to the training data almost doubled the accuracy of the models. This study highlights the potential benefits of incorporating GAN-generated data in the training of deep learning models to improve their robustness against adversarial attacks.  © 2023 Owner/Author."
"The question of who should be held responsible when machines cause harm in high-risk environments is open to debate. Empirical research examining laypeople's opinions has been largely restricted to the moral domain and has only inspected a limited set of negative outcomes. This study collects lay perceptions of legal responsibility for a wide range of machine-caused harms. We investigated how much people (N = 572) expect users and developers of machines to pay as legal damages in 37 diverse scenarios from the book ""How Humans Judge Machines""by Hidalgo et al. [37]. Our results suggest that people's expectations of legal damages for machine-caused harms are influenced by several factors, including perceived moral wrongness and the presence of victims. The scenarios exhibited substantial variation in how they were perceived and thus in the amount of legal damages they called for. People viewed both users and developers as legally responsible and expected the latter to pay higher damages. We discuss our findings in the context of future regulations of machines. © 2023 Owner/Author."
"Experiments in immersive environments allow the collection of large amounts of data that are closely related to individual behaviour. The recording of such experiments allows for the complex study of under-constrained tasks. That is, tasks that allow for a high degree of contingency in their resolution. This contingency allows for better discrimination of individual behaviour. However, the high complexity of the tasks makes them difficult to analyse. My thesis aims to discuss the advantages of Immersive Analytics for analysing hybrid sequential data (trajectory and events) generated in immersive environments. The analysis needs to be performed at a very high level of abstraction due to the high contingency of behaviours extracted from immersive environments. The massive amount of data generated highlights the need to build a model that allows feature extraction at a high level of abstraction. Since the exploration scheme is unknown in advance, the visualisations provided to the analyst should be highly interactive and adaptable to follow the analyst's queries as he or she searches for new insights in the data.  © 2023 Owner/Author."
"Information technology to some extent has changed the landscape of the accounting process and thus, the accountants will have to be alert of the technological changes to remain relevant. The emerging of IR 4.0 has been discussed in many platforms abroad, yet little is known if any, on Artificial Intelligence (AI) and accountants especially in the context of Malaysia. Artificial Intelligence (AI) is a technology that enables machine to learn quickly in real time and it can be applied to the whole organization. It was predicted that when industries adopt AI at full scale, most of the traditional accounting processes would be driven by machine and this implied that the traditional accountants' functions must be redefined or remodelled. Thus, the objective of the study is to investigate the job scope and roles of accountants in AI environment companies as well as to develop an AI implementation model. This study adopts qualitative method using interviews with accountants in the industry and in accounting/audit firms who work in the AI setting companies. Accountant's nature of work, tasks and changes to the traditional accountant's task would be investigated. Further subjects could be identified via snowballing methods, by asking for similar companies that practice AI in their operations. Thus, it will help to resurface the important elements of how accountants job scope and roles would be under the AI settings. Data will be analysed using thematic analysis. It is expected that the outcomes from the findings will be a model to assist the government especially in accelerating the adoption as well as the implementation and extending the full benefits of Artificial Intelligence among professionals. This study could contribute to achieve the national agenda outlined in NITA and TN50 by increasing the implementation of AI (innovation) among accountants.  © 2023 Author(s)."
"AI-assisted tools have become more prevalent than ever in the last few years. However, applying them to build a lifelog retrieval system is still non-trivial due to the disparity in interfaces and interactions. The Lifelog Search Challenge (LSC) aims to provide a testing ground where systems can be benchmarked in a highly competitive setting. In this paper, we present the fourth iteration of our participating system FIRST. For this year, we adopt generative models to equip the system with predictive ability rather than entirely relying on the user to input the query. We also index a sequence of images as an event for improved search speed. Finally, we demonstrate how the additional features can assist users in searching.  © 2023 ACM."
"Society's increasing dependence on Artificial Intelligence (AI) and AI-enabled systems require a more practical approach from software engineering (SE) executives in middle and higher-level management to improve their involvement in implementing AI ethics by making ethical requirements part of their management practices. However, research indicates that most work on implementing ethical requirements in SE management primarily focuses on technical development, with scarce findings for middle and higher-level management. We investigate this by interviewing ten Finnish SE executives in middle and higher-level management to examine how they consider and implement ethical requirements. We use ethical requirements from the European Union (EU) Trustworthy Ethics guidelines for Trustworthy AI as our reference for ethical requirements and an Agile portfolio management framework to analyze implementation. Our findings reveal a general consideration of privacy and data governance ethical requirements as legal requirements with no other consideration for ethical requirements identified. The findings also show practicable consideration of ethical requirements as technical robustness and safety for implementation as risk requirements and societal and environmental well-being for implementation as sustainability requirements. We examine a practical approach to implementing ethical requirements using the ethical risk requirements stack employing the Agile portfolio management framework.  © 2023 Owner/Author."
"The prevalence and significance of web services in our daily lives make it imperative to ensure that they are - as much as possible - free from vulnerabilities. However, developing a complex piece of software free from any security vulnerabilities is hard, if not impossible. One way to progress towards achieving this holy grail is by using static code analysis tools to root out any common or known vulnerabilities that may accidentally be introduced during the development process. Static code analysis tools have significantly contributed to addressing the problem above, but are imperfect. It is conceivable that static code analysis can be improved by using AI-powered tools, which have recently increased in popularity. However, there is still very little work in analysing both types of tools' effectiveness, and this is a research gap that our paper aims to fill. We carried out a study involving 11 static code analysers, and one AI-powered chatbot named ChatGPT, to assess their effectiveness in detecting 92 vulnerabilities representing the top 10 known vulnerability categories in web applications, as classified by OWASP. We particularly focused on PHP vulnerabilities since it is one of the most widely used languages in web applications. However, it has few security mechanisms to help its software developers. We found that the success rate of ChatGPT in terms of finding security vulnerabilities in PHP is around 62-68%. At the same time, the best traditional static code analyser tested has a success rate of 32%. Even combining several traditional static code analysers (with the best features on certain aspects of detection) would only achieve a rate of 53%, which is still significantly lower than ChatGPT's success rate. Nonetheless, ChatGPT has a very high false positive rate of 91%. In comparison, the worst false positive rate of any traditional static code analyser is 82%. These findings highlight the promising potential of ChatGPT for improving the static code analysis process but reveal certain caveats (especially regarding accuracy) in its current state. Our findings suggest that one interesting possibility to explore in future works would be to pick the best of both worlds by combining traditional static code analysers with ChatGPT to find security vulnerabilities more effectively.  © 2023 Owner/Author."
"The use of artificial intelligence (AI) in healthcare has the potential to improve patient outcomes and increase efficiency in the delivery of care. However, the design, deployment and use of AI in healthcare must be guided by a set of ethical principles that prioritize the well-being of patients and the community, and ensure that care is delivered equitably. A growing body of literature on algorithmic injustice illustrates that AI systems in healthcare have the potential to cause social harm, especially to vulnerable communities. Existing ethical principles in healthcare are based on, and mostly influenced by, Western epistemology, which emphasizes individual rights, often at the expense of collective well-being. The African philosophy of Ubuntu, which emphasises the interconnectedness and interdependence of all people, is an attractive framework for addressing ethical concerns in AI for healthcare because healthcare is intrinsically a community-wide issue. This paper discusses the relevance of Ubuntu ethics in the context of AI for healthcare and proposes principles to reinvigorate the spirit of ""I am because we are""in design, deployment and use. These principles are fairness, community good, safeguarding humanity, respect for others and trust, and we believe their application will support co-designing, deploying and using inclusive AI systems that will enable clinicians to deliver equitable care for all. Highlighting the relational aspect of Ubuntu, this paper not only calls for rethinking AI ethics in healthcare but also endorses discussions about the need for non-Western ethical approaches to be utilised in AI ethics more broadly. © 2023 ACM."
"With the advancement of green energy technology and rising public and political acceptance, electric vehicles (EVs) have grown in popularity. Electric motors, batteries, and charging systems are considered major components of EVs. The electric power infrastructure has been designed to accommodate the needs of EVs, with an emphasis on bidirectional power flow to facilitate power exchange. Furthermore, the communication infrastructure has been enhanced to enable cars to communicate and exchange information with one another, also known as Vehicle-to-Everything (V2X) technology. V2X is positioned to become a bigger and smarter system in the future of transportation, thanks to upcoming digital technologies like Artificial Intelligence (AI), Distributed Ledger Technology, and the Internet of Things. However, like with any technology that includes data collection and sharing, there are issues with digital privacy and cybersecurity. This paper addresses these concerns by creating a multi-layer Cyber-Physical-Social Systems (CPSS) architecture to investigate possible privacy and cybersecurity risks associated with V2X. Using the CPSS paradigm, this research explores the interaction of EV infrastructure as a very critical part of the V2X ecosystem, digital privacy, and cybersecurity concerns.  © 2023 Owner/Author."
"In the field of clinical imaging, computer-aided assessment (CAD) is a dynamic and rapidly developing area of research. Recently, so much effort has been put into further development of symptomatic PC applications, where failures in clinical presentation systems can lead to real abuse. Artificial intelligence plays an important role in diagnosing computers. Artificial Intelligence (AI) is the investigation of PC calculations that can consequently work on thorough experience and information use. It is viewed as a feature of computerized reasoning. AI calculations construct a model from an example of information, called preparing information, to settle on forecasts or choices without being modified to do as such. Machine learning calculations are utilized in a wide assortment of uses, for instance in medication, email separating, discourse acknowledgment and PC vision, where it is troublesome or difficult to foster customary calculations to play out the necessary assignments. The subset of artificial intelligence is closely related to computational understanding, which focuses onmaking predictions using personal computers. The study of numerical advances provides strategies, hypotheses, and applications in the field of artificial intelligence. Information mining is a related field of research that focuses on the exploratory exploration of information through self-directed learning. Some Machine Learning (ML) applications use information and brain networks that emulate how the organic mind functions. In its application to business issues, ML is likewise alluded to as prescient examination. Plan acknowledgment is essentially about learning as a visual show. In the field of biomedicine, tolerating plan and manmade reasoning vows to runafter a more profound comprehension of contamination and control. This adds greater objectivity to thepowerful collaboration. For the investigation of multi-faceted and blended biomedical data, computerized reasoning furnishes a decent method for managing dynamic and robotized information handling. This article gives a similar investigation of various ML calculations for diagnosing different illnesses like coronary illness, diabetes, liver sickness, dengue fever and hepatitis. ©2023 Nova Science Publishers, Inc. All rights reserved."
[No abstract available]
"'Emotion AI' is a subset of artificial intelligence (AI) technologies that claim to be able to detect the inner emotional states of individuals by collecting biometric information such as face scans, voice recordings, and traces of physical movement. Despite their growing popularity in education, these systems have the potential to produce serious harm. In this paper, we argue that a major concern with emotion AI technologies has to do with the theories of emotion that undergird them. Most emotion AI technologies are built on the foundations of anti-intentionalist theories of human emotion, which claim that emotions can be understood as discrete, universal states that arise as automatic physiological responses. Anti-intentionalists suggest that emotions are not directed at any object, or subject to cognitive reasons. In our work, we focus on the increasing use of these technologies in education to illustrate the ways in which these anti-intentionalist systems are problematic, as they dissolve the space for pushback against the judgements they make. We argue that their use thereby contributes to harms towards children broadly centered around student disempowerment, surveillance, and classification. We then consider three alternative policy approaches to emotion AI use in schools in light of their role with this political agenda of emotion commodification, assessing each of these options - interpretability, technical reform, and non-use - for their desirability and feasibility. In doing so, we underscore the conceptual harms produced by emotion AI systems in the context of education, and the criteria by which these technologies should be judged by educators and policymakers. © 2023 ACM."
"BACKGROUND Recently, artificial intelligence (AI) has been widely used in gastrointestinal endoscopy examinations. AIM To comprehensively evaluate the application of AI-assisted endoscopy in detecting different digestive diseases using bibliometric analysis. METHODS Relevant publications from the Web of Science published from 1990 to 2022 were extracted using a combination of the search terms “AI” and “endoscopy”. The following information was recorded from the included publications: Title, author, institution, country, endoscopy type, disease type, performance of AI, publication, citation, journal and H-index. RESULTS A total of 446 studies were included. The number of articles reached its peak in 2021, and the annual citation numbers increased after 2006. China, the United States and Japan were dominant countries in this field, accounting for 28.7%, 16.8%, and 15.7% of publications, respectively. The Tada Tomohiro Institute of Gastroenterology and Proctology was the most influential institution. “Cancer” and “polyps” were the hotspots in this field. Colorectal polyps were the most concerning and researched disease, followed by gastric cancer and gastrointestinal bleeding. Conventional endoscopy was the most common type of examination. The accuracy of AI in detecting Barrett’s esophagus, colorectal polyps and gastric cancer from 2018 to 2022 is 87.6%, 93.7% and 88.3%, respectively. The detection rates of adenoma and gastrointestinal bleeding from 2018 to 2022 are 31.3% and 96.2%, respectively. CONCLUSION AI could improve the detection rate of digestive tract diseases and a convolutional neural network-based diagnosis program for endoscopic images shows promising results. ©The Author(s) 2023. Published by Baishideng Publishing Group Inc. All rights reserved."
"Artificial intelligence can facilitate the management of large amounts of media content and enable media organisations to extract valuable insights from their data. Although AI for media understanding has made rapid progress over the recent years, its deployment in applications and professional sectors poses challenges, especially to organizations with no AI expertise. This motivated the creation of the Media Asset Annotation and Management platform (MAAM) that employs state-of-the-art deep learning models to annotate and facilitate the management of image and video assets. Annotation models provided by MAAM include automatic captioning, object detection, action recognition and moderation models, such as NSFW and disturbing content classifiers. By annotating media assets with these models, MAAM can support easy navigation, filtering and retrieval of media assets. In addition, our platform leverages the power of deep learning to support advanced visual and multi-modal retrieval capabilities. That allows accurately identifying assets that convey a similar idea, or concept even if they are not visually identical, and support a state-of-the-art reverse search facility for images and videos.  © 2023 Owner/Author."
"Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate ""ethics washing.""Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies. © 2023 Owner/Author."
"Developments in object-based media and IP-based delivery offer an opportunity to create superior audience experiences through personalisation. Towards the aim of making personalised experiences regularly available across the breadth of audio-visual media, we conducted a study to understand how personalised experiences are being created. This consisted of interviews with producers of six representative case studies, followed by a thematic analysis. We describe the workflows and report on the producers' experiences and obstacles faced. We found that the metadata models, enabling personalisation, were developed independently for each experience, restricting interoperability of personalisation affordances provided to users. Furthermore, the available tools were not effectively integrated into preferred workflows, substantially increasing role responsibilities and production time. To ameliorate these issues, we propose the development of a unifying metadata framework and novel production tools. These tools should be integrated into existing workflows; improve efficiency using AI; and enable producers to serve more diverse audiences.  © 2023 ACM."
"There is no scientific consensus on what is meant by ""emotion""- researchers have examined various phenomena spanning brain modes, feelings, sensations, and cognitive structures, among others, in their study of emotional experiences. For the purposes of developing an AI speech emotion recognition (SER) system, however, emotion must be defined, bounded, and instantiated as ground truth in the training data. This means practical choices must be made in which particular emotional ontologies are prioritized over others in the construction of SER datasets. In this paper, I explore these tensions around fairness, accountability, and transparency by analyzing open-source datasets used for SER applications along with their accompanying methodology papers. Specifically, I critique the centrality of discrete emotion theory in SER applications as a contestable emotional framework that is invoked primarily for its practical utility and alignment - as opposed to scientific rigor - with machine learning epistemologies. In so doing, I also shed light on the role of the dataset creators as emotional designers in their attempt to produce, elicit, record, and index emotional expressions for the purposes of crafting SER training datasets. Ultimately, by further querying SER through the aperture of Critical Disability Studies, I use this empirical work to examine the sociopolitical stakes of SER as a normative and regulatory technology that siphons emotion into a broader agenda of capitalistic productivity in the context of call center optimization. © 2023 Owner/Author."
"Warning: This paper contains examples of gender non-affirmative language which could be offensive, upsetting, and/or triggering. Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization of TGNB persons contributes to and persists within Open Language Generation (OLG). This social knowledge serves as a guide for evaluating popular large language models (LLMs) on two key aspects: (1) misgendering and (2) harmful responses to gender disclosure. To do this, we introduce TANGO, a dataset of template-based real-world text curated from a TGNB-oriented community. We discover a dominance of binary gender norms reflected by the models; LLMs least misgendered subjects in generated text when triggered by prompts whose subjects used binary pronouns. Meanwhile, misgendering was most prevalent when triggering generation with singular they and neopronouns. When prompted with gender disclosures, TGNB disclosure generated the most stigmatizing language and scored most toxic, on average. Our findings warrant further research on how TGNB harms manifest in LLMs and serve as a broader case study toward concretely grounding the design of gender-inclusive AI in community voices and interdisciplinary literature. © 2023 Owner/Author."
"Warning: The content of this paper may be upsetting or triggering. Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body or a collection of body parts. We replicate three experiments in the psychology literature quantifying sexual objectification and show that the phenomena persist in trained AI models. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d > 0.80) and sadness (d > 0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images where subjects are partially clothed. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of ""a [age] year old girl""generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP (age 17), and up to 42% of the time for Stable Diffusion (ages 14 and 18); the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on automatically collected web scrapes learn biases of sexual objectification, which propagate to downstream applications. © 2023 Owner/Author."
"Smart scenarios related to industries or cities, characterized by intensive technology transfer and use of innovative and disruptive technologies, have been in the spotlight either on academic or organizational discussions, especially those with a technocentric focus. Among these technologies, artificial intelligence (AI) emerges as the most challenging one due to its complexity. Therefore, this chapter aims to address AI, in particular the future of the labor market, exploring the challenges regarding the skills required in the context of AI technology, addressing its uses, challenges, and benefits. In order to achieve this goal, a systematic review was conducted on the extant literature using the methodology Methodi Ordinatio. The results show that the current literature is gradually changing from a more critical and negative view of AI to a more optimistic one, with more positive approaches and expectations regarding its benefits. As practical implications, the findings can be used as a guide for governments to develop strategies aiming to deal with upcoming challenges, especially regarding future jobs and employability. © 2023 by Regina Negri Pagani, Clayton Pereira De Sá, Alana Corsi, and Fabiane Florêncio De Souza Published under exclusive licence by Emerald Publishing Limited. All rights reservesd."
"Warning: The content of this paper may be upsetting or triggering. The rapid deployment of artificial intelligence (AI) models de- demands a thorough investigation of biases and risks inherent in these models to understand their impact on individuals and society. A growing body of work has shown that social biases are encoded in language models and their downstream tasks. This study extends the focus of bias evaluation in extant work by examining bias against social stigmas on a large scale. It focuses on 93 stigmatized groups in the United States, including a wide range of conditions related to disease, disability, drug use, mental illness, religion, sexuality, socioeconomic status, and other relevant factors. We investigate bias against these groups in English pre-trained Masked Language Models (MLMs) and their downstream sentiment classification tasks. To evaluate the presence of bias against 93 stigmatized conditions, we identify 29 non-stigmatized conditions to conduct a comparative analysis. Building upon a psychology scale of social rejection, the Social Distance Scale, we prompt six MLMs that are trained with different datasets: RoBERTa-base, RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to analyze the predicted words from these models, with which we measure the extent of bias against stigmatized groups. When prompts include stigmatized conditions, the probability of MLMs predicting negative words is, on average, 20 percent higher than when prompts have non-stigmatized conditions. Bias against stigmatized groups is also reflected in four downstream sentiment classifiers of these models. When sentences include stigmatized conditions related to diseases, disability, education, and mental illness, they are more likely to be classified as negative. For example, the sentence ""They are people who have less than a high school education.""is classified as negative consistently across all models. We also observe a strong correlation between bias in MLMs and their downstream sentiment classifiers (Pearson's r =0.79). The evidence indicates that MLMs and their downstream sentiment classification tasks exhibit biases against socially stigmatized groups. © 2023 Owner/Author."
"Data quality assessment has become a prominent component in the successful execution of complex data-driven artificial intelligence (AI) software systems. In practice, real-world applications generate huge volumes of data at speeds. These data streams require analysis and preprocessing before being permanently stored or used in a learning task. Therefore, significant attention has been paid to the systematic management and construction of high-quality datasets. Nevertheless, managing voluminous and high-velocity data streams is usually performed manually (i.e. offline), making it an impractical strategy in production environments. To address this challenge, DataOps has emerged to achieve life-cycle automation of data processes using DevOps principles. However, determining the data quality based on a fitness scale constitutes a complex task within the framework of DataOps. This paper presents a novel Data Quality Scoring Operations (DQSOps) framework that yields a quality score for production data in DataOps workflows. The framework incorporates two scoring approaches, an ML prediction-based approach that predicts the data quality score and a standard-based approach that periodically produces the ground-truth scores based on assessing several data quality dimensions. We deploy the DQSOps framework in a real-world industrial use case. The results show that DQSOps achieves significant computational speedup rates compared to the conventional approach of data quality scoring while maintaining high prediction performance.  © 2023 Owner/Author."
[No abstract available]
"The ""impossibility theorem""- which is considered foundational in algorithmic fairness literature - asserts that there must be trade-offs between common notions of fairness and performance when fitting statistical models, except in two special cases: when the prevalence of the outcome being predicted is equal across groups, or when a perfectly accurate predictor is used. However, theory does not always translate to practice. In this work, we challenge the implications of the impossibility theorem in practical settings. First, we show analytically that, by slightly relaxing the impossibility theorem (to accommodate a practitioner's perspective of fairness), it becomes possible to identify abundant sets of models that satisfy seemingly incompatible fairness constraints. Second, we demonstrate the existence of these models through extensive experiments on five real-world datasets. We conclude by offering tools and guidance for practitioners to understand when - and to what degree - fairness along multiple criteria can be achieved. This work has an important implication for the community: achieving fairness along multiple metrics for multiple groups (and their intersections) is much more possible than was previously believed. © 2023 Owner/Author."
[No abstract available]
"Automatic facial expression recognition (FER) has a lot of potential applications. However, even if it can be beneficial for some areas, e.g. security and healthcare, several legal and ethical challenges arise. In this article, we first present such challenges related to the deployment of FER. Then, we introduce the conduct of a focus group which allowed to highlight interesting points regarding the use of FER in a medical context. Particularly, transparency, data management, diagnoses, liability, best endeavours obligation, and non-discrimination principle are debated. We finally discuss on our study's limitations and directions for future work.  © 2023 Owner/Author."
"In the last years, the raise of Artificial Intelligence (AI), and its pervasiveness in our lives, has sparked a flourishing debate about the ethical principles that should lead its implementation and use in society. Driven by these concerns, we conduct a rapid review of several frameworks providing principles, guidelines, and/or tools to help practitioners in the development and deployment of Responsible AI (RAI) applications. We map each framework w.r.t. the different Software Development Life Cycle (SDLC) phases discovering that most of these frameworks fall just in the Requirements Elicitation phase, leaving the other phases uncovered. Very few of these frameworks offer supporting tools for practitioners, and they are mainly provided by private companies. Our results reveal that there is not a ""catching-all""framework supporting both technical and non-technical stakeholders in the implementation of real-world projects. Our findings highlight the lack of a comprehensive framework encompassing all RAI principles and all (SDLC) phases that could be navigated by users with different skill sets and with different goals.  © 2023 ACM."
"AI systems are adopted in numerous domains due to their increasingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop common frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other's work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making. © 2023 ACM."
"In this paper, we argue for a paradigm shift from the current model of explainable artificial intelligence (XAI), which may be counter-productive to better human decision making. In early decision support systems, we assumed that we could give people recommendations and that they would consider them, and then follow them when required. However, research found that people often ignore recommendations because they do not trust them; or perhaps even worse, people follow them blindly, even when the recommendations are wrong. Explainable artificial intelligence mitigates this by helping people to understand how and why models give certain recommendations. However, recent research shows that people do not always engage with explainability tools enough to help improve decision making. The assumption that people will engage with recommendations and explanations has proven to be unfounded. We argue this is because we have failed to account for two things. First, recommendations (and their explanations) take control from human decision makers, limiting their agency. Second, giving recommendations and explanations does not align with the cognitive processes employed by people making decisions. This position paper proposes a new conceptual framework called Evaluative AI for explainable decision support. This is a machine-in-the-loop paradigm in which decision support tools provide evidence for and against decisions made by people, rather than provide recommendations to accept or reject. We argue that this mitigates issues of over- and under-reliance on decision support tools, and better leverages human expertise in decision making. © 2023 ACM."
[No abstract available]
"While recent studies indicate that AI could play an important role in detecting early signs of Alzheimer's disease in speech, this use of data from individuals with cognitive decline raises numerous ethical concerns. In this paper, we identify and explain concerns related to autonomy (including consent, depersonalization and disclosure), privacy and data protection (including the handling of personal content and medical information), welfare (including distress, discrimination and reliability), transparency (including the interpretability of language features and AI-based decision-making for developers and clinicians), and fairness (including bias and the distribution of benefits). Our aim is to not only raise awareness of the ethical concerns posed by the use of AI in speech-based Alzheimer's detection, but also identify ways in which these concerns might be addressed. To this end, we conclude with a list of suggestions that could be incorporated into ethical guidelines for researchers and clinicians working in this area. © 2023 Owner/Author."
"The Ozone Mapping and Profiler Suite (OMPS) has been on board the Suomi National Polar-orbiting Partnership (S-NPP) satellite since October 2011 and was followed by an OMPS on NOAA-20 (N20) in November 2017 as part of the US Joint Polar Satellite System (JPSS) program. The OMPS measurements are processed to yield various products of atmospheric composition data for near-real-time monitoring and offline study, including retrievals of total column ozone (TCO) and an ultraviolet-absorbing aerosol index (AI) based on the version-8 total ozone (V8TOZ) algorithm. With the implementation of changes to employ a broadband channel approach in the NOAA OMPS V8TOZ, the retrieved TCO and AI products have become more stable and consistent between S-NPP and N20. Two particular regions have been chosen for building soft-calibration adjustments for both OMPS S-NPP and N20, which force the V8TOZ retrievals to be in quite good agreement from both sensors with little change by season. However, bias analysis shows that some noticeable errors and differences still exist after soft-calibration, and those errors appear to be quite persistently associated with solar zenith angle (SZA) and satellite viewing angle (SVA) in the retrievals of TCO and AI for both OMPS S-NPP and N20. Comparisons of TCO and AI from NOAA OMPS retrievals with other products such as those from the Tropospheric Monitoring Instrument (TROPOMI) and the Earth Polychromatic Imaging Camera (EPIC) show that, although the sensor, algorithm, and solar spectra are different among them, the overall retrievals from those products are quite similar and consistent.  © Copyright 2023 Authors"
"Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users' trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users' preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem. © 2023 Owner/Author."
"Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators. © 2023 Owner/Author."
"The development of processes and tools for ethical, trustworthy, and legal AI is only beginning. At the same time, legal requirements are emerging in various jurisdictions, following a deluge of ethical guidelines. It is therefore key to explore the necessary practices that must be adopted to ensure the quality of AI systems, mitigate their potential risks and enable legal compliance. Ensuring that the potential negative impacts of AI on individuals, society, and the environment are mitigated will depend on many factors, including the capacity to properly regulate its deployment and to mandate necessary internal best practices along lifecycles. Regulatory frameworks must evolve from abstract requirements to providing concrete operational mandates that enable better oversight mechanisms in the way AI systems operate, how they are developed, and how they are deployed. In view of the above, this paper explores the necessary practices that can be adopted throughout a comprehensive lifecycle audit as a key practice to ensure the quality of AI systems and enable the development of compliance mechanisms. It also discusses novel governance tools that enable bridging the current operational gaps. Such gaps were identified by interviewing experts, analysing adaptable tools and methodologies from the software engineering domain, and by exploring the state of the art of auditing. The results present recommendations for novel tools and oversight mechanisms for governing AI systems. © 2023 ACM."
"As the need for societal and organizational cybersecurity increases, artificial intelligence (AI) driven intelligent virtual and physical assistants (robots) appear to offer promising solutions to help address future gaps in the work force. However, some important and as yet unanswered questions on trust arise. This extended abstract begins to tackle how trust may function in high-risk cybersecurity environments in the context of zero trust. Leveraging well-established trust building theory allows the first steps towards novel solutions to be developed and proposed. Implications and promising directions for future work are presented.  © 2023 Owner/Author."
"AI is becoming increasingly popular in artistic work. Yet tools for calculating environmental impact of AI are more adapted for other contexts than creative practices, making them sometimes hard to comprehend for the non-expert. In this study, based on interviews with AI artists, a design artifact called The Green Notebook was developed: a physical notebook where the AI artist could discuss ideas and receive feedback of their expected environmental impact. The conversational experience between the artist and the interface was informed by online content analysis of artistic work processes. The Notebook was explored and assessed with five artists in Wizard-of-Oz and focus group studies. Generally, the participants found a co-creation process with the enhanced ability to reflect on sustainability an accessible way to engage with sustainability considerations of their AI artistic practices. We provide insights of the Notebook's perceived role and the conversational strategies used by the artists. Furthermore, we discuss trade-offs between politeness vs. efficiency and focus vs. integration to inform future research.  © 2023 Owner/Author."
"Online social networks use AI techniques to automatically infer profiles from users' shared data. However, these inferences and their effects remain, to a large extent, opaque to the users themselves. We propose a method which raises user awareness about the potential use of their profiles in impactful situations, such as searching for a job or an accommodation. These situations illustrate usage contexts that users might not have anticipated when deciding to share their data. User photographic profiles are described by automatic object detections in profile photos, and associated object ratings in situations. Human ratings of the profiles per situation are also available for training. These data are represented as graph structures which are fed into graph neural networks in order to learn how to automatically rate them. An adaptation of the learning procedure per situation is proposed since the same profile is likely to be interpreted differently, depending on the context. Automatic profile ratings are compared to one another in order to inform individual users of their standing with respect to others. Our method is evaluated on a public dataset, and consistently outperforms competitive baselines. An ablation study gives insights about the role of its main components.  © 2023 ACM."
"Artificial Intelligence (AI), and in particular generative models, are transformative tools for knowledge work. They problematise notions of creativity, originality, plagiarism, the attribution of credit, and copyright ownership. Critics of generative models emphasise the reliance on large amounts of training data, and view the output of these models as no more than randomised plagiarism, remix, or collage of the source data. On these grounds many have argued for stronger regulations on the deployment, use, and attribution of the output of these models. However, these issues are not new or unique to artificial intelligence. In this position paper, using examples from literary criticism, the history of art, and copyright law, I show how creativity and originality resist definition as a notatable or information-theoretic property of an object, and instead can be seen as the property of a process, an author, or a viewer. Further alternative views hold that all creative work is essentially reuse (mostly without attribution), or that randomness itself can be creative. I suggest that creativity is ultimately defined by communities of creators and receivers, and the deemed sources of creativity in a workflow often depend on which parts of the workflow can be automated. Using examples from recent studies of AI in creative knowledge work, I suggest that AI shifts knowledge work from material production to critical integration. This position paper aims to begin a conversation around a more nuanced approach to the problems of creativity and credit assignment for generative models, one which more fully recognises the importance of the creative and curatorial voice of the users of these models, and moves away from simpler notational or information-theoretic views.  © 2023 ACM."
"Robo-advisors are democratizing access to life-insurance by enabling fully online underwriting. In Europe, financial legislation requires that the reasons for recommending a life insurance plan be explained according to the characteristics of the client, in order to empower the client to make a ""fully informed decision"". In this study conducted in France, we seek to understand whether legal requirements for feature-based explanations actually help users in their decision-making. We conduct a qualitative study to characterize the explainability needs formulated by non-expert users and by regulators expert in customer protection. We then run a large-scale quantitative study using Robex, a simplified robo-advisor built using ecological interface design that delivers recommendations with explanations in different hybrid textual and visual formats: either ""dialogic""- more textual - or ""graphical""- more visual. We find that providing feature-based explanations does not improve appropriate reliance or understanding compared to not providing any explanation. In addition, dialogic explanations increase users' trust in the recommendations of the robo-advisor, sometimes to the users' detriment. This real-world scenario illustrates how XAI can address information asymmetry in complex areas such as finance. This work has implications for other critical, AI-based recommender systems, where the General Data Protection Regulation (GDPR) may require similar provisions for feature-based explanations. © 2023 ACM."
"Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance (i.e. institutional governance) surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO.org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare. © 2023 Owner/Author."
"In recent years, discussions of responsible AI practices have seen growing support for 'participatory AI' approaches, intended to involve members of the public in the design and development of AI systems. Prior research has identified a lack of standardised methods or approaches for how to use participatory approaches in the AI development process. At present, there is a dearth of evidence on attitudes to and approaches for participation in the sites driving major AI developments: commercial AI labs. Through 12 semi-structured interviews with industry practitioners and subject-matter experts, this paper explores how commercial AI labs understand participatory AI approaches and the obstacles they have faced implementing these practices in the development of AI systems and research. We find that while interviewees view participation as a normative project that helps achieve 'societally beneficial' AI systems, practitioners face numerous barriers to embedding participatory approaches in their companies: participation is expensive and resource intensive, it is 'atomised' within companies, there is concern about exploitation, there is no incentive to be transparent about its adoption, and it is complicated by a lack of clear context. These barriers result in a piecemeal approach to participation that confers no decision-making power to participants and has little ongoing impact for AI labs. This paper's contribution is to provide novel empirical research on the implementation of public participation in commercial AI labs, and shed light on the current challenges of using participatory approaches in this context. © 2023 ACM."
"Recent advances in diffusion models and large language models have underpinned a new generation of powerful and accessible tools, and some of the most publicly visible applications are for artistic endeavour. Such tools, however, provide little scope for deeper understanding of AI systems, while the growing public interest in them can eclipse notice of the vibrant community of artists who have long worked with other forms of AI. We explore the potential for AI Art - particularly work in which AI is both tool and topic - to facilitate public AI literacies and consider how tactics developed before the current generative AI boom have continued relevance today. We look at the strategies of critical AI artists to scaffold public understanding of AI and enhance legibility for non-experts. This paper also investigates how collaborations between artists and AI researchers and designers can illuminate key technical and social issues relevant to the development of AI. The study entailed workshops between three professional artists who work with AI and a cross-disciplinary set of academic participants. This paper reports on these workshops and presents the intentions and strategies expressed by the artists, as well as insights of relevance to the research community on public AI literacies. We find that critical AI art can link underlying technical systems to structural issues of power and facilitate experiential learning that is situated and embodied, valuing interpretation over explanation. The findings also demonstrate the importance of transdisciplinary conversations around art, ethics and the political economy of AI technologies and how these dialogues may feed into AI design processes. © 2023 ACM."
"Network analysis and machine learning techniques have been widely applied for building malware detection systems. Though these systems attain impressive results, they often are (i) not extensible, being monolithic, well tuned for the specific task they have been designed for but very difficult to adapt and/or extend to other settings, and (ii) not interpretable, being black boxes whose inner complexity makes it impossible to link the result of detection with its root cause, making further analysis of threats a challenge. In this paper we present RADAR, an extensible and explainable system that exploits the popular TTP (Tactics, Techniques, and Procedures) ontology of adversary behaviour described in the industry-standard MITRE ATT&CK framework in order to unequivocally identify and classify malicious behaviour using network traffic. We evaluate RADAR on a very large dataset comprising of 2,286,907 malicious and benign samples, representing a total of 84,792,452 network flows. The experimental analysis confirms that the proposed methodology can be effectively exploited: RADAR's ability to detect malware is comparable to other state-of-the-art non-interpretable systems' capabilities. To the best of our knowledge, RADAR is the first TTP-based system for malware detection that uses machine learning while being extensible and explainable.  © 2023 ACM."
"Big data has enabled commuters to mimic human behavior and simulate it through artificial intelligence. However, when it comes to performing complex tasks where tacit knowledge is involved, AI has limitations. The purpose of this paper was to explore the use of machine learning and deep learning. The paper presents the description of ML and DL and compares their functions. Findings indicated that ML can cover the limitation of AI; however, ML application is also limited. DL can address some of the issues of ML but the literal understanding and intentionality yet to be learnt by DL. Besides, ML and DL are being employed in different fields.  © 2023 Author(s)."
"Multiaccess edge computing (MEC) is a promising approach to enhancing IoT devices running AI-based services. Especially, the edge-cloud architecture acts as a strong supporter of the resource-limited IoT devices. How to optimize the system resources efficiently to improve the service performance is the key issue in this scenario. Motivated by this, in this article, we focus on a multi-base station (BS) and multiservice edge-cloud-assisted IoT environment, where both the BSs (with edge servers deployed) and the cloud can assist the IoT devices to process multitype deep learning (DL) tasks via task offloading. DNN partition mechanism and both the communication and computing resources allocation are utilized to enable a collaborative optimization to minimize the processing delay of all the DL tasks in the system. Due to the mixed-integer nonlinear programming (MINLP) characteristic of our optimization problem, we propose an algorithm that decomposes the original problem into two subproblems, solves them separately, and then obtains the near-optimal solution efficiently. Extensive simulations are conducted by varying five different crucial parameters. The superiority of our scheme is demonstrated in comparisons with several other schemes proposed by existing works. Our scheme can achieve a notable 28.3% delay reduction on average. © 2014 IEEE."
"Investigative journalists and public defenders conduct the essential work of examining, reporting, and arguing critical cases around police use-of-force and misconduct. In an ideal world, they would have access to well-organized records they can easily navigate and search. In reality, records can come as large, disorganized data dumps, increasing the burden on the already resource-constrained teams. In a cross-disciplinary research team of stakeholders and computer scientists, we worked closely with public defenders and investigative journalists in the United States to co-design an AI-augmented tool that addresses challenges in working with such data dumps. Our Document Organization Tool (DOT) is a Python library that has data cleaning, extraction, and organization features. Our collaborative design process gave us insights into the needs of under-resourced teams who work with large data dumps, such as how some domain experts became self-taught programmers to automate their tasks. To understand what type of programming paradigm could support our target users, we conducted a user study (n=18) comparing visual, programming-by-example, and traditional text-based programming tools. From our user study, we found that once users passed the initial learning stage, they could comfortably use all three paradigms. Our work offers insights for designers working with under-resourced teams who want to consolidate cutting-edge algorithms and AI techniques into unified, expressive tools. We argue user-centered tool design can contribute to the broader fight for accountability and transparency by supporting existing practitioners in their work in domains like criminal justice. © 2023 Owner/Author."
"Developing AI tools that preserve fairness is of critical importance, specifically in high-stakes applications such as those in healthcare. However, health AI models' overall prediction performance is often prioritized over the possible biases such models could have. In this study, we show one possible approach to mitigate bias concerns by having healthcare institutions collaborate through a federated learning paradigm (FL; which is a popular choice in healthcare settings). While FL methods with an emphasis on fairness have been previously proposed, their underlying model and local implementation techniques, as well as their possible applications to the healthcare domain remain widely underinvestigated. Therefore, we propose a comprehensive FL approach with adversarial debiasing and a fair aggregation method, suitable to various fairness metrics, in the healthcare domain where electronic health records are used. Not only our approach explicitly mitigates bias as part of the optimization process, but an FL-based paradigm would also implicitly help with addressing data imbalance and increasing the data size, offering a practical solution for healthcare applications. We empirically demonstrate our method's superior performance on multiple experiments simulating large-scale real-world scenarios and compare it to several baselines. Our method has achieved promising fairness performance with the lowest impact on overall discrimination performance (accuracy). © 2023 ACM."
"The EU's proposed AI Act sets out a risk-based regulatory framework to govern the potential harms emanating from use of AI systems. Within the AI Act's hierarchy of risks, the AI systems that are likely to incur ""high-risk""to health, safety, and fundamental rights are subject to the majority of the Act's provisions. To include uses of AI where fundamental rights are at stake, Annex III of the Act provides a list of applications wherein the conditions that shape high-risk AI are described. For high-risk AI systems, the AI Act places obligations on providers and users regarding use of AI systems and keeping appropriate documentation through the use of harmonised standards. In this paper, we analyse the clauses defining the criteria for high-risk AI in Annex III to simplify identification of potential high-risk uses of AI by making explicit the ""core concepts""whose combination makes them high-risk. We use these core concepts to develop an open vocabulary for AI risks (VAIR) to represent and assist with AI risk assessments in a form that supports automation and integration. VAIR is intended to assist with identification and documentation of risks by providing a common vocabulary that facilitates knowledge sharing and interoperability between actors in the AI value chain. Given that the AI Act relies on harmonised standards for much of its compliance and enforcement regarding high-risk AI systems, we explore the implications of current international standardisation activities undertaken by ISO and emphasise the necessity of better risk and impact knowledge bases such as VAIR that can be integrated with audits and investigations to simplify the AI Act's application. © 2023 Owner/Author."
"In order to unlock the full potential of Extended Reality (XR) and its application to societal sectors such as health (e.g., training) or Industry 5.0 (e.g., remote control of infrastructure) there is a need for very realistic environments to enhance the presence of the user. However, current photo-realistic content generation methods (such as Light Fields) require a massive amount of data transmission (i.e., ultra-high bandwidths) and extreme computational power for displaying. Thus, they are not suited for interactive immersive and realistic applications. In this research, we hypothesize that is possible to generate realistic dynamic 3D environments by means of Deep Generative Networks. The work will consist of two parts: (1) a computer vision system that generates the 3D environment based on 2D images, and (2) a Human-Computer Interaction system (HCI) that predicts Region of Interest (RoI) for efficient 3D rendering, subjective and objective assessment of user perception (by means of presence) to enhance the 3D scene quality. This work aims to gain insights into how well deep generative methods can create realistic and immersive environments. This will significantly help future developments in realistic and immersive XR content creation.  © 2023 Owner/Author."
"Pair Programming (PP) is an Agile software development methodology that involves two developers working together on a single computer. However, the physical presence of two developers has become a challenge in recent years due to the pandemic, necessitating remote collaboration methods such as Distributed Pair Programming (DPP). DPP has been found to have similar benefits to in-person PP, but the issue of team compatibility remains unresolved. These are more evident in the educational field of Agile methodologies. To address these challenges, we developed a novel approach by creating a Mixed Reality (MR) application that enables users to learn PP with the assistance of a conversational intelligent virtual avatar. The application uses the HoloLens MR device and a Conversational Agent (CA) extension integrated into Visual Studio Code to provide suggestions for improving the code written by the user. The virtual avatar animates these suggestions, making it appear to speak and interact with the user in real time. This system aims to overcome the limitations of common DPP methods, allowing a single developer to learn and apply the PP methodology even when a human partner is unavailable.  © 2023 ACM."
"This study aims at describing the percentage of protected and unprotected antibody titer on Avian Influenza (AI) in Ayam Filipina (Clarets) in Gorontalo City. The study is conducted at Animal Laboratory of Class II Gorontalo Agricultural Quarantine Agency from September 2020 to April 2021. The study employs a qualitative descriptive method where AI antibody titer in blood sample of Ayam Filipina that into or out of area is tested with Haemaglutination Inhibition (HI) assay. The recommended standard of AI antibody titer value from OIE (Office International de Epozooties) in 2018 where a criterion of AI antibody value for 20-23 is declared unprotected and a value for 24-26 is declared protected. The data are analyzed descriptively. The finding denotes that out of 130 samples, 125 samples (96,2%) have antibody titer for about 20-23 or it is declared as unprotected samples of AI whereas the rest 5 samples (3,8%) have an antibody titer for about 24-26 or it is declared as protected samples of AI. In conformity with the previous findings, it is conclude that during the span of September 2020 and April 2021, the percentage of Ayam Filipina samples with unprotected antobody of AI is higher than the samples with protected antibody of AI. © 2023 Author(s)."
The proceedings contain 7 papers. The topics discussed include: controllable image generation and manipulation; multimedia forensics versus disinformation in images and videos: lesson learnt and new challenges; synthetic speech detection through audio folding; SpoTNet: a spoofing-aware transformer network for effective synthetic speech detection; autoencoder-based data augmentation for deepfake detection; improving synthetically generated images detection in cross-concept settings; and synthetic Misinformers: generating and combating multimodal misinformation.
"This paper concerns the double standard debate in the ethics of AI literature. This debate revolves around the question of whether we should subject AI systems to different normative standards than humans. So far, the debate has centered around transparency. That is, the debate has focused on whether AI systems must be more transparent than humans in their decision-making processes in order for it to be morally permissible to use such systems. Some have argued that the same standards of transparency should be applied to AI systems and humans. Others have argued that we should hold AI systems to higher standards than humans in terms of transparency. In this paper, we first highlight that debates concerning double standards, which have a similar structure to those related to transparency, exist in relation to other values such as predictive accuracy. Second, we argue that when we focus on predictive accuracy, there are at least two reasons for holding AI systems to a lower standard than humans. © 2023 ACM."
"With the rapid growth of large language models, conversational recommender systems (CRSs) are on the rise. When receiving a CRS recommendation, one may encounter phrases of varying confidence levels such as ""you might like⋯""or ""I think you will like⋯,""but to the best of our knowledge, no work has investigated how the pattern of confidence signals from one recommendation to the next affects trust in and reliance on CRSs. In a mixed-methods user study, we explore how 30 participants interact with two Wizard of Oz music CRSs that grow increasingly confident, but only one uses natural language and color-coding to communicate confidence, which is accurate, random, always low, or always high. Through semi-structured interviews, survey responses, and recommendation ratings, we find evidence suggesting that an accurate confidence signal generates the greatest increase in trust-related metrics without encouraging over-reliance but potentially under-reliance. Furthermore, we identify design guidelines for CRS confidence signals associated with each trust-related metric: desire to use, perceived transparency, perceived ability, perceived benevolence, and perceived anthropomorphism. © 2023 Owner/Author."
"Assessing visual aesthetics is important for organizing and retrieving photos. That is one reason why several works aim to automate such an assessment using deep neural networks. The underlying models, however, lack explainability. Due to the subjective nature of aesthetics, it is challenging to find objective ground truths and explanations for aesthetics based on them. Hence, such models are prone to socio-cultural biases that come with the data, which raises questions on a wide range of ethical and technical issues. This paper presents an explainable artificial intelligence framework that adapts and combines three types of explanations for the concept of aesthetic assessment: 1) model constraints for built-in interpretability, 2) analysis of perturbation impacts on decisions, and 3) generation of artificial images that represent maxima or minima of values in the latent feature space. The objective is to improve human understanding through the explanations by creating an intuition for the model's decision making. We identify issues that arise when humans interact with the explanations and derive requirements from human feedback to address the needs of different user groups. We evaluate our novel interactive explainable artificial intelligence technology in a study with end users (N=20). Our participants have different levels of experience in deep learning, allowing us to include experts, intermediate users, and laypersons. Our results show the benefits of the interactivity of our approach. All users found our system helpful in understanding how the aesthetic assessment was executed, reporting varying needs for explanatory details.  © 2023 ACM."
"Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA's content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers. © 2023 Owner/Author."
[No abstract available]
"There is growing interest within the medical sector about the diagnostic potential of voice analysis-based artificial intelligence (AI) for monitoring mental health, such as depression detection. However, insufficient attention has been paid to the societal consequences of such technologies rendering depression and similar disabilities into purely technical problems. We provide a critical case study of Sonde Health, a Boston-based startup that purports to offer ""objective""depression detection and monitoring via its Mental Fitness app that extracts and analyzes the acoustic features of the user's voice. Using a critical disability studies lens, we conducted a textual analysis of the publicly available developer documentation for Sonde's application programming interface, examining each of these acoustic features (""vocal biomarkers""), and problematizing Sonde's claims that these vocal biomarkers are objective universal indicators of depression. Through our case study, we identify and illustrate three hegemonic norms that contribute to troubling social implications of the technology: the fallacy that complex psychometrics can be meaningfully flattened into a single encompassing score, the aesthetic of ""objectivity"", and the presumptive universalizing of easily-available voice data sets. We discuss how all three are tied up in the legacy of eugenics and reflect a fundamental mismatch in values between mainstream AI technology and the humanistic requirements of mental health care. © 2023 ACM."
"Artificial intelligence (AI) methods have been used widely in power transformer fault diagnosis with notable developments in solutions for big data problems. Training data is essential to accurately train AI models. The volume, scope and variety of data samples contribute significantly to the success and reliability of diagnostic outcomes. This paper provides a comprehensive review and comparison of different augmentation methods used to generate reliable data samples for minority and majority classes to balance the diversity and distribution of dissolved gas analysis (DGA) datasets. The augmentation method presented in this paper combines three common AI models—the Support Vector Machine (SVM), Decision Tree, and k-Nearest Neighbour (KNN)—to assess performance for diagnostic fault determination and classification, with comparator assessment using no data augmentation. Comparative analysis of the hybrid models uses evaluation metrics including accuracy, precision, recall, specificity, F-score, G-mean, and the area under receiver operation characteristic (Auc). Experimental results presented in this paper confirm that the data augmentation applied to AI models can resolve difficulties in imbalanced data distribution and provide significant improvements for fault diagnosis, particularly for minority classes. © 2023 The Authors. IET Renewable Power Generation published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology."
"Some researchers have recognized that privileged communities dominate the discourse on AI Ethics, and other voices need to be heard. As such, we identify the current ethics milieu as arising from WEIRD (Western, Educated, Industrialized, Rich, Democratic) contexts, and aim to expand the discussion to non-WEIRD global communities, who are also stakeholders in global sociotechnical systems. We argue that accounting for honor, along with its values and related concepts, would better approximate a global ethical perspective. This complex concept already underlies some of the WEIRD discourse on AI ethics, but certain cultural forms of honor also bring overlooked issues and perspectives to light. We first describe honor according to recent empirical and philosophical scholarship. We then review ""consensus""principles for AI ethics framed from an honor-based perspective, grounding comparisons and contrasts via example settings such as content moderation, job hiring, and genomics databases. A better appreciation of the marginalized concept of honor could, we hope, lead to more productive AI value alignment discussions, and to AI systems that better reflect the needs and values of users around the globe. © 2023 ACM."
[No abstract available]
"An emerging body of research indicates that ineffective cross-functional collaboration - the interdisciplinary work done by industry practitioners across roles - represents a major barrier to addressing issues of fairness in AI design and development. In this research, we sought to better understand practitioners' current practices and tactics to enact cross-functional collaboration for AI fairness, in order to identify opportunities to support more effective collaboration. We conducted a series of interviews and design workshops with 23 industry practitioners spanning various roles from 17 companies. We found that practitioners engaged in bridging work to overcome frictions in understanding, contextualization, and evaluation around AI fairness across roles. In addition, in organizational contexts with a lack of resources and incentives for fairness work, practitioners often piggybacked on existing requirements (e.g., for privacy assessments) and AI development norms (e.g., the use of quantitative evaluation metrics), although they worry that these tactics may be fundamentally compromised. Finally, we draw attention to the invisible labor that practitioners take on as part of this bridging and piggybacking work to enact interdisciplinary collaboration for fairness. We close by discussing opportunities for both FAccT researchers and AI practitioners to better support cross-functional collaboration for fairness in the design and development of AI systems. © 2023 Owner/Author."
"Many safety-critical systems are required to have their correctness validated prior to deployment. Such validation is typically performed using models of the run-time behaviour that the system is expected to exhibit and experience during run-time. However, these systems may be subject to different requirements under different circumstances; also, there may be multiple stakeholders involved, each with a somewhat different perspective on correctness. We examine the use of a multi-model framework based on assumptions (Pre and Rely conditions) and obligations (Post and Guarantee conditions) to represent the workload and resource related needs of complex AI system components such as DNN classifiers. We identify three kinds of multi-models that are of particular interest: Independent, Integrated and Hierarchical. All the individual models comprising an independent multi-model must remain valid at all times during run-time; at least one of the models comprising an integrated multi-model must always be valid. With hierarchical multi-models all models are initially valid but the component's behaviour may gracefully degrade through a series of models with successively weaker assumptions and commitments (we show that Mixed-Criticality Systems, widely studied in the real-time computing community, are particularly well-suited for representation via hierarchical multi-models). We explain how this modelling framework is intended to be used, and present algorithms for determining the worst-case timing behaviour of systems that are specified using multi-models.  © 2023 Owner/Author."
"Micro/nano optical materials and devices are the key to many optical fields such as optical communication, optical sensing, biophotonics, laser, and quantum optics, etc. At present, the design of micro/nano optics mainly relies on the numerical methods such as Finite-difference time-domain (FDTD), Finite element method (FEM) and Finite difference method (FDM). These methods bottleneck the current micro/nano optical design because of their dependence on computational resources, low innovation efficiency, and difficulties in obtaining global optimal design. Artificial intelligence (AI) has brought a new paradigm of scientific research: AI for Science, which has been successfully applied to chemistry, materials science, quantum mechanics, and particle physics. In the area of micro/nano design AI has been applied to the design research of chiral materials, power dividers, microstructured optical fibers, photonic crystal fibers, chalcogenide solar cells, plasma waveguides, etc. According to the characteristics of the micro/nano optical design objects, the datasets can be constructed in the form of parameter vectors for complex micro/nano optical designs such as hollow core anti-resonant fibers with multi-layer nested tubes, and in the form of images for simple micro/nano optical designs such as 3dB couplers. The constructed datasets are trained with artificial neural network, deep neural network and convolutional neural net algorithms to fulfill the regression or classification tasks for performance prediction or inverse design of micro/nano optics. The constructed AI models are optimized by adjusting the performance evaluation metrics such as mean square error, mean absolute error, and binary cross entropy. In this paper, the application of AI in micro/nano optics design is reviewed, the application methods of AI in micro/nano optics are summarized, and the difficulties and future development trends of AI in micro/nano optics research are analyzed and prospected. © 2023 Chinese Physical Society."
"Over evolution, organisms develop complex material structures fit to their environments. Based on these time-tested designs, human-engineered bioinspired structures offer exciting possible materials configurations. However, navigating diverse structure spaces for attaining desired properties remains non-trivial. We focus on the hardest biological tissue in humans, tooth enamel, to examine the structure-property relationship. While typical hardness measurements are time consuming and destructive, we propose that artificial intelligence models can predict properties directly and enable high-throughput, non-destructive characterization. We train a deep image regression neural network as a surrogate model and visualize with gradient ascent and saliency maps to identify structural features contributing most to hardness. This model demonstrates improved spatial resolution and sensitivity compared with experimental hardness maps. Using this rapid hardness testing model, a generative adversarial model, and a genetic algorithm that operates in latent space, allows for guided materials design, yielding proposed designs for bioinspired structures with precisely controlled hardness. © 2023 Elsevier Inc."
"Purpose: Currently, the vision and depth information obtained from the eye-to-hand RGB-D camera can apply to the reconstruction of the three-dimensional (3D) environment for a robotic operation workspace. The reconstructed 3D space contributes to a symmetrical and equal observation view for robots and humans, which can be considered a digital twin (DT) environment. The purpose of this study is to enhance the robot skill in the physical workspace, although the artificial intelligence (AI) technique has high performance of the robotic operation in the known environments. Design/methodology/approach: A multimodal interaction framework is proposed in DT operation environments. Findings: A fast image-based target segmentation technique is combined in the 3D reconstruction of the robotic operation environment from the eye-to-hand camera, thus expediting the 3D DT environment generation without accuracy loss. A multimodal interaction interface is integrated into the DT environment. Originality/value: The users are supported to operate the virtual objects in the DT environment using speech, mouse and keyboard simultaneously. The humans’ operations in 3D DT virtual space are recorded, and cues are provided for the robot’s operations in practice. © 2023, Emerald Publishing Limited."
"This systematic literature review explores how gamification in legal education might be In recent years, ChatGPT has become a noteworthy subject in the educational field due to the popularity it gained amongst students across different levels of education all over the world, who use this technology to assess their academic homework, transforming ChatGPT in some sort of auxiliary tool that aids them with the completion of certain tasks that would take more time to complete, such as research and data comparison, to name a few examples; but this form of AI assisted learning, as it were, has also become a problematic subject. This artificial intelligence chatbot is, undeniably, a remarkable advancement in AI regarding the improvements it presents compared to other similar technologies, and it clearly paves the way for future applications not only in education, but also at a social level, in a world more driven towards the development and optimization of digital tools with the help of machine learning. Nevertheless, this sort of technology should be question ed when its application permeates deeply in the performance and development of students and their learning process, especially when taking in consideration the level of accessibility that ChatGPT has worldwide. Students should have an ethical standpoint on whether they use ChatGPT to complement their learning process and how much input is this technology having in their academic work, so they learn to use it more effectively and avoid the abuse of ChatGPT usage, in order to seize the benefits that this AI may have on education. This study's objective is to analyze the current literature around the use of ChatGPT in education, for which we conducted a Systematic Literature Review (SLR) across multiple journal databases such as Scopus, ScienceDirect, ProQuest, IEEE Xplore and ACM Digital Library.  © 2023 ACM."
"Test paper analysis plays an important role in the field of educational statistics and evaluation. The rapid development of machine learning has brought new opportunities for test paper analysis. This study uses decision tree and XGBoost method, combined with SHAP interpretation framework, to analyze the key and difficult questions in the university physics test paper, and quantify how each key and difficult question affects students' passing rate and excellence rate. The research results show that the performance of XGBoost method is better compared with decision tree method. Using explainable artificial intelligence technology to show how each question affects students' test performance can help the teachers explain the pass rate and excellent rate, and provide a reference for the application of explainable artificial intelligence in test paper analysis and evaluation.  © 2023 ACM."
"Health is a state of total physical, mental, and social wellbeing. chatbots have been applied to this industry frequently and in a variety ofways in the past, there is still room for more inventive uses. Healthcareconversational AI use cases are flexible and may be tailored to the industry. Patients might use them to gain additional knowledge about their disease, the therapies that are available, or even their insurance coverage. Because research has shown that healthcare chatbots can improve patient satisfaction and significantly reduce wait times, many healthcare organisations are considering incorporating them into their operations. Chatbots for healthcare can be used for a number of purposes, such as monitoring, anonymity, personalization, in-person involvement, and more. In this case study, the user's input on the patient's symptoms will be used to determine the patient's likely ailment type. According on the type of sickness, precautions will be suggested, and the patient will be sent to a doctor who specialises in that field. A sequential model was utilised to extract the text's symptoms, and the KNN method was then applied to predict the patient's ailment type.1 © 2023 EDP Sciences. All rights reserved."
"Pathologists worldwide are facing remarkable challenges with increasing workloads and lack of time to provide consistently high-quality patient care. The application of artificial intelligence (AI) to digital whole-slide images has the potential of democratizing the access to expert pathology and affordable biomarkers by supporting pathologists in the provision of timely and accurate diagnosis as well as supporting oncologists by directly extracting prognostic and predictive biomarkers from tissue slides. The long-awaited adoption of AI in pathology, however, has not materialized, and the transformation of pathology is happening at a much slower pace than that observed in other fields (eg, radiology). Here, we provide a critical summary of the developments in digital and computational pathology in the last 10 years, outline key hurdles and ways to overcome them, and provide a perspective for AI-supported precision oncology in the future. © The Author(s) 2023. Published by Oxford University Press. All rights reserved. For permissions, please email: journals.permissions@oup.com."
"Big data and artificial intelligence (AI) techniques not only enable the accurate and comprehensive analysis of massive biomedical data, but also assist the development of predictive models in the field of drug design. With the development of computational and statistical methods, big data and AI techniques have been used for computer-aided drug design (CADD). CADD can be used to overcome troubles in the field of drug design, so as to effectively and efficiently design and develop new drugs. The steps of data pre-processing and development of models during drug design and development, AI-based modeling methods during drug design and development, recent applications of big data and AI-driven technologies in CADD were introduced, so as to provide some references for drug design and development in China. © 2023 The Author(s). All rights reserved."
"Background: Hepatocellular carcinoma (HCC) is unique among malignancies, and its characteristics on contrast imaging modalities allow for a highly accurate diagnosis. The radiological differentiation of focal liver lesions is playing an increasingly important role, and the Liver Imaging Reporting and Data System adopts a combination of major features including arterial phase hyper-enhancement (APHE) and the washout pattern. Summary: Specific HCCs such as well or poorly differentiated type, subtypes including fibrolamellar or sarcomatoid and combined hepatocellular-cholangiocarcinoma do not often demonstrate APHE and washout appearance. Meanwhile, hypervascular liver metastases and hypervascular intrahepatic cholangiocarcinoma can demonstrate APHE and washout. There are still other hypervascular malignant liver tumors (i.e., angiosarcoma, epithelioid hemangioendothelioma) and hypervascular benign liver lesions (i.e., adenoma, focal nodular hyperplasia, angiomyolipoma, flash filling hemangioma, reactive lymphoid hyperplasia, inflammatory lesion, arterioportal shunt), which need to be distinguished from HCC. When a patient has chronic liver disease, differential diagnosis of hypervascular liver lesions can be even more complicated. Meanwhile, artificial intelligence (AI) in medicine has been widely explored, and recent advancement in the field of deep learning has provided promising performance for the analysis of medical images, especially radiological imaging data contain diagnostic, prognostic, and predictive information which AI can extract. The AI research studies have demonstrated high accuracy (over 90% accuracy) for classifying lesions with typical imaging features from some hepatic lesions. The AI system has a potential to be implemented in clinical routine as decision support tools. However, for the differential diagnosis of many types of hypervascular liver lesions, further large-scale clinical validation is still required. Key Messages: Clinicians should be aware of the histopathological features, imaging characteristics, and differential diagnoses of hypervascular liver lesions to a precise diagnosis and more valuable treatment plan. We need to be familiar with such atypical cases to prevent a diagnostic delay, but AI-based tools also need to learn a large number of typical and atypical cases.  © 2023 S. Karger AG. All rights reserved."
"On the strength of the new quantum impedance Lorentz oscillator (QILO) model, a charge-transfer method in molecular photon-absorption is proposed and imaged via the numerical simulations of 1- and 2-photon-absorption (1PA and 2PA) behaviors of the organic compounds LB3 and M4 in this paper. According to the frequencies at the peaks and the full width at half-maximums (FWHMs) of the linear absorptive spectra of the two compounds, we first calculate the effective quantum numbers before and after the electronic transitions. Thus, we obtain the molecular average dipole moments, i.e., 1.8728 × 10-29 C·m (5.6145 D) for LB3 and 1.9626 × 10-29 C·m (5.8838 D) for M4 in the ground state in the tetrahydrofuran (THF) solvent. Then, the molecular 2PA cross sections corresponding to wavelength are theoretically inferred and figured out by QILO. As a result, the theoretical cross sections turn out to be in good agreement with the experimental ones. Our results reveal such a charge-transfer image in 1PA near wavelength 425 nm, where an atomic electron of LB3 jumps from the ground-state ellipse orbit with the semimajor axis ai = 1.2492 × 10-10 m = 1.2492 Å and semiminor axis bi = 0.4363 Å to the excited-state circle (aj = bj = 2.5399 Å). In addition, during its 2PA process, the same transitional electron in the ground state is excited to the elliptic orbit with aj = 2.5399 Å and bj =1.3808 Å, in which the molecular dipole moment reaches as high as 3.4109 × 10-29 C·m (10.2256 D). In addition, we obtain a level-lifetime formula with the microparticle collision idea of thermal motion, which indicates that the level lifetime is proportional (not inverse) to the damping coefficient or FWHM of an absorptive spectrum. The lifetimes of the two compounds at some excited states are calculated and presented. This formula may be used as an experimental method to verify 1PA and 2PA transition selection rules. The QILO model exhibits the advantage of simplifying the calculation complexity and reducing the high cost associated with the first principle in dealing with quantum properties of optoelectronic materials. © 2023 The Authors. Published by American Chemical Society."
"Industry 4.0 is an amalgamation of digital technologies with the industries; it is required for enhancing production, flexibility and scalability in industries. This field of research is a rapidly changing domain. It is also a multifaceted area of research including signal processing, computer vision, artificial intelligence, manufacturing, production engineering, etc. This book brings together professionals from academia and industry to present a review of state of knowledge in the fields of advanced signal and vision processing, the Industrial Internet of Things, AI and machine learning, signal processing for smart manufacturing, cyber-physical systems and intelligent systems for industries as applied to the implementation of Industry 4.0. The book will help readers to understand future needs of industries. © IOP Publishing Ltd 2023. All rights reserved."
"As increasingly powerful generative AI systems are developed, the release method greatly varies. We propose a framework to assess six levels of access to generative AI systems: fully closed; gradual or staged access; hosted access; cloud-based or API access; downloadable access; and fully open. Each level, from fully closed to fully open, can be viewed as an option along a gradient. We outline key considerations across this gradient: release methods come with tradeoffs, especially around the tension between concentrating power and mitigating risks. Diverse and multidisciplinary perspectives are needed to examine and mitigate risk in generative AI systems from conception to deployment. We show trends in generative system release over time, noting closedness among large companies for powerful systems and openness among organizations founded on principles of openness. We also enumerate safety controls and guardrails for generative systems and necessary investments to improve future releases. © 2023 ACM."
"Layer-wise mixed-precision quantization (MPQ) has become prevailing for edge inference since it strikes a better balance between accuracy and efficiency compared to the uniform quantization scheme. Existing MPQ strategies either lacked hardware awareness or incurred huge computation costs, which gated their deployment at the edge. In this work, we propose a novel MPQ search algorithm that obtains an optimal scheme by ""sampling""layer-wise sensitivity with respect to a newly proposed metric that incorporates both accuracy and proxy of hardware cost. To further efficiently deploy post-training MPQ on edge chips, we propose to tightly integrate the quantized inference units as part of the processor pipeline through micro-architecture and Instruction Set Architecture (ISA) co-design. Evaluation results show that the proposed search algorithm achieves 3% ∼ 11% higher inference accuracy with similar hardware cost compared to the state-of-the-art MPQ strategies. In addition, the tightly integrated MPQ units achieve speedup of 15.13x ∼ 29.65x compared to a baseline RISC-V processor.  © 2023 ACM."
"Recently, activity detection using sensor data is a widely growing field throughout the world, employing the concepts of artificial intelligence (AI) and deep learning (DL). Fitness tracking, sleep monitoring, fall detection, and smart SOS triggering applications are just a few of the use cases. We can use the built-in mobile sensors in smartphones, such as accelerometers and gyroscopes, to achieve smart human activity detection. To perform real-time activity detection for typical behaviors such as walking, jogging, upstairs movement, downstairs movement, stand-to-sit, sit-to-stand, and fall, we must design an iterative mathematical formulation implemented using a deep learning model. As an input to the proposed model, we will use real-time raw data from the sensors. We used two available datasets: the MobiAct dataset and the SisFall dataset, however owing to a lack of training data, we had to create our unique dataset for activities such as falling and jumping. We constructed a 3D matrix of size (20, 20, 3), scaled it in the range of [1, 255], and sent it to a recurrent convolution neural network-long short term memory (RCNN-LSTM) for feature extraction using a sliding window of 200 rows of time series data. This proposed model has been proven to be most effective in this use case and has a training accuracy of 96.24% and test accuracy of 97.85%. The latency of the model is very low thus making it efficient to be used for real-time human activity detection. This model can be used for real-time activity detection in a variety of use cases and tested to be robust in different places and conditions.  © 2023 Author(s)."
"We present Scallop, a language which combines the benefits of deep learning and logical reasoning. Scallop enables users to write a wide range of neurosymbolic applications and train them in a data-and compute-efficient manner. It achieves these goals through three key features: 1) a flexible symbolic representation that is based on the relational data model; 2) a declarative logic programming language that is based on Datalog and supports recursion, aggregation, and negation; and 3) a framework for automatic and efficient differentiable reasoning that is based on the theory of provenance semirings. We evaluate Scallop on a suite of eight neurosymbolic applications from the literature. Our evaluation demonstrates that Scallop is capable of expressing algorithmic reasoning in diverse and challenging AI tasks, provides a succinct interface for machine learning programmers to integrate logical domain knowledge, and yields solutions that are comparable or superior to state-of-The-Art models in terms of accuracy. Furthermore, Scallop's solutions outperform these models in aspects such as runtime and data efficiency, interpretability, and generalizability.  © 2023 Owner/Author."
"Facial Expression Recognition (FER) plays an important role in human-computer interactions and is used in a wide range of applications. Convolutional Neural Networks (CNN) have shown promise in their ability to classify human facial expressions, however, large CNNs are not well-suited to be implemented on resource-and energy-constrained IoT devices. In this work, we present a hierarchical framework for developing and optimizing hardware-aware CNNs tuned for deployment at the edge. We perform a comprehensive analysis across various edge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick, and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of 99.49% when testing on the CK+ facial expression recognition dataset. Additionally, we achieved a minimum inference latency of 0.39 milliseconds and a minimum power consumption of 0.52 Watts.  © 2023 ACM."
[No abstract available]
"Time-domain (TD) computing has attracted attention for its high computing efficiency and suitability for applications on energy-constrained edge devices. In this paper, we present a time-domain compute-in-memory (TDCIM) macro for binary neural networks (BNNs) realized by standard as well as custom delay cells. Multiply-and-accumulate (MAC) operations, batch normalization (BN) and binarization (Bin) are all processed in the time-domain, avoiding costly digital domain post-processing. In addition, it supports flexible mapping for different kernel sizes, achieving 100% utilization. Starting from a standard cell-based implementation, we propose two custom cells that provide interesting trade-offs between energy efficiency, area and accuracy. The two proposed custom designs can achieve 1.5 and 2.06 POPS/W energy efficiencies at 0.5V and 0.6V with less cell area while maintaining model test accuracy.  © 2023 ACM."
"With the development of edge computing and artificial intelligence (AI) technologies, edge devices are witnessed to generate data at unprecedented volume. The edge intelligence (EI) has led to the emergence of edge devices in various application domains. The EI can provide efficient services to delay-sensitive applications, where the edge devices are deployed as edge nodes to host the majority of execution, which can effectively manage services and improve service discovery efficiency. The multilevel index model is a well-known model used for indexing service, such a model is being introduced and optimized in the edge environments to efficiently services discovery while managing large volumes of data. However, effectively updating the multilevel index model by adding new services timely and precisely in the dynamic edge computing environments is still a challenge. Addressing this issue, this article proposes a designated key selection method to improve the efficiency of adding services in the multilevel index models. Our experimental results show that in the partial index and the full index of multilevel index model, our method reduces the service addition time by around 84% and 76%, respectively when compared with the original key selection method and by around 78% and 66%, respectively when compared with the random selection method. Our proposed method significantly improves the service addition efficiency in the multilevel index model, when compared with existing state-of-the-art key selection methods, without compromising the service retrieval stability to any notable level. © 2021 John Wiley & Sons Ltd."
"Recently, video/image intelligent analytics has been widely used in industrial Artificial Intelligence (AI) applications, such as defect detection, face recognition, and security monitoring. To provide better applicability and compatibility in such applications, the embedded AI models must be developed, compiled, and deployed under different development frameworks, such as cuDNN, RKNN, etc. Unfortunately, these frameworks are supported by various Graphic Processing Unit (GPU) hardware vendors, resulting in different model parameter structures and increased development costs. To address these issues, we propose LiGo, a low cost cross-platform video/image process framework, that simplifies and accelerates video intelligent processing in practical heterogeneous hardware systems. LiGo1 provides video processing pipeline, cross-platform development environments, and unified model serving structures. We demonstrate LiGo's efficiency and flexibility in model generation and deployment through its use in supporting multiple real-world commercial industrial systems. © 2023 ACM."
"Traditional von Neumann architecture bottlenecks such as the “memory wall” limit artificial intelligence (AI) development, and in-memory computing (IMC) as a new computing architecture can solve the above problems. Spin-orbit-torque-magnetic random access memory (SOT-MRAM) has very good advantages in IMC architecture because of its good compatibility with CMOS, high tunneling magnetoresistance (TMR) ratio, and high energy efficiency. In this paper, we propose a 6T-3M-based MRAM-IMC architecture with reconfigurable memory mode and logical operation mode. The functionality of the proposed architecture is validated using the 28 nm process design kit and the SOT-MTJ model. Copyright © 2023 The Institute of Electronics, Information and Communication Engineers."
"This study anticipates to identify Covid-19 using Lung CT (Computed Tomography) scan image dataset with the help of effective DL(Deep Learning) based algorithms. Though several clinical procedures and imaging modalities exists to diagnose Covid-19, these methods are time-consuming processes and sometimes the predictions are incorrect. Concurrently, AI (Artificial Intelligence) based DL models have gained attention in this area due to its innate capability for efficient learning. Though conventional systems have tried to perform better prediction, they lacked in accuracy with prediction rate. Moreover, the conventional systems have not utilized attention model completely for Covid-19 detection. This research is intended to resolve these pitfalls of covid-19 detection methods with the help of deep feature wise attention based Convolutional Neural Network. For this purpose, the data has been pre-processed by image resizing, the Residual Descriptor with Conv-BAM(Convolutional Block Attention Module) has been employed to obtain refined features from spatial and channel wise attention based module. The obtained features are used in the present study to improvise the classification as covid positive or negative. The performance of the proposed system has been assessed with regard to metrics to prove better efficiency. The proposed method achieved high accuracy rate of 97.82%. This DL based model can be used as a supplementary tool in the diagnosis of Covid-19 alongside other diagnostic method. © 2023, Intellectual Research and Development Education Foundation (YRPI). All rights reserved."
"Purpose: This study aims to understand a customer-purchase mechanism in the artificial intelligence (AI)-powered chatbot context based on the elaboration likelihood model (ELM) and technology acceptance model (TAM). The first objective is to examine how to boost chatbot adoption. The second objective is to investigate the role of information characteristics, technology-related characteristics and attitude toward AI in purchase intention. Design/methodology/approach: Data was collected from a sample of 492 users in Vietnam, who are potential customers of chatbots for purchase. Structural equation modeling was applied for data analysis. Findings: Results illustrate that chatbot adoption is significantly influenced by information credibility, technology-related factors (i.e. interactivity, relative advantage and perceived intelligence), attitude toward AI and perceived usefulness. Moreover, information quality and persuasiveness motivate information credibility. Information credibility and attitude toward AI are the essential motivations for perceived usefulness. Finally, chatbot adoption and information credibility determine purchase intention. Practical implications: The results are insightful for practitioners to envisage the importance of chatbot use for customer purchase in the AI scenario. Additionally, this research offers a framework to practitioners for shaping customer engagement in chatbots. Originality/value: The value of this work lies in the incorporation of technology-related characteristics into the two well-established theories, the ELM and TAM, to identify the importance of AI and its applications (i.e. chatbots) for purchase and to understand the formation of perceived usefulness and chatbot use through information credibility and attitude toward AI. © 2023, Emerald Publishing Limited."
"Internet-of-Things (IoT) edge devices have limited resources in terms of area and power. Machine Learning based intelligent filtering can be effective in reducing the data footprint. In this work, we report a feasibility study of using decision trees (DTs) on the edge. The main contribution of this work is to demonstrate that decision trees are equally effective compared to popular neural networks (multi-layer perceptrons). We trained four datasets (Iris, Heart Disease, Breast Cancer, and Credit Card) with supervised decision tree-based learning with accuracy comparable to that of MLPs. We synthesized the DTs to gate-level implementation with the Synopsys Design Compiler in 32 nm CMOS technology node. Compared to MLP implementations, DTs can save approximately 97-98% in both area and power.  © 2023 Owner/Author."
"Fairness is central to the ethical and responsible development and use of AI systems, with a large number of frameworks and formal notions of algorithmic fairness being available. However, many of the fairness solutions proposed revolve around technical considerations and not the needs of and consequences for the most impacted communities. We therefore want to take the focus away from definitions and allow for the inclusion of societal and relational aspects to represent how the effects of AI systems impact and are experienced by individuals and social groups. In this paper, we do this by means of proposing the ACROCPoLis framework to represent allocation processes with a modeling emphasis on fairness aspects. The framework provides a shared vocabulary in which the factors relevant to fairness assessments for different situations and procedures are made explicit, as well as their interrelationships. This enables us to compare analogous situations, to highlight the differences in dissimilar situations, and to capture differing interpretations of the same situation by different stakeholders. © 2023 Owner/Author."
"Fourier Harmonic Analysis or FHA, a computer image analysis of sperm nuclear shape to assess DNA fragmentation, was used to classify water buffalo bull into High and Low fertilities. Oocytes sired in vitro by sperm cells of FHA classified High fertility bulls had significantly higher developmental competence than Low fertility bulls, 38.1±3.2 vs. 25.5±3.2 (p=0.021), respectively. The pregnancy rate after AI through synchronized estrus (n=2600) is higher in High fertility bulls (23.2±0.4%) but not different (p=0.07) to those sired by Low fertility bulls (21.0±0.4). To acquire more data and assess calf production performance of the classified High and Low fertility bulls, a four-year record of artificial insemination from the regional centers of the Philippine Carabao Center involving 10 Low fertility and 11 High fertility bulls classified by FHA were analyzed. A total of monitored 12,848 AI services of which the High fertility bull served 6,846 females out of natural estrus and 1,339 females out of synchronized estrus while the Low fertility bulls served 3,432 females out of natural estrus and 1,231 females out of synchronized estrus, respectively, were assessed. Calf production record showed a significantly higher calves produced out of the inseminated cows by High fertility bulls than Low fertility bulls; 61.3±4.4% vs. 41.5±2.6 (p=0.035) from AI through natural estrus and 38.1±6.2% vs. 21.8±0.6 (p=0.040) from synchronized estrus. Male and female calf sired by High fertility bull was 56.1±1.4 male and 43.8±0.1 female while Low fertility bull was 55.4±0.3 male and 44.6±0.3 female, respectively. These results show that FHA is a promising tool in classifying High and Low fertility water buffalo bulls and using bulls classified as high fertility by FHA for AI program can significantly improve calf production. © 2023 American Institute of Physics Inc.. All rights reserved."
"Calls for representation in artificial intelligence (AI) and machine learning (ML) are widespread, with ""representation""or ""representativeness""generally understood to be both an instrumentally and intrinsically beneficial quality of an AI system, and central to fairness concerns. But what does it mean for an AI system to be ""representative""? Each element of the AI lifecycle is geared towards its own goals and effect on the system, therefore requiring its own analyses with regard to what kind of representation is best. In this work we untangle the benefits of representation in AI evaluations to develop a framework to guide an AI practitioner or auditor towards the creation of representative ML evaluations. Representation, however, is not a panacea. We further lay out the limitations and tensions of instrumentally representative datasets, such as the necessity of data existence and access, surveillance vs expectations of privacy, implications for foundation models and power. This work sets the stage for a research agenda on representation in AI, which extends beyond instrumentally valuable representation in evaluations towards refocusing on, and empowering, impacted communities. © 2023 Owner/Author."
"In this paper, we introduce LifeInsight - an interactive lifelog retrieval system developed for the sixth annual Lifelog Search Challenge (LSC'23). LifeInsight incorporates semantic search mechanisms from state-of-the-art lifelog retrieval systems while focusing on providing insights into the lifelogger's routine using spatial information to support question-answering tasks. The system employs the Bootstrapping Language-Image Pre-training (BLIP) model for zero-shot image-text retrieval, which has been shown to achieve higher recall scores than the CLIP model on the Flickr30K dataset. In addition, the Elastic Search filtering mechanism is utilized to remove irrelevant images. Apart from semantic search mechanisms, the system also supports visual similarity search by comparing the inner product distance between the vectors in the lifelog image corpus and the query image. Furthermore, the system includes an explicit relevance feedback function, AI-based query description rewriting, and visual-example-generating features to re-phrase the query to describe it better and support end-users envisioning the targeted image for retrieval.  © 2023 ACM."
"The run-time reconfigurability and high parallelism offered by Field Programmable Gate Arrays (FPGAs) make them an attractive choice for implementing hardware accelerators for Machine Learning (ML) algorithms. In the quest for designing efficient FPGA-based hard-ware accelerators for ML algorithms, the inherent error-resilience of ML algorithms can be exploited to implement approximate hard-ware accelerators to trade the output accuracy with better over-all performance. As multiplication and addition are the two main arithmetic operations in ML algorithms, most state-of-the-art approximate accelerators have considered approximate architectures for these operations. However, these works have mainly considered the exploration and selection of approximate operators from an existing set of operators. To this end, we provide an efficient methodology for synthesizing and implementing novel approximate operators. Specifically, we propose a novel operator synthesis approach that supports multiple operator algorithms to provide new approximate multiplier and adder designs for AI inference applications. We report up to 27% and 25% lower power than state-of-the-art approximate designs, with equivalent error behavior, for 8-bit unsigned adders and 4-bit signed multipliers respectively. Further, we propose a correlation-aware Design Space Exploration (DSE) method that can improve the efficacy of randomized search algorithms in synthesizing novel approximate operators.  © 2023 ACM."
"Operationalizing AI fairness at LinkedIn's scale is challenging not only because there are multiple mutually incompatible definitions of fairness but also because determining what is fair depends on the specifics and context of the product where AI is deployed. Moreover, AI practitioners need clarity on what fairness expectations need to be addressed at the AI level. In this paper, we present the evolving AI fairness framework used at LinkedIn to address these three challenges. The framework disentangles AI fairness by separating out equal treatment and equitable product expectations. Rather than imposing a trade-off between these two commonly opposing interpretations of fairness, the framework provides clear guidelines for operationalizing equal AI treatment complemented with a product equity strategy. This paper focuses on the equal AI treatment component of LinkedIn's AI fairness framework, shares the principles that support it, and illustrates their application through a case study. We hope this paper will encourage other big tech companies to join us in sharing their approach to operationalizing AI fairness at scale, so that together we can keep advancing this constantly evolving field. © 2023 ACM."
"This paper introduces the application of artificial intelligence video feedback system in college teaching. With the rise of online education and distance education, artificial intelligence technology is becoming an important way of university education, and also an effective means to improve the teaching effect and quality. This video feedback system provides personalized feedback by analyzing students' performance, and helps teachers better guide students' learning. In practice, AI video feedback system can be combined with existing online education resources such as online education platform to further deepen online education mode and innovate teaching methods. Students can learn in free time, and teachers can also use the online education platform to interact with students, and constantly improve and adjust teaching strategies in the teaching process.  © 2023 ACM."
"One of artificial insemination (AI) technology development today is the use sexing methods or the separation of X and Y spermatozoa. In this method, sexing media is needed to separate X and Y sperms, one of them is using freeze-dried albumin media. The objective of this study was to determine the quality of Bali bull sexed semen separated using freeze-dried albumin media and extended using soybean extender. Semen of a Bali bull was collected five times and diluted with three different extender treatments. T1 was soy extender, T2 was tris extender and T3 was tris-soy extender. Treatment T0 was dilution of fresh semen in tris extender with a ratio of 1:1. Parameters measured in the study were motility and viability of sexed semen. The results of the study showed that the motility and viability of the sperms after sexing using freeze-dried albumin were decreased in all treatments. Sperms motility and viability in lower layer (Y) after sexing in T1, T2, and T3 tended to be lower than sperms motility and viability before sexing (T0). In the upper layer (X) T1, T2, and T3 were significantly lower (P<0.05) than T0. Before sexing, motility and viability of the sperms were 89.39% and 91.13%, respectively. While after sexing, motility of the sperms decreased to 57% to 68% in upper layer and 71% to 81% in lower layer in the three treatments. Similarly, the sperms viability decreased to 58% to 70% in upper layer and 72% to 82% in lower layer. It can be concluded that the motility and viability of sexed sperms using freeze-dried albumin media on soy extender tend to be better than tris and tris-soy extender. This suggests that soy extender is suitable to be used as semen extender. Freeze-dried albumin media can be used as media of sperms sexing. © 2023 American Institute of Physics Inc.. All rights reserved."
"A form of chronic disease-related kidney that causes a progressive decrease in kidney function over time is Chronic kidney disease (CKD). The kidney's primary role is to filter waste from the body's fluids and waste from the blood. CKD is a severe health disease that affects the lives of millions of individuals throughout the globe and leads to substantial medical, social, and economical difficulties in its wake. The most frequent kind of kidney illness is CKD, which is identified by three-month-long inflammation and degenerative changes in kidney tissue. In the early stages of CKD, patients may be readily treated. Hence, the early detection of CKD is a need, that can be possible by employing the concepts of artificial intelligence (AI), and its extensions like machine learning (ML), ensemble learning (EL), deep learning (DL), etc. In this paper, we have ensembled the DL approach named artificial neural network (ANN) using a bagging classifier and voting approaches on the CKD dataset sourced at the UCI-ML warehouse which consists of 25 features and 400 instances. We first build three EDL models for experiment purposes Model -1 (ANN with Voting Classifier), Model -2 (ANN with Bagging Classifier), and Model -3 (ANN with both Bagging and Voting Classifier). The experiments are performed on a Jupyter notebook, a python programming environment. By considering the performance parameters like accuracy, recall, precision, f-measure, etc. in the experiments, it is observed that the proposed model outperforms all other discussed approaches.  © 2023 Author(s)."
The proceedings contain 54 papers. The topics discussed include: the ethics of AI assisted learning: a systematic literature review on the impacts of ChatGPT usage in education; an intelligent evaluation application for English teaching in college; the impact of information and communication technologies on skills in remote education: a diversified approach to improve the assessment of competencies in higher education; study on the recent development of metaverse application in college teaching; research on the framework model of man-machine collaborative teaching system in the context of artificial intelligence; design and development of a knowledge service platform in the field of computer science: knowledge service platform in computer science: design and development; case study of immersive digital learning in public space for common good education; and virtual gamification strategies and their impact on legal education experiences: a systematic review.
"The field of machine learning (ML) has long struggled with a principles-to-practice gap, whereby careful codes and commitments dissipate on their way to practical application. The present work bridges this gap through an applied affordance framework. 'Affordances' are how the features of a technology shape, but do not determine, the functions and effects of that technology. Here, I demonstrate the value of an affordance framework as applied to ML, considering ML systems through the prism of design studies. Specifically, I apply the mechanisms and conditions framework of affordances, which models the way technologies request, demand, encourage, discourage, refuse, and allow technical and social outcomes. Illustrated through three case examples across work, policing, and housing justice, the mechanisms and conditions framework reveals the social nature of technical choices, clarifying how and for whom those choices manifest. This approach displaces vagaries and general claims with the particularities of systems in context, empowering critically minded practitioners while holding power - and the systems power relations produce - to account. More broadly, this work pairs the design studies tradition with the ML domain, setting a foundation for deliberate and considered (re)making of sociotechnical futures. © 2023 Owner/Author."
"Recidivism risk assessment instruments are presented as an 'evidence-based' strategy for criminal justice reform - a way of increasing consistency in sentencing, replacing cash bail, and reducing mass incarceration. In practice, however, AI-centric reforms can simply add another layer to the sluggish, labyrinthine machinery of bureaucratic systems and are met with internal resistance. Through a community-informed interview-based study of 23 criminal judges and other criminal legal bureaucrats in Pennsylvania, I find that judges overwhelmingly ignore a recently-implemented sentence risk assessment instrument, which they disparage as ""useless,""""worthless,""""boring,""""a waste of time,""""a non-thing,""and simply ""not helpful.""I argue that this algorithm aversion cannot be accounted for by individuals' distrust of the tools or automation anxieties, per the explanations given by existing scholarship. Rather, the instrument's non-use is the result of an interplay between three organizational factors: county-level norms about pre-sentence investigation reports; alterations made to the instrument by the Pennsylvania Sentencing Commission in response to years of public and internal resistance; and problems with how information is disseminated to judges. These findings shed new light on the important role of organizational influences on professional resistance to algorithms, which helps explain why algorithm-centric reforms can fail to have their desired effect. This study also contributes to an empirically-informed argument against the use of risk assessment instruments: they are resource-intensive and have not demonstrated positive on-the-ground impacts. © 2023 ACM."
"Three-dimensional (3D) bioprinting is an emerging biofabrication technique that shows great potential in the field of tissue engineering, regenerative medicine and advanced drug delivery. Despite the current advancement of bioprinting technology, it faces several obstacles such as the challenge of optimizing the printing resolution of 3D constructs while retaining cell viability before, during, and after bioprinting. Therefore, it is of great significance to fully understand factors that influence the shape fidelity of printed structures and the performance of cells encapsulated in bioinks. This review presents a comprehensive analysis of bioprinting process parameters that influence bioink printability and cell performance, including bioink properties (composition, concentration, and component ratio), printing speed and pressure, nozzle characteristics (size, length, and geometry), and crosslinking parameters (crosslinker types, concentration, and crosslinking time). Key examples are provided to analyze how these parameters could be tailored to achieve the optimal printing resolution as well as cell performance. Finally, future prospects of bioprinting technology, including correlation between process parameters and particular cell types with predefined applications, applying statistical analysis and artificial intelligence (AI)/machine learning (ML) technique in parameter screening, and optimizing four-dimensional (4D) bioprinting process parameters, are highlighted. © 2023 Elsevier B.V."
"This timely book investigates emerging efforts to govern artificial intelligence (AI) at an international level. It emphasizes the complex interactions involved when creating international norms related to potential and current developments in AI regulation. Organized into four parts, The International Governance of Artificial Intelligence demonstrates how formal and informal standards for AI are emerging from stakeholder interactions. With the objective of describing a nascent transnational law on AI use, chapters survey the various global realities that affect AI governance, concluding that AI law should ultimately be evaluated against the measure of international human rights. Students of law and governance will benefit from this book, particularly when studying emerging technologies, international economic law and general international law. Those researching policy creation and regulation will additionally find it to be an enlightening read. © Mark Chinen 2023. All rights reserved."
"The XAI community is currently studying and developing symbolic knowledge-extraction (SKE) algorithms as a means to produce human-intelligible explanations for black-box machine learning predictors, so as to achieve believability in human-machine interaction. However, many extraction procedures exist in the literature, and choosing the most adequate one is increasingly cumbersome, as novel methods keep on emerging. Challenges arise from the fact that SKE algorithms are commonly defined based on theoretical assumptions that typically hinder practical applicability. This paper focuses on hypercube-based SKE methods, a quite general class of extraction techniques mostly devoted to regression-specific tasks. We first show that hypercube-based methods are flexible enough to support classification problems as well, then we propose a general model for them, and discuss how they support SKE on datasets, predictors, or learning tasks of any sort. Empirical examples are reported as well -based upon the PSyKE framework -, showing the applicability of hypercube-based methods to actual classification tasks.  © 2023 - IOS Press. All rights reserved."
"Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world's population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems' fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases. © 2023 ACM."
"The increased water scarcity, depletion of freshwater resources, and rising environmental awareness are stressing for the development of sustainable wastewater treatment processes. Microalgae-based wastewater treatment has resulted in a paradigm shift in our approach toward nutrient removal and simultaneous resource recovery from wastewater. Wastewater treatment and the generation of biofuels and bioproducts from microalgae can be coupled to promote the circular economy synergistically. A microalgal biorefinery transforms microalgal biomass into biofuels, bioactive chemicals, and biomaterials. The large-scale cultivation of microalgae is essential for the commercialization and industrialization of microalgae biorefinery. However, the inherent complexity of microalgal cultivation parameters regarding physiological and illumination parameters renders it challenging to facilitate a smooth and cost-effective operation. Artificial intelligence (AI)/machine learning algorithms (MLA) offer innovative strategies for assessing, predicting, and regulating uncertainties in algal wastewater treatment and biorefinery. The current study presents a critical review of the most promising AI/MLAs that demonstrate a potential to be applied in microalgal technologies. The most commonly used MLAs include artificial neural networks, support vector machine, genetic algorithms, decision tree, and random forest algorithms. Recent developments in AI have made it possible to combine cutting-edge techniques from AI research fields with microalgae for accurate analysis of large datasets. MLAs have been extensively studied for their potential in microalgae detection and classification. However, the ML application in microalgal industries, such as optimizing microalgae cultivation for increased biomass productivity, is still in its infancy. Incorporating smart AI/ML-enabled Internet of Things (IoT) based technologies can help the microalgal industries to operate effectively with minimum resources. Future research directions are also highlighted, and some of the challenges and perspectives of AI/ML are outlined. As the world is entering the digitalized industrial era, this review provides an insightful discussion about intelligent microalgal wastewater treatment and biorefinery for researchers in the field of microalgae. © 2023"
"Purpose: Media exaggerations of health research may confuse readers' understanding, erode public trust in science and medicine, and cause disease mismanagement. This study built artificial intelligence (AI) models to automatically identify and correct news headlines exaggerating obesity-related research findings. Design/methodology/approach: We searched popular digital media outlets to collect 523 headlines exaggerating obesity-related research findings. The reasons for exaggerations include: inferring causality from observational studies, inferring human outcomes from animal research, inferring distant/end outcomes (e.g., obesity) from immediate/intermediate outcomes (e.g., calorie intake), and generalizing findings to the population from a subgroup or convenience sample. Each headline was paired with the title and abstract of the peer-reviewed journal publication covered by the news article. We drafted an exaggeration-free counterpart for each original headline and fined-Tuned a BERT model to differentiate between them. We further fine-Tuned three generative language models-BART, PEGASUS, and T5 to autogenerate exaggeration-free headlines based on a journal publication's title and abstract. Model performance was evaluated using the ROUGE metrics by comparing model-generated headlines with journal publication titles. Findings: The fine-Tuned BERT model achieved 92.5% accuracy in differentiating between exaggeration-free and original headlines. Baseline ROUGE scores averaged 0.311 for ROUGE-1, 0.113 for ROUGE-2, 0.253 for ROUGE-L, and 0.253 ROUGE-Lsum. PEGASUS, T5, and BART all outperformed the baseline. The best-performing BART model attained 0.447 for ROUGE-1, 0.221 for ROUGE-2, 0.402 for ROUGE-L, and 0.402 for ROUGE-Lsum. Originality/value: This study demonstrated the feasibility of leveraging AI to automatically identify and correct news headlines exaggerating obesity-related research findings.  © 2023 Ruopeng An et al., published by Sciendo."
"Processing-in-Memory (PIM) has shown great potential for a wide range of data-driven applications, especially Deep Learning and AI. However, it is a challenge to facilitate the computational sophistication of a standard processor (i.e. CPU or GPU) within the limited scope of a memory chip without contributing significant circuit overheads. To address the challenge, we propose a programmable LUT-based area-efficient PIM architecture capable of performing various low-precision floating point (FP) computations using a novel LUT-oriented operand-decomposition technique. We incorporate such compact computational units within the memory banks in a large count to achieve impressive parallel processing capabilities, up to 4x higher than state-of-the-art FP-capable PIM. Additionally, we adopt a highly-optimized low-precision FP format that maximizes computational performance at a minimal compromise of computational precision, especially for Deep Learning Applications. The overall result is a 17% higher throughput and an impressive 8-20x higher compute Bandwidth/bank compared to the state-of-the-art of in-memory acceleration.  © 2023 ACM."
"Purpose: Poor performance remains a challenge for the construction industry worldwide. One of the key performance indicators of the construction industry is the timely delivery of projects. Despite the recent methodological and technological advances in the field, project-overrun remains a significant challenge for the industry. This paper seeks to propose practical solutions that allow overcoming the challenges and promote the opportunities for improving the performance of the construction projects in Dubai. Design/methodology/approach: This study focussed on the construction projects in Dubai; therefore, this research adopted a sequential mixed approach in two stages. The first stage involved face-to-face interviews with seven carefully selected construction professionals. Their answers were analysed to provide with the literature study “the informed-basis for the development of the online questionnaire”. The second stage involved an online survey administrated to 425 carefully selected construction organisations working in Dubai. Accordingly, a meticulous analysis for the prime causes of project overruns has also been undertaken. This analysis assisted proposing the most suitable solutions-based technologies that enabled alleviating overruns in the construction projects. Findings: The findings revealed that, there was a consensus agreement on the formidable opportunities for improving the performance of the construction industry in general and in particular in Dubai. These opportunities are intrinsically linked with the adoption of the latest technologies such as building information modelling, augmented reality, virtual reality and the artificial intelligence (AI). Whereas, adopting AI has already assisted two public authorities to release No Objection Certificates and work permits effectively within one day instead of 14 working days, which has saved 90% of the time and cost. Likewise, adoption of the AI has assisted delivering the construction project with a 9% of time saving and a 6% of cost saving due to embracing an automated system that enabled them to instantly detect and report the delays, once occurred. Research limitations/implications: The main limitation of this study is that the study was limited to the construction industry in the Emirate of Dubai. Therefore, future research could target the whole United Arab Emirates construction industry to propose the practical solution on the country level. Practical implications: The literature study is replete with solutions, which tend to be theoretical more than practical. Therefore, the proposed practical recommendations will significantly assist the construction industry to improve its suboptimal performance to rescind the sovereignty of the irrelevant involvements. The research recommended establishing independent entity to lead the change in the construction industry; this entity will have the power of enacting rules and legislations. Furthermore, this independent entity will have the power and authority of dictations and impose sanctions on the non-committed organisations that are reluctant to adopt the recommended technologies and approaches. Originality/value: Based on the findings of the study, this paper draws a road map for the construction industry by determining practical solutions for improvements starting with an establishment of an independent authority that selects and tests the most appropriate technologies and approaches to contribute to performance improvements. © 2022, Emerald Publishing Limited."
"Numerous methods have been proposed in predicting formability of sheet metals based on microstructural and macro-scale properties of sheets. However, there are limited number of papers on the optimization problem to increase formability of sheet metals. In the present study, we aim to use novel optimization algorithms in neural networks to maximize the formability of sheet metals based on tensile curve and texture of aluminum sheet metals. In this regard, experimental and numerical evaluations of effects of texture and tensile properties are conducted. The texture effects evaluation is performed using Taylor homogenization method. The data obtained from these evaluations are gathered and utilized to train and validate an artificial neural network (ANN) with different optimization methods. Several optimization method including grey wolf algorithm (GWA), chimp optimization algorithm (ChOA) and whale optimization algorithm (WOA) are engaged in the optimization problems. The results demonstrated that in aluminum alloys the most preferable texture is cube texture for the most formable sheets. On the other hand, slight differences in the tensile behavior of the aluminum sheets in other similar conditions impose no significant decreases in the forming limit diagram under stretch loading conditions. Copyright © 2023 Techno-Press, Ltd."
"Importance: The rapid expansion of virtual health care has caused a surge in patient messages concomitant with more work and burnout among health care professionals. Artificial intelligence (AI) assistants could potentially aid in creating answers to patient questions by drafting responses that could be reviewed by clinicians. Objective: To evaluate the ability of an AI chatbot assistant (ChatGPT), released in November 2022, to provide quality and empathetic responses to patient questions. Design, Setting, and Participants: In this cross-sectional study, a public and nonidentifiable database of questions from a public social media forum (Reddit's r/AskDocs) was used to randomly draw 195 exchanges from October 2022 where a verified physician responded to a public question. Chatbot responses were generated by entering the original question into a fresh session (without prior questions having been asked in the session) on December 22 and 23, 2022. The original question along with anonymized and randomly ordered physician and chatbot responses were evaluated in triplicate by a team of licensed health care professionals. Evaluators chose ""which response was better"" and judged both ""the quality of information provided"" (very poor, poor, acceptable, good, or very good) and ""the empathy or bedside manner provided"" (not empathetic, slightly empathetic, moderately empathetic, empathetic, and very empathetic). Mean outcomes were ordered on a 1 to 5 scale and compared between chatbot and physicians. Results: Of the 195 questions and responses, evaluators preferred chatbot responses to physician responses in 78.6% (95% CI, 75.0%-81.8%) of the 585 evaluations. Mean (IQR) physician responses were significantly shorter than chatbot responses (52 [17-62] words vs 211 [168-245] words; t = 25.4; P <.001). Chatbot responses were rated of significantly higher quality than physician responses (t = 13.3; P <.001). The proportion of responses rated as good or very good quality (≥ 4), for instance, was higher for chatbot than physicians (chatbot: 78.5%, 95% CI, 72.3%-84.1%; physicians: 22.1%, 95% CI, 16.4%-28.2%;). This amounted to 3.6 times higher prevalence of good or very good quality responses for the chatbot. Chatbot responses were also rated significantly more empathetic than physician responses (t = 18.9; P <.001). The proportion of responses rated empathetic or very empathetic (≥4) was higher for chatbot than for physicians (physicians: 4.6%, 95% CI, 2.1%-7.7%; chatbot: 45.1%, 95% CI, 38.5%-51.8%; physicians: 4.6%, 95% CI, 2.1%-7.7%). This amounted to 9.8 times higher prevalence of empathetic or very empathetic responses for the chatbot. Conclusions: In this cross-sectional study, a chatbot generated quality and empathetic responses to patient questions posed in an online forum. Further exploration of this technology is warranted in clinical settings, such as using chatbot to draft responses that physicians could then edit. Randomized trials could assess further if using AI assistants might improve responses, lower clinician burnout, and improve patient outcomes. © 2023 American Medical Association. All rights reserved."
"Saying ‘no’ to this kind of visual content is a question of research integrity, consent, privacy and intellectual-property protection. [Figure not available: see fulltext.]. © 2023, Springer Nature Limited."
"The aesthetic appeal of a website has strong effects on users' reactions, appraisals, and even behaviors. However, evaluating website aesthetics through user ratings is resource intensive, and extant models to predict website aesthetics are limited in performance and ability. We contribute a novel and more precise approach to predict website aesthetics that considers rating distributions. Moreover, we use this approach as a baseline model to illustrate how future research might be conducted using predictions instead of participants. Our approach is based on a deep convolutional neural network model and uses innovations in the field of image aesthetic prediction. It was trained with the dataset from Reinecke and Gajos [2014] and was validated using two independent large datasets. The final model reached an unprecedented cross-validated correlation between the ground truth and predicted rating of LCC = 0.752. We then used the model to successfully replicate prior findings and conduct original research as an illustration for AI-based research.  © 2023 Copyright held by the owner/author(s)."
"With the availability of advanced packaging technology and its attractive features, the chiplet-based architecture has gained traction among chip designers. The large design space and the lack of system and package-level co-design methods make it difficult for the designers to create the optimum design choices. In this research, considering the colossal design space of advanced packaging technologies, resource allocation, and chiplet placement, we design an optimizer that looks for the design choices that maximize the Power, Performance, and Area (PPA) and minimize the cost of the chiplet-based AI accelerator. Inspired by the Bayesian approach for black-box function optimization, our optimizer guides the search space toward global maxima instead of randomly traversing through the search space. We analytically synthesize a dataset from the search space and train an ML model to predict the target value of our defined cost function at the optimizer-suggested points. The optimizer locates the optimum design choices from the specified search space (≥ 1M data points) with minimal iterations (≤ 200 iterations) and trivial run time.  © 2023 ACM."
"Ultra High-Performance Concrete (UHPC) has superior mechanical properties, including high compressive strength, tensile strain hardening behavior, and self-healing capacity. However, there has been limited focus on developing predictive models for UHPC's self-healing properties, despite extensive research in the aforesaid respect. While multi-physics modeling has made progress in predicting the coupled chemical, physical, and mechanical phenomena in cement-based materials, data-driven models, including Artificial Intelligence (AI) and Machine Learning (ML), are gaining popularity in predicting some concrete properties. In this study, a machine learning model was developed to predict UHPC's self-healing performance using three meta-heuristic algorithms, i.e., whales optimization algorithm (WOA), grey wolf optimization (GWO), and flower pollination algorithm (FPA), combined with extreme gradient boosting tree (Xgboost). The dataset used for the model was obtained from original experimental tests on UHPC's crack sealing performance under sustained through crack tensile stress and exposure to various aggressive environments for up to six months. The model's predictive performance was assessed using four mathematical indicators. The regression error characteristic (REC) and Taylor diagrams also showed the optimal models’ performance were found to be consistent and reliable across different optimization algorithms. SHapley Additive exPlanation (SHAP) results revealed that exposure time and crack width were most critical features for predicting self-healing performance. The study demonstrated the potential of using machine learning for predicting UHPC's self-healing performance and provided insights into the most critical factors affecting the process. © 2023 Elsevier Ltd"
"Introduction Artificial intelligence (AI) has been on the rise in the field of pathology. Despite promising results in retrospective studies, and several CE-IVD certified algorithms on the market, prospective clinical implementation studies of AI have yet to be performed, to the best of our knowledge. In this trial, we will explore the benefits of an AI-assisted pathology workflow, while maintaining diagnostic safety standards. Methods and analysis This is a Standard Protocol Items: Recommendations for Interventional Trials-Artificial Intelligence compliant single-centre, controlled clinical trial, in a fully digital academic pathology laboratory. We will prospectively include prostate cancer patients who undergo prostate needle biopsies (CONFIDENT-P) and breast cancer patients who undergo a sentinel node procedure (CONFIDENT-B) in the University Medical Centre Utrecht. For both the CONFIDENT-B and CONFIDENT-P trials, the specific pathology specimens will be pseudo-randomised to be assessed by a pathologist with or without AI assistance in a pragmatic (bi-)weekly sequential design. In the intervention group, pathologists will assess whole slide images (WSI) of the standard hematoxylin and eosin (H&E)-stained sections assisted by the output of the algorithm. In the control group, pathologists will assess H&E WSI according to the current clinical workflow. If no tumour cells are identified or when the pathologist is in doubt, immunohistochemistry (IHC) staining will be performed. At least 80 patients in the CONFIDENT-P and 180 patients in the CONFIDENT-B trial will need to be enrolled to detect superiority, allocated as 1:1. Primary endpoint for both trials is the number of saved resources of IHC staining procedures for detecting tumour cells, since this will clarify tangible cost savings that will support the business case for AI. Ethics and dissemination The ethics committee (MREC NedMec) waived the need of official ethical approval, since participants are not subjected to procedures nor are they required to follow rules. Results of both trials (CONFIDENT-B and CONFIDENT-P) will be published in scientific peer-reviewed journals.  © 2023 BMJ Publishing Group. All rights reserved."
"Unpredictable natural disasters, disease outbreaks, climate change, pollution, and war constantly threaten food crop production. Smart and precision farming encourages using information or data obtained by using advanced technology (sensors, AI, and IoT) to improve decision-making in agriculture and achieve high productivity. For instance, weather prediction, nutrient information, pollutant assessment, and pathogen determination can be made with the help of new analytical and bioanalytical methods, demonstrating the potential for societal impact such as environmental, agricultural, and food science. As a rising technology, biosensors can be a potential tool to promote smart and precision farming in developing and underdeveloped countries. This review emphasizes the role of on-field, in vivo, and wearable biosensors in smart and precision farming, especially those biosensing systems that have proven with suitably complex and analytically challenging samples. The development of various agricultural biosensors in the past five years that fulfill market requirements such as portability, low cost, long-term stability, user-friendliness, rapidity, and on-site monitoring will be reviewed. The challenges and prospects for developing IoT and AI-integrated biosensors to increase crop yield and advance sustainable agriculture will be discussed. Using biosensors in smart and precision farming would ensure food security and revenue for farming communities. © 2023 The Royal Society of Chemistry."
"Artificial intelligence (AI) is gaining strength, and materials science can both contribute to and profit from it. In a simultaneous progress race, new materials, systems, and processes can be devised and optimized thanks to machine learning (ML) techniques, and such progress can be turned into innovative computing platforms. Future materials scientists will profit from understanding how ML can boost the conception of advanced materials. This review covers aspects of computation from the fundamentals to directions taken and repercussions produced by computation to account for the origins, procedures, and applications of AI. ML and its methods are reviewed to provide basic knowledge of its implementation and its potential. The materials and systems used to implement AI with electric charges are finding serious competition from other information-carrying and processing agents. The impact these techniques have on the inception of new advanced materials is so deep that a new paradigm is developing where implicit knowledge is being mined to conceive materials and systems for functions instead of finding applications to found materials. How far this trend can be carried is hard to fathom, as exemplified by the power to discover unheard of materials or physical laws buried in data. © 2023 The Authors. Advanced Materials published by Wiley-VCH GmbH."
"Sustaining high fidelity and high throughput of perception tasks over vision sensor streams on edge devices remains a formidable challenge, especially given the continuing increase in image sizes (e.g., generated by 4K cameras) and complexity of DNN models. One promising approach involves criticality-Aware processing, where the computation is directed selectively to ""critical""portions of individual image frames. We introduce MOSAIC, a novel system for such criticality-Aware concurrent processing of multiple vision sensing streams that provides a multiplicative increase in the achievable throughput with negligible loss in perception fidelity. MOSAIC determines critical regions from images received from multiple vision sensors and spatially bin-packs these regions using a novel multi-scale Mosaic Across Scales (MoS) tiling strategy into a single 'canvas frame', sized such that the edge device can retain sufficiently high processing throughput. Experimental studies using benchmark datasets for two tasks, Automatic License Plate Recognition and Drone-based Pedestrian Detection, shows that MOSAIC, executing on a Jetson TX2 edge device, can provide dramatic gains in the throughput vs. fidelity tradeoff. For instance, for drone-based pedestrian detection, for a batch size of 4, MOSAIC can pack input frames from 6 cameras to achieve (a) 4.75X (475%) higher throughput (23 FPS per camera, cumulatively 138FPS) with ≤ 1% accuracy loss, compared to a First Come First Serve (FCFS) processing paradigm.  © 2023 ACM."
"In the past decades, the rapid development of the Internet of Things (IoT) technology and artificial intelligence (AI) has driven the research boom of physical sensors. Material selection, structure design, and performance research for physical sensors have attracted extensive attention from worldwide researchers in the field of advanced manufacturing. Significant technological progress has been made in the area of physical sensors for applications in various fields such as electronic skin, biomedicine, and tissue engineering. There are many methods (e.g., electrospinning, screen printing, or rotary coating) to prepare physical sensors. Among them, nanofibers or nanofiber membranes prepared by electrospinning have the advantages of a nanosize effect, high specific surface area, and high porosity over other reported materials used for physical sensors. In this review, the working principles of various physical sensors including pressure sensors, strain sensors, temperature sensors, and humidity sensors are first introduced; recent research progress of electrospun nanofiber-based physical sensors is then summarized. Finally, future research trends and associated challenges of large-scale adoption of electrospun physical sensors are proposed. © 2023 American Chemical Society."
[No abstract available]
"Brain tumors are caused by the proliferation of abnormal cells in the brain. Because of the various nature of aberrant cells in the brain, diagnosing and classifying brain tumors has become very important in recent years. Treatment for brain tumors is mostly determined by characteristics such as the kind of tumor, the abnormality of the cells, and the tumor's location in the brain. Deep learning models are being utilized to diagnose brain malignancies using the Magnetic Resonance Imaging (MRI) approach, thanks to the fast growth of Artificial Intelligence (AI) libraries. Strong magnetic fields and radio waves are used in MRI scanning to obtain detailed pictures of the interior body. In this study, we use a convolutional neural network (CNN) to create a deep learning model that diagnoses, classifies, and locates the afflicted area of the tumor region in scanned brain pictures. This research is only focused on building a deep-learning application for brain tumor detection and classification.  © 2023 Author(s)."
"Emerging AI applications such as ChatGPT, graph convolutional networks, and other deep neural networks require massive computational resources for training and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs are struggling to keep up with the demands of these AI applications. Non-coherent optical computing represents a promising approach for light-speed acceleration of AI workloads. In this paper, we show how cross-layer design can overcome challenges in non-coherent optical computing platforms. We describe approaches for optical device engineering, tuning circuit enhancements, and architectural innovations to adapt optical computing to a variety of AI workloads. We also discuss techniques for hardware/ software co-design that can intelligently map and adapt AI software to improve performance on non-coherent platforms.  © 2023 ACM."
"Automation of manufacturing processes has been an essential and necessary aspect in the industrial set up since its very inception. Development in artificial intelligence (AI) methodology and robotics has made it possible to deploy AI in general and machine learning (ML) methods in particular to enhance the automation in production management, process planning, and intelligent scheduling. AI methodologies have also been used in preventive maintenance, process automation, supply chain management, product development, warehouses, enterprise resource planning, and product design. This chapter covers the use of some of the expert systems (ESs) in various manufacturing processes in flexible manufacturing systems. A rule-based system has been described in the process planning. The components and procedure of a knowledge-based scheduling strategy has been described in detail. An outline of ML methods in some processes has also been discussed in this work. © IOP Publishing Ltd 2023. All rights reserved."
"With rapid progress made in artificial intelligence, visual content can be generated that is often indistinguishable from “the real thing.” Potential misuse of microscopy image generation is a growing concern, as it may evade current fraud checks. We urge studies on methods to detect such fabrication to circumvent misconduct. © 2023 Elsevier Inc."
"The advent of AI-supported, cloud-based collaborative translation platforms has enabled a new form of online collaborative translation – ‘concurrent translation’ (CT). CT refers to commercial translation performed on such platforms by multiple agents (translators, editors, subject-matter experts etc.) simultaneously, via concurrent access. Although the practice has recently gained more ground, research on CT is scarce. The present article reports on selected key findings of a study that investigates translators’ experiences with CT via a survey of 804 professional translators working in CT mode across different commercial platforms. Despite the affordances such as peer learning, positive competition, speed, flexibility of the volume of work and working time, and reduced responsibility and reduced stress, CT workflow comes with its substantial challenges such as time pressure, negative competition, reduced self-revision and research, all of which result in quality compromised for speed. © John Benjamins Publishing Company."
"The proceedings contain 144 papers. The topics discussed include: machine explanations and human understanding; broadening AI ethics narratives: an Indic art view; how to explain and justify almost any decision: potential pitfalls for accountability in AI decision-making; ‘we are adults and deserve control of our phones’: examining the risks and opportunities of a right to repair for mobile apps; fairness in machine learning from the perspective of sociology of statistics: how machine learning is becoming scientific by turning its back on metrological realism; two reasons for subjecting medical AI systems to lower standards than humans; optimization’s neglected normative commitments; humans, AI, and context: understanding end-users’ trust in a real-world computer vision application; multi-dimensional discrimination in law and machine learning – a comparative overview; reconciling individual probability forecasts; the gradient of generative AI release: methods and considerations; and in the name of fairness: assessing the bias in clinical record de-identification."
"the purpose of this study was to devise an efficient solution, known as SMART_EYE, aimed at assisting visually impaired individuals in navigating unfamiliar environments and detecting obstacles. The motivation behind this research stemmed from the significant population affected by vision impairment and the limitations of existing navigation alternatives, which are often heavy and expensive and thus infrequently adopted. To address this problem, we employed a method that utilized smart applications with AI and sensor technology. The smart app captured and classified images, while obstacle detection was performed using ultrasonic sensors. Voice commands were used to provide users with real-time information about obstacles in their path. The results of this study demonstrated the effectiveness of the SMART_EYE model in improving both qualitative and quantitative performance measures for visually impaired individuals. The model offered a cost-effective alternative that enabled independent navigation and obstacle detection, thereby enhancing the quality of life for this population. The practical implications of this study are twofold: the SMART_EYE model provides a viable solution for visually impaired individuals to navigate unfamiliar environments, and its cost-effectiveness addresses the limitations of existing assistive devices. From a theoretical perspective, this study contributes to the field of assistive technology by integrating AI and sensor technology in a smart app to aid visually impaired individuals. Furthermore, the evaluation and ranking of different systems based on their impact on the lives of visually impaired people provide a basis for further research and development in this area. In conclusion, this study's value lies in its contribution to both theory and practice, showcasing the potential for future advancements in assistive technology for visually impaired individuals, ultimately improving their quality of life. © 2023, Intellectual Research and Development Education Foundation (YRPI). All rights reserved."
"Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by 'many hands'. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accountability (particularly regarding general purpose AI services). To highlight ways forward and areas warranting attention, we further discuss some implications raised by supply chains: challenges for allocating accountability stemming from distributed responsibility for systems between actors, limited visibility due to the accountability horizon, service models of use and liability, and cross-border supply chains and regulatory arbitrage. © 2023 Owner/Author."
[No abstract available]
"PURPOSEArtificial intelligence (AI) algorithms improve breast cancer detection on mammography, but their contribution to long-term risk prediction for advanced and interval cancers is unknown.METHODSWe identified 2,412 women with invasive breast cancer and 4,995 controls matched on age, race, and date of mammogram, from two US mammography cohorts, who had two-dimensional full-field digital mammograms performed 2-5.5 years before cancer diagnosis. We assessed Breast Imaging Reporting and Data System density, an AI malignancy score (1-10), and volumetric density measures. We used conditional logistic regression to estimate odds ratios (ORs), 95% CIs, adjusted for age and BMI, and C-statistics (AUC) to describe the association of AI score with invasive cancer and its contribution to models with breast density measures. Likelihood ratio tests (LRTs) and bootstrapping methods were used to compare model performance.RESULTSOn mammograms between 2-5.5 years prior to cancer, a one unit increase in AI score was associated with 20% greater odds of invasive breast cancer (OR, 1.20; 95% CI, 1.17 to 1.22; AUC, 0.63; 95% CI, 0.62 to 0.64) and was similarly predictive of interval (OR, 1.20; 95% CI, 1.13 to 1.27; AUC, 0.63) and advanced cancers (OR, 1.23; 95% CI, 1.16 to 1.31; AUC, 0.64) and in dense (OR, 1.18; 95% CI, 1.15 to 1.22; AUC, 0.66) breasts. AI score improved prediction of all cancer types in models with density measures (PLRT values <.001); discrimination improved for advanced cancer (ie, AUC for dense volume increased from 0.624 to 0.679, Î ""AUC 0.065, P =.01) but did not reach statistical significance for interval cancer.CONCLUSIONAI imaging algorithms coupled with breast density independently contribute to long-term risk prediction of invasive breast cancers, in particular, advanced cancer.  © American Society of Clinical Oncology."
"Ethnophamacological relevance: Simiao Pill (SM) as a classic prescription of traditional Chinese medicine treatment of damp-heat arthralgia, the earliest from ‘Cheng Fan Bian Du ', written by the Qing Dynasty doctor Zhang Bingcheng. Previous studies have shown that SM has obvious curative effect on rheumatoid arthritis, which provides a basis for the application of SM in rheumatoid arthritis related complications. Aim of the study: Interstitial lung disease (ILD), as the most severe complication of rheumatoid arthritis (RA), lacks effective clinical treatments and a corresponding animal model. Simiao pill (SM) is a traditional Chinese medicine prescription extensively used as a complementary and alternative treatment for RA. However, the effect and mechanism of SM on RA-ILD have not yet been reported. This study aimed to investigate an appropriate animal model that can simulate RA-ILD, and the efficacy, safety, and mechanism of SM on RA-ILD. Methods: Collagen-induced arthritis (CIA) and bleomycin-induced pulmonary fibrosis model were combined to construct the CIA-BLM model. After the intervention of SM, the protective effects of SM on RA-ILD were determined by detecting the CIA mouse arthritis index (AI), Spleen index, and the extent of pulmonary fibrosis. The joint inflammation and pulmonary fibrosis were detected by immunohistochemistry, H&E staining, safranin- O fast green Sirius red staining, trap staining, and Masson staining. Finally, the mechanism was verified by Western blot and immunohistochemistry. Results: Our work showed that SM significantly reduced joint swelling, arthritis index, pulmonary fibrosis score, and spleen index in CIA mice. The pathological examination results indicated Si-Miao Pill suppressed inflammation, pulmonary fibrosis, bone erosion, and cartilage degradation of the ankle joint. Besides, SM up-regulated expressions of E-cadherin, whereas down-regulated expressions of α-SMA. Further studies confirmed that SM regulated JAK2/STAT3 and TGF-β/SMAD2/3. Conclusion: SM can not only effectively improve joint inflammation by JAK2/STAT3 Pathway but also inhibit pulmonary fibrosis by TGF-β/SMAD2/3. The fibrosis induced by CIA-BLM model was more stable and obvious than that induced by CIA model alone. © 2023 The Authors"
"Artificial intelligence (AI) and machine learning (ML) methods are currently widely employed in medicine and healthcare. A PubMed search returns more than 100,000 articles on these topics published between 2018 and 2022 alone. Notwithstanding several recent reviews in various subfields of AI and ML in medicine, we have yet to see a comprehensive review around the methods' use in longitudinal analysis and prediction of an individual patient's health status within a personalized disease pathway. This review seeks to fill that gap. After an overview of the AI and ML methods employed in this field and of specific medical applications of models of this type, the review discusses the strengths and limitations of current studies and looks ahead to future strands of research in this field. We aim to enable interested readers to gain a detailed impression of the research currently available and accordingly plan future work around predictive models for deterioration in health status. © 2023 Annual Reviews Inc.. All rights reserved."
"The rate of technological evolution is increasing rapidly, not just to solve the problem of humanity but also to explore what problems can occur to find the most significant issue that needs to be resolved first on a priority basis. An appropriate evolution requires a lot of data and computing power to make it possible to understand esoteric details quickly. Higher computing power is not enough to produce the most desired outcome. It requires a concise and advanced algorithm to generate satisfactory results. The development of artificial intelligence has been gaining additional weight over the last two decades because the availability of data and computing power is increasing rapidly. The field where the study of algorithms and information is performed to train the machine to get the response more likely to an intelligent human being is now a part of study for numerous science groups across the globe. It has changed everyone's life and consistently impacted almost every industry on a large scale. The development of industrial sectors such as IT, automobile, communication, energy, food, and healthcare is remarkable as these industries greatly grow in their domain. This chapter aims to highlight some of those important impressions of AI on industries responsible for bringing a significant shift in their timeline and consistently altering the future with unprecedented potential efforts. © IOP Publishing Ltd 2023. All rights reserved."
[No abstract available]
"The effects of climate change on the reproduction of water buffaloes need to be checked in order to employ necessary interventions to maintain or improve the management system and achieve optimal semen quality for the nationwide artificial insemination (AI) program. Fourteen (14) Bulgarian Murrah buffalo bulls were used to determine the effects of weekly changes in ambient temperature, relative humidity, and barometric pressure on the motility characteristics of buffalo sperm cells. Three studies were conducted; 1) Effect of the environmental parameters on motility characteristics of fresh semen, 2) Assessment if the effect of environmental parameters was reflected to the motility characteristics of frozen-thawed spermatozoa, and 3) Assessment of the direct effect of body and scrotal temperature on the motility characteristics of the spermatozoa. In Study 1, correlation analysis showed positive correlation between ambient temperature and motility parameters on distance travelled characteristics and the velocity of movement. Relative humidity had negligible correlation but barometric pressure has significant negative correlation with all the motility parameters except linearity and wobble. In Study 2, the effects of relative humidity and barometric pressure were still distinct in frozen semen but the effect of ambient temperature was negligible. The distance travelled characteristics and velocity of movement including BCF and ALH were negatively affected. THI of 28.5 ºC ambient temperature with 82% relative humidity fall to Moderate level of stress and 32.60 ºC with 70% relative humidity fall to Severe heat stress level showed significant effect on motility characteristics. Analysis on body and scrotal temperatures of the buffalo bulls showed a difference of 0.94 ºC with scrotal temperature lower than body temperature. A significant positive correlation was observed in body and scrotal to ambient temperature and significant negative correlation to barometric pressure. Correlation analysis showed significant positive correlation between testicular temperature and the sperm motility characteristics. Based on the results, it was concluded that in spite of being kept under confinement system of management, high ambient temperature (>28°C), relative humidity (>70%) and low barometric pressure (<1,000 hPA) affect the motility characteristics of water buffalo bulls. Hence, monitoring and intervention to maintain optimal sperm motility characteristics were recommended. © 2023 American Institute of Physics Inc.. All rights reserved."
"Digitalization is an important, and in many cases the most important driver of innovation in industry. This also applies to the real estate industry and, in particular, to real estate and facility management (FM). Established technologies such as CAFM work closely with modern digitalization trends such as IoT. In general where it is possible to adopt and use such IT-based technologies, the economic return on investment usually quickly follows. The main reason is these systems reduce time-consuming and error-prone transformation and coordination processes. In this chapter, digitalization trends are presented, which are already important for the real estate and FM industry today, but even more in the future. These include CAFM/IWMS, BIM, Mobile and Cloud Computing, Augmented Reality, Big Data, IoT, AI, Digital Workplce and Simulation and Integration Techniques. The chapter presents the diversity of these technologies and their development potentials. © The Author(s), under exclusive license to Springer Fachmedien Wiesbaden GmbH, part of Springer Nature 2023. All rights reserved."
"Despite stringent quality control checks, some bulls with apparently normal semen quality yield lower than expected pregnancy rates. This study profiled the transcriptome and performed histological analysis of the bovine uterus in response to sperm from high-fertility (HF) and low-fertility (LF) bulls. Postmortem uterine biopsies and uterine explants were collected from heifers 12 h after a fixed time artificial insemination (AI) to a synchronized estrus with frozen-thawed semen from five HF (fertility rate 4.01% ± 0.25) and five LF (fertility rate - 11.29% ± 1.11; mean ± SEM) bulls. Uterine biopsies were also collected from control (CTRL) heifers, which were not inseminated. RNA-sequencing and histological analysis were performed for differential gene expression and neutrophil quantification. In the HF treatment relative to CTRL heifers, there were 376 genes significantly differentially expressed in the endometrium with just one gene differentially expressed in the LF treatment relative to CTRL heifers. Comparing the HF and LF treatments directly, there were 40 significantly differentially expressed genes (P < 0.05). Transcriptomic analysis shows a predominant role for the inflammatory marker Interleukin-1 alpha, which was further confirmed by immunohistochemistry. Quantification of neutrophils in the endometrium showed a significant effect of sperm; however, there was no difference in neutrophil numbers between HF and LF groups. In conclusion, this novel study clearly shows a distinct inflammatory response to sperm in the endometrium and a divergent transcriptomic response to semen from HF and LF bulls. © The Author(s) 2023. Published by Oxford University Press behalf of Society for the Study of Reproduction."
"Purpose: Apart from, the smart edge computing (EC) robot (SECR) provides the tools to manage Internet of things (IoT) services in the edge landscape by means of real-world test-bed designed in ECR. Eventually, based on the results from two experiments held in little constrained condition, such as the maximum data size is 2GB, the performance of the proposed techniques demonstrate the effectiveness, scalability and performance efficiency of the proposed IoT model. Design/methodology/approach: Certainly, the proposed SECR is trying primarily to take over other traditional static robots in a centralized or distributed cloud environment. One aspect of representation of the proposed edge computing algorithms is due to challenge to slow down the consumption of time which happened in an artificial intelligence (AI) robot system. Thus, the developed SECR trained by tiny machine learning (TinyML) techniques to develop a decentralized and dynamic software environment. Findings: Specifically, the waste time of SECR has actually slowed down when it is embedded with Edge Computing devices in the demonstration of data transmission within different paths. The TinyML is applied to train with image data sets for generating a framework running in the SECR for the recognition which has also proved with a second complete experiment. Originality/value: The work presented in this paper is the first research effort, and which is focusing on resource allocation and dynamic path selection for edge computing. The developed platform using a decoupled resource management model that manages the allocation of micro node resources independent of the service provisioning performed at the cloud and manager nodes. Besides, the algorithm of the edge computing management is established with different path and pass large data to cloud and receive it. In this work which considered the SECR framework is able to perform the same function as that supports to the multi-dimensional scaling (MDS). © 2022, Emerald Publishing Limited."
"Illustrating the development of Artificial Intelligence (AI) and the changes it has generated in the economy, society and culture, this expansive book continues the debate concerning the digital revolution and the rise of the algorithmic society. Examining technological, economic and social transformations, and the role played by culture in terms of risks and new opportunities, Luciana Lazzeretti expertly reviews the issues surrounding the economics of innovation and the interaction with culture, creativity and local development to establish a future agenda for research. Commencing with a historical overview, Lazzeretti discusses how culture and creativity allow us to face the challenges of the new digital revolution and provides insightful antidotes to the risks generated by the rise and evolution of an algorithmic society. The key elements of the art of imagination and human intelligence are examined together with their mutual interactions and relationship with AI as they continue to remain intertwined. With a contemporary approach, this invaluable book will be an excellent resource for researchers and scholars interested in cultural economy and digitalization of cultural heritage. It will also be of interest to professionals who want to develop competencies relating to new technologies and the role of cultural organizations in the digital revolution. © Luciana Lazzeretti 2023. All rights reserved."
"Purpose: To assess the ability of generative AI to assist in crisis management planning and response. Design/methodology/approach: The viewpoint is built on a “conversation” with ChatGPT (CGPT) on the subject of crisis management. As such, portions of the text were generated by CGPT, a Large Language Model (LLM) and not by the author. Findings: While CGPT has mastered the language of crisis management, its ability to assist in real-life situations is probably limited. Paradoxically, it believes it can help provide predictive analytics even though it claims not to be able to assess future events. Originality/value: The author believes that the paradoxes inherent in CGPT’s claims to be able to assist in crisis management have not previously been examined. © 2023, Emerald Publishing Limited."
"Nowadays, skin cancer is one of the most important problems faced by the world, due especially to the rapid development of skin cells and excessive exposure to UV rays. Therefore, early detection at an early stage employing advanced automated systems based on AI algorithms plays a major job in order to effectively identifying and detecting the disease, reducing patient health and financial burdens, and stopping its spread in the skin. In this context, several early skin cancer detection approaches and models have been presented throughout the last few decades to improve the rate of skin cancer detection using dermoscopic images. This work proposed a model that can help dermatologists to know and detect skin cancer in just a few seconds. This model combined the merits of two major artificial intelligence algorithms: Deep Learning and Reinforcement Learning following the great success we achieved in the classification and recognition of images and especially in the medical sector. This research included four main steps. Firstly, the pre-processing techniques were applied to improve the accuracy, quality, and consistency of a dataset. The input dermoscopic images were obtained from the HAM10000 database. Then, the watershed algorithm was used for the segmentation process performed to extract the affected area. After that, the deep convolutional neural network (CNN) was utilized to classify the skin cancer into seven types: actinic keratosis, basal cell carcinoma, benign keratosis, dermatofibroma melanocytic nevi, melanoma vascular skin lesions. Finally, in regards to the reinforcement learning part, the Deep Q_Learning algorithm was utilized to train and retrain our model until we found the best result. The accuracy metric was utilized to evaluate the efficacy and performance of the proposed method, which achieved a high accuracy of 80%. Furthermore, the experimental results demonstrate how reinforcement learning can be effectively combined with deep learning for skin cancer classification tasks. © 2023, J.J. Strossmayer University of Osijek, Faculty of Electrical Engineering, Computer Science and Information Technology. All rights reserved."
"Northern highbush blueberry is an important fresh market product in New Jersey where the plant was first domesticated in the early 20th century. Because of the short period for safely and timely applying postemergence (POST) herbicides, reliance on residual herbicides that provide season-long control of weeds is essential for blueberry growers to minimize the detrimental effect of weed competition on berry yield and quality and bush growth. Field studies were conducted from 2018 to 2020 in Chatsworth, New Jersey, on 'Bluecrop', 'Duke', and 'Elliott' blueberry cultivars growing on sandy acidic soil to evaluate weed control and crop tolerance in response to repeated annual applications of indaziflam at 73 or 146 g ai ha-1 applied in fall or spring. The efficacy of indaziflam treatments were compared to those of fall-applied dichlobenil at 3,300 g ai ha-1 or a spring-applied mix of diuron at 1,800 g ai ha-1, oryzalin at 3,360 g ai ha-1, and mesotrione at 210 g ai ha-1. Indaziflam at the currently labeled rate of 73 g ai ha-1 provided ≥85% and season-long control of horseweed, Canadian toadflax, and large crabgrass with fall applications on dormant blueberry, whereas spring applications were less effective. Whereas minor (≤8%) and transient leaf crinkling was noted in response to spring-applied indaziflam at 146 g ai ha-1, a fall application never caused leaf crinkling greater than that observed in the nontreated weedy and weed-free controls, regardless of rate. No negative effects on plant growth or fruit production were observed from indaziflam applied at 73 or 146 g ai ha-1 in fall or spring. Findings of this study suggest that indaziflam applied at 73 (1× commercial use rate) and 146 g ai ha-1 is safe to use on blueberry grown on New Jersey sandy acidic soils despite restrictions for using this herbicide on such soils. © The Author(s), 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
"For AI-based systems in safety-critical domains, it is inevitable to understand the impact of random hardware faults affecting the target hardware accelerators. The high degree of data reuse makes Deep Neural Network (DNN) accelerators susceptible to significant fault propagation and hence hazardous predictions. Therefore, we present SiFI-AI, a simulation framework for fault injection in DNN accelerators. SiFI-AI proposes a hybrid simulation approach combining fast AI inference with cycle-accurate RTL simulation. Time-expensive RTL simulation is only used to accurately target registers in the hardware through condition-based fault injection. This enables to reveal vulnerable DNN layers and the related fault origin. In a resilience study with 1.5∼M fault injection experiments, we analyze representative DNNs and a state-of-the-art DNN accelerator to identify vulnerable layers. The study only takes 1.15 days which is 7x faster than state-of-the-art. Our experiments show the high impact of control register faults and that narrow and deep layers are 10x more resilient compared to the wide and shallow layers of a DNN.  © 2023 ACM."
"Recent years have seen the rapid deployment of Artificial Intelligence (AI) which allows systems to take intelligent decisions. AI breakthroughs could radically change modern libraries' operations. However, introducing AI in modern libraries is a challenging task. This research explores the potential for smart libraries to improve the caliber of user services through the use of machine learning (ML) techniques. The proposed work investigates machine learning methods such as Random Forest (RF) and boosting algorithms, including Light Gradient Boosting Machine (LGBM), Histogram-based gradient boosting (HGB), Extreme gradient boosting (XGB), CatBoost (CB), AdaBoost (AB), and Gradient Boosting (GB) for the task of identifying and classifying Favorite books and compares their performances. Comprehensive experiments performed on the publicly available dataset (Art Garfunkel's Library) show that the proposed model can effectively handle the task of identifying and classifying Favorite books. Experimental results show that LGBM has achieved outstanding performance with an accuracy rate of 94.9367% than Random Forest and other boosting ML algorithms. This empirical research work takes advantage of AI adoption in libraries using machine learning techniques. To the best of our knowledge, we are the first to develop an intelligent application for the modern library to automatically identify and classify Favorite books. © 2023, Intellectual Research and Development Education Foundation (YRPI). All rights reserved."
"Digital transformation is both an opportunity and a challenge. To take advantage of this opportunity for humans and the environment, the transformation process must be understood as a design process that affects almost all areas of life. In this paper, we investigate AI-Based Self-Adaptive Cyber-Physical Process Systems (AI-CPPS) as an extension of the traditional CPS view. As contribution, we present a framework that addresses challenges that arise from recent literature. The aim of the AI-CPPS framework is to enable an adaptive integration of IoT environments with higher-level process-oriented systems. In addition, the framework integrates humans as actors into the system, which is often neglected by recent related approaches. The framework consists of three layers, i.e., processes, semantic modeling, and systems and actors, and we describe for each layer challenges and solution outlines for application. We also address the requirement to enable the integration of new networked devices under the premise of a targeted process that is optimally designed for humans, while profitably integrating AI and IoT. It is expected that AI-CPPS can contribute significantly to increasing sustainability and quality of life and offer solutions to pressing problems such as environmental protection, mobility, or demographic change. Thus, it is all the more important that the systems themselves do not become a driver of resource consumption.  © 2023 the author(s), published by De Gruyter, Berlin/Boston."
"Purpose: This research aimed to examine the current status of artificial intelligence's (AI's) integration into Chinese adult education, by analyzing the influences that AI has had on current adult education practices in China and by discussing the opportunities and challenges that adult education in China is faced with under the rapid AI development in the past 12 years. Design/methodology/approach: This research employed systematic literature analysis. CNKI (China National Knowledge Infrastructure) Chinese Journals Full-text Database was used to collect scholarly publications on the use of AI in adult education in China that was published in the past decade. Data analysis included the following steps: identifying key words and phrases, detecting underlying meanings, searching for logical connections and relationships, collecting and connecting evidence to the research questions, and drawing logical and credible conclusions. Findings: The findings indicated that AI has been gradually integrated into Chinese adult education through innovations and explorations and AI's influence is broad and profound. More specifically, the following five main themes were identified. The field's understanding of AI technology and AI's influence on adult education has evolved and become more comprehensive; AI challenges traditional Chinese adult education practices by helping to actualize personalized learning and precision education; AI transforms adult learning resource development; AI helps to turn learning environment into an open intelligent learning system; and lastly, AI urges the shift of adult educator's role in adult learning. Research limitations/implications: This study is not without limitations. Contextualized in China, this study shares the limitations with other single country studies. One such limitation is “cumulation” issue. This study should be replicated in other country contexts to further validate the generalizability of the five main themes identified in this research. Practical implications: The five themes identified in this study can help understand the promises and challenges that AI brings to the field of adult education in China. These five themes can also serve as an integrated lens through which one can make sense of AI's integration into other countries' adult education practices. Originality/value: This paper fulfills an identified need of understanding the current status of AI's integration into and influence on the field of adult education in China. © 2023, Emerald Publishing Limited."
"The proposed EU regulation for Artificial Intelligence (AI), the AI Act, has sparked some debate about the role of explainable AI (XAI) in high-risk AI systems. Some argue that black-box AI models will have to be replaced with transparent ones, others argue that using XAI techniques might help in achieving compliance. This work aims to bring some clarity as regards XAI in the context of the AI Act and focuses in particular on the AI Act requirements for transparency and human oversight. After outlining key points of the debate and describing the current limitations of XAI techniques, this paper carries out an interdisciplinary analysis of how the AI Act addresses the issue of opaque AI systems. In particular, we argue that neither does the AI Act mandate a requirement for XAI, which is the subject of intense scientific research and is not without technical limitations, nor does it ban the use of black-box AI systems. Instead, the AI Act aims to achieve its stated policy objectives with the focus on transparency (including documentation) and human oversight. Finally, in order to concretely illustrate our findings and conclusions, a use case on AI-based proctoring is presented. © 2023 Owner/Author."
"Recent works in Explainable AI mostly address the transparency issue of black-box models or create explanations for any kind of models (i.e., they are model-agnostic), while leaving explanations of interpretable models largely underexplored. In this paper, we fill this gap by focusing on explanations for a specific interpretable model, namely pattern-based logistic regression (PLR) for binary text classification. We do so because, albeit interpretable, PLR is challenging when it comes to explanations. In particular, we found that a standard way to extract explanations from this model does not consider relations among the features, making the explanations hardly plausible to humans. Hence, we propose AXPLR, a novel explanation method using (forms of) computational argumentation to generate explanations (for outputs computed by PLR) which unearth model agreements and disagreements among the features. Specifically, we use computational argumentation as follows: we see features (patterns) in PLR as arguments in a form of quantified bipolar argumentation frameworks (QBAFs) and extract attacks and supports between arguments based on specificity of the arguments; we understand logistic regression as a gradual semantics for these QBAFs, used to determine the arguments' dialectic strength; and we study standard properties of gradual semantics for QBAFs in the context of our argumentative re-interpretation of PLR, sanctioning its suitability for explanatory purposes. We then show how to extract intuitive explanations (for outputs computed by PLR) from the constructed QBAFs. Finally, we conduct an empirical evaluation and two experiments in the context of human-AI collaboration to demonstrate the advantages of our resulting AXPLR method.  © 2023 - The authors. Published by IOS Press."
"Artificial intelligence/machine learning (AI/ML) applied to battery research is considered to be a powerful tool for accelerating the research cycle. However, the development of appropriate materials descriptors is often the first hurdle toward implementing meaningful and accurate AI/ML. Currently, rational solvent selection remains a significant challenge in electrolyte development and is still based on experiments. The dielectric constant (ε) and donor number (DN) in electrolyte design are insufficient. Finding theoretically computable solvent descriptors for evaluating Li+ solvation is a significant step toward accelerating electrolyte development. Here, based on the electrostatic interaction between Li+ and solvent, the electrostatic potential (ESP) of electrolyte solvent is calculated by density functional theory calculations and reveals significant regularity. ESP as a direct and simple solvent descriptor for conveniently designing electrolytes is proposed. The lowest negative electrostatic potential (ESPmin) ensures the nucleophilic capacity of the solvating solvent and the weak ESPmin means decreased solvation energy. Weak ESPmin and strong highest positive electrostatic potential (ESPmax) are the main characteristics of non-solvating antisolvents. Using the plot of ESPmin – ESPmax strong solvating solvent, weakly solvating solvent, or antisolvent are identified that have been used in electrolyte engineering. This solvent descriptor can boost AI/ML to develop high performance electrolytes. © 2023 Wiley-VCH GmbH."
"To the aim of constructing effective human-AI teams, that can be useful for improving caregiving in medicine and enhancing human performance also in other sectors (i.e., teaching), agents which interact with humans should be endowed with an emotion recognition and management module, capable of empathy, and of modeling aspects of the Theory of Mind, in the sense of being able to reconstruct what someone is thinking or feeling. In this paper, we propose an architecture for such a module, based upon an enhanced notion of Behavior Trees. We illustrate the effectiveness of the proposed architecture on a significant example and on a wider case study.  © 2023 - IOS Press. All rights reserved."
"Emotion-aware services are increasingly used in different applications such as gaming, mental health tracking, video conferencing, and online tutoring. The core of such services is usually a machine learning model that automatically infers its user's emotions based on different biological indicators (e.g., physiological signals and facial expressions). However, such machine learning models often require a large number of emotion annotations or ground truth labels, which are typically collected as manual self-reports by conducting long-term user studies, commonly known as Experience Sampling Method (ESM). Responding to repetitive ESM probes for self-reports is time-consuming and fatigue-inducing. The burden of repetitive self-report collection leads to users responding arbitrarily or dropping out from the studies, compromising the model performance. To counter this issue, we, in this paper, propose a Human-AI Collaborative Emotion self-report collection framework, HACE, that reduces the self-report collection effort significantly. HACE encompasses an active learner, bootstrapped with a few emotion self-reports (as seed samples), and enables the learner to query for only not-so-confident instances to retrain the learner to predict the emotion self-reports more efficiently. We evaluated the framework in a smartphone keyboard-based emotion self-report collection scenario by performing a 3-week in-the-wild study (N = 32). The evaluation of HACE on this dataset (≈11,000 typing sessions corresponding to more than 200 hours of typing data) demonstrates that it requires 46% fewer self-reports than the baselines to train the emotion self-report detection model and yet outperforms the baselines with an average self-report detection F-score of 85%. These findings demonstrate the possibility of adopting such a human-AI collaborative approach to reduce emotion self-report collection efforts.  © 2023 ACM."
"This systematic mapping review sheds light on how emerging technologies have been introduced and taught in various K-12 learning settings, particularly with regard to artificial intelligence (AI), machine learning (ML), the internet of things (IoT), augmented reality (AR), and virtual reality (VR). These technologies are rapidly being integrated into children's everyday lives, but their functions and implications are rarely understood due to their complex and distributed nature. The review provides a rigorous overview of the state of the art based on 107 records published across the fields of human-computer interaction, learning sciences, computing education, and child-computer interaction between 2010 and 2020. The findings show the urgent need on a global scale for inter- and transdisciplinary research that can integrate these dispersed contributions into a more coherent field of research and practice. The article presents nine discussion points for developing a shared agenda to mature the field. Based on the HCI community's expertise in human-centred approaches to technology and aspects of learning, we argue that the community is ideally positioned to take a leading role in the realisation of this future research agenda.  © 2023 Association for Computing Machinery."
"Algorithmic recourse discloses the internal procedures of a black-box decision process where decisions have significant consequences by providing recommendations to empower beneficiaries to achieve a more favorable outcome. To ensure an effective remedy, suggested interventions must not only be cost-effective but also robust and fair. To that end, it is essential to provide similar explanations to similar individuals. This study explores the concept of individual fairness and adversarial robustness in causal algorithmic recourse and addresses the challenge of achieving both. To resolve the challenges, we propose a new framework for defining adversarially robust recourse. That setting observes the protected feature as a pseudometric and demonstrates that individual fairness is a special case of adversarial robustness. Finally, we introduce the fair robust recourse problem and establish solutions to achieve both desirable properties both theoretically and empirically. © 2023 ACM."
"In recent years, algorithms have been incorporated into fact-checking pipelines. They are used not only to flag previously fact-checked misinformation, but also to provide suggestions about which trending claims should be prioritized for fact-checking - a paradigm called 'check-worthiness.' While several studies have examined the accuracy of these algorithms, none have investigated how the benefits from these algorithms (via reduction in exposure to misinformation) are distributed amongst various online communities. In this paper, we investigate how diverse representation across multiple stages of the AI development pipeline affects the distribution of benefits from AI-assisted fact-checking for different online communities. We simulate information propagation through the network using our novel Topic-Aware, Community-Impacted Twitter (TACIT) simulator on a large Twitter followers network, tuned to produce realistic cascades of true and false information across multiple topics. Finally, using simulated data as a test bed, we implement numerous algorithmic fact-checking interventions that explicitly account for notions of diversity. We find that both representative and egalitarian methods for sampling and labeling check-worthiness model training data can lead to network-wide benefit concentrated in majority communities, while incorporating diversity into how fact-checkers use algorithmic recommendations can actively reduce inequalities in benefits between majority and minority communities. These findings contribute to an important conversation around the responsible implementation of AI-assisted fact-checking by social media platforms and fact-checking organizations. © 2023 Owner/Author."
"Tritium (3H) is a radioactive isotope of hydrogen that is abundantly released from nuclear industries. It is extremely mobile in the environment and in all biological systems, representing an increasing concern for the health of both humans and non-human biota (NHB). The present review examines the sources and characteristics of tritium in the environment, and evaluates available information pertaining to its biological effects at different levels of biological organisation in NHB. Despite an increasing number of publications in the tritium radiobiology field, there exists a significant disparity between data available for the different taxonomic groups and species, and observations are heavily biased towards marine bivalves, fish and mammals (rodents). Further limitations relate to the scarcity of information in the field relative to the laboratory, and lack of studies that employ forms of tritium other than tritiated water (HTO). Within these constraints, different responses to HTO exposure, from molecular to behavioural, have been reported during early life stages, but the potential transgenerational effects are unclear. The application of rapidly developing “omics” techniques could help to fill these knowledge gaps and further elucidate the relationships between molecular and organismal level responses through the development of radiation specific adverse outcome pathways (AOPs). The use of a greater diversity of keystone species and exposures to multiple stressors, elucidating other novel effects (e.g., by-stander, germ-line, transgenerational and epigenetic effects) offers opportunities to improve environmental risk assessments for the radionuclide. These could be combined with artificial intelligence (AI) including machine learning (ML) and ecosystem-based approaches. © 2023 The Authors"
"Industry 4.0 signifies a new step in the control and organization of the manufacturing value chain. It can provide better digital solutions for our day-to-day lives during pandemics and post-pandemic life, including better activity planning, global public health emergencies, and risk assessment. The Internet of Things (IoT), as a part of Industry 4.0, brings new opportunities in various applications, including smart healthcare and cities. The combination of an IoT system and artificial intelligence (AI) has the following capabilities in pandemic management: public security improvement, contact tracing to access public places, and automated diagnosis and post-pandemic prognosis. In this chapter, we discuss the contribution of Industry 4.0 (IoT, digitalization, Big Data, AI, and cloud-based computing) for automatic surveillance and health monitoring during pandemics and post-pandemic life. More than fifty papers published in 2020-2 were analyzed in this chapter. © IOP Publishing Ltd 2023. All rights reserved."
"Artificial intelligence (AI) has been widely used in various fields of physics and engineering in recent decades. In this work, we introduce model-based reinforcement learning (MBRL), which is an important branch of machine learning in the AI domain, to the broadband frequency-swept laser control for frequency modulated continuous wave (FMCW) light detection and ranging (LiDAR). With the concern of the direct interaction between the optical system and the MBRL agent, we establish the frequency measurement system model on the basis of the experimental data and the nonlinearity property of the system. In light of the difficulty of this challenging high-dimensional control task, we propose a twin critic network on the basis of the Actor-Critic structure to better learn the complex dynamic characteristics of the frequency-swept process. Furthermore, the proposed MBRL structure would stabilize the optimization process greatly. In the training process of the neural network, we apply a delaying strategy to the policy update and introduce a smoothing regularization strategy to the target policy to further enhance the network stability. With the well-trained control policy, the agent generates the excellent and regularly updated modulation signals to control the laser chirp precisely and an excellent detection resolution is obtained eventually. Our proposed work demonstrates that the integration of data-driven reinforcement learning (RL) and optical system control gives an opportunity to reduce the system complexity and accelerate the investigation and optimization of control systems. © 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement."
"With the development of the era of artificial intelligence (AI), China has put forward the cultivation of computational thinking (CT) in the compulsory education curriculum standard. CT includes three dimensions: CT concept, CT practice and CT perspective. As a part of CT, the development of CT perspective can promote the growth of students ' connection ability, questioning ability and expression ability. CT perspective promotes the formation of computational identity through the internalization of concepts. At present, there are relatively few studies on the CT perspective. Based on this, this study proposes a design-based STEM + AI teaching model, aiming to create a combination of artificial intelligence and interdisciplinary to cultivate pupils ' CT perspective. In this study, a single group of pre-test and post-test experiments were conducted to test the CT perspective of students in third grade of a primary school in Wuhan. The research shows that the design-based STEM + AI teaching has significantly improved the expression ability and questioning ability of primary school students, but the improvement of connection ability is not significant. The implementation of STEM + AI teaching helps students to internalize ideas from the perspective of CT, thus cultivating students ' computational identity.  © 2023 ACM."
"Around the world due to pests and pathogens almost 50% of the agricultural produce is lost which is so alarming given the fact that many people die everyday due to starvation in poor nations. Crop diseases disturb the normal growth and physiological processes. It is estimated that every year 20-40% of crop loss is reported and, in some cases, whole production gets destroyed. So, to produce higher yield and for sustainable agriculture it is important to identify any diseases from the early stage itself. Technology can do a great help in this cause to detect plant disease by using various AI techniques. It is also important to recommend proper pesticides for the persisting disease. The model proposed is based upon a 9 layer resnet deep learning algorithm that takes in present time images of various crops and detects the disease & also recommends the suitable pesticide. Plant Village Dataset taken from Kaggle comprising 87000 images (38 Classes,13 Crops) is used. A custom dataset is also built consisting of disease-description-measures to be taken-pesticide or fertilizer to be used. The end system developed also has two other models integrated that are used for crop and fertilizer recommendations. They are built using the Random Forest Classifier algorithm and a parameter conditional statements function. © 2023 EDP Sciences. All rights reserved."
"Artificial Intelligence (AI) is widely used in dermatology to analyze trichoscopy imaging and assess Alopecia Areata (AA) and scalp hair problems. From this viewpoint, the Attention-based Balanced Multi- Tasking Ensembling Deep (AB-MTEDeep) network was developed, which combined the Faster Residual Convolutional Neural Network (FRCNN) and Long Short-Term Memory (LSTM) network with cross residual learning to classify scalp images into different AA classes. This article presents a new data augmentation model called AA-Generative Adversarial Network (AA-GAN) to produce a huge number of images from a set of input images. The structure of AA-GAN and its loss functions are comparable to those of standard GAN, which encompasses a generator and a discriminator network. To generate high-quality AA structure-based images, the generator was trained to extract the 2D orientation and confidence maps along with the bust depth map from real hair and scalp images. The discriminator was also used to separate real from generated images, which were provided as feedback to the generator to create synthetic images that are extremely close to the real input images. The created images were used to train the ABMTEDeep model for AA classification. Finally, the experimental results exhibited that the AA-GAN-ABMTEDeep achieved 96.94% accuracy. © 2023, Dr D. Pylarinos. All rights reserved."
"Artificial intelligence (AI) can be harnessed to create sophisticated social and moral scoring systems-enabling people and organizations to form judgments of others at scale. However, it also poses significant ethical challenges and is, subsequently, the subject of wide debate. As these technologies are developed and governing bodies face regulatory decisions, it is crucial that we understand the attraction or resistance that people have for AI moral scoring. Across four experiments, we show that the acceptability of moral scoring by AI is related to expectations about the quality of those scores, but that expectations about quality are compromised by people's tendency to see themselves as morally peculiar. We demonstrate that people overestimate the peculiarity of their moral profile, believe that AI will neglect this peculiarity, and resist for this reason the introduction of moral scoring by AI. © The Author(s) 2023. Published by Oxford University Press on behalf of National Academy of Sciences. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited."
"Enterprises invest in data platforms with the aim of extracting meaningful information through analytics. Typically, experts create analytics pipelines that feed into dashboards and provide answers to predetermined questions. This approach makes analytics a spectator sport for most people and introduces operational bottlenecks to leveraging those investments. To improve the value derived from data, many organizations are opting to open up their data assets and allow access to a wider range of users. However, using programming languages such as SQL and Python for analytics can be difficult for most enterprise users. DataChat provides a simplified data science approach that is intuitive, powerful, and accessible to all data users. The platform is built on a library of data functions that are cleanly abstracted to maximize efficiency and ease of use while maintaining a rich suite of tools necessary for data science. With these functions, users can create data analysis pipelines by using a simple point-and-click interface in a spreadsheet view or by using natural English interfaces. Modern sharing and collaboration features are central to all aspects of the platform, allowing teams to easily bridge expertise gaps. A deeper understanding of results is facilitated by providing automatically-generated English explanations of how they were derived. By enhancing these aspects of data science and human-to-human communication, the platform addresses the needs that many organizations are encountering as their analytics needs mature.  © 2023 Owner/Author."
"In the era of big data and artificial intelligence (AI), advanced data storage and processing technologies are in urgent demand. The innovative neuromorphic algorithm and hardware based on memristor devices hold a promise to break the von Neumann bottleneck. In recent years, carbon nanodots (CDs) have emerged as a new class of nano-carbon materials, which have attracted widespread attention in the applications of chemical sensors, bioimaging, and memristors. The focus of this review is to summarize the main advances of CDs-based memristors, and their state-of-the-art applications in artificial synapses, neuromorphic computing, and human sensory perception systems. The first step is to systematically introduce the synthetic methods of CDs and their derivatives, providing instructive guidance to prepare high-quality CDs with desired properties. Then, the structure–property relationship and resistive switching mechanism of CDs-based memristors are discussed in depth. The current challenges and prospects of memristor-based artificial synapses and neuromorphic computing are also presented. Moreover, this review outlines some promising application scenarios of CDs-based memristors, including neuromorphic sensors and vision, low-energy quantum computation, and human–machine collaboration. © 2023 The Authors. Advanced Science published by Wiley-VCH GmbH."
"As a growing country, Zimbabwe must adopt these technologies since they offer numerous benefits, including precision and accuracy. Because a significant volume of MRI data must be reviewed, this procedure takes long and is unsuitable for big data. Because automated solutions are more cost-effective, they are essential. Automated medical imaging has become a hot issue in a variety of medical diagnostic checking. Magnetic Resonance Imaging (MRI) tumor diagnosis that is automated is crucial because it provides information about abnormal tissues that are needed for treatment planning. Human inspection is the traditional approach for detecting defects in magnetic resonance brain imaging. This method is impractical when dealing with large amounts of data. As a result, radiologists are developing automated tumor detection technologies to save time. It is necessary to use MATLAB to train an artificial neural network to identify brain cancers. In addition, an algorithm that can distinguish and categorize tumors into carcinogenic and non-cancerous tumors must be developed. The complexity and variety of malignancies make MRI brain tumor diagnosis tough task. Machine learning approaches are utilized to detect malignancies in brain MRI in this study. In this research paper work, three stages are implemented in preprocessing on the brain. To extract texture features, the Gray Level Co-occurrence Matrix (GLCM) is used. They are then classified using a machine learning technique, indicating the feasibility of utilizing machine vision to discriminate between cancerous and non-cancerous brain tumors. As a result, an AI process is used to arrange them correctly, demonstrating the feasibility of using machine vision to distinguish between cancerous and non-malignant cerebrum growths. It has the potential to make patient management easier in the future. Computer vision is one of the most widely utilized technology technologies for skin cancer detection systems in most industrialized countries. © 2023 Author(s)."
"In this article, we provide an overview of efforts to regulate the various phases of the artificial intelligence (AI) life cycle. In doing so, we examine whether—and, if so, to what extent—highly fragmented legal frameworks are able to provide safeguards capable of preventing the dangers that stem from AI- and algorithm-driven organisational practices. We critically analyse related developments at the European Union (EU) level, namely the General Data Protection Regulation, the draft AI Regulation, and the proposal for a Directive on improving working conditions in platform work. We also consider bills and regulations proposed or adopted in the United States and Canada via a transatlantic comparative approach, underlining analogies and variations between EU and North American attitudes towards the risk assessment and management of AI systems. We aim to answer the following questions: Is the widely adopted risk-based approach fit for purpose? Is it consistent with the actual enforcement of fundamental rights at work, such as privacy, human dignity, equality and collective rights? To answer these questions, in section 2 we unpack the various, often ambiguous, facets of the notion(s) of ‘risk’—that is, the common denominator with the EU and North American legal instruments. Here, we determine that a scalable, decentralised framework is not appropriate for ensuring the enforcement of constitutional labour-related rights. In addition to presenting the key provisions of existing schemes in the EU and North America, in section 3 we disentangle the consistencies and tensions between the frameworks that regulate AI and constrain how it must be handled in specific contexts, such as work environments and platform-orchestrated arrangements. Paradoxically, the frenzied race to regulate AI-driven decision-making could exacerbate the current legal uncertainty and pave the way for regulatory arbitrage. Such a scenario would slow technological innovation and egregiously undermine labour rights. Thus, in section 4 we advocate for the adoption of a dedicated legal instrument at the supranational level to govern technologies that manage people in workplaces. Given the high stakes involved, we conclude by stressing the salience of a multi-stakeholder AI governance framework. © The Author(s) 2023."
"The pace of US Food and Drug Administration-approved medical devices that incorporate artificial intelligence (AI) or machine learning as part of the device is accelerating. As of September 2021, 350 such devices have been approved for commercial sale in the United States. As much as AI has become ubiquitous in our lives—keeping our cars between the lines on the highway, converting speech to text on the fly, recommending movies, books, or restaurants, and so much more, AI also appears destined to become a routine aspect in daily spine surgery. Neural network types of AI programs have achieved extraordinary pattern recognition and predictive abilities—far surpassing human capabilities—and thus appears well suited to back pain and spine surgery diagnostic and treatment pattern recognition and prediction tasks. These AI programs are also data hungry. As luck would have it, surgery generates an estimated 80 MB per patient per day collected in a variety of datasets. When aggregated, this represents a 200+ billion patient record data ocean of diagnostic and treatment patterns. Such Big Data, when combined with a new generation of convolutional neural network (CNN) AI, set the stage for a cognitive revolution in spine surgery. However, there are important issues and concerns. Spine surgery is a mission-critical task. Because AI programs lack explainability and are absolutely reliant on correlative, not causative, data relationships, the emerging role of AI and Big Data in spine surgery will likely come first in productivity tools and later in narrowly defined spine surgery tasks. The purpose of this article is to review the emergence of AI in spine surgery applications and examine spine surgery heuristics and “expert” decision models within the context of AI and Big Data. © International Society for the Advancement of Spine Surgery."
"Human activity recognition (HAR) - the exciting time series based classification task has been categorize into vision, sensor and hybrid approaches based on the input modality. The availability of these modalities to capture data for HAR is a motivation factor in building AI / ML enabled applications in emerging areas such as health care, surveillance, etc. Based on the input data captured, HAR helps in identifying the specific movement of an individual. It also helps in inferring the current behavior and goals of the human body depending upon the environment, through a series of observations. A generic process flow of HAR involves data acquisition followed by pre-processing; feature extraction, feature selection and time series based classification process. The pros, and cons of the HAR approaches were analyzed along with a detailed report on the datasets being used which provides a comprehensive review on the usage of appropriate ML algorithms and analyzes the risks in the existing HAR models for the future scopes in this field. © 2023 Author(s)."
[No abstract available]
"Criteria identify and define the aspects on which what we evaluate is judged and play a central role in evaluation practice. While work on the use of AI in evaluation is burgeoning, at the time of writing, a set of criteria to consider in evaluating the use of AI in evaluation has not been proposed. As a first step in this direction, Teasdale's Criteria Domains Framework was used as the lens through which to critically read articles included in this special issue. This resulted in the identification of eight criteria domains for evaluating the use of AI in evaluation. Three of these criteria domains relate to the conceptualization and implementation of AI in evaluation practice. Five criteria domains are focused on outcomes, specifically those stemming from the use of AI in evaluation. More work is needed to further identify and deliberate possible criteria domains for AI use in evaluation. © 2023 American Evaluation Association and Wiley Periodicals LLC."
"Recently, some effort went into explaining intransparent and black-box models, such as deep neural networks or random forests. So-called model-agnostic methods typically approximate the prediction of the intransparent black-box model with an interpretable model, without considering any specifics of the black-box model itself. It is a valid question whether direct learning of interpretable white-box models should not be preferred over post-hoc approximations of intransparent and black-box models. In this paper, we report the results of an empirical study, which compares post-hoc explanations and interpretable models on several datasets for rule-based and feature-based interpretable models. The results seem to underline that often directly learned interpretable models approximate the black-box models at least as well as their post-hoc surrogates, even though the former do not have direct access to the black-box model. © 2023 by the authors."
"This study conducted a hardware design approach of critical smart warehouse key components with the desire to best support digital transformation and AI implementation. Specifically, three important constituents related to the three main areas of a smart warehouse (i.e., mechanical, controller, and data collection algorithm), including telescopic fork mechanisms for AS/RS stacker cranes, multilayer industrial controller, and an effective algorithmic data collection solution was created and demonstrated. The strict requirements of pallet storage smart warehouse comprise (1) mechanical mechanism: high payload, high positioning accuracy, low taping deformation, high acceleration transmission, and low noise; (2) controller: industrial standard, cloud database communication, and storage, accurate inventory, stock keeping unit planning, inventory management, circulating conveyor system, and control of incoming and exiting items; (3) data collection: high capacity with 1000 pallets storage, pallet circulation, and management code reassignment, and continuous operation; and the desire for the compatibility and possibility of complementing IoT and AI add-ins in the future has been considered and guaranteed. The results show that the input design parameters and constraints are well captured and assured. © 2023 River Publishers. All rights reserved."
"AI-empowered sweat metabolite analysis is an emerging and open research area with great potential to add a third category to biometrics: chemical. Current biometrics use two types of information to identify humans: physical (e.g., face, eyes) and behavioral (i.e., gait, typing). Sweat offers a promising solution for enriching human identity with more discerning characteristics to overcome the limitations of current technologies (e.g., demographic differential and vulnerability to spoof attacks). The analysis of a biometric trait’s chemical properties holds potential for providing a meticulous perspective on an individual. This not only changes the taxonomy for biometrics, but also lays a foundation for more accurate and secure next-generation biometric systems. This paper discusses existing evidence about the potential held by sweat components in representing the identity of a person. We also highlight emerging methodologies and applications pertaining to sweat analysis and guide the scientific community towards transformative future research directions to design AI-empowered systems of the next generation. © 2023 by the authors."
"Four late season cauliflower genotypes namely Pusa Snowball 1 (PSB 1), Pusa Snowball K1 (PSB K1), Pusa Snowball K25 (PSB K25) and Plantsman Snowball (PMS) were evaluated for their resistance to Diamond Back Moth (DBM), Plutella xylostella (Linnaeus), during 2018-19. Among the genotypes screened PSB1 and PMS were comparatively less susceptible. Biochemical analyses of leaves from 85 days old healthy plants showed that PMS had higher phenol content (89.9 mg 100g-1) which on par with PSB-1 with 81.43 mg 100g-1 of leaf. This was followed by PSB K25 and PSB K1 with 69.5 and 54.23 mg 100g-1 respectively. The varieties with higher phenol content offered more resistance to DBM. Protein content of the plant showed no correlation with DBM infestation. Foliar application of spinosad (45 % SC @ 56.25 g ai ha-1 +NSKE 2.5 %) recorded a higher reduction (94.40 %) of DBM population over control, with comparatively higher number of syrphids and coccinellids indicating its safety to natural enemies. © 2023 Association for Advancement of Entomology."
"The energy sector is enduring a momentous transformation with new technological advancements and increasing demand leading to innovative pathways. Artificial intelligence (AI) is emerging as a critical driver of the change, offering new ways to optimize energy systems operations, control, automation, etc. Developing a competitive policy framework aligned with circular economy practices to adapt to the trends of the rapid revolution is crucial, shaping the future of energy and leading the sector in a sustainable, equitable, and impartial direction. This study aims to propose an AI-driven policy framework that aligns with the circular economy business model to address the transformation trend in the development of energy policies through a multidisciplinary approach. The study identifies key trends, various approaches, and evaluates the potential of AI in addressing the challenges. The AI-driven policy paradigm outlines a comprehensive framework and roadmap to harness the potential of AI through a forward-looking policy framework that considers the rapidly changing landscape and the essence of the circular economy. The proposed novel framework provides a roadmap for researchers, governments, and other stakeholders to navigate the future of energy and unlock the potential of AI for a sustainable energy future. © 2023 The Author(s)"
"The era of advanced artificial intelligence has arrived with the development of chatbots like ChatGPT (Chat Generative Pre-trained Transformer). As described by Ouyang et al. (2022), ChatGPT demonstrates an impressive ability to generate human-like responses and solve practical problems, surpassing original expectations for its capabilities. The rapid release and adoption of ChatGPT signals a new phase in AI development, powered by large language models that can be fine-tuned through human feedback. However, risks remain regarding how such powerful models may be misused. Further research is needed to ensure safe and ethical deployment of these transformative technologies.  © 2023 the author(s), published by De Gruyter, Berlin/Boston."
"Wild aquatic birds are natural reservoirs of influenza A viruses and H3 subtype is one of the most prevalent subtypes in waterfowl. Two H3N8 viruses of low pathogenic avian influenza (LPAI) were isolated via egg inoculation technique from the fecal swab specimens from imported barnacle goose and paradise shelduck in Veterinary Research Institute Ipoh, Malaysia. The full length of eight gene segments of the two viruses were amplified and sequenced with specific primers. The sequences were molecularly characterized, and the sequence identity were assessed with other published sequences. The two viruses are identical and they possess the same amino acid sequences for all the eight gene segments. The viruses were highly similar to the H3 virus from Netherlands and N8 virus from Belgium respectively. Phylogenetic analysis revealed that all the eight gene segments were grouped in the Eurasian lineage, and genetic reassortment may occur between the internal genes of the H3 viruses and other AI subtypes. Though four amino acid substitutions were identified in the hemagglutinin gene, the viruses retained most of the avian-type receptor binding preference. Few amino acid substitutions were observed in all internal genes. Most of the neuraminidase inhibitors and adamantine resistance related mutation were not seen in the viruses. The replicative capacity, cross species transmissibility, and potential zoonotic risk of the viruses are worth further investigation. As H3 virus poses potential threats to both human and animals, and with the increase in the international trade of birds; strict quarantine practice at the entry point and good laboratory diagnostic capabilities is crucial to prevent the introduction of new AI virus into our country. © 2023, Malaysian Society for Parasitology. All rights reserved."
[No abstract available]
"In this article, I discuss the use of artificial intelligence (AI) in evaluation and its relevance to the evolution of the field. I begin with a background on how AI models are developed, including how machine learning makes sense of data and how the algorithms it develops go on to power AI models. I go on to explain how this foundational understanding of machine learning and natural language processing informs where AI might and might not be effectively used. A critical concern is that AI models are only as strong as the data on which they are trained, and evaluators should consider important limitations when using AI, including its relevance to structural inequality. In considering the relationship between AI and evaluation, evaluators must consider both AI's use as an evaluative tool and its role as a new subject of evaluation. As AI becomes more and more relevant to a wider array of fields and disciplines, evaluators will need to develop strategies for how good the AI is (or is not), and what good the AI might (or might not) do. © 2023 American Evaluation Association and Wiley Periodicals LLC."
"The European Society of Toxicologic Pathology (ESTP) initiated a survey through its Pathology 2.0 workstream in partnership with sister professional societies in Europe and North America to generate a snapshot of artificial intelligence (AI) usage in the field of toxicologic pathology. In addition to demographic information, some general questions explored AI relative to (1) the current status of adoption across organizations; (2) technical and methodological aspects; (3) perceived business value and finally; and (4) roadblocks and perspectives. AI has become increasingly established in toxicologic pathology with most pathologists being supportive of its development despite some areas of uncertainty. A salient feature consisted of the variability of AI awareness and adoption among the responders, as the spectrum extended from pathologists having developed familiarity and technical skills in AI, to colleagues who had no interest in AI as a tool in toxicologic pathology. Despite a general enthusiasm for these techniques, the overall understanding and trust in AI algorithms as well as their added value in toxicologic pathology were generally low, suggesting room for the need for increased awareness and education. This survey will serve as a basis to evaluate the evolution of AI penetration and acceptance in this domain. © The Author(s) 2023."
"Big data analytical applications like deep learning-based AI applications more and more rely on distributed file system to store and manage large scale data sets. File systems often need to provide standard POSIX interfaces to en⁃ hance their access compatibility with upper-layer applications. However, it is complicated to develop POSIX-compatible file systems in kernel space. In recent years, FUSE (File System in User Space) has been used by many well-known file sys⁃ tems, including Alluxio, Ceph, etc., because it significantly simplifies the file system development. The popular FUSE li⁃ brary libfuse is developed in the C language. However, the popular distributed file systems (like HDFS and Alluxio) for big data applications are developed with the Java language. To make the Java-based distributed file systems use the FUSE mechanism, the cross-language FUSE frameworks are needed to bridge the gap, which becomes a potential performance bot⁃ tleneck. The cross-language FUSE framework uses a cross-programming language function callback mechanism to enable the C functions of the FUSE library to call the programming interface provided by the distributed file system in Java. In this way, we can provide the access to the standard POSIX interface for the Java-based distributed file systems. However, the existing cross-language FUSE frameworks are inefficient in performance. It makes the data I/O in data-intensive appli⁃ cations (like deep learning and big data analysis) occupy noticeable proportion of their execution costs. To address the prob⁃ lem, we first systematically evaluate the performance of the widely-used cross-language FUSE framework, and find the bot⁃ tlenecks of throughput performance in high concurrency and small file scenarios. We then analyze the bottlenecks of the cross-language FUSE framework from multiple perspectives, and propose several directions for optimizing the cross-lan⁃ guage FUSE framework. According to the optimization directions, we design and implement JNI-FUSE (Java Native Inter⁃ face-Filesystem in User SpacE), an efficient cross-language FUSE framework. In JNI-FUSE, we propose the defer detach and meta cache techniques to reduce execution costs of cross-programming language function callbacks. Experiment results show that JNI-FUSE improves the average framework performance from 1.15 times to 6.04 times compared to the cuttingedge cross-language FUSE framework JNR-FUSE. JNI-FUSE improves the end-to-end performance by 1.90 time to 2.71 times, and accelerates the deep learning training by 1.06 times to 1.73 times compared to JNR-FUSE. JNI-FUSE has been accepted and integrated by the well-known open-source distributed file system Alluxio due to its good performance. © 2023 Chinese Institute of Electronics. All rights reserved."
"With the development and progress of science and technology, artificial intelligence (AI) has been widely used in the field of Stomatology, which promotes the intelligent, accurate and minimally invasive development of Stomatological treatment. Orthodontics, as a branch of Stomatology, has carried out many researches on the clinical application of AI, including cephalometric analysis, making treatment plan, determination of growth and development stage, prediction of postoperative effect, etc. In this paper, the application progress of AI technology in orthodontics is reviewed, hence to promote the application of AI in the field of orthodontics. © 2023 Chinese Medical Association. All rights reserved"
"Cereal rye cover crop (cereal rye) and preemergence (PRE) herbicides are becoming common practices for managing herbicide-resistant weeds in soybean production. Adopting these two practices in combination raises concerns regarding herbicide fate in soil, given that the cereal rye biomass can intercept the herbicide spray solution, preventing it from reaching the soil. Delaying cereal rye termination until soybean planting (planting green) optimizes biomass accumulation but might also increase PRE interception. To better understand the dynamics between cereal rye and PRE herbicides, a field experiment was conducted to evaluate two soil management practices (tillage and no-till) and two cereal rye termination practices in the planting-green system (glyphosate [1,260 g ae ha-1] and roller-crimper) on the spray deposition and fate of PRE herbicides and soybean yield. The spray deposition was assessed by placing water-sensitive paper cards on the soil surface before spraying the PRE herbicides (sulfentrazone [153 g ai ha-1] + S-metolachlor [1,379 g ai ha-1]). Herbicide concentration in soil (0 to 7.6 cm) was quantified 25 d after treatment (DAT). The presence of no-till stubble and cereal rye biomass reduced the spray coverage compared to tillage at PRE application, which reflected in a reduction in the concentration of both herbicides in soil 25 DAT. Soybean yield was reduced in all three years when the cereal rye was terminated with a roller-crimper but only reduced in one year when terminated with glyphosate. Our findings indicate that mainly cereal rye biomass reduced the concentration of PRE herbicides in the soil due to the interception of the spray solution during application. Although higher cereal rye biomass accumulation can provide better weed suppression according to the literature, farmers should be aware that the biomass can lower the concentration of PRE herbicides reaching the soil, thus intensifying field scouting to ensure that weed control is not being negatively affected. © The Author(s), 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
"This special issue of the European Labour Law Journal, edited by Jeremias Adams-Prassl, Halefom Abraha, Aislinn Kelly-Lyth, Sangh Rakshita and Michael ‘Six’ Silberman, explores the regulation of Algorithmic Management in the European Union and beyond. In our guest editorial, we set out the background to the project, introduce the reader to the key themes and highlights of the papers to follow, and acknowledge the support that the project has enjoyed. © The Author(s) 2023."
"Cryptography is defined as the analysis of encryption or secretive writing of information with the use of mathematical & logical concepts in order to prevent data from being compromised. Because of the growing security issues around Internet of Things (IoT) & artificial intelligence (AI) based applications, this method has gained in significance in computer technologies for banking & healthcare systems, transportation, as well as other implementations. Although each cryptographic strategy is designed to have its own unique strength, the application of a single cryptographic strategy into a systems has certain drawbacks, as will be discussed below. For example, the symmetric key encryption approach is a cost-effective way of protecting information that does not sacrifice security in the process. The distribution of the private key, on the other hand, is a significant issue. The asymmetrical method, on either side, overcomes the problem of private key transmission; nevertheless, the independent approach is slower and uses more computer resources as comparing to symmetric encryption. While a hash function, on the other hand, produces a distinctive & fixed-length signatures for a communication in order to ensure information security, the technique is just a one-way function that is not possible to reverse. As an option to addressing the security flaws of individual cryptographic schemes, the inclusion of many cryptographic schemes, also known as the hybridization approach, is being suggested, which has the advantage of increasing the efficiency of information security while also discussing the problem of key transfer. Existing IoT & AI domains that have adopted hybrid methods have been recognized, and a study has been carried out as per classification of the domain under consideration. The security of the networks and the data sent over the network is a top priority for network providers or network operators. As a result, cryptographic methods are used to protect the data throughout the data exchange process and during different interactions. Traditional cryptographic methods, on either side, are well-known, because hackers are aware of the answer to the problem. As a result, a fresh type of cryptographic method is needed, one that increases the security & complexities of the data encryption while maintaining its simplicity. An innovative hybrid cryptographic approach for enhancing data security throughout networks transmission is presented in this article, and the consequences of its implementation and evaluation are discussed. Throughout a comparative performance study, the suggested cryptographic method was able to identify the most efficient & enhanced encrypted message. © 2023 Novel Carbon Resource Sciences. All rights reserved."
"Advancements in Artificial Intelligence (AI) signal a paradigmatic shift with the potential for transforming many various aspects of society, including evaluation education, with implications for subsequent evaluation practice. This article explores the potential implications of AI for evaluator and evaluation education. Specifically, the article discusses key issues in evaluation education including equitable language access to evaluation education, navigating program, social science, and evaluation theory, understanding evaluation theorists and their philosophies, and case studies and simulations. The paper then considers how chatbots might address these issues, and documents efforts to prototype chatbots for three use cases in evaluation education, including a guidance counselor, teaching assistant, and mentor chatbot for young and emerging evaluations or anyone who wants to use it. The paper concludes with ruminations on additional research and activities on evaluation education topics such as how to best integrate evaluation literacy training into existing programs, making strategic linkages for practitioners, and evaluation educators. © 2023 The Authors. New Directions for Evaluation published by American Evaluation Association and Wiley Periodicals LLC."
"Natural Language Processing is a branch of artificial intelligence (AI) that focuses on the interaction between computers and human language. Speech recognition systems utilize machine learning algorithms and statistical models to analyze acoustic features of speech, such as pitch, duration, and frequency, to convert spoken words into written text. The Student English Oral Proficiency Assessment and Feedback System provides students with a comprehensive evaluation of their spoken English skills and offers tailored feedback to help them improve. It can be used in language learning institutions, universities, or online platforms to support language education and enhance oral communication abilities. In this paper constructed a framework stated as Latent Dirichlet Integrated Deep Learning (LDiDL) for the assessment of student English proficiency assessment. The system begins by collecting a comprehensive dataset of spoken English samples, encompassing various proficiency levels. Relevant features are extracted from the samples, including acoustic characteristics and linguistic attributes. Leveraging Latent Dirichlet Allocation (LDA), the system uncovers latent topics within the data, enabling a deeper understanding of the underlying themes present in the spoken English. To further enhance the analysis, a deep learning model is developed, integrating the LDA topics with the extracted features. This model is trained using appropriate techniques and evaluated using performance metrics. Utilizing the predictions made by the model, the system generates personalized feedback for each student, focusing on areas of improvement such as vocabulary, grammar, fluency, and pronunciation. Simulation mode uses the native English speech audio for the LDiDL training and classification. The experimental analysis stated that the proposed LDiDL model achieves an accuracy of 99% for the assessment of English Proficiency. © 2023 Auricle Global Society of Education and Research."
"In this study, the effect of quorum sensing signal molecule AI-2 on EPS production of L. fermentum TG4-1 and P. acidilactici 11-3 was investigated to explore the regulation mechanism of LuxS/AI-2 QS system on EPS secretion of lactic acid bacteria. EPS production and AI—2 activity of strains TG4—1 —1 and 11—3 were measured by phenol—sulfuric; acid method and V. harveyi BB170 bioluminescence method. The optimal concentration of exogenous AI—2 was selected and the growth, EPS production and AI -2 activity of the strain were measured. The morphology of the strain and polysaccharide after freeze-drying were observed by scanning electron microscopy. The results showed that both strains TG4-1-1 and 11-3 had the strongest AI-2 activity at 10 h, and the highest EPS yield at 22 h, reaching (195.863±1. 643) mg/L and (125.179+1.458) mg/L, respectively. After screening, 100 p.mol/L exogenous AI—2 was determined as the optimal supplemental concentration of strains TG4-1-1 and 11-3, which had no significant effect on the growth of the two strains at each stage (-ft>0.05). EPS production of strain TG4-1-1 at 16-22 h and strain 11-3 at 13~22h were significantly promoted (P<0.05); meanwhile, the activity of AI-2 was significantly increased at 10 h (P<0.05). After 100 fxmol/L exogenous AI-2 was added for 13 h, the cells had smooth surface, regular shape and full shape. However, it had no obvious effect on the surface structure and morphology of exopolysaccharides. These results indicated that exogenous AI-2 at a certain concentration could promote the production of EPS in strains TG4-1-1 and 11-3, providing a certain reference for the mechanism of LuxS/AI-2 QS system regulating EPS. © 2023 Chinese Institute of Food Science and Technology. All rights reserved."
[No abstract available]
"Artificial Intelligence (AI) has emerged as a transformative technology in the scientific community with the potential to accelerate and enhance research in various fields. ChatGPT, a popular language model, is one such AI-based system that is increasingly being discussed and being adapted in scientific research. However, as with any technology, there are challenges and limitations that need to be addressed. This paper focuses on the challenges and limitations that ChatGPT faces in the domain of organic materials research. This paper will take organic materials as examples in the use of ChatGPT. Overall, this paper aims to provide insights into the challenges and limitations of researchers working in the field of organic materials. © 2023 by the author."
"The Italian Society of Clinical Pathology and Laboratory Medicine (SIPMeL) has joined Choosing Wisely Italy (CW) together with many other scientific societies and professional associations and has produced to date by the Society and the GdS-EMM and GdS-AI three lists totaling fifteen “recommendations of tests at risk of inappropriateness that physicians and patients should talk about.” CW promotes periodic review of the recommendations taking into account recent studies and continuous evaluation of their quality. The five procedures proposed by SIPMeL six years ago can now be considered acquired in the knowledge and competence of the laboratory professionals and clinicians, and no revision and updating of the recommendations and their rationale are required. Therefore, steps were taken: 1) to update the bibliographic references of the first list according to recently published scientific studies and 2) to complete the preparation of a second list of recommendations that began with the publication of an article and a session of the 7th National Congress of SIPMeL in October 2022. The following five “Practices at risk of inappropriateness that physicians and patients should talk about” were identified through a multi-step process and were included in a second SIPMeL-CW list: • do not request amylase in addition to lipase when acute pancreatitis is suspected; • do not request erythrocyte sedimentation rate to screen asymptomatic patients or as a general test to look for inflammatory states in patients with undiagnosed conditions; • do not request blood ammonium for diagnosis or management of hepatic encephalopathy in patients with chronic hepatopathy; • do not request procalcitonin outside of evidence-based protocols defined by scientific societies or at the health organization/ regional/national level; • do not request uric acid as part of routine assessment of cardiovascular risk, obesity, or diabetes. The purpose of the article is to present the critical appraisal of relevant available literature and the rationale supporting the inclusion of the single recommendations in the list. © 2023 EDIZIONI MINERVA MEDICA."
"Primary care has the potential to be transformed by artificial intelligence (AI) and, in particular, machine learning (ML). This review summarizes the potential of ML and its subsets in influencing two domains of primary care: pre-operative care and screening. ML can be utilized in preoperative treatment to forecast postoperative results and assist physicians in selecting surgical interventions. Clinicians can modify their strategy to reduce risk and enhance outcomes using ML algorithms to examine patient data and discover factors that increase the risk of worsened health outcomes. ML can also enhance the precision and effectiveness of screening tests. Healthcare professionals can identify diseases at an early and curable stage by using ML models to examine medical pictures, diagnostic modalities, and spot patterns that may suggest disease or anomalies. Before the onset of symptoms, ML can be used to identify people at an increased risk of developing specific disorders or diseases. ML algorithms can assess patient data such as medical history, genetics, and lifestyle factors to identify those at higher risk. This enables targeted interventions such as lifestyle adjustments or early screening. In general, using ML in primary care offers the potential to enhance patient outcomes, reduce healthcare costs, and boost productivity. © 2023 by the authors."
"The Proposal for an Artificial Intelligence Act is one of the most representative features of the new regulatory model displayed by the European Union to regulate activities in the digital environment. The purpose of this work is to analyze the implications of the future Regulation from a Private international law perspective. A relevant provision in this regard is art. 2 (1) that establishes the territorial scope of application of the Regulation. The provision plays though a different role depending on the perspective we adopt to analyze it: that of the economic actors, that of the national competent authorities or that of the judicial courts. The provision has been criticized because it provides for an extraterritorial application of the Regulation. The work analyzes the justifications of such broad application and the connecting factors that determine the territorial application of the Regulation. An analysis is also included of the problems that may exist to ensure the effective enforcement of the decisions adopted by national surveillance authorities in cases where the provider or user of the AI systems are established in a third state. Finally, the work examines if, similar to what has happened with the GDPR, the future AI Act will produce the Brussels effect - i.e. if it will imply a de facto and de iure harmonization of the regulatory standards of AI in third states. © 2023 American Psychiatric Association. All rights reserved."
"The use of data-driven methods for metal additive manufacturing (AM) is currently gaining importance as indicated by the increasing number of scientific literature in this field. Incorporation of data-driven methods has the potential to eliminate current bottlenecks in microstructure design given the diverse and complex nature of microstructures in additively manufactured metals. So far, coupling of existing simulation methods, e.g. physics-based process and microstructure models, to simulate AM microstructures with desired morphological characteristics requires extensive computational resources, high computation times and therefore, allows no scalable output. The extension of experimental- and simulation-based approaches by machine learning (ML) algorithms enables fast and computationally efficient predictions. However, the underlying architecture of ML algorithms often does not allow domain experts to interpret how predictions of the model were made and which features are responsible to what extent. This is why ML models are often referred to as black- box models. In this study, we present a data-driven framework based on physics-based simulation data to reveal explainable process-(micro)structure (P-S) linkages for metal AM. We provide an open-source dataset of 960 unique 3D microstructures created by simulation of powder bed fusion in metal AM. We employed the stochastic parallel particle kinetic simulator (SPPARKS) that is based on the kinetic Monte Carlo (kMC) method as an exemplary AM microstructure generator. Selected ML regression algorithms aim to predict 3D chord length distributions (CLDs), as a morphology descriptor, depending on the associated process parameter combinations. Various dimension reduction algorithms are applied for computationally efficient use of the data space. The proposed methodology allows (i) microstructure predictions under given processing conditions and (ii) to navigate experts in the process parameter space to achieve target microstructures. In this context, SHAP (SHapley Additive exPlanations) values are used to decipher the contribution of individual process parameters to the microstructure evolution. In particular, SHAP values calculated in this study unfold the width of the melt pool and the heat-affected zone as dominant features on the model output. We provide open-access to the used dataset and methods for the scientific community to gain experience with the proposed approach. © 2023 Elsevier B.V."
"One of the most common, helpful practices of data scientists, when starting the exploration of a given dataset, is to examine existing data exploration notebooks prepared by other data analysts or scientists. These notebooks contain curated sessions of contextually-related query operations that together demonstrate interesting hypotheses and conjectures on the data. Unfortunately,relevant such notebooks, that had been prepared on the same dataset, and in light of thesame analysis task, are often nonexistent or unavailable. In this work, we describe ATENA-PRO, a framework for auto-generating such relevant, personalized exploratory sessions. Using a novel specification language, users first describe their desired output notebook. Our language contains dedicated constructs for contextually connecting future output queries. These specifications are then used as input for a Deep Reinforcement Learning (DRL) engine, which auto-generates the personalized notebook. Our DRL engine relies on an existing, general-purpose, DRL framework for data exploration. However, augmenting the generic framework with user specifications requires overcoming a difficult sparsity challenge, as only a small portion of the possible sessions may be compliant with the specifications. Inspired by solutions for constrained reinforcement learning, we devise a compound, flexible reward scheme as well as specification-aware neural network architecture. Our experimental evaluation shows that the combination of these components allows ATENA-PRO to consistently generate interesting, personalized exploration sessions for various analysis tasks and datasets.  © 2023 Owner/Author."
[No abstract available]
"Modern cloud has turned data services into easily accessible commodities. With just a few clicks, users are now able to access a catalog of data processing systems for a wide range of tasks. How- ever, the cloud brings in both complexity and opportunity. While cloud users can quickly start an application by using various data services, it can be difficult to configure and optimize these services to gain the most value from them. For cloud providers, managing every aspect of an ever-increasing set of data services, while meeting customer SLAs and minimizing operational cost is becoming more challenging. Cloud technology enables the collection of significant amounts of workload traces and system telemetry. With the progress in data science (DS) and machine learning (ML), it is feasible and desirable to utilize a data-driven, ML-based approach to automate various aspects of data services, resulting in the creation of autonomous data services. This paper presents our perspectives and insights on creating autonomous data services on Azure. It also covers the future endeavors we plan to undertake and unresolved issues that still need attention.  © 2023 ACM."
"In the last few years, the field of quantum computing has experienced remarkable progress. The prototypes of quantum computers already exist and have been made available to users through cloud services (e.g., IBM Q experience, Google quantum AI, or Xanadu quantum cloud). While fault-tolerant and large-scale quantum computers are not available yet (and may not be for a long time, if ever), the potential of this new technology is undeniable. Quantum algorithms have the proven ability to either outperform classical approaches for several tasks, or are impossible to be efficiently simulated by classical means under reasonable complexity-theoretic assumptions. Even imperfect current-day technology is speculated to exhibit computational advantages over classical systems. Recent research is using quantum computers to solve machine learning tasks. Meanwhile, the database community has already successfully applied various machine learning algorithms for data management tasks, so combining the fields seems to be a promising endeavour. However, quantum machine learning is a new research field for most database researchers. In this tutorial, we provide a fundamental introduction to quantum computing and quantum machine learning and show the potential benefits and applications for database research. In addition, we demonstrate how to apply quantum machine learning to the join order optimization problem in databases.  © 2023 ACM."
"Aim: The regulation of inflammation and the host immune response relies significantly on apoptosis. It aids in tissue homeostasis, and a disruption of this is frequently linked to disease. The use of histochemical stains like hematoxylin and eosin (H&E) and Feulgen reaction for DNA can provide a simple and cost-effective method for the detection of apoptotic cells. The aim of this study was to analyze the expression of apoptosis in the gingival epithelium of gingivitis subjects and in patients with chronic periodontitis, using H&E and Feulgen reaction for DNA, aided by immunohistochemistry (IHC). Materials and methods: A total of 20 gingival biopsies were harvested from gingivitis subjects (n = 10) and 10 subjects who suffered from chronic periodontitis (n = 10). On the day the samples were collected, a University of North Carolina (UNC) 15 periodontal probe was used to record the periodontal data which includes the bleeding index, plaque index, gingival index, probing depth, and attachment loss. Apoptotic cells were analyzed using Feulgen reaction for DNA and H&E under light microscopy. This was followed by immunohistochemical analysis of p53 and Bcl-2 biomarkers. Results: Apoptotic cell count was higher in the chronic periodontitis group with a mean apoptotic index (AI) of 15.00 compared to gingivitis the group where the mean AI was 7.48. The mean difference was found to be −7.52, however the difference was not statistically significant (p = 0.18). Conclusion: p53 plays a pivotal role in periodontal ligament cell homeostasis and seems to be upregulated in oral inflammatory diseases. Bcl-2 being an antiapoptotic marker was associated more with gingivitis as compared to chronic periodontitis. © The Author(s)."
"Let ς be an alphabet and μ be a distribution on ςk for some k ≥ 2. Let α > 0 be the minimum probability of a tuple in the support of μ (denoted supp(μ)). Here, the support of μ is the set of all tuples in ςk that have a positive probability mass under μ. We treat the parameters ς, k, μ, α as fixed and constant. We say that the distribution μ has a linear embedding if there exist an Abelian group G (with the identity element 0G) and mappings σi : ς → G, 1 ≤ i ≤ k, such that at least one of the mappings is non-constant and for every (a1, a2, ak) supp(μ), 'i=1k σi(ai) = 0G. Let fi: ςn→ [-1,1] be bounded functions, such that at least one of the functions fi essentially has degree at least d, meaning that the Fourier mass of fi on terms of degree less than d is negligible, say at most-. In particular, |E[fi]| ≤-. The Fourier representation is w.r.t. the marginal of μ on the ith co-ordinate, denoted (ς, μi). If μ has no linear embedding (over any Abelian group), then is it necessarily the case that |E(x1,.,x2,.,.,.,xk)1/4.,μ-.,n[f1(x1)f2(x2)¯.,fk(xk)].,.,=.,od,.,-(1),where the right hand side → 0 as the degree d → ∞ and δ→ 0? In this paper, we answer this analytical question fully and in the affirmative for k=3. We also show the following two applications of the result. The first application is related to hardness of approximation. We show that for every 3-ary predicate P:ς3 → {0,1} such that P has no linear embedding, an SDP integrality gap instance of a P-CSP instance with gap (1,s) can be translated into a dictatorship test with completeness 1 and soundness s+o(1), under certain additional conditions on the instance. The second application is related to additive combinatorics. We show that if the distribution μ on ς3 has no linear embedding, marginals of μ are uniform on ς, and (a,a,a) supp(μ) for every a ς, then every large enough subset of ςn contains a triple (x1, x2,x3) from μ-n (and in fact a significant density of such triples). © 2023 Owner/Author."
"Objective. This study aimed to assess the social attention received by the top 100 highly cited scientific publications focusing on mHealth (mobile health) research during the Covid-19 outbreak. Design/Methodology/Approach. This study employed altmetric tools to assess the social attention received by mHealth research publications. The study collected bibliographical data from the Scopus database of the top 100 highly cited articles published between 2019 and 2022. Altmetric data was collected from the Dimensions.ai database and analyzed using MS Excel, Tableau, and SPSS software. Results/Discussion. The study found that mHealth research has received significant social attention from various social media, mass media, and reference manager platforms. However, it needs to be promoted in order to reach a wider audience. Twitter was the leading channel for disseminating research highlights on mHealth, and articles have many readers on the Mendeley platform. However, correlation analysis revealed a weak positive correlation between citation and AAS of mHealth research publications. Conclusions. The study contributes to understanding the societal impact of mHealth research during the COVID-19 outbreak and emphasizes the role of Altmetric tools in assessing social attention in scientific publications. The study concludes by suggesting future research directions in the field. © 2023 The author(s)."
"Since the public launch of ChatGPT in November 2022, disciplines across the globe have grappled with questions about how emerging artificial intelligence will impact their fields. In this article I explore a set of foundational concepts in artificial intelligence (AI), then apply them to the field of evaluation broadly, and the American Evaluation Association's evaluator competencies more specifically. Given recent developments in narrow AI, I then explore two potential frameworks for considering which evaluation competencies are most likely to be impacted—and potentially replaced—by emerging AI tools. Building on Moravec's Landscape of Human Competencies and Lee's Risk of Replacement Matrix I create an exploratory Landscape of Evaluator Competencies and an Evaluation-Specific Risk of Replacement Matrix to help conceptualize which evaluator competencies may be more likely to contribute to long-term sustainability for the field. Overall, I argue that the interpersonal, and contextually-responsive aspects of evaluation work—in contrast to the more technical, program management, or methodological aspects of the field—may be the competencies least likely to be impacted or replaced by AI. As such, these may be the competencies we continue to emphasize, both in the day-to-day aspects of our operations, and in the training of new and emerging evaluators. This article is intended to be a starting point for discussions that continue throughout the remainder of this issue. © 2023 American Evaluation Association and Wiley Periodicals LLC."
"With the advantages of real-time analysis and visual evaluation results, intelligent technology-enabled teaching behavior evaluation has gradually become a powerful means to help teachers adjust teaching behaviors and improve teaching quality. However, at present, the evaluation of intelligent teachers’ behaviors is still in the preliminary exploration stage, and the application research is not deep enough. This paper analyzes the application of intelligent technology in the evaluation of teachers’ classroom teaching behaviors from the perspectives of evaluation data, methods, and results. Voice print recognition technology is used to recognize the teachers’ identities and track the speech in the classroom videos, and the videos are segmented. Then, the evaluation framework of teachers’ classroom teaching behaviors is constructed using three dimensions of emotion, posture, and position preference. Finally, evaluation results are presented to teachers in a more intuitive and easy-to-understand visual way, to help teachers reflect on teaching. This paper aims to promote the transformation of teachers’ classroom teaching behavior evaluation toward an intelligent, efficient, and sustainable direction through current research. © 2023, Higher Education Press Limited Company. All rights reserved."
"Much research in robotic artificial intelligence (AI) and Artificial Life has focused on autonomous agents as an embodied and situated approach to AI. Such systems are commonly viewed as overcoming many of the philosophical problems associated with traditional computationalist AI and cognitive science, such as the grounding problem (Harnad) or the lack of intentionality (Searle), because they have the physical and sensorimotor grounding that traditional AI was argued to lack. Robot lawn mowers and self-driving cars, for example, more or less reliably avoid obstacles, approach charging stations, and so on—and therefore might be considered to have some form of artificial intentionality or intentional directedness. It should be noted, though, that the fact that robots share physical environments with people does not necessarily mean that they are situated in the same perceptual and social world as humans. For people encountering socially interactive systems, such as social robots or automated vehicles, this poses the nontrivial challenge to interpret them as intentional agents to understand and anticipate their behavior but also to keep in mind that the intentionality of artificial bodies is fundamentally different from their natural counterparts. This requires, on one hand, a “suspension of disbelief ” but, on the other hand, also a capacity for the “suspension of belief.” This dual nature of (attributed) artificial intentionality has been addressed only rather superficially in embodied AI and social robotics research. It is therefore argued that Bourgine and Varela’s notion of Artificial Life as the practice of autonomous systems needs to be complemented with a practice of socially interactive autonomous systems, guided by a better understanding of the differences between artificial and biological bodies and their implications in the context of social interactions between people and technology. © 2023 Massachusetts Institute of Technology."
"The advent of generative AI such as ChatGPT has propelled the field of evaluation into conversations about the use of AI in the field and the ethics of knowledge generation. While there are many benefits of AI, as with any new technology there can be collateral damage. The discourse about AI and evaluation provides another opportunity to center equity in our work as evaluators by asking, how can evaluation contribute to the public good in an AI world? This article highlights contextual concerns with AI from an ecosystem perspective, placing emphasis on structural and racial/ethnic inequities, bias, and prejudice. The author issues a clarion call for the field of evaluation to act collectively to incite change by being proactive, embracing our professional responsibility and critical voice, and employing evidence-based practice. Evaluators are encouraged to exercise our social and political responsibility through courageous leadership and advocacy to attend to the values of stakeholders and advance an equitable AI world. © 2023 The Authors. New Directions for Evaluation published by American Evaluation Association and Wiley Periodicals LLC."
"In the past few years, there has been much work on incorporating fairness requirements into the design of algorithmic rankers, with contributions from the data management, algorithms, information retrieval, and recommender systems communities. In this tutorial, we give a systematic overview of this work, offering a broad perspective that connects formalizations and algorithmic approaches across subfields. During the first part of the tutorial, we present a classification framework for fairness-enhancing interventions, along which we will then relate the technical methods. This framework allows us to unify the presentation of mitigation objectives and of algorithmic techniques to help meet those objectives or identify trade-offs. Next, we discuss fairness in score-based ranking and in supervised learning-to-rank. We conclude with recommendations for practitioners, to help them select a fair ranking method based on the requirements of their specific application domain.  © 2023 Owner/Author."
"Recent advances in AI techniques, as well as enabling hardware and infrastructure, has led to the integration of AI in wide-ranging domains and tasks. In particular, AI has been used to handle various types of data (including numerical, textual and image data) and has been adopted in large-scale distributed systems. From a data management perspective, this calls for the harnessing of state-of-the-art AI solutions for data management tasks and systems. aiDM is a full-day workshop that offers a stage for innovative interdisciplinary research that studies the interaction between AI and data management and develops new AI technologies for data-related tasks. This year, aiDM'23 particularly focuses on the transparent exploitation of AI techniques in existing enterprise-level data management workloads.  © 2023 Owner/Author."
"We report about the first ever symposium on the assessment of AI trustworthiness, leading to the birth of a new research community on the matter. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence."
"Goldschmidt's evocation of Leviticus 19:18 in Contradiction Set Free accomplishes heavy lifting within the distinction of the dialogic from the dialectic. Analogized to a necessary recognition of each particular and unique fulfillment of the immediate command to ""love your neighbor as yourself,""dialogue is temporalized within an already near, yet not ever complete, messianic infinite. As an ongoing, active and unfinished composition of unique ""nows,""dialogue's structure is likewise epistemically distinct from the structure of dialectical synthesis. How might this distinction's lens of Leviticus 19:18 illumine opportunities-and obstacles-for ""dialogue""between humans and artificial systems? Does examining the specific case of OpenAI's ChatGPT under the lens of Goldschmidt's text suggest that certain questions about generative AI learning capacity might actually be questions of time?.  © Philosophy Today."
"In the era of artificial intelligence, teachers’ subjectivity presents new characteristics, such as the interweaving of virtual and reality, the integration of individual and groups, and the coexistence of exuberance and disorientation. While reshaping teachers’ subjectivity, intelligent technologies also easily make teachers fall into the subjectivity dilemmas, such as self-crisis, distortion of interaction, and alienation of action. Therefore, to reinstate teachers’ subjectivity, it is necessary to know oneself in pursuit of truth by sticking to the nature of life, to become oneself by constructing the nature of choice in practical activities, and to achieve oneself by returning to the nature of creation in lifelong learning. © 2023, Higher Education Press Limited Company. All rights reserved."
"The agricultural sky is a dominant natural entity that has influenced, interacted with, and guided the evolution of crops, farming practices, and cropping systems. The skyand all its componentsabove and near agricultural areas is an important aspect of an agricultural enterpriseas important as soils, water, and crop species. The blue sky above crops that is seemingly clear, tranquil or sometimes filled with clouds, is really a repository of a large number of gases, mineral or organic particulate matter, dust, mist, turbulent wind, innumerable species of micro-organisms, tiny biotic flora/fauna, seeds, insects, etc. The agrarian sky supports complex interactions of biotic and abiotic aspects with perhaps immediate and/or delayed influence on crops sown on the ground. This volume helps us to better understand the importance of the sky above crop fields, with the goal to encourage revolutionary agronomic procedures that lead to higher yield. It is a comprehensive treatise on the agriculture sky, covering basic definitions, limits, and explanations about atmospheric layers like troposphere, stratosphere, and the phyllosphere. The volume addresses the nutrient dynamics in the sky and their relevance to crop productivity. It looks at both natural biotic and manmade abiotic factors in the sky and how they affect what goes on below, such as from dust storms, at cloudy and/or windy locations, and from high-altitude jet streams. The author discusses wind and solar power generation in the agrarian sky and explores aeroponics to revolutionize crop production. The volume delves into several types of aerial robots, employing AI and other technology, to provide aerial spectral data that are capable of analyzing procedures, soil conditions, irrigation, insect pests, weed detection, herbicide application, soil fertility, and much more. The book includes examples from the North American Great Plains, Pampas of Argentina, Sahelian production zones of West Africa, Indo-Gangetic Plains, etc. This eye-opening book, The Agricultural Sky: A Concept to Revolutionize Farming, will be useful to students and professors in universities as well as to researchers in industry dealing with aerial aspects of farming. © 2023 by Apple Academic Press, Inc. All rights reserved."
"Wastewater treatment is essential because it reduces the pollutant in the water, promotes the water quantity, and protects the ecosystem from harmful and toxic elements in wastewater. Many uncertainties appear in wastewater treatment systems since the natural condition is complex, and the technology of wastewater treatment is limited. Artificial Intelligence (AI) is a novel and influential technology assisting with complicated work, including modeling. The advantages of AI are evident in wastewater treatment because of the high accuracy, which leads to cost, energy, and material saving. This article mainly focuses on introducing Artificial Intelligence in wastewater treatment, displaying the application of Artificial Intelligence Neural Networks in wastewater treatment, and analyzing the advantages and problems. Overall, the research demonstrates that applying Artificial Intelligence in wastewater treatment provides a promising future with benefits, such as cost-saving and high accuracy. © 2023 EDP Sciences. All rights reserved."
"Modern power management systems are highly recommended for institutes to enhance power saving, as they effectively stratify their activities. These systems are essential to integrate intelligent methods, such as machine learning and deep learning, to make optimal decisions in managing consumed power and significantly minimize energy usage. In this review, we delve into the concept of smart energy management, focusing on three key areas: Wireless Sensor Networks (WSN), Building Information Modeling (BIM), and Artificial Intelligence (AI) techniques represented by deep learning (DL) and machine learning (ML) approaches. The primary objective of this review is to propose an optimized model for an energy management system based on a clustered WSN that collects the required information. Additionally, we explore how data from buildings' BIM systems can be effectively utilized to create an optimized method for managing power consumption using ML/DL techniques, specifically applicable to smart buildings. Implementing this solution can efficiently manage power consumption in institute buildings, leading to significant energy savings and reduced related costs. © 2023 Lavoisier. All rights reserved."
[No abstract available]
"Objective To establish a comprehensive diagnostic classification model of lateral cephalograms based on artificial intelligence (AI) to provide reference for orthodontic diagnosis. Methods A total of 2 894 lateral cephalograms were collected in Department of Orthodontics, Capital Medical University School of Stomatology from January 2015 to December 2021 to construct a data set, including 1 351 males and 1 543 females with a mean age of (26.4± 7.4) years. Firstly, 2 orthodontists (with 5 and 8 years of orthodontic experience, respectively) performed manual annotation and calculated measurement for primary classification, and then 2 senior orthodontists (with more than 20 years of orthodontic experience) verified the 8 diagnostic classifications including skeletal and dental indices. The data were randomly divided into training, validation, and test sets in the ratio of 7∶2∶1. The open source DenseNet121 was used to construct the model. The performance of the model was evaluated by classification accuracy, precision rate, sensitivity, specificity and area under the curve (AUC). Visualization of model regions of interest through class activation heatmaps. Results The automatic classification model of lateral cephalograms was successfully established. It took 0.012 s on average to make 8 diagnoses on a lateral cephalogram. The accuracy of 5 classifications was 80%‑90%, including sagittal and vertical skeletal facial pattern, mandibular growth, inclination of upper incisors, and protrusion of lower incisors. The acuracy rate of 3 classifications was 70%‑80%, including maxillary growth, inclination of lower incisors and protrusion of upper incisors. The average AUC of each classification was ≥0.90. The class activation heat map of successfully classified lateral cephalograms showed that the AI model activation regions were distributed in the relevant structural regions. Conclusions In this study, an automatic classification model for lateral cephalograms was established based on the DenseNet121 to achieve rapid classification of eight commonly used clinical diagnostic items. © 2023 Authors. All rights reserved."
"Exterior wall inspections are critical to ensuring public safety around aging buildings in urban cities. Conventional manual approaches are dangerous, time-consuming and labor-intensive. AI-enabled drone platforms have recently become popular and provide an alternative to serving automated and intelligent inspections. However, current identification only investigates RGB image of visual defects or thermal images of thermal anomalies without considering the continuous monitoring and the conversion between multiple defects. To gain new insights with modality-specific information, this research therefore compares the performance of early, intermediate, and late multimodal RGB-Thermal images fusion techniques for multi-defect detection in facades, especially for detached tiles and missing tiles. Numerous RGB and thermals images from an ageing campus building were collected as a dataset and the classical UNet for image segmentation was modified as a benchmark. The comparative results regarding accuracy (mAP, ROC, and AUC) proved that early fusion model performed well in distinguishing detached tiles and missing tiles from complex and congested facades. Nevertheless, intermediate and late fusion models were proven to be more efficient and effective with an optimal architecture, achieving high mean average accuracy with much less parameters. In addition, the results also showed that multi-modal fusion techniques can significantly improve the performance of multi-defects detection without adding a large number of parameters to single-modal AI models. © 2023 The Author(s)"
"In a work by Raz (J. ACM and FOCS 16), it was proved that any algorithm for parity learning on n bits requires either ω(n2) bits of classical memory or an exponential number (inn) of random samples. A line of recent works continued that research direction and showed that for a large collection of classical learning tasks, either super-linear classical memory size or super-polynomially many samples are needed. All these works consider learning algorithms as classical branching programs, which perform classical computation within bounded memory. However, these results do not capture all physical computational models, remarkably, quantum computers and the use of quantum memory. It leaves the possibility that a small piece of quantum memory could significantly reduce the need for classical memory or samples and thus completely change the nature of the classical learning task. Despite the recent research on the necessity of quantum memory for intrinsic quantum learning problems like shadow tomography and purity testing, the role of quantum memory in classical learning tasks remains obscure. In this work, we study classical learning tasks in the presence of quantum memory. We prove that any quantum algorithm with both, classical memory and quantum memory, for parity learning on n bits, requires either ω(n2) bits of classical memory or ω(n) bits of quantum memory or an exponential number of samples. In other words, the memory-sample lower bound for parity learning remains qualitatively the same, even if the learning algorithm can use, in addition to the classical memory, a quantum memory of size c n (for some constant c>0). Our result is more general and applies to many other classical learning tasks. Following previous works, we represent by the matrix M: A × X → {-1,1} the following learning task. An unknown x is sampled uniformly at random from a concept class X, and a learning algorithm tries to uncover x by seeing streaming of random samples (ai, bi = M(ai, x)) where for every i, ai A is chosen uniformly at random. Assume that k,,r are integers such that any submatrix of M of at least 2-k·|A| rows and at least 2-·|X| columns, has a bias of at most 2-r. We prove that any algorithm with classical and quantum hybrid memory for the learning problem corresponding to M needs either (1) ω(k · ) bits of classical memory, or (2) ω(r) qubits of quantum memory, or (3) 2ω(r) random samples, to achieve a success probability at least 2-O(r). Our results refute the possibility that a small amount of quantum memory significantly reduces the size of classical memory needed for efficient learning on these problems. Our results also imply improved security of several existing cryptographical protocols in the bounded-storage model (protocols that are based on parity learning on n bits), proving that security holds even in the presence of a quantum adversary with at most c n2 bits of classical memory and c n bits of quantum memory (for some constant c>0). © 2023 ACM."
"With the development of artificial intelligence (AI) technology, it has a wide range of explorations in orthodontics. AI has greater application prospects in precise measurement, multidimensional diagnosis, treatment planning and efficacy prediction. At the same time, there are certain limitations in the application of AI, such as risks caused by individual variability, black box properties and unclear delineation of medical responsibilities. This paper summarized the history and current status of AI applications in orthodontics and discussed future development trends, to provide reference for clinical orthodontics. © 2023 Authors. All rights reserved."
"Complexity theory provides a framework for understanding intricate systems and their interactions. Its use in media and communication is relatively recent and has been used for examining, for instance, the spread of online communication, the formation of public opinions and the development of misinformation. Building on the concept of VUCA, this article outlines the emerging field of ‘complexity communication’. In a light-hearted experiment, we turned to the lately introduced artificial intelligence (AI) agent, ChatGPT, to ask what it knew about complexity thinking in contemporary communication practice. We found that the AI accepted the global environment was characterized by volatility, uncertainty, complexity and ambiguity, and these had transformed the field of communication, both for academics and professionals. It identified a range of complexity-based concepts relevant to communication but not specifically designed for communication situations. It struggled to find any extant practical strategies for complexity communication. This relative lack of bespoke concepts and applied knowledge on managing unpredictable situations indicates an urgent need for the development of complexity thinking in communication. © 2023 Intellect Ltd."
"Internet learning has advanced a great deal in the beyond couple of years. Web based learning can be partitioned into two classes: live classes and recorded classes. This paper mostly centers around live classes. In live web-based classes, ensure whether the understudy is paying attention to the live meeting or not. It is hard for the guide/teacher to ensure that everybody in that live meeting is paying attention to the class as he can't outwardly see each understudy's face on his PC/work area screen. With the assistance of AI, it's feasible to tackle the previously mentioned issue. A nostalgic based look identification model is utilized to discover the mindfulness of the understudies. For this at first numerous edges from the video are taken at a customary span and these edges are given as info. Two principle classifiers are utilized in this task, VGG based Convolutional Neural Network for wistful investigation and Haar Cascade Classifier for Face discovery. This model predicts the state of mind of the understudy like dismal, exhausted, lazy, and others. The expectation made by the model is to be sure used to discover the mindfulness of each understudy. Our model has been tried with numerous experiments and is effective. This framework can be utilized in applications through which online classes are led. © 2023 Author(s)."
[No abstract available]
"Besides many sectors, artificial intelligence (AI) will drive energy sector transformation, offering new approaches to optimize energy systems’ operation and reliability, ensuring techno-economic advantages. However, integrating AI into the energy sector is associated with unforeseen obstacles that might change optimistic approaches to dealing with AI integration. From a multidimensional perspective, these challenges are identified, categorized based on common dependency attributes, and finally, evaluated to align with the viable recommendations. A multidisciplinary approach is employed through the exhaustive literature to assess the main challenges facing the integration of AI into the energy sector. This study also provides insights and recommendations on overcoming these obstacles and highlights the potential benefits of successful integration. The findings suggest the need for a coordinated approach to overcome unforeseen obstacles and can serve as a valuable resource for policymakers, energy practitioners, and researchers looking to unlock the potential of AI in the energy sector. © 2023 by the author."
"This study is aimed to design shape and realization of the Internet of Thing (IoT) system on smart agricultural irrigation system which monitors the water necessity for plants in agricultural land. This system is so beneficial that it can help make it easier for millennial farmers in managing the irrigation system and easing the plants' death risk that is caused by water deficiency and surplus. The development of irrigation in the screen house is still conventional at present, where the water pump is still being operated manually by the workers. The irrigation system that is operated has not yet been conducted based on the real-time data on the field due to the constraint of wide land coverage. The land has not used the smart system that can control and monitor the irrigation automatically, so the irrigation system becomes inefficient in order to overcome the water necessity for plants in agricultural land. This research suggests a solution for the problem by developing a smart irrigation system with the IoT network, which can operate the irrigation process automatically based on the data sensor that is laid on the field. IoT system consists of layers which include the piezometer, temperature sensor, humidity, soil moisture sensor and output it uses the solar power system to operate the irrigation water pump. The system has process knowledge decision-making that is equipped by Artificial Intelligence (AI) which uses the algorithm's rule-based expert system in screen house with NodeMCU. The result shows the monitoring of agricultural land with server cloud by using the internet network. It can be seen that it can do the watering process automatically with accurate and efficient water usage on plants in agricultural land.  © 2023 Author(s)."
"Large language models (LLMs) are a type of generative artificial intelligence (AI) designed to produce text-based content. LLMs use deep learning techniques and massively large data sets to understand, summarize, generate, and predict new text. LLMs caught the public eye in early 2023 when ChatGPT (the first consumer facing LLM) was released. LLM technologies are driven by recent advances in deep-learning AI techniques, where language models are trained on extremely large text data from the internet and then re-used for downstream tasks with limited fine-tuning required. They offer exciting opportunities for evaluators to automate and accelerate time-consuming tasks involving text analytics and text generation. We estimate that over two-thirds of evaluation tasks will be affected by LLMs in the next 5 years. Use-case examples include summarizing text data, extracting key information from text, analyzing and classifying text content, writing text, and translation. Despite the advances, the technologies pose significant challenges and risks. Because LLM technologies are generally trained on text from the internet, they tend to perpetuate biases (racism, sexism, ethnocentrism, and more) and exclusion of non-majority languages. Current tools like ChatGPT have not been specifically developed for monitoring, evaluation, research, and learning (MERL) purposes, possibly limiting their accuracy and usefulness for evaluation. In addition, technical limitations and challenges with bias can lead to real world harm. To overcome these technical challenges and ethical risks, the evaluation community will need to work collaboratively with the data science community to co-develop tools and processes and to ensure the application of quality and ethical standards. © 2023 American Evaluation Association and Wiley Periodicals LLC."
"-The integration of Cloud technology (CT) and Artificial Intelligence (AI) presents significant potential benefits for the real estate industry, particularly in the areas of REITs and crowdfunding. By providing greater transparency, reducing costs, and increasing liquidity, the cloud can make real estate investments more accessible and appealing to individuals. Moreover, AI can assist with decision-making processes and data analysis, leading to better investment strategies and higher returns. To measure individual perspectives on this integration in India, a survey questionnaire involving 236 individuals will be designed using a Likert scale to measure attitudes and opinions on this topic. This will be followed by data collection through a convenient sampling method and analysis using the SPSS tool. The survey aims to identify the level of awareness and adoption of integrating cloud and AI among real estate professionals and investors, intending to enhance REITs and Crowdfunding Synergy (CS). The findings of the study revealed that the use of CT can enhance transparency in real estate investments, making it easier for investors to understand how their money is being used and what returns they can expect. © 2023 Auricle Global Society of Education and Research."
"Algorithmic outputs are increasingly shaping the employee experience, presenting a host of risks and impacts with far-reaching consequences. This contribution considers how algorithmic impact assessments should complement, as well as inform, an overarching ‘top-down’ framework for the governance of algorithmic management systems. While generalised obligations are crucial, identifying risk mitigations on a case-by-case basis can provide significant added value by (i) identifying and evaluating risks and impacts, and facilitating context-specific responses; (ii) striking a balance between generalised requirements and complete self-regulation; and (iii) ensuring that due regard to anticipated impacts and risk mitigation is built in from the design and development stages, through to deployment in the workplace. The criteria for an effective impact assessment obligation in the algorithmic management context are identified, including the appropriate stages, actors, and procedure. The Good Work Charter, which operates as a synthesis of legal principles, rights, and obligations, as well as ethical principles as they apply to the workplace, is proposed as an assessment framework. Finally, the article compares the proposed model with the existing obligation to carry out data protection impact assessments for high-risk data processing. The shortcomings of the latter obligation are explored, and a legislative approach to avoid duplication is proposed. © The Author(s) 2023."
"Objective To study the agreements between transperineal ultrasound (TPUS) and endoanal ultrasound in assessing obstetrics anal sphincter injury (OAS1), and to analyse the diagnostic efficacy of OAS1 in predicting AI relationship between OASI and anal incontinence (AI). Methods A total of 217 women were prospectively recruited from the clinic in the Second Xiangya Hospital of Central South University from January 2021 to May 2022. Symptoms of AI were determined using the St Mark s Incontinence Score (SMIS). TPUS and EAUS were performed by the same operator with the same machine on every participant for detecting OASI: OASI grades 3a, 3b, 3c, and 4 were performed according to the extent of the injuries in the anal sphincter complex. The angle of the defect in the external anal sphincter (EAS) was measured. A ""significant EAS defect"" was diagnosed as a defect affecting at least 2/3 of the length of the EAS with a defect angle of JS30° in each slice. Ultrasound findings were compared between the two methods. The diagnostic efficacy of ""ultrasound OASI"" in predicting AI was analysed by logistic-regression. Results Of 217 women, twenty-eight (12.9%) suffered from AI with SMIS ranging from 5~ 20(11.9 ±4. 5). On TPUS, 79 (36.4%) cases were suspected of OASI, that was 50 OASI 3a, 13 OASI 3b, and 16 OASI 3c/4. On EAUS,78 (35.9%) cases were suspected of OASI that was 23 OASI 3a,22 OASI 3b, 15 OASI 3c, and 18 OASI 4. Twenty-four ""significant EAS defects"" were diagnosed by TPUS and twenty-eight by EAUS,TPUS had excellent agreement with EAUS (weighted Kappa = 0.91, P <0.001). Logistic regression analysis showed that ""ultrasound OASI"" was associated with AI symptoms. ROC curve analysis showed that the area under the curve (AUG) was 0.92,0.87,0.89,0.92 for TPUS OASI 3b +, EAUS OASI 3b + ,TPUS ""Significant EAS defect"", and EAUS ""Significant EAS defect"" for predicting AI, respectively. Conclusions TPUS has good agreement with EAUS in detecting OASI. OASI 3b + and ""significant EAS defect"" on TPUS and EAUS had good performance in predicting AI symptoms. © 2023 Ｃｈｉｎ Ｊ Ｌｅｐｒ Ｓｋｉｎ Ｄｉｓ. All rights reserved."
"In a prior practice and policy article published in Healthcare Science, we introduced the deployed application of an artificial intelligence (AI) model to predict longer-term inpatient readmissions to guide community care interventions for patients with complex conditions in the context of Singapore's Hospital to Home (H2H) program that has been operating since 2017. In this follow on practice and policy article, we further elaborate on Singapore's H2H program and care model, and its supporting AI model for multiple readmission prediction, in the following ways: (1) by providing updates on the AI and supporting information systems, (2) by reporting on customer engagement and related service delivery outcomes including staff-related time savings and patient benefits in terms of bed days saved, (3) by sharing lessons learned with respect to (i) analytics challenges encountered due to the high degree of heterogeneity and resulting variability of the data set associated with the population of program participants, (ii) balancing competing needs for simpler and stable predictive models versus continuing to further enhance models and add yet more predictive variables, and (iii) the complications of continuing to make model changes when the AI part of the system is highly interlinked with supporting clinical information systems, (4) by highlighting how this H2H effort supported broader Covid-19 response efforts across Singapore's public healthcare system, and finally (5) by commenting on how the experiences and related capabilities acquired from running this H2H program and related community care model and supporting AI prediction model are expected to contribute to the next wave of Singapore's public healthcare efforts from 2023 onwards. For the convenience of the reader, some content that introduces the H2H program and the multiple readmissions AI prediction model that previously appeared in the prior Healthcare Science publication is repeated at the beginning of this article. © 2023 The Authors. Health Care Science published by John Wiley & Sons Ltd on behalf of Tsinghua University Press."
[No abstract available]
"Objective: To explore the development and research hotspots on the application of artificial intelligence (AI) in traditional Chinese medicine (TCM) diagnosis and predict research trends in the area. Methods: All articles were retrieved from China National Knowledge Infrastructure (CNKI), Wanfang Data (Wanfang), China Science and Technology Journal Database (VIP), and Web of Science Core Collection (WoSCC). All related papers published in journals from the foundation of the databases to December 31, 2022 were included. NoteExpress, Co-Occurrence (COOC), VOSviewer, and CiteSpace were used to visualize data about publication volumes, journals, authors, research institutions, and keywords as well as to analyze hotspots trending topics in the field. Results: A total of 686 articles were retrieved from the databases, among which 610 papers were published in Chinese and 76 in English. In terms of the journals in which these papers were published, 238 of them were Chinese journals and 52 were English ones. The number of the papers published in journals presented a slow growth. According to the results from Chinese article analysis, WANG Yiqin from Shanghai University of Traditional Chinese Medicine published the most papers in the field. The authors of Chinese papers belonged to six long-term research teams, led by WANG Yiqin and XU Jiatuo (Shanghai University of Traditional Chinese Medicine), WEI Yuke (Guangdong University of Technology), LI Gang (Tianjin University), XI Guangcheng (Institute of Automation of the Chinese Academy of Sciences), and NIU Xin (Beijing University of Chinese Medicine), respectively. In accordance with results from English paper analysis, four authors equally publishing the most papers were YAN Haixia, HU Xiaojuan, and JIANG Tao (Shanghai University of Traditional Chinese Medicine), and WEN Chuanbiao (Chengdu University of Traditional Chinese Medicine). The authors of English papers were from two major research teams in the field of Shanghai University of Traditional Chinese Medicine. Currently, research hotspots on AI such as neural networks, data mining, machine learning, feature recognition, image processing, and expert systems, have been centered on tongue diagnosis, pulse diagnosis, and syndrome research in TCM. Additionally, it was found that research on the topic was gradually evolving from explorations of a single diagnosis method to investigations on the combination of multiple TCM diagnosis methods. Conclusion: Research on AI application in TCM diagnosis is still in a slowly growing stage. As technology develops, AI has been applied to many aspects of TCM diagnosis. Therefore, how to combine the two for improving TCM diagnosis is something worthy of our brainstorming and exploring. © 2023 Digital Chinese Medicine"
"Weeds pose a great threat to modern agricultural production. The unrestrained growth of weeds has also a significant impact on crop yields and quality. Therefore, it is a high demand to effectively manage and control weeds in recent years, in order to optimize agricultural production activities. The specific area of weed management can be expected to serve as the promising research direction for weed control operations, with the rapid development of unmanned aerial vehicle (UAV) platforms and artificial intelligence (AI). Accurate and efficient identification of weeds can be one of the most key steps to realizing automated weed management in the field. However, the current efficient models of weed identification are often required a significant amount of labeled agricultural data. The major bottleneck can be also limited in the model. In this study, a semi-supervised semantic segmentation network was proposed, called UANP-MT (uncertainty aware and network perturbed mean teacher), in order to reduce the dependence on labeled data. The PSPNet network model was also selected to leverage the teacher-student network concept from MT (Mean Teacher). Several key improvements were then incorporated during this time. Firstly, the improved model reduced the errors that caused by random network factors. The teacher network output was also augmented to take the mean, thereby ensuring the robustness of the network prediction. Secondly, the improved model constructed the uncertainty constraints for the different uncertainties between networks and assigns weightings, in order to balance the differences in the output between different networks. This approach enhanced the confidence and reliability of the prediction, ultimately leading to the higher recognition accuracy of an improved model. A series of ablation experiments were conducted to evaluate the effectiveness of the improved model. These experiments included the setting values of network parameters, the selection of feature extraction network backbones, and the model performance testing on datasets with varying volumes of data. Some optimal parameter settings were identified for the improved model, with the ResNet50 as the preferred choice for the model backbone. The comparison experiments showed that the UANP-MT model outperformed the original supervised network, in terms of the three evaluation indicators of F1 score, pixel accuracy (PA), and intersection over union (IoU), when the labeled data was less than that of the original supervised network, indicating the superiority of the semi-supervised network. Furthermore, the effectiveness of the improved model was further validated to compare with the classic semantic segmentation networks, including SegNet, U-Net, and Deeplabv3+ on the CaiXin weed dataset. UANP-MT model achieved better performance with an F1 score of 81.83%, a PA of 95.84%, and an IoU of 90.70% when only 1/4 of the labeled data was used for training. These evaluation metrics were superior to those of the Deeplabv3+ model by percentage point of 4.71, 7.94, and 8.27, respectively. The UANP-MT model was achieved in the high-quality weed detection and recognition with the low labeled data, thus significantly reducing the dependence on labeled data with the manpower and time cost saving. The findings can provide valuable implications for the subsequent weed recognition research and the development of automated weeding operations on UAV platforms. © 2023 Chinese Society of Agricultural Engineering. All rights reserved."
"The present study recorded the complete femoral geometry of healthy as well as dysplastic hip joint in Labrador retriever breed young dogs. In dogs below 6 months of age, all the proximal femoral, acetabular and other measurements except FSC and TW were significantly different between healthy hip joint. HAL, FNALa, AW, HD, FSD, AA, EAA DI and SI were higher in CHD where as FNALb, ND, FOA, HNI, AHI, PC and NA were significantly higher in healthy hip joint. In dogs with healthy hip joint FNALa, FSC, HD, ND, TW, AI and PC were higher in male dogs where as in CHD, FSC, TW were higher in female dogs and FIA, AA were higher in male dogs. No significant difference in hip geometry was seen between right and left limbs in healthy hip as well as CHD. AW and ND were higher in bilateral CHD and FSC was higher in unilateral CHD. In unilateral CHD, FSD and PC were higher in female dogs where as FIA was significantly higher in male dogs. © 2023, Indian Society for Advancement of Canine Practice. All rights reserved."
"Quantifying the spatiotemporal dynamic of potential evapotranspiration (PET) in topographically complex country are often limited by the scarcity and difficulty in obtaining ground-based measured climate data. Remote sensing products have multiple advantage, which allows rapid acquisition of information and status over large areas and long time series. This study compared and explored the spatiotemporal variation of PET across Nepal using three widely-used remote sensing and reanalysis data PET products (CRU_PET, MODIS_PET and PML_PET), and PET estimation using Hargreaves-Samani method based on CHIRTS datasets. On the basis of this, the spatiotemporal dynamic at seasonal and annual level and the periodical change of PET and meteorological aridity index AI (precipitation P/PET) were identified using Mann-Kendall test and Morlet Wavelet method. These PET products showed general spatial consistency over most areas in Nepal with higher PET in the south and less in the north. However, great differences were also found in PET values for four different products. Compared with the three remote sensing and reanalysis data products, CHIRTS_PET performs better in Nepal. The AI based on CHIRTS_PET showed an insignificant increasing (wetting) trend in the western and southern regions, while an insignificant decreasing (drying) trend in the central and northeastern region, which is generally consistent with the spatial pattern of precipitation. In the western and southern regions, the positive impact of increasing precipitation on AI masked the negative effect of increasing PET, and thus making this region getting humid. However, in the central and northeastern region, the increase in PET aggravates the impact of reduced precipitation on the drying trend. Our investigation have broad implications for sustainable water resources management under climate change in a topographically complex country like Nepal. © 2023 National Institute of Natural Hazards, Ministry of Emergency Management of China"
"The article offers a survey of currently notable artificial intelligence methods (released between 2019-2023), with a particular emphasis on the latest advancements in detecting rheumatoid arthritis (RA) at an early stage, providing early treatment, and managing the disease. We discussed challenges in these areas followed by specific artificial intelligence (AI) techniques and summarized advances, relevant strengths, and obstacles. Overall, the application of AI in the fields of RA has the potential to enable healthcare professionals to detect RA at an earlier stage, thereby facilitating timely intervention and better disease management. However, more research is required to confirm the precision and dependability of AI in RA, and several problems such as technological and ethical concerns related to these approaches must be resolved before their widespread adoption. © 2023 Jiaqi Wang, Yu Tian, Tianshu Zhou, Danyang Tong, Jing Ma, Jingsong Li, published by De Gruyter on behalf of the SMP."
"To explore the inhibitory activity and structural characteristics of α - amylase inhibitors(α - AI)under the simulated gastric digestion in vitro, white kidney beans was used as the raw materials to prepare α - AI. Its inhibitory activity, particle size distribution, Zeta potential, secondary structure, sulphydryl and disulfide bond contents of the products were measured during the simulated digestion in vitro. The results indicated that the molecular weight of α - AI from white kidney beans was 34 ku, the inhibitory activity of the product on α - amylase increased significantly after the digestion time was longer than 90 min (P < 0. 05), however, the particle size and absolute value of Zeta potential were reduced significantly (P < 0. 05);after simulated gastric digestion, it was found that the relative content of β - turn was increased, and the relative content of random curls was decreased, the content of total sulfhydryl groups and free sulfhydryl groups were reduced significantly, and the content of disulfide bonds increased significantly in the α - AI from white kidney beans. The results indicated that the interaction and aggregation of white kidney bean α - AI protein molecules continued to intensify, and some sulphydryl groups were oxidized to form disulfide bonds during the simulated gastric digestion in vitro, forming a stable protein conformation, which could better exert its activity of inhibiting α - amylase in the body. © 2023 Editorial Department, Chinese Cereals and Oils Association. All rights reserved."
"In the upcoming 6G era, network communication will no longer be focused solely on indicators, such as bandwidth and latency. The influx of distributed service nodes and gradually perfected deep learning theories are now pushing the network communication environment into a wave of intelligence. In the 6G era, more emphasis will be placed on the immersive experience of humans in the space where perception, transmission, and computing are integrated. Consequently, this promotes a paradigm shift from traditional sensing devices by enabling non-perceptual devices with the capability of data perception. On the other hand, it also requires researchers to realize the mining and utilization of massive data. In addition, digital twin technology has also gradually merged into traditional application fields and expanded to a wider field of enabling intelligent network services. © 2002-2012 IEEE."
[No abstract available]
"This study determined the effect of ovarian status at the beginning of the modified Double-Ovsynch program on reproductive performance in dairy cows. In the study, 1,302 cows were treated with a modified Double-Ovsynch program at 56 days after calving. This program comprises administering gonadotropin- releasing hormones (GnRH), prostaglandin F2α (PGF2α) 10 days later, GnRH 3 days later, GnRH 7 days later, and GnRH 56 h later, followed by timed artificial insemination (TAI) 16 h later. At the beginning of the program, cows were categorized according to the size of the largest follicle and the presence of a corpus luteum (CL) in the ovaries as follows: 1) small follicle (<5 mm, SF group, n = 100), 2) medium follicle (8-20 mm, MF group, n = 538), and 3) large follicle (≥25 mm, LF group, n = 354) without a CL, or 4) the presence of a CL (CL group, n = 310). The pregnancies per AI after the first TAI were analyzed by logistic regression using the LOGISTIC procedure, and the logistic model included the fixed effects of the herd size, parity, body condition score (BCS) at the first TAI, TAI period, and ovarian status. A larger herd size, higher BCS at the first TAI, and TAI period with no heat stress increased (p < 0.05) the probability of pregnancy per AI after the first TAI. However, ovarian status at the beginning of the program did not affect (p > 0.05) the pregnancies per AI (ranges of 37.9% to 42.9%). These results show that the modified Double-Ovsynch program can be used effectively while maintaining good fertility regardless of the ovarian status in dairy herds. © The Korean Society of Veterinary Clinics."
"The increasing prevalence of disinformation has led to a growing interest in leveraging artificial intelligence (AI) for detecting and combating this phenomenon. This article presents a thematic analysis of the potential benefits of automated disinformation detection from the perspective of information sciences. The analysis covers a range of approaches, including fact checking, linguistic analysis, sentiment analysis, and the utilization of human-in-the-loop systems. Furthermore, the article explores how the combination of blockchain and AI technologies can be used to automate the process of disinformation detection. Ultimately, the article aims to consider the integration of AI into journalism and emphasizes the importance of ongoing collaboration between these fields to effectively combat the spread of disinformation. The article also addresses ethical considerations related to the use of AI in journalism, including concerns about privacy, transparency, and accountability. © 2023 by the author."
"The COVID-19 virus has made a huge impact on people’s lives ever since the outbreak happened in December 2019. Unfortunately, the COVID-19 virus has not completely vanished from the world yet, and thus, global agitation is still increasing with mutations and variants of the same. Early diagnosis is the best way to decline the mortality risk associated with it. This urges the necessity of developing new computational approaches that can analyze a large dataset and predict the disease in time. Currently, automated virus diagnosis is a major area of research for accurate and timely predictions. Artificial intelligent (AI)-based techniques such as machine learning (ML) and deep learning (DL) can be deployed for this purpose. In this, compared to traditional machine learning techniques, deep Learning approaches show prominent results. Yet it still requires optimization in terms of complex space problems. To address this issue, the proposed method combines deep learning predictive models such as convolutional neural network (CNN), long short-term memory (LSTM), auto-encoder (AE), cross-validation (CV), and synthetic minority oversampling techniques (SMOTE). This method proposes six different combinations of deep learning forecasting models such as CV-CNN, CV-LSTM+CNN, IMG-CNN, AE+CV-CNN, SMOTE-CV-LSTM, and SMOTE-CV-CNN. The performance of each model is evaluated using various metrics on the standard dataset that is approved by The Montefiore Medical Center/Albert Einstein College of Medicine Institutional Review Board. The experimental results show that the SMOTE-CV-CNN model outperforms the other models by achieving an accuracy of 98.29%. Moreover, the proposed SMOTE-CV-CNN model has been compared to existing mortality risk prediction methods based on both machine learning (ML) and deep learning (DL), and has demonstrated superior accuracy. Based on the experimental analysis, it can be inferred that the proposed SMOTE-CV-CNN model has the ability to effectively predict mortality related to COVID-19. © 2023 by the authors."
"Insect pests pose major threat to the rice production and amongst these, brown plant hopper, Nilaparvata lugens (Stal) and whitebacked planthopper, Sogatella furcifera (Horvath) are the major culprit for huge economic crop losses of rice. The efficacy of newer insecticide flonicamid (Ulala 50 WG) against these insect pests with an aim of least disturbance to ecosystem and compared with the checks pymetrozine (Chess 50 WG) and imidacloprid was evaluated. The insecticides were applied when population exceeded economic threshold level (ETL) of 5 planthoppers per hill. The results revealed that flonicamid @ 75 g ai ha-1 for the management of planthoppers in rice/basmati was statistically at par with the check pymetrozine applied @ 150 g ai ha-1 but significantly superior to imidacloprid @ 25 g ai ha-1. © 2023, Society of Pesticide Science India. All rights reserved."
[No abstract available]
"This research paper presents a novel Cloud Computing User Experience (CCUE) approach to reconstructing the brand image of traditional Shanghai cosmetic brands by leveraging virtual reality (VR) technology and user experience (UX) research. Traditional Shanghai cosmetic brands possess rich cultural heritage and unique product offerings, but often face challenges in maintaining relevance in the modern market. The proposed CCUE uses the VR technology to create immersive and interactive experiences that allow consumers to explore and engage with the brand in a virtual environment. The developed CCUE model integrates the Artificial Intelligence (AI) integrated Imperialist Competitive Algorithm (ICA) for the user-machine interaction. With the CCUE a combination of VR simulations, product showcases, and interactive storytelling, users can experience the essence and history of traditional Shanghai cosmetic brands, fostering a deep connection and emotional attachment. Additionally, UX research techniques are employed to gather user feedback and insights, enabling the refinement and optimization of the VR experience. The findings of this CCUE contribute to the field of brand reconstruction and provide practical insights for traditional brands seeking to revitalize their image in a rapidly evolving market. © 2023 Auricle Global Society of Education and Research."
"Aroma is one of the most important factors to evaluate the quality of tea. In order to systematically analyze the difference of aroma components in new and aged Dahongpao oolong teas, and thus provided a scientific basis for storing Dahongpao teas, headspace -solid -phase microextraction combined with gas chromatography -mass spectrometry (HS-SPME-GC-MS) and gas chromatography-olfactometry (GC-O) were mainly used in this study. A total of 185 aroma compounds were identified from new Dahongpao and aged Dahongpao oolong teas by HS-SPME-GC-MS, and 45 aroma-active components were identified by using GC-O method. Partial least squares discriminant analysis (PLS-DA) and nonparametric test indicated that there were 31 key aroma compounds with statistical differences between the two groups (P< 0.05). Whereafter, the key aroma compounds with statistical differences and aroma-active components in new and aged Dahongpao were further investigated by combining the peak areas and aroma intensity (AI). Results showed that the peak areas of 2-acetylfuran, 3,5-dimethyl-2-ethylpyrazine, linalool, phenylethyl alcohol, phenethyl acetate, indole and hexyl hexanoate had the same trend as those of aroma intensity. Therefore, these key differential aroma-active components have important contribution for distinguishing the aroma between the new and aged Dahongpao oolong teas, which present a potential applications in the identification of aged Wuyi rock teas. The results of this study provide some technical references for the scientific evaluation of Dahongpao aroma and the discrimination of ""new and aged"" teas. © 2023 Chinese Institute of Food Science and Technology. All rights reserved."
"Background: Alzheimer's disease (AD) leads to cognitive dysfunction among older people worldwide, making it nearly impossible for them to carry out their daily lives. Due to the inherent characteristics of Alzheimer's disease and its impact on the brain, timely intervention is crucial to delay its onset and mitigate its progression. Currently, the diagnosis of Alzheimer's disease often occurs at a stage where it is too late for effective prevention measures, allowing the disease to cause significant damage to the brain. The use of machine learning and deep learning models is critical for the classification of demented and non-demented cases, but most highly accurate models are non-linear and less transparent, not revealing the logic behind the predictions. Therefore, incorporating interpretability components into the models will make them more transparent and trustworthy. This study is aimed to develop appropriate diagnostic methods capable of assessing Mild Cognitive Impairment (MCI), the early stage of Alzheimer's disease that occurs before the irreversible loss of neurons. Methods: Explainable artificial intelligence (XAI) refers to AI systems that can provide explanations for their decisions or predictions. In the context of AD classification, explainable AI systems aim to provide insights into the features or characteristics of the model used to make a prediction. This XAI provides a mechanism to understand and interpret the basis of a model's predictions which is more important for improving the trust in the system and its results. As such, a non-linear neural network is employed in this work to distinguish between demented and non-demented cases while local post hoc explanations are incorporated to make it a glass-box model using the XAI techniques such as SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). Results: The application of LIME provided valuable insights into the impact of various factors on predictions. Notably, factors such as CDR, Age, and ASF aligned with clinical knowledge and proved instrumental in predicting dementia cases. Conversely, features like nWBV, MMSE, and eTIV adversely affected the predictions, highlighting their significance in identifying non-demented cases. Similarly, exploring SHAP values yielded a comprehensive understanding of the decision-making process employed by the model in detecting Alzheimer's disease. Conclusion: Through the utilization of explainable artificial intelligence (XAI) methods, this study endeavors to develop a dependable and transparent technique for early detection, monitoring, and personalized interventions in the realm of Alzheimer's disease. © 2023, Bahrain Medical Bulletin. All rights reserved."
"Design science research (DSR) is taught in university courses and used by students for their final theses. For successfully learning DSR, it is important to learn to apply it to real-world problems. However, students not only need to learn the new DSR paradigm (meta-level) but also need to develop an understanding of the problem domain (content-level). In this paper, we focus on content-level support (CLS), proposing an illustrative tool to aid students when learning to develop a conceptual design with DSR (e.g., for a prototype). Following the DSR paradigm, we deductively identify students’ issues and use the scaffolding approach to develop design requirements (DRs) and design principles (DPs). To offer AI-generated scaffolding, we use the generative language model (GLM) “GPT-3.” We evaluate our illustrative design through 13 expert interviews. Our results show that providing students with CLS is perceived to be helpful, but the interaction with the student needs to be designed carefully to circumvent unintended usage patterns. We contribute DPs and an illustrative instantiation thereof toward a DSR tool support ecosystem. More broadly, we contribute to the understanding of how humans can be supported by AI to solve problems, an important challenge in human-AI collaboration research. © 2023 by the Information Systems & Computing Academic Professionals, Inc. (ISCAP)."
"This paper seeks to analyse the role of technology and memory in Kawakami Hiromi’s novel Ōkinatorinisarawarenaiyō. Set thousands of years in the future, in a world that has been subjected to several catastrophes, the novel highlights the interconnections between humanity and technology in order to defamiliarize ideas of human centrality and exceptionalism. Kawakami’s characters are forced to face the cyclical possibility of human extinction, which turns into a recurring threat rather than an abstract and discrete event. In this narrative time chronically affected by crises, the definition of what it means to be human is constantly challenged and renegotiated, allowing for the development of non-anthropocentric forms of existence. This paper will demonstrate how, in a world in which the human species is chronically experiencing the threat of extinction, technologies – such as AI entities, clones and factories – allow the renegotiation of ideas of human centrality and singularity, effectively suggesting possibilities for survival outside of a humanist and anthropocentric framework. © 2023 Edizioni Ca' Foscari. All rights reserved."
"We give a simple proof of the matrix Spencer conjecture up to poly-logarithmic rank: given symmetric d × d matrices A1,.,An each with ||Ai||op ≤ 1 and rank at most n/log3 n, one can efficiently find ± 1 signs x1,.,xn such that their signed sum has spectral norm ||'i=1n xi Ai||op = O(n). This result also implies a logn-ω( loglogn) qubit lower bound for quantum random access codes encoding n classical bits with advantage ≫ 1/n. Our proof uses the recent refinement of the non-commutative Khintchine inequality in [Bandeira, Boedihardjo, van Handel, 2021] for random matrices with correlated Gaussian entries. © 2023 ACM."
"BACKGROUND In prostate cancer (PCa) diagnosis, many developed machine learning (ML) models using ultrasound images show good accuracy. This study aimed to analyze the accuracy of neural network ML models in PCa diagnosis using ultrasound images. METHODS The protocol was registered with PROSPERO registration number CRD42021277309. Three reviewers independently conducted a literature search in 5 online databases (PubMed, EBSCO, Proquest, ScienceDirect, and Scopus). We included all cohort, case-control, and cross-sectional studies in English, that used neural networks ML models for PCa diagnosis in humans. Conference/review articles and studies with combination examination with magnetic resonance imaging or had no diagnostic parameters were excluded. RESULTS Of 391 titles and abstracts screened, 9 articles relevant to the study were included. Risk of bias analysis was conducted using the QUADAS-2 tool. Of the 9 articles, 5 used artificial neural networks, 1 used deep learning, 1 used recurrent neural networks, and 2 used convolutional neural networks. The included articles showed a varied area under the curve (AUC) of 0.76–0.98. Factors affecting the accuracy of artificial intelligence (AI) were the AI model, mode and type of transrectal sonography, Gleason grading, and prostate-specific antigen level. CONCLUSIONS The accuracy of neural network ML models in PCa diagnosis using ultrasound images was relatively high, with an AUC value above 0.7. Thus, this modality is promising for PCa diagnosis that can provide instant information for further workup and help doctors decide whether to perform a prostate biopsy. © 2023 Authors."
"This article explores the interaction between artificial intelligence (AI) and validity and identifies areas where AI can help build validity arguments, and where AI might not be ready to contribute to our work in establishing validity. The validity of claims made in an evaluation is critical to the field, since it highlights the strengths and limitations of findings and can contribute to the utilization of the evaluation. Within this article, validity will be discussed within two broad categories: quantitative validity and qualitative trustworthiness. Within these categories, there are multiple types of validity, including internal validity, measurement validity, establishing trustworthiness, and credibility, to name a few. Each validity type will be discussed within the context of AI, examining if and how AI can be leveraged (or not) to help establish a specific validity type, or where it might not be possible for AI (in its current form) to contribute to the development of a validity argument. Multiple examples will be provided throughout the article to highlight the concepts introduced. © 2023 American Evaluation Association and Wiley Periodicals LLC."
"Recently, the domestic railway industry is developing an 'unmanned automation system for safety inspection of railway facilities' that automatically checks the condition of railway facilities with drones and artificial intelligence (AI) technology; the Ministry of Land, Infrastructure, and Transport plans to utilize UAM (Urban Air Mobility), an eco-friendly, low-noise, three-dimensional transportation method, by introducing it to existing transportation methods between airports and train stations with the goal of commercialization in 2025. The power source of these drone systems is mostly lithium-ion batteries, but due to the chemical properties of lithium, there is a risk of fire (thermal runaway) depending on the use environment and external factors. This paper summarizes in detail the fire prevention and early response technologies for lithium-ion batteries and verifies their feasibility when installed in actual drone systems. In particular, the weight and volume changes of the battery pack were analyzed when the appropriate fire prevention and early response technologies were selected and applied according to the maximum takeoff weight of the drone. © 2023 Korean Society for Railway. All rights reserved."
"The post-COVID period drastically accelerated the development and increased use of artificial intelligence. These changes have not been limited to industry but have begun to be introduced into education. The digital literacy of all involved in education must also incorporate ethical considerations related to the application of AI in education. The article presents the ethical codes of AI and analyse the four basic building blocks of an ethical attitude towards AI in education (autonomy, privacy, trust and responsibility). Understanding the concepts of AI and their ethical implications is a condition for acting in accordance with them and for their introduction in education. © 2023 University of Maribor Press. All rights reserved."
"Variations and trade-offs between leaf stoichiometric characteristics and photosynthetic traits are indicative of ecological adaptation strategies of plants and their responses to environment changes. In a common garden of Maoershan, we measured leaf stoichiometric characteristics (carbon content (C), nitrogen content (N), phosphorus content (P), C/ N, C/ P, N/ P) and photosynthetic traits (maximum net photosynthetic rate (Amax), maximum electron transport rate (Jmax), maximum carboxylation rate (Vmax)) of Larix gmelinii from 17 geographical provenances. We examined the provenance differences in stoichiometric characteristics and photosynthetic traits, and analyzed their trade-offs and influencing factors. The results showed leaf stoichiometric characteristics and photosynthetic traits significantly differed among provenances. The climatic factors of seed-source sites explained 54.8% and 67.2% of the variation in stoichiometric characteristics and photosynthetic traits, respectively. Aridity index (AI) of seed-source sites was positively correlated with C, N, P, Amax, Jmax, Vmax, but negatively with C/ N, C/ P, and N/ P. Results of redundancy analysis showed that stoichiometric characteristics accounted for 75.0% of the variation in photosynthetic traits. Amax, Jmax, Vmax were positively correlated with C, N, P, and negatively correlated with C/ N, C/ P, N/ P. The provenance differences in stoichiometric characteristics, photosynthetic traits, and their synergistic relationship suggested the long-term adaptation of trees to the climate of seed-source sites. These findings were of great significance for understanding ecological adaptation strategies of trees in response to climate change. © 2023 Editorial Board of Chinese Journal of Applied Ecology. All rights reserved."
"The honey bee Apis cerana indica is an ecologically and economically important pollinator species worldwide. Honey bees are major pollinators of brinjal (Solnum melongena L.). Carbosulfan is ascertained to be efficient for managing both lepidopteran and sucking pests of brinjal. A laboratory and field experiments were conducted to evaluate the safety of carbosulfan to honey bees in brinjal eco system. Carbosulfan at 1000 g ai ha-1 recorded 100% mortality after 24 h of treatment, followed by carbosulfan at 500 g ai ha-1 (98.0%). The recommended dose of carbosulfan @ 250 g ai ha-1 recorded 58.7 and 88.0% mortality after 12 and 24 h of treatment. In the field, the population of foraging honey bees visiting brinjal field showed negative response after carbosulfan spray at higher doses, indicating that the chemical is lethal to honey bees. However, at the recommended dose of 250 g ai ha-1, the normal 28 visits by honey bees reduced to 11 visits on 1 DAT, 17 visits on 2 DAT and rebounds to 20 visits on 3 DAT suggesting it is to be used in brinjal ecosystem only in case of severe infestation. © 2023, Society of Pesticide Science India. All rights reserved."
"In order to support the production of maize at an Arenosol a study was conducted during the first cropping season 2016-2017 on an Arenosol in the Democratic Republic of Congo to evaluate the effects of Tithonia diversifolia leaves, cow manure, lime and inorganic fertilizer (NPK) on maize yield and economic profitability. The trial was conducted in a randomized complete block design with ten treatments: T0 = Control; T1 = Cow manure; T2 = Guano; T3 = Tithonia diversifolia leaves; T4 = NPK fertilizer 17-17-17; T5 = Tithonia diversifolia leaves + Lime; T6 = Cow manure + Lime; T7 = Guano + Lime; T8=NPK fertilizer 17-17-17 + Lime and T9=Lime. Significant effects of treatments were observed on plant growth, grain yield and profit margins. Treatments T3 and T5 gave the best yields. However, the T3 treatment showed the highest gross profit and acceptability index (AI) and can then be proposed to farmers with more chance of adoption. © 2023 International Formulae Group. All rights reserved."
"Source search is an important problem in our society, relating to finding fire sources, gas sources, or signal sources. Particularly, in an unexplored and potentially dangerous environment, an autonomous source search algorithm that employs robotic searchers is usually applied to address the problem. Such environments could be completely unknown and highly complex. Therefore, novel search algorithms have been designed, combining heuristic methods and intelligent optimization, to tackle search problems in large and complex search spaces. However, these intelligent search algorithms were not designed to address completeness and optimality, and therefore commonly suffer from the problems such as local optimums or endless loops. Recent studies have used crowd-powered systems to address the complex problems that cannot be solved by machines on their own. While leveraging human intelligence in an AI system has been shown to be effective in making the system more reliable, whether using the power of the crowd can improve autonomous source search algorithms remains unanswered. To this end, we propose a crowd-powered source search approach enabling human-AI collaboration, which uses human intelligence as external supports to improve existing search algorithms and meanwhile reduces human efforts using AI predictions. Furthermore, we designed a crowd-powered prototype system and carried out an experiment with both experts and non-experts, to complete 200 source search scenarios (704 crowdsourcing tasks). Quantitative and qualitative analysis showed that the sourcing search algorithm enhanced by crowd could achieve both high effectiveness and efficiency. Our work provides valuable insights in human-AI collaborative system design.  © 2020 Tsinghua University Press."
"Old man's beard is a woody liana that has become an invasive weed in many areas of its introduction, through its vigorous spread and negative impacts on the tree hosts it climbs. Control techniques that improve precision and reduce non-target damage are increasingly preferred for weed control yet have not been compared in published research for use against old man's beard. Field experiments in New Zealand were conducted to: (i) assess targeted herbicide techniques for control of this weed's climbing stems when growing among trees and (ii) assess foliar herbicides for control of creeping stems in ruderal sites. For climbing stems, triclopyr in oil was applied around the circumference of woody stems near their base, which was compared with cutting the stems and applying concentrated glyphosate gel (45% ai) to each cut end. Herbicides were applied in autumn directly to individual stem bases of the weed, thereby protecting tree hosts and other non-target vegetation. The basal application of triclopyr to intact stems was highly effective (>95% mortality) with no damage to nearby trees noted. The glyphosate gel applications to cut stems were less effective (56% mortality by 2 yr after treatment). For creeping stems in grass-dominated ruderal sites, selective foliar herbicide sprays had not been previously juxtaposed to compare control of old man's beard. Three selective sprays that do not damage existing grass cover were applied in autumn at their recommended rates: (i) metsulfuron; (ii) triclopyr; and (iii) a mixture of triclopyr, picloram, and aminopyralid. All herbicide treatments provided effective control, although metsulfuron had a negative effect on grass vigor, which might allow new establishment of old man's beard seedlings by competitive release. These results provide effective options that reduce non-target damage for control of both climbing and creeping old man's beard stems. © The Author(s), 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
[No abstract available]
"Data preparation - the process of discovering, integrating, transforming, cleaning, and annotating data - is one of the oldest, hardest, yet inevitable data management problems. Unfortunately, data preparation is known to be iterative, requires high human cost, and is error-prone. Recent advances in artificial intelligence (AI) have shown very promising results on many data preparation tasks. At a high level, AI for data preparation (AI4DP) should have the following abilities. First, the AI model should capture real-world knowledge so as to solve various tasks. Second, it is important to easily adapt to new datasets/tasks. Third, data preparation is a complicated pipeline with many operations, which results in a large number of candidates to select the optimum, and thus it is crucial to effectively and efficiently explore the large space of possible pipelines. In this tutorial, we will cover three important topics to address the above issues: demystifying foundation models to inject knowledge for data preparation, tuning and adapting pre-trained language models for data preparation, and orchestrating data preparation pipelines for different downstream applications.  © 2023 ACM."
"While many knowledge workers may fear that the rise of artificial intelligence (AI) will threaten their jobs, this article argues that small evaluation businesses should embrace AI tools to increase their value in the marketplace and remain relevant. In this article, consultants from a research, evaluation, and strategy firm, Intention 2 Impact, Inc., make a case for using AI tools to disrupt business as usual in evaluation from theoretical and practical perspectives. Theoretically, AI may be another example of technology that was initially feared but is now ubiquitous in society. Using concrete examples, the authors describe how businesses and evaluators have evolved to keep up with changes in supply and demand. Lastly, it is posited that embracing AI will save time for those working in small businesses, which can ultimately increase added value and profitability. © 2023 American Evaluation Association and Wiley Periodicals LLC."
"In recent years, the developing artificial intelligence (AI) represented by deep learning has created many new methods for clinical research in ophthalmology, improving the efficiency of screening and diagnosing ophthalmic diseases. AI has been well applied in the diagnosis of diabetic retinopathy, cataract, retinopathy of prematurity, and keratitis. In the field of glaucoma, AI can be used to analyze multimodal images based on fundus photography, optical coherence tomography, and perimetry, so as to evaluate the structural and functional changes in the retina, improving the diagnostic level of glaucoma. This article reviews the application of AI in diagnosing glaucoma and discusses its advantages and limitations at the current stage. © 2023 Xinxiang Medical University. All rights reserved."
"Background: The commercial coronary computed tomographic angiography artificial intelligence (CCTA-AI) platform has made great progress in clinical application. However, research is needed to elucidate the current stage of commercial AI platforms and the role of radiologists. This study compared the diagnostic performance of the commercial CCTA-AI platform with that of a reader based on a multicenter and multidevice sample. Methods: A total of 318 patients with suspected coronary artery disease (CAD) who underwent both CCTA and invasive coronary angiography (ICA) were included in a multicenter and multidevice validation cohort between 2017 and 2021. The commercial CCTA-AI platform was used to automatically assess coronary artery stenosis by using ICA findings as the gold standard. The CCTA reader was completed by radiologists. The diagnostic performance of the commercial CCTA-AI platform and CCTA reader was evaluated at the patient and segment levels. The cutoff values of models 1 and 2 were 50% and 70% stenosis, respectively. Results: It took 20.4 seconds to accomplish post-processing per patient when using the CCTA-AI platform, which was significantly shorter than the time taken to complete this task with the CCTA reader (1,112.1 s). In the patient-based analysis, the area under the curve (AUC) was 0.85 using the CCTA-AI platform and 0.61 using the CCTA reader in model 1 (stenosis ratio: 50%). In contrast, the AUC was 0.78 using the CCTA-AI platform and 0.64 using the CCTA reader in model 2 (stenosis ratio: 70%). In the segment-based analysis, the AUCs of CCTA-AI were slightly better than those of the readers. The negative predictive value (NPV) increased from model 1 to model 2. Furthermore, the diagnostic performance was better for larger-diameter arteries. Conclusions: The commercial CCTA-AI platform may provide a feasible solution for the diagnosis of coronary artery stenosis, and it has a diagnostic performance that is slightly better than that of a radiologist with a moderate level of experience (5–10 years of experience). © 2023 AME Publishing Company. All rights reserved."
"Breast cancer is a common cancer affecting women worldwide, and it progresses from breast tissue to other parts of the body through a process called metastasis. Albizia lebbeck is a valuable plant with medicinal properties due to some active biological macromolecules, and it’s cultivated in subtropical and tropical regions of the world. This study reports the phytochemical compositions, the cytotoxic, anti-proliferative and anti-migratory potential of A. lebbeck methanolic (ALM) extract on strongly and weakly metastatic MDA-MB 231 and MCF-7 human breast cancer cells, respectively. Furthermore, we employed and compared an artificial neural network (ANN), an adaptive neuro-fuzzy inference system (ANFIS), and multilinear regression analysis (MLR) to predict cell migration on the treated cancer cells with various concentrations of the extract using our experimental data. Lower concentrations of the ALM extract (10, 5 & 2.5 μg/mL) showed no significant effect. Higher concentrations (25, 50, 100 & 200 μg/mL) revealed a significant effect on the cytotoxicity and proliferation of the cells when compared with the untreated group (p < 0.05; n ≥ 3). Furthermore, the extract revealed a significant decrease in the motility index of the cells with increased extract concentrations (p < 0.05; n ≥ 3). The comparative study of the models observed that both the classical linear MLR and AI-based models could predict metastasis in MDA-MB 231 and MCF-7 cells. Overall, various ALM extract concentrations showed promising an-metastatic potential in both cells, with increased concentration and incubation period. The outcomes of MLR and AI-based models on our data revealed the best performance. They will provide future development in assessing the anti-migratory efficacies of medicinal plants in breast cancer metastasis. © 2023 by the authors."
"In this study we present AI Prediction of Equatorial Plasma Bubbles (APE), a machine learning model that can accurately predict the Ionospheric Bubble Index (IBI) on the Swarm spacecraft. IBI is a correlation (R2) between perturbations in plasma density and the magnetic field, whose source can be Equatorial Plasma Bubbles (EPBs). EPBs have been studied for a number of years, but their day-to-day variability has made predicting them a considerable challenge. We build an ensemble machine learning model to predict IBI. We use data from 2014 to 2022 at a resolution of 1s, and transform it from a time-series into a 6-dimensional space with a corresponding EPB R2 (0–1) acting as the label. APE performs well across all metrics, exhibiting a skill, association and root mean squared error score of 0.96, 0.98 and 0.08 respectively. The model performs best post-sunset, in the American/Atlantic sector, around the equinoxes, and when solar activity is high. This is promising because EPBs are most likely to occur during these periods. Shapley values reveal that F10.7 is the most important feature in driving the predictions, whereas latitude is the least. The analysis also examines the relationship between the features, which reveals new insights into EPB climatology. Finally, the selection of the features means that APE could be expanded to forecasting EPBs following additional investigations into their onset. © 2023. The Authors."
"On March 27–29, 2023, the AAAI symposium on “HRI in Academia and Industry: Bridging the Gap” was held in a hybrid format, with both in-person and remote participants, gathering Human-Robot Interaction (HRI) researchers and practitioners from academia, industry, and national research laboratories to find common ground, understand the different constraints at play, and determine how to work together. The use of robots that operate in spaces in which humans are physically co-present is growing at a dramatic rate. We are seeing more and more robots in our warehouses, on our streets, and even in our homes. All of these robots will interact with humans in some way, whether intentionally or unintentionally. To be successful, their interactions with humans will have to be carefully designed. For more than a decade, the field of HRI has been growing at the intersection of robotics, Artificial Intelligence (AI), human-computer interaction (HCI), psychology, and other fields; however, until quite recently, it has been a largely academic area, with university researchers proposing, implementing, and reporting on experiments at a limited scale. With the current increase of commercially-available robots, HRI is starting to make its way into the robotics industry in a meaningful way. This symposium brought together HRI researchers and practitioners from academia, industry, and national research laboratories to find common ground, understand the different constraints at play, and determine how to effectively work together. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence."
"To satisfy the participation of the public with different disciplines in the design process of high-speed rails, we conduct a research on computer-aided conceptual design of high-speed rails based on human-AI collaboration. This work aims to lower the barrier of designing through human-machine co-creation and collaboration, inspire the creativity and imagination of designers, visualize their design concepts that flash in mind, and simplify the time and efforts while creating detailed sketches. First, based on creative support theory and the different stages in the conceptual design of high-speed rails, we propose three human-AI collaborative ideation strategies to promote divergent thinking and convergent thinking. Next, we create a high-quality dataset of high-speed rails using the deep generative adversarial neural network (GAN) to implement the proposed creative ideation strategies. The system takes the designer's input as a limitation to generate both continuity and novelty designs in line with the aesthetics. Finally, we invite participants to verify the validity of our method and report the results of the qualitative analysis, aiming to provide a reference for future research on the collaborative design of crowd intelligence. © 2023 Editorial Office of Chinese Journal of Mechanical Engineering. All rights reserved."
"The New-fangled strategy called Artificial Intelligence (AI) Assisted Language Learning (AI-ALL) incorporates Google Assistant (GA) to support the learning activities of learners. At present, it is considered Google Assistant Assisted Language Learning (GAALL). For many years, academicians have been exploring ways to use AI for tasks that are related to education. The major objectives of this study are to emphasize 1) The ESL learners' perceptions of using AI-powered Google Assistant for learning the English language; 2). The ESL learners' problems are concerned with utilizing AI-powered Google Assistant to support language learning, especially in English. A survey instrument was employed to the data, including primary research objects, from (n = 141) engineering stream undergraduates. The data was gathered using a questionnaire with 5 points Likert scale. According to the survey, the vast majority of students, especially individuals who were learning English, had a positive opinion of AI-driven GA. The major problem is the lack of quality in GA on smartphones. However, it is envisaged that AI-powered GA in language learning, also known as GAALL, would be deployed as one of the instructional media that might help learners to learn English efficiently as a Second Language. This study suggests that further research studies in this field could be conducted to test the efficacy of AI in ESL contexts. © 2023 Redfame Publishing Inc. All rights reserved."
"Decentralized machine learning (FL) is a system that uses federated learning (FL). Without disclosing locally stored sensitive information, FL enables multiple clients to work together to solve conventional distributed ML problems coordinated by a central server. In order to classify FLs, this research relies heavily on machine learning and deep learning techniques. The next generation of wireless networks is anticipated to incorporate unmanned aerial vehicles (UAVs) like drones into both civilian and military applications. The use of artificial intelligence (AI), and more specifically machine learning (ML) methods, to enhance the intelligence of UAV networks is desirable and necessary for the aforementioned uses. Unfortunately, most existing FL paradigms are still centralized, with a singular entity accountable for network-wide ML model aggregation and fusion. This is inappropriate for UAV networks, which frequently feature unreliable nodes and connections, and provides a possible single point of failure. There are many challenges by using high mobility of UAVs, of loss of packet frequent and difficulties in the UAV between the weak links, which affect the reliability while delivering data. An earlier UAV failure is happened by the unbalanced conception of energy and lifetime of the network is decreased; this will accelerate consequently in the overall network. In this paper, we focused mainly on the technique of security while maintaining UAV network in surveillance context, all information collected from different kinds of sources. The trust policies are based on peer-to-peer information which is confirmed by UAV network. A preshared UAV list or used by asymmetric encryption security in the proposal system. The wrong information can be identified when the UAV the network is hijacked physically by using this proposed technique. To provide secure routing path by using Secure Location with Intrusion Detection System (SLIDS) and conservation of energy-based prediction of link breakage done by location-based energy efficient routing (LEER) for discovering path of degree connectivity. Thus, the proposed novel architecture is named as Decentralized Federate Learning- Secure Location with Intrusion Detection System (DFL-SLIDS), which achieves 98% of routing overhead, 93% of end-to-end delay, 92% of energy efficiency, 86.4% of PDR and 97% of throughput. © 2023 International Journal on Recent and Innovation Trends in Computing and Communication. All rights reserved."
[No abstract available]
[No abstract available]
[No abstract available]
"Artificial intelligence (AI) in healthcare raises significant legal and ethical concerns. Al has been deployed rapidly in healthcare systems despite a lack of legal oversight, leaving policymakers and lawmakers scrambling to catch up. This article develops a four-stage framework for multidisciplinary audiences to understand more clearly the path that has emerged from 'Al to law' using healthcare as a case study. First, Al is introduced into the healthcare system, posing unique legal challenges surrounding algorithmic autonomy, explainability and data biases. Second, legal research interprets current regulations and mainly tort law to determine whether the law can be adapted to these unique challenges, but the law can only be adapted to a point which will then require new legislation. Third, from the absence of legal oversight, policies and guidelines are created as a stopgap measure from governments and bodies such as the World Health Organization (WHO), the Food and Drug Administration (FDA), and the National Health Service (NHS). The policies and guidelines form part of a growing body of research that considers what new laws should be created-research that informs high-level governmental and intergovernmental consultations on developing such new laws. Fourth, following consultations, new laws are devised to address the unique challenges posed by Al, such as the European Union's Artificial Intelligence Act (AIA). While this process is slow, multifaceted, and highly complex, it is argued that it is necessary owing to the unique challenge posed by Al technology. © 2023, William S. Hein & Co., Inc. All rights reserved."
"To address the limitations of the current proactive content caching technology for the 6th generation (6G) mobile network, this article comprehensively analyzes the complex application scenarios of proactive content caching technology for wireless edge networks. It constructs an accurate content popularity prediction model, develops a user-device-oriented proactive content caching mechanism, establishes an interpretable cached content replacement strategy, and designs a reliable interdevice content sharing service model to achieve accurate, effective, trustworthy, and practical results. In this article, we analyze the proactive content caching technology for wireless edge networks. Based on the analysis of the core theory and application scenarios of proactive content caching in wireless edge networks, this article focuses on improving the hit rate of content caching in edge devices, improving the quality-of-experience (QoE) of end-users accessing content, enhancing the robustness of proactive content caching schemes, and conducting in-depth research on the key technologies and methods involved. The proposed proactive content caching technology for wireless edge networks is validated and improved through experimental research. © 2002-2012 IEEE."
"Decision systems for solving real-world combinatorial problems must be able to report infeasibility in such a way that users can understand the reasons behind it, and determine how to modify the problem to restore feasibility. Current methods mainly focus on reporting one or more subsets of the problem constraints that cause infeasibility. Methods that also show users how to restore feasibility tend to be less flexible and/or problem-dependent. We describe a problem-independent approach to feasibility restoration that combines existing techniques from the literature in novel ways to yield meaningful, useful, practical, and flexible user support. We evaluated the resulting framework on three real-world applications and conducted a qualitative expert user study with participants from different application domains. © 2023, The Author(s)."
"Recent advances in artificial intelligence (AI) have significantly intensified research in the geoscience and remote sensing (RS) field. AI algorithms, especially deep learning-based ones, have been developed and applied widely to RS data analysis. The successful application of AI covers almost all aspects of Earth-observation (EO) missions, from low-level vision tasks like superresolution, denoising, and inpainting, to high-level vision tasks like scene classification, object detection, and semantic segmentation. Although AI techniques enable researchers to observe and understand the earth more accurately, the vulnerability and uncertainty of AI models deserve further attention, considering that many geoscience and RS tasks are highly safety critical. This article reviews the current development of AI security in the geoscience and RS field, covering the following five important aspects: adversarial attack, backdoor attack, federated learning (FL), uncertainty, and explainability. Moreover, the potential opportunities and trends are discussed to provide insights for future research. To the best of the authors' knowledge, this article is the first attempt to provide a systematic review of AI security-related research in the geoscience and RS community. Available code and datasets are also listed in the article to move this vibrant field of research forward.  © 2013 IEEE."
"Context: The number of people with dementia is increasing dramatically. With the outbreak of the COVID-19 pandemic, digital screening tests can play a significant role in the remote and timely detection of people with dementia. This study aimed to review digital cognitive tests for dementia screening. Methods: We searched Web of Science, ProQuest, PubMed, Scopus, and Cochrane using related terms such as “dementia,” “mobile,” “digital,” “computer,” and “cognitive assessment,” leading to the emergence of 1,348 articles. Titles, abstracts, and full texts were screened to select the relevant articles based on inclusion/exclusion criteria. Study characteristics and digital test features such as diagnostic performance and deploying platforms were extracted from selected articles. The risk of bias and reporting quality were evaluated in the included studies. Results: Out of 1,348 identified articles, 32 were eligible for inclusion. We categorized digital cognitive tests into 3 groups based on deploying platforms: 1) Mobile-based screening tests (59.5%), 2) desktop-based screening tests (28%), and 3) web-based screening tests (12.5%). Conclusions: Digital cognitive tests, especially mobile-based screening tests, facilitate the timely diagnosis of dementia. The development of AI-based screening tests and the use of technologies such as virtual reality and chatbots will set a bright future in the early detection of dementia. © 2023, Author(s)."
"The field experiments were carried out to determine the bioefficacy of new insecticidal molecules chlorantraniliprole (Coragen 18.5 SC), pyridalyl (Sumipleo 10 EC) and emamectin benzoate (Proclaim 5 SG) against shoot and fruit borer, Earias vittella in okra crop. The experiments were conducted at farmer’s field and PAU Regional Research Station, Gurdaspur during 2017-18 and 2018-19, respectively. The significantly lower infested shoots 0.49, 0.54 and 0.59 per plant were recorded ten days after application of chlorantraniliprole, emamectin benzoate and pyridalyl @ 30, 8.75 and 62.5 g ai ha-1, respectively. Maximum (87.1%) reduction in fruit infestation over control was recorded with application of chlorantraniliprole and it was closely followed by emamectin benzoate and pyridalyl @ 8.75 and 62.5 g ai ha-1 with reduction in fruit infestation of 83.2 and 82.6%, respectively. Significantly, more marketable fruit yield (122.40 q ha-1) was obtained from plots treated with of chlorantraniliprole @ 30 g ai ha-1 and it was at par with emamectin benzoate and pyridalyl @ 8.75 and 62.5 g ai ha-1 with an average marketable fruit yield of 121.7 and 120.0 q ha-1, respectively. Maximum cost benefit ratio (1: 36.5) was achieved by spray of emamectin benzoate @ 8.75 g ai ha-1 and it was followed by chlorantraniliprole and pyridalyl @ 30 and 62.5 g ai ha-1 with mean C:B ratio of 1: 21.7 and 1: 21.6, respectively. All new insecticidal molecules under study had neither any significant adverse effect on population of natural enemies of okra insect-pests nor any phytotoxicity to okra crop. © 2023, Society of Pesticide Science India. All rights reserved."
"This article delves into the academic discourse surrounding the security implications of artificial intelligence (AI). It highlights the lack of consensus on what AI truly encompasses, which poses challenges for regulating the technology. While some express concerns about the security risks of AI, there are proponents of ethical AI who believe in the possibility of programming the technology to operate ethically, and supporters of AI for Good (or AI for Social Good) who see potential for AI to address sustainable development challenges. Scholars agree that AI will have a significant impact on global security, but they disagree on the specific mechanisms and dimensions of this impact. Some focus on changes in the balance of power and the effectiveness of autonomous weapons, while others highlight the tactical advantages, defensive and offensive capabilities, and even national differences in AI development and deployment. Overall, the article concludes that more critical studies are needed to explore how AI is already affecting security in a broad sense. © International Political Science Association 2023."
"This paper describes different machine learning methods for recognizing and distinguishing brick types in masonry debris. Certain types of bricks, such as roof tiles, facing bricks and vertically perforated bricks can be reused and recycled in different ways if it is possible to separate them by optical sorting. The aim of the research was to test different classification methods from machine learning for this task based on high-resolution images. For this purpose, image captures of different bricks were made with an image acquisition system, the data was pre-processed, segmented, significant features selected and different AI methods were applied. A support vector machine (SVM), multilayer perceptron (MLP), and k-nearest neighbour (k-NN) classifier were used to classify the images. As a result, a recognition rate of 98 % and higher was achieved for the classification into the three investigated brick classes. © 2023 International Measurement Confederation (IMEKO). All rights reserved."
"Many of our generation’s most pressing environmental science problems are wicked problems, which means they cannot be cleanly isolated and solved with a single “correct” answer. The NSF AI Institute for Research on Trustworthy AI in Weather, Climate, and Coastal Oceanography (AI2ES) seeks to address such problems by developing synergistic approaches with a team of scientists from three disciplines: environmental science (including atmospheric, ocean, and other physical sciences), artificial intelligence (AI), and social science including risk communication. As part of our work, we developed a novel approach to summer school, held from 27 to 30 June 2022. The goal of this summer school was to teach a new generation of environmental scientists how to cross disciplines and develop approaches that integrate all three disciplinary perspectives and approaches in order to solve environmental science problems. In addition to a lecture series that focused on the synthesis of AI, environmental science, and risk communication, this year’s summer school included a unique “trust-a-thon” component where participants gained hands-on experience applying both risk communication and explainable AI techniques to pretrained machine learning models. We had 677 participants from 63 countries register and attend online. Lecture topics included trust and trustworthiness (day 1), explainability and interpretability (day 2), data and workflows (day 3), and uncertainty quantification (day 4). For the trust-a-thon, we developed challenge problems for three different application domains: 1) severe storms, 2) tropical cyclones, and 3) space weather. Each domain had associated user persona to guide user-centered development. © 2023 American Meteorological Society."
"It is undeniable that Artificial Intelligence is present in our daily life producing innumerable advantages in the whole society, although an indiscriminate use of AI entails risks that can directly affect the fundamental rights of individuals. This work aims to analyze the implications on the fundamental rights of citizens and the ethical principles that should prevail in the use AI, as well as the risks involved, being aware that we are facing a matter on which there is even more debate than positive law. © 2023 Authors. All rights reserved."
"Artificial Intelligence (AI) and AI-powered machine translation bring opportunities and challenges for L2 educators and students. Most recently, the emergence of AI-based chatbots, such as ChatGPT, has led to calls for a revision of traditional teaching methods to prioritize reflective reasoning and critical thinking. This article studies the potentialities of Applied Translation (AT) to promote essential critical thinking skills needed to engage effectively with AI-based tools in the L2 classroom. We present the IMI+ framework (Integration, Multimodality, and Interaction) for integrating AT in language education, which helps support the development of digital literacy and critical thinking in L2 classrooms. Furthermore, given the challenges related to privacy and ethics inherent in these new technologies, we propose applying a Critical Ecological Approach (CEA) to AT to help learners navigate those challenges by identifying power imbalances and societal inequities. Finally, we explain how the seven articles in this special issue showcase the potential applications of AT in Spanish language education. AATSP Copyright © 2023."
"The study was conducted to assess the bioefficacy of new combi-product of b-cyfluthrin 9% + imidacloprid 21% (Solomon 300 OD) @ 150, 175 and 200 mL ha-1 (13.5 + 31.5, 15.75 + 36.75 and 18 + 42 g ai ha-1) along with imidacloprid (Confidor 200 SL) @ 125 mL and 210 mL ha-1 (25 and 42 g ai ha-1) and diafenthiuron (Polo 50 WP) @ 500 g ha-1 (250 g ai ha-1) as standards check against whitefly and leafhopper, respectively, on Bt cotton during 2017 and 2018 crop season, Maximum reduction in whitefly (64.8 and 73.5%) and leafhopper (78.6 and 84.9%) was recorded in Solomon 300 OD @ 175 and 200 mL ha-1, respectively. Solomon 300 OD @ 200 mL ha-1 recorded higher seed cotton yield (23.71 q ha-1) and was at par with the diafenthiuron @ 500 g ha-1 (23.45q ha-1) and Solomon 300 OD @ 175 mL ha-1 (22.78 q ha-1) without causing any phytotoxicity to Bt cotton. © 2023, Society of Pesticide Science India. All rights reserved."
"BACKGROUND On March 2022, the United States Food and Drug Administration(FDA) announced the approval of radiolabeled drug lutetium Lu 177 vipivotide tetraxetan for treatment of adult patients with prostate specific membrane antigen(PSMA)-positive metastatic castration-resistant prostate cancer who have been treated with androgen-receptor pathway inhibition and taxane-based chemotherapy. As PSMA is barely expressed on non-prostatic tissue, it has a very low background accumulation in healthy tissue, consequently, avoiding severe adverse drug reaction of lutetium Lu 177 vipivotide tetraxetan. A multicenter phase Ⅲ VISION study(NCT 03511664) showed that about 30% of patients with evaluable disease at baseline demonstrated an overall response with lutetium Lu 177 vipivotide tetraxetan plus standard care, compared to only 2% in the control arm. The high efficacy and mild adverse drug reaction of lutetium Lu 177 vipivotide tetraxetan cause opportunities for the healthcare systems and represent an important next step towards novel oncotherapy, but also cause great challenges in its clinical use due to lack of practical experience. Currently, data on the large sample and real-world comprehensive safety of lutetium Lu 177 vipivotide tetraxetan are still limited. Therefore, it is necessary to employ data mining algorithms to seek out the potential adverse event signals of lutetium Lu 177 vipivotide tetraxetan by post-marketing monitoring. METHODS FDA Adverse Event Reporting System(FAERS) is a publicly available, voluntary, and spontaneous reporting database. In the present study, the adverse events reported from the second quarter of 2022 to the fourth quarter of 2022 with lutetium Lu 177 vipivotide tetraxetan from FAERS were retrospectively analyzed. Seven types of datasets, including patient demographic and administrative information(DEMO), drug information(DRUG), therapy start dates and end dates for reported drugs(THER), adverse event results(OUTC), adverse event sources(PRSP), coded for the adverse events(REAC), and indications for use/diagnosis(INDI) were used. The reports of lutetium Lu 177 vipivotide tetraxetan were identified using generic name(LUTETIUM LU-177 VIPIVOTIDE TETRAXETAN in prod_ai column) and trade name(PLUVICTO in drug name column) in the DRUG dataset. The adverse event reports with the role_cod as the primary suspected(PS) were chose. Next, the report characteristics, demographic characteristics and onset time of lutetium Lu 177 vipivotide tetraxetan-associated adverse events were analyzed. The adverse events were coded using preferred terms(PT) derived from the standardized Medical Dictionary for Regulatory Activities 25.1(MedDRA), which contained 27 system organ classes(SOCs). Four algorithms, including reporting odds ratio(ROR), proportional reporting ratio(PRR), Bayesian confidence propagation neural network(BCPNN) and multi-item gamma Poisson shrinker(MGPS) were used to detect the signals. All the four data mining algorithms were based on the disproportionality analysis. An adverse event signal was detected only when it conformed to all of the four algorithms criteria simultaneously. RESULTS A total of 634 reports associated with lutetium Lu 177 vipivotide tetraxetan were considered. As a whole, the number of reports had increased gradually month-on-month, and 568(89.6%) reports occurred in the United States. The most common age and body weight groups were 61−80 years(75.4%) and 61−80 kg(50.9%), respectively. Most reports occurred within 30 d after administration of lutetium Lu 177 vipivotide tetraxetan, accounting for 41.5%. Based on 4 algorithms of ROR, PRR, BCPNN and MGPS, six effective signals at the PT level were detected, including anaemia(PT: 10002034), thrombocytopenia(PT: 10043554), laboratory test abnormal(PT: 10023547), platelet count decreased(PT: 10035528), full blood count decreased(PT: 10017413) and dry mouth(PT: 10013781). CONCLUSION When using lutetium Lu 177 vipivotide tetraxetan, it is important to strengthen clinical monitoring within one month and pay attentions to laboratory results including complete blood cell and platelet count. This study might provide powerful support for clinical monitoring of lutetium Lu 177 vipivotide tetraxetan. © 2023 Authors. All rights reserved."
"In this article, we consider the space-time fractional (nonlocal) diffusion equation ətβ u(t,x)=Lα1,a2 D u(t,x), t ≥ 0, x ∈ D, where ətβ is the Caputo fractional derivative of order β ∈ (0,1) and the differential operator Lα1,a2 D is the generator of a Lévy process, sum of two symmetric independent a1-stable and a2-stable processes and D is the open unit interval in R. We consider a nonlocal inverse problem and show that the fractional exponents β and ai, i = 1,2 are determined uniquely by the data u(t,0) =g(t), 0< t < T. The uniqueness result is a theoretical background for determining experimentally the order of many anomalous diffusion phenomena, which are important in many fields, including physics and environmental engineering. We also discuss the numerical approximation of the inverse problem as a nonlinear least-squares problem and explore parameter sensitivity through numerical experiments. © 2023, Element D.O.O.. All rights reserved."
"Terahertz and higher frequency band wireless communication technologies represent the most promising spectrums for 6G networks. Compared to 4G/5G networks, 6G networks operate in a higher frequency band with greater propagation and penetration losses, which may bring serious challenges to network planning and green communications. The digital twin (DT) technology is able to model and simulate wireless networks to improve network performance and deployment efficiency. The artificial intelligence (AI) algorithms provide strong self-evolution and self-optimization capabilities, enabling the generation of an intelligent network. To achieve intelligent, energy-efficient, and cost-effective deployment of 6G networks in smart factories, this article proposes a DT-based system architecture and a mobile-enhanced edge computing-cloud collaborative mechanism for handling diverse and complex data. Moreover, a DT and AI-based method is developed to enable intelligent planning and deployment of 6G networks in factories, which improves network performance while reducing operational costs. © 2002-2012 IEEE."
"In the cloud environment, microservices are implemented through Kubernetes, and these services can be expanded or reduced through the autoscaling function under Kubernetes, depending on the service request or resource usage. However, the increase in the number of nodes or distributed microservices in Kubernetes and the unpredictable autoscaling function make it very difficult for system administrators to conduct operations. Artificial Intelligence for IT Operations (AIOps) supports resource management for cloud services through AI and has attracted attention as a solution to these problems. For example, after the AI model learns the metric or log data collected in the microservice units, failures can be inferred by predicting the resources in future data. However, it is difficult to construct data sets for generating learning models because many microservices used for autoscaling generate different metrics or logs in the same timestamp. In this study, we propose a cloud data refining module and structure that collects metric or log data in a microservice environment implemented by Kubernetes; and arranges it into computing resources corresponding to each service so that AI models can learn and analogize service-specific failures. We obtained Kubernetes-based AIOps learning data through this module, and after learning the built dataset through the AI model, we verified the prediction result through the differences between the obtained and actual data. Copyright © 2023 KSII."
[No abstract available]
"It is widely recognized that AI is beginning to profoundly impact society around the globe. These developments are introducing new opportunities, presenting new risks, and fundamentally reshaping the current and future workforce. As such, we must now answer critically important questions: How can we prepare K-12 students for an AI-permeated future? How do K-12 students conceive of AI and what do they need to know to be effective consumers of AI technologies? What competencies do K-12 students need to acquire to be prepared for workplaces where human-AI teaming is the norm? What do future knowledge workers, including but not limited to those in STEM, need to learn in primary and secondary school to set the stage for their careers, which will no doubt require the ability to effectively interact with AI tools? How can K-12 education best prepare future AI developers, engineers, and researchers? This special issue explores the emerging field of K-12 AI education research. © 2023, International Artificial Intelligence in Education Society."
"The environment is getting contaminated drastically by introducing harmful materials into the atmosphere through the excessive activities of human in adding comfort and luxurious style in their living. The pollutant level in air has high impact of healthiness of the person inhaling it. Air not outside as well indoor is infected by various hazardous particles and gases. To assess the air quality in the particular environment, it stimulates the need to monitor the hazardous elements listed. The internet of things (IoT) and artificial intelligence (AI) became the part of human life by adding smartness in their daily routines from facilitating control over appliances to own health factor as well automate operations. The primary objective of the effort is to identify the gases that cause air pollution, measure the air quality, and assess the level of pollution so that we can determine which gases cause pollution and at what place is the air being impacted. An IoT based indoor air quality monitoring system is built through incorporating carbon monoxide (CO), carbon dioxide (CO2), Ozone (O3), particulate matters (PM) and volatile organic components (VOC) sensors into Arduino board. The design ensures a complete air monitor, extends reliable service at low cost. A rule based system is developed to automate events upon the estimated air quality index (AQI) out of the sensory circuit. © 2023 International Journal on Recent and Innovation Trends in Computing and Communication. All rights reserved."
"An investigation was conducted to assess the bio-efficacy of the newly introduced formulation of an insect-growth regulator, pyriproxyfen 10 EW against whitefly, Bemisia tabaci (Gennadius) in Bt cotton hybrid, RCH 773. Spraying pyriproxyfen 10 EW @ 125 g ai ha-1 led to a significantly higher seed cotton yield (24.93 and 24.88 q ha-1 during the respective years, 2016 and 2017).The maximum reduction (83.9 and 82.1%) in the whitefly adult population was achieved at 10 days after the second spray during the respective years, 2016 and 2017. Further, the nymphal population was reduced by up to 84.8%, after 10 days of second spray in 2017, and the corresponding figure was 81.0% for the year 2016. Hence, pyriproxyfen 10 EW can be a good product for the effective management of cotton whitefly which should be rotated with other group of insecticides to reduce chance of development of resistance. © 2023, Society of Pesticide Science India. All rights reserved."
"Artificial intelligence (AI) is changing the world of work and leading to a dynamic redesign of the division of labor between humans and technology in companies. By automating even cognitively demanding activities, AI systems can, for example, support managers in the performance of their tasks and, in some cases, take over administrative coordination and control tasks from employees. For the redesign of work systems in companies, suitable instruments are needed on the one hand. On the other hand, well-structured change management is important that is oriented toward human-centered work design and takes AI-specific design criteria into account.  © 2023 Walter de Gruyter GmbH, Berlin/Boston, Germany."
"This study aims to highlight some of the implications that automation of certain interactions or relations with citizens, especially the use of algorithms and artificial intelligence, may have on citizens language rights, especially given that some of these new technological opportunities could (or in some legal cases, must) be used to improve opportunities for effectively exercising these rights. In this sense, the study also analyses recent legal developments in both constitutional and ordinary law moving towards limiting the capacity of public authorities to address citizens in minority languages by default as a public incentive policy. It also looks at how such developments might see their effects overcome or at least diluted by the implementation of predictive or artificial intelligence systems that can correctly identify citizens competence in these languages in order to use them to initiate engagement. © 2023 Escola d'Administracio Publica de Catalunya. All rights reserved."
[No abstract available]
"Anticipatory thinking is necessary for managing risk in the safety- and mission-critical domains where AI systems are being deployed. We analyze the intersection of anticipatory thinking, the optimization paradigm, and metaforesight to advance our understanding of AI systems and their adaptive capabilities when encountering low-likelihood/high-impact risks. We describe this intersection as the anticipatory paradigm. We detail these challenges in concrete examples and propose new types of anticipatory thinking, towards a paradigm shift in how AI systems are evaluated. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence."
"In Part I (February 2023) of this two-part series on artificial intelligence (AI), and its subfield machine learning (ML), we presented the variety of chemometric algorithms used to compare AI, ML, and chemometrics. These algorithms included those used for classification, regression, clustering, ensemble learning, signal processing, and component analysis. Now, in Part II, we discuss the applications of AI to electronic and vibrational spectroscopy. We also touch on some applications of deep learning (DL), which is a subfield of machine learning where more complex artificial neural networks (ANNs) with more hidden layers are used. This column article includes a number of selected references that discuss the application of AI in analytical chemistry and in molecular spectroscopy. We give a few early and late examples of AI and ML as applied to different vibrational spectroscopy methods, such as Raman, infrared (FT-IR), near-infrared (NIR), and ultraviolet–visible (UV-vis) spectroscopic techniques. This article is intended only as a sampling of the numerous research manuscripts addressing this subject. © 2023 Advanstar Communications Inc.. All rights reserved."
"The excess pore water pressure (we) in saturated coral sand with different relative densities (Dr) features complicated under complex cyclic loading conditions. In order to investigate its generation characteristics, a series of unDrained cyclic shear tests subjected to 90° jump rotation of cyclic principal stresses with various orientations was conducted on such saturated coral sand. These laboratory cyclic tests by using the GDS hollow cylinder torsional apparatus showed that the patterns of uc generation in those test specimens are related to the paths of cyclic stress, levels of cyclic stress, and Dr. Then, the expression of excess pore water pressures ratio (ru) was established for different Dr values and cyclic loading modes. The unit cyclic stress ratio (USR) was introduced as a stress index for characterizing complex cyclic stress path, and the expressions of model parameters Ai and 6 based on USR and Dr were proposed. The established expression of ru can predict the generation of ru in saturated coral sand with various Dr under complex cyclic loading modes. Experimental data of different sand types in the literature independently verify the applicability of the proposed equation for ru © 2023 Academia Sinica. All rights reserved."
"In recent times, ML algorithms that plays a significant role right from drug discovery to clinical decision making. The recent advances in DL technologies contribute towards improved performance for carrying out computer aided medical image analysis and disease diagnosis. The key benefit of AI in processing of medical big data offers spectacular insights into the hierarchal relationships that exist among data which can be algorithmically explored thus replacing the tedious manual processes to extract and localize specific areas of interests in medical images thus considerably changing the way medicine has been practiced so far. In bio medical related clinical applications, there is a constant demand pertaining the research and development with respect to deploying AI as a mainstream tool to perform several medical imaging activities like analysis, diagnosis, segmentation as well as classification. The increased usage of electronic health records and medical images being its integral component the need for appropriate and efficient AI assisted medical image analysis system that takes care of accurate and automated decision making could be of great help to radiologists and medical practitioners. Molecular image analysis is a dynamic field that makes use of ML and DL algorithms that utilizes labeled and structured information which also proves to be helpful to the patients as they serve as an initial interface before further diagnosis and treatments. Thus our research aims to offer a novel and efficient AI based medical analysis system that can assist clinical practitioners to focus on enhancing the disease diagnosis through DL based medical image analysis and decision making. In addition, we also address specific challenges related to disease diagnosis and propose novel GAN model for improved diagnosis and implementation. Our proposed technique can also be generalized to generate synthetic data for further issues related to molecular image analysis in the field of medicine and help towards building a better disease diagnosis model. © 2023 - IOS Press. All rights reserved."
"The objective of AI-based resume screening is to automate the screening process, and text, keyword, and named entity recognition extraction are critical. This paper discusses segmenting resumes in order to extract data and perform text analysis. The raw CV file has been imported, and the resume data cleaned to remove extra spaces, punctuation and stop words. To extract names from resumes, regular expressions are used. We have also used the spaCy library which is considered the most accurate natural language processing library. It includes already-trained models for entity recognition, parsing, and tagging. The experimental method is used with resume data sourced from Kaggle, and external Source (MTIS). © 2023 Authors. All rights reserved."
[No abstract available]
"QuEChERS technique was validated and measurement uncertainty determined for residue analysis of fipronil and imidacloprid on chilli. LOQ of fipronil and imidacloprid was 0.001 and 0.05 mg kg-1, respectively. Pesticide was applied as foliar spray at the recommended (50 + 50 g ai ha-1) and double the recommended dose (100 + 100 g ai ha-1), and drenching of soil @ 200 + 200 and 400 + 400 g ai ha-1, respectively. Residues of fipronil and imidacloprid were estimated using GC and HPLC, respectively. Following foliar application, initial residues of fipronil and imidacloprid in chilli were 0.498 and 0.993 mg kg-1 and 0.505 and 1.007 mg kg-1, at recommend and double dose, respectively. Drenching resulted in initial residues of 0.097 and 0.136 mg kg-1 for fipronil and 0.280 and 0.443 mg kg-1 for imidacloprid at recommend and double dose, respectively in fruits on 3rd day of treatment. © 2023, Society of Pesticide Science India. All rights reserved."
"Ensuring the well-being of fetuses and their timely diagnosis for potential abnormalities is a critical aspect of healthcare. Early identification of intrauterine growth restriction can facilitate appropriate interventions and improve neonatal outcomes. This study presents a novel approach incorporating the Internet of Things (IoT) and Artificial Intelligence (AI) in the medical domain for the automatic detection of fetal abnormalities. IoT sensors were employed to gather maternal clinical data, including temperature, blood pressure, oxygen saturation levels, and fetal heart rate. A Fast Mask Recurrent Convolutional Neural Network (FMRCNN) was proposed to predict and accurately classify a range of conditions affecting pregnant women and their unborn children. The developed FMRCNN model learns, segments, and classifies fetal abdominal images to identify abnormalities. Additionally, a unified fetal abnormality prediction model was established to process and classify both fetal abdomen and brain ultrasound images. Comparative performance analysis was conducted using Convolutional Neural Networks (CNN), Random Forest (RF), and Support Vector Machine (SVM) algorithms. Evaluation metrics, such as F1-score, accuracy, precision, recall, and sensitivity, were employed to assess the effectiveness of the proposed approach. The results indicate that the presented FMRCNN model holds promise for IoT-based maternal and fetal monitoring in high-risk pregnancies. © 2023 Lavoisier. All rights reserved."
"Objectives: This study aimed to examine the impacts of the wide range of concentrations of glucose and trehalose on the tris-citric acid-egg yolk-fructose (TCEF) extenders for cryopreservation of goat semen. Materials and Methods: The sperm sample was pooled, washed, and diluted in control (TCEF without glucose and trehalose), TCEF + glucose (75, 150, 450, and 900 mm), and TCEF + trehalose (75, 150, 450, and 900 mm). After equilibrations, the semen straws were frozen under LN2 in the LN2 tank. After LN2 storage, the straws were thawed at 37°C for 30 seconds. The sperm parameters of all study groups were checked after equilibration and freezing. Results: After equilibration, the progressive motility (PM), total motility (TM), and viability of sperm in G-75, G-150, G-450, T-75, T-150, and T-450 were not significantly different (p < 0.05) from those in control. After cryopreservation and thawing, the PM, TM, and plasma membrane integrity (PMI) of T-150 were significantly higher (p < 0.05) than in control, G-75, G-900, T-75, and T-900. The viability of sperm in T-150 was substantially higher (p < 0.05) than in the control, whereas there was no significant difference among the control, G-75, G-900, T-75, and T-900. However, the acrosome integrity (AI) of sperm in G-900 was significantly decreased (p < 0.05) compared to the control, G-75, G-150, G-450, T-75, T-150, and T-450. Conclusion: According to the findings, the supplementation of 150 mm trehalose in the TCEF diluent was more efficient for sperm cryopreservation in the buck as reflected by PM, TM, viability, PMI, and AI. © The authors. This is an Open Access article distributed under the terms of the Creative Commons Attribution 4.0 License (http://creativecommons.org/licenses/by/4.0)"
"Artificial neural networks (ANNs) have revolutionized the field of science in the last few decades. Unlike classical machine learning (ML) algorithms, which require human effort to craft well-structured features, an ANN automatically extracts complex patterns as features and passes them into ML to perform various downstream tasks, such as classification and segmentation. Hence, ANNs have made most classical ML algorithms obsolete for many tasks. In addition, deep learning-based models, such as convolutional neural networks, recurrent neural networks, graph neural networks, and generative adversarial neural networks, accelerate artificial intelligence (AI) applications. Therefore, it is essential for novices in ML to understand the basic functionality of ANN to pursue deep learning-related algorithms. Considering this importance, this paper explains the major functionalities of ANN algorithms, such as loss function and backpropagation. © 2023 Institute of Electronics and Information Engineers. All rights reserved."
[No abstract available]
"The journey to the next decade of smart cellular connectivity, sixth-generation (6G) networks, has already begun, even though 6G is still in its nascent stages and far from its deployment. In telecommunications, 6G networks have gained the attention of the industry and academia. 6G is planned to succeed the 5G standard with almost 100 times greater speed. One of the exciting features of 6G is Edge Intelligence (EI), which is the coupling of Edge Computing with Artificial Intelligence (AI). So far, EI has yet to be a component of the existing and predecessor communication standards; thus, 6G will open up many opportunities with its deployment in the future. Nonetheless, integration of 6G with EI, in other words, Edge and AI, is also susceptible to various challenges, particularly security and privacy. Therefore, this article proposes a trusted AI-enabled intelligent architecture for the 6G-envisioned Edge Computing platform. The proposed architecture is based on the Explainable AI concept and is mainly used to ensure the security and privacy of the future 6G networks at the Edge. Following this, the work presents a detailed case study of employing the proposed framework. The preliminary discussion indicates some exciting findings and lays the foundation for future research. In a nutshell, the proposed architecture can be extended to different verticals, including, but not limited to, life-critical systems, like e-healthcare, autonomous vehicles, and traffic monitoring. © 2002-2012 IEEE."
"Objectives: The aim of the study was to quantify and compare craniofacial asymmetry in subjects with and without symptoms of temporomandibular joint disorders (TMDs). Materials and Methods: A total of 126 adult subjects were categorized into two groups (63 with a TMDs and 63 without a TMDs), based on detection of symptoms using the Temporomandibular Joint Disorder-Diagnostic Index (TMD-DI) questionnaire. Posteroanterior cephalograms of each subject were traced manually and 17 linear and angular measurements were analyzed. Craniofacial asymmetry was quantified by calculating the asymmetry index (AI) of bilateral parameters for both groups. Results: Intra- and intergroup comparisons were analyzed using independent t-test and Mann–Whitney U test, respectively, with a P<0.05 considered statistically significant. An AI for each linear and angular bilateral parameter was calculated; higher asymmetry was found in TMD-positive patients compared with TMD-negative patients. An intergroup comparison of AIs found highly significant differences for the parameters of antegonial notch to horizontal plane distance, jugular point to horizontal plane distance, antegonial notch to menton distance, antegonial notch to vertical plane distance, condylion to vertical plane distance, and angle formed by vertical plane, O point and antegonial notch. Significant deviation of the menton distance from the facial midline was also evident. Conclusion: Greater facial asymmetry was seen in the TMD-positive group compared with the TMD-negative group. The mandibular region was characterized by asymmetries of greater magnitude compared with the maxilla. Patients with facial asymmetry often require management of temporomandibular joint (TMJ) pathology to achieve a stable, functional, and esthetic result. Ignoring the TMJ during treatment or failing to provide proper management of the TMJ and performing only orthognathic surgery may result in worsening of TMJ-associated symptoms (jaw dysfunction and pain) and re-occurrence of asymmetry and malocclusion. Assessments of facial asymmetry should take into account TMJ disorders to improve diagnostic accuracy and treatment outcomes. Copyright © 2023 The Korean Association of Oral and Maxillofacial Surgeons."
"Online Social Network (OSN) is frequently used to carry out cyber-criminal actions such as cyberbullying. As a developing country in Asia that keeps abreast of ICT advancement, Malaysia is no exception when it comes to cyberbullying. Author Identification (AI) task plays a vital role in social media forensic investigation (SMF) to unveil the genuine identity of the offender by analysing the text written in OSN by the candidate culprits. Several challenges in AI dealing with OSN text, including limited text length and informal language full of internet jargon and grammatical errors that further impact AI's performance in SMF. The traditional AI system that analyses long text documents seems inadequate to analyse short OSN text's writing style. N-gram features are proven to efficiently represent the authors' writing style for shot text. However, representing N-grams in traditional representation like Tf-IDF resulted in sparse and difficult in grasping the semantic information from text. Besides, most AI works have been done in English but receive less attention in indigenous languages. In West Malaysia, the supreme languages that transcend ethnic boundaries are Iban of Sarawak and KadazanDusun of Sabah, which both are inherently under-resourced. This paper presented a proposed workflow of AI for short OSN text using two Under-Resourced Language (U-RL), Iban and KadazanDusun tweets, to curb the cyberbullying issue in Malaysia. This paper compares Tf-Idf (sparse) and SoA embedding-based (dense) feature representations to observe which representations best represent the stylistic features of the authors’ writing. N-grams of word, character, and POS were extracted as the features. The representation models were learned by different classifiers using machine learning (Naïve Bayes, Random Forest, and SVM). The convolutional neural network (CNN), a SoA deep learning model in sentence classification, was tested against the traditional classifiers. The result was observed by combining different representation models and classifiers on three datasets (English, Iban, and KadazanDusun). The best result was achieved when CNN learned embedding-based models with a combination of all features. KadazanDusun achieved the highest accuracy with 95.76%, English with 95.02%, and Iban with 94%.. © 2022 Penerbit UTM Press. All rights reserved."
"Recent advances in remote sensing hyperspectral imaging and artificial intelligence (AI) bring exciting opportunities to various fields of science and industry that can directly benefit from in-orbit data processing. Taking AI into space may accelerate the response to various events, as massively large raw hyperspectral images (HSIs) can be turned into useful information onboard a satellite; hence, the images' transfer to the ground becomes much faster and offers enormous scalability of AI solutions to areas across the globe. However, there are numerous challenges related to hardware and energy constraints, resource frugality of (deep) machine learning models, availability of ground truth data, and building trust in AI-based solutions. Unbiased, objective, and interpretable selection of an AI application is of paramount importance for emerging missions, as it influences all aspects of satellite design and operation. In this article, we tackle this issue and introduce a quantifiable procedure for objectively assessing potential AI applications considered for onboard deployment. To prove the flexibility of the suggested technique, we utilize the approach to evaluate AI applications for two fundamentally different missions: the Copernicus Hyperspectral Imaging Mission for the Environment (CHIME) [European Union/European Space Agency (ESA)] and the 6U nanosatellite Intuition-1 (KP Labs). We believe that our standardized process may become an important tool for maximizing the outcome of Earth observation (EO) missions through selecting the most relevant onboard AI applications in terms of scientific and industrial outcomes.  © 2013 IEEE."
"Following rapid technological advancements that have taken place throughout the late twentieth and early twenty-first centuries, this intriguing book provides a dynamic agenda for the study of artificial intelligence (AI) within finance. Through an in-depth consideration of the use of AI, it utilizes case study examples to investigate AI's effectiveness within investment and banking. Artificial Intelligence and Financial Behaviour examines to what extent AI can guide people to improve their financial wellbeing. It explores potential effects of, and problems with, specific technologies, as well as describing current regulatory considerations regarding the use of AI and machine learning. Chapters succinctly portray the impact AI may have on investor and trader behaviour. This highly informative book will be beneficial for students and researchers studying behavioural and regulatory economics. It will also be immensely useful for financial regulators who are analysing problems from contemporary points of view. © Riccardo Viale, Shabnam Mousavi, Umberto Filotto and Barbara Alemanni 2023. All rights reserved."
"This article analyses the EU's regulation of medical artificial intelligence (AI) from a product safety perspective, concentrating on the interplay between the proposed AI Act (AIA) and the Medical Device Regulation (MDR). Recent advances in AI development illustrate the future potential of generative AI technologies, including those based on Large LanguageModels (LLMs). In a medical context, AI systems with different degrees of generativity are conceivable. These AI systems can pose new types of risks that are specific to AI technologies, as well as more traditional risks that are typical of medical devices. The proposed AIA is intended to address AI-specific risks foreseen by the EU legislature, whereas the MDR addresses more traditional medical risks. Through two case studies which display different degrees of generativity, this article identifies regulatory lacunae in the intersection between the AIA and the MDR. The article suggests that the emerging regulatory framework for medical AI systems potentially leaves certain AI-specific risks as well as certain typical medical device risks unregulated. Finally, the article discusses possible solutions that are compatible with the intentions of the EU legislature pertaining to the regulation of medical AI systems.  © 2023 Author(s)."
[No abstract available]
"Network agility, automation, and intelligence are at the forefront of the next-generation networks (NGNs) vision, which aims to provide zero-touch service management and self-optimizing networks. In this article, we give an overview of the significance of artificial intelligence (Ali-enabled NGNs, their projected benefits, design requirements, and critical challenges for evolving heterogeneous softwarized networks where microservices can be autonomously orchestrated, scaled, and maintained. The convergence of emerging disruptive technologies, for example, AI, network softwarization, hybrid cloud/edge-native computing architecture, with NGNs accelerates the enhanced service-oriented architecture at the network core/edge level to support on-demand microservices, such as visibility services for intelligent network management. In addition, we present a use case study and conduct experiments based on a novel design of an edge intelligence framework that orchestrates and deploys AI microservices utilizing the testbed resources of a multisite cloud/edge-native NGNs. We use a deep learning-based forecaster model to predict near real-time edge network flow between a centralized service orchestrator hub and multiple edge devices, geographically apart. The obtained results show that the deployed forecaster model accurately predicts the throughput and latency of edge network flow (verified against the groundtruth observations), which is additionally validated through two performance metrics obtained, low root-mean-square error, and high coefficient of determination values. Finally, we outline some of the potential future prospects for AI-enabled NGNs research. © 2002-2012 IEEE."
[No abstract available]
[No abstract available]
"Gastric cancer (GC) is one of the commonest cancers with high morbidity and mortality in the world. How to realize precise diagnosis and therapy of GC owns great clinical requirement. In recent years, artificial intelligence (AI) has been actively explored to apply to early diagnosis and treatment and prognosis of gastric carcinoma. Herein, we review recent advance of AI in early screening, diagnosis, therapy and prognosis of stomach carcinoma. Especially AI combined with breath screening early GC system improved 97.4% of early GC diagnosis ratio, AI model on stomach cancer diagnosis system of saliva biomarkers obtained an overall accuracy of 97.18%, specificity of 97.44%, and sensitivity of 96.88%. We also discuss concept, issues, approaches and challenges of AI applied in stomach cancer. This review provides a comprehensive view and roadmap for readers working in this field, with the aim of pushing application of AI in theranostics of stomach cancer to increase the early discovery ratio and curative ratio of GC patients.  © 2023 the author(s), published by De Gruyter, Berlin/Boston."
"Background: A dynamic artificial intelligence (AI) ultrasonic intelligent assistant diagnosis system (dynamic AI) is a joint application of AI technology and medical imaging, which can conduct real-time synchronous dynamic analysis of nodules from multiple sectional views with different angles. This study explored the diagnostic value of dynamic AI for benign and malignant thyroid nodules in patients with Hashimoto thyroiditis (HT) and its significance in guiding surgical treatment strategies. Methods: Data of 487 patients (154 with and 333 without HT) with 829 thyroid nodules who underwent surgery were collected. Differentiation of benign and malignant nodules was performed using dynamic AI, and diagnostic effects (specificity, sensitivity, negative predictive value, positive predictive value, accuracy, misdiagnosis rate and missed diagnosis rate) was assessed. Differences in diagnostic efficacy were compared among AI, preoperative ultrasound based on the American College of Radiology (ACR) Thyroid Imaging Reporting and Data System (TI-RADS), and fine needle aspiration cytology (FNAC) diagnoses. Results: The accuracy, specificity and sensitivity of dynamic AI reached 88.06%, 80.19%, and 90.68%, respectively; besides, there was consistency with postoperative pathological consequences (κ=0.690; P<0.001). The diagnostic efficacy of dynamic AI was equivalent between patients with and without HT, and there were no significant differences in sensitivity, specificity, accuracy, positive predictive value, negative predictive value, missed diagnosis rate, and misdiagnosis rate. In patients with HT, dynamic AI had significantly higher specificity and a lower misdiagnosis rate than did preoperative ultrasound based on the ACR TI-RADS (P<0.05). Compared with FNAC diagnosis, dynamic AI had a significantly higher sensitivity and a lower missed diagnosis rate (P<0.05). Conclusions: Dynamic AI possessed an elevated diagnostic worth of malignant and benign thyroid nodules in patients with HT, which can provide a new method and valuable information for the diagnosis and development of management strategy of patients. © 2023 AME Publishing Company. All rights reserved."
"Companies are faced with the challenge of securing the experiential knowledge of long-serving employees. Previous approaches to this are time-consuming and are becoming more difficult due to increasing labor shortages. There is a need for solutions that can be integrated into the workflow and perform automated knowledge storage. In the KI_eeper project, an AI-based assistance system is being developed that takes into account the needs of both the company and the employees.  © 2023 Walter de Gruyter GmbH, Berlin/Boston, Germany."
"This paper reports on the comparison of the accuracy and quality of the responses produced by the three artificial intelligence (AI) chatbots, ChatGPT, YouChat, and Chatsonic, based on the prompts (use cases) related to selected areas of applied English language studies (AELS). An exploratory research design was employed and we utilised purposive sampling. The three aforementioned AI chatbots were used to collect data sets. Of the three chatbots, YouChat was technically unstable and unreliable, and had some inconsistency in generating responses. The other two chatbots, ChatGPT and Chatsonic, consistently exhibited a tendency to plagiarise responses from internet information without acknowledging the sources. In certain cases, the three chatbots all generated almost similar responses for different and unrelated prompts. This made their responses look like run-of-the-mill responses that lacked credibility, accuracy, and quality. One chatbot (ChatGPT) could not recognise a scholar mentioned in one of the prompts, while the other one (Chatsonic) misrecognised this scholar, and ended up rambling parts of its response. Additionally, the three chatbots all mechanically and superficially generated phrases and ideas in their responses without detecting the related critical nuances in the original sources in which they were used. This made the knowledge communicated by those responses appearing too fluffy. In this paper the educational and knowledge implications of the generated responses for AELS were educed. Based on the shortcomings the three AI chatbots displayed, I concluded that these three chatbots are not yet credible and reliable generators of knowledge for the aspects of AELS discussed. ©Authors This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC BY-NC-ND 4.0)."
[No abstract available]
[No abstract available]
"In the metaverse, the concept of the digital twin has been expanded from modeling industrial manufacturing to the counterpart of physical objects in cyberspace. The cyber digital twin is updated using real-time data and reasoning to improve decision-making, which imposes a high computational demand on the mobile edge. Mobile edge computing (MEC) provides computing resources for mobile devices to handle complex tasks, addressing the shortcomings of mobile devices in performance. Cyber digital twins with artificial intelligence (AI) capability have great advantages in addressing complex and changing environments. In this article, we propose a cyber digital twin-based mobile edge computing framework, which integrates artificial intelligence into mobile edge networks to enable intelligent resource management. We address the edge computation offloading task through formulating an optimization problem that minimizes the latency of a mobile user via MEC server selection and power allocation. Our solution employs a reinforcement learning-based algorithm, which we demonstrate to be effective. The experimental results show that the cyber digital twin based framework with artificial intelligence capability can further reduce task processing latency and improve the quality of service provided to users. © 2002-2012 IEEE."
"Telemedicine in physical therapy has increased rapidly since the COVID-19 pandemic erupted. Recuperation can help survivors resume their lives by restoring lost skills, regaining independence, and improving their well-being. With the help of innovative technologies, researchers have created new methods to aid clinicians in patient evaluation and assessment, and more people than ever have access to physiotherapy. The focus of this study is the use of deep learning and machine learning algorithms in conjunction with virtual, augmented, and mixed reality (VR, AR, and MR) technologies for experimental analysis to help patients recover from intracranial hemorrhage, stroke, musculoskeletal and neurological trauma, scoliosis, etc. We present evaluation frameworks systematically categorized into three groups: detecting emotions, identifying movements, and mimicking clinical assessments. We also examine the most popular sensors, body regions, and outcome metrics, and we review plans in evaluating AI strategies (from element design to grouping). Finally, some challenges and future directions for reviewing the field are presented. Copyrights © 2023 The Institute of Electronics and Information Engineers."
"Smart tourism uses artificial intelligence (AI) technology to provide easy and convenient travel services to tourists. The task-oriented chatbot system is a way to provide tourists more efficiently with travel services that were previously provided on the web or apps. In this paper, we develop the question answering (QA) dataset for an AI-based tourism information chatbot system. The tourism information QA dataset is developed in JSON format of KLUE MRC based on the tourism information database and tourism knowledge base built for smart tourism apps and rule-based chatbot services, respectively. To apply the QA model along with the DST and NER models to the smart tourism chatbot system, we develop the QA dataset by considering the previously developed the tourism information NER dataset and the smart tourism DST dataset. We evaluate the tourism information QA datasets with the koBigBird model, which can handle sequences of 4,096 tokens, and the EM (Exact Match) and F1 score are 96.85 and 98.84, respectively. © 2023 Tarih Dergisi - Turkish Journal of History. All rights reserved."
"With the breakthroughs of sixth-generation (6G), immersive services are beginning to receive a tremendous amount of interest, that is, 6G immersive services. The 6G immersive services involve various wearable devices to provide a high-quality perception of virtual scenes for users. The active participation of service users (SUs) and service providers (SPs) makes the rapid proliferation of wearable devices. Emerging technologies, such as cloud computing and edge computing, have also promoted the rapid development of the 6G immersive service market, making ubiquitous immersive services possible. However, the proliferation of wearable devices has brought severe challenges, including hierarchical resource provisioning, temporal dependencies between services and resources, as well as heterogeneous resource requirements. To fill this gap, we propose an AI-driven 6G immersive service resource provisioning approach, Almers-6G, from the perspective of large and small regions. In the large region, heterogeneous resources are allocated to satisfy the requirements of perception experience from SUs. The problem of resource provisioning is solved by a context-immersive learning-based Lyapunov optimization algorithm. While in the small region, the well-designed blockchain-based double dutch auction (SDDA) mechanism is used for heterogeneous resources matching and pricing determination. Finally, illustrative simulations are provided to show the effectiveness of the proposed scheme. © 2002-2012 IEEE."
"Concept sketching, as a higher order human visual cognitive activity, is an important tool to assist designers in recording, ideating, creating, and evaluating ideas and has a positive impact on the generation of innovative concepts. In order to simulate this higher order visual cognitive behaviors of designers and to achieve intelligent assistance in creative sketching, a deep learning-based design integrated framework for intelligent generation of product concept sketches is proposed, which includes two core modules: an end-to-end sketch design GAN (Sketch2Render-GAN) and a sketch-neural style transfer network (Sketch-NST). The first module implements sketch generation and rendering, while the second performs sketch style features transformation. The hand drill and bicycle helmet were used as design objects respectively, and experimental results show that the proposed approach framework can quickly obtain many innovative concept sketches and implement automatic sketch rendering and style transformation. The findings also show that the approach framework helps designers to break through design solidification at the visual perception level and increase design efficiency. Furthermore, a smart-sketch design generator (S-SDG_v0.1) was developed to facilitate human-machine design collaboration between designers and AI models, which effectively reduces the threshold of designers to apply intelligent algorithms to assist design. © 2023 Editorial Office of Chinese Journal of Mechanical Engineering. All rights reserved."
[No abstract available]
[No abstract available]
"The emergence of text-to-image generative models (e.g., Midjourney, DALL-E 2, Stable Diffusion) in the summer of 2022 impacted architectural visual culture suddenly, severely, and seemingly out of nowhere. To contextualize this phenomenon, this text offers a socio-technical history of text-to-image generative systems. Three moments in time, or “scenes,” are presented here: the first at the advent of AI in the middle of the last century; the second at the “reawakening” of a specific approach to machine learning at the turn of this century; the third that documents a rapid sequence of innovations, dubbed “clever little tricks,” that occurred across just 18 months. This final scene is the crux, and represents the first formal documentation of the recent history of a specific set of informal innovations. These innovations were produced by non-affiliated researchers and communities of creative contributors, and directly led to the technologies that so compellingly captured the architectural imagination in the summer of 2022. Across these scenes, we examine the technologies, application domains, infrastructures, social contexts, and practices that drive technical research and shape creative practice in this space. © The Author(s) 2023."
"To explore the distribution of and the mechanical properties (compressive strength) of the hardened body of alkali slag-fly ash cementitious materials, this study was conducted by using the XRD, FT-IR, SEM/EDS, and other test methods in three conditions: airtight drying (AD), airtight immersion (AI), and airtight soaking (AS). The 1D distribution law of free of hardened body under standard curing conditions was explored. The experimental results show that under standard curing conditions, the 1D distribution of within 0 d-3 d shows a ∨-shaped distribution, within 3–7 d show a ∧-shaped distribution, and within 7–28 d tends to be balanced. The test results of leaching rate show that the free was the most stable under AD conditions and the hardened body bound the most by XRD, FTIR and SEM/EDS. And the compressive strength of the hardened body was the highest. The compressive strength of 28th reached 95.9 MPa. The definite distribution of provides an important reference for the strength development and durability evaluation of the hardened body of alkali-excited cementitious materials. © 2023, Wuhan University of Technology and Springer-Verlag GmbH Germany, Part of Springer Nature."
[No abstract available]
"Knowledge graphs are an important data infrastructure in AI technologies and applications, and have become a hot research topic in geosciences. The size and topological features in geographic knowledge graphs are usually different from universal knowledge graphs, which are not typical small- world networks. However, existing studies often use the default network search depth when learning geographic knowledge graph representations, and its rationality needs further demonstration. For this purpose, this paper constructs a metro travel knowledge graph based on the topological structure features of metro line network, combined with passenger flow data, POI (Point of Interest) data and built environment data, etc.; then GraphSAGE model is used to learn node multidimensional feature embedding and combine POI data for semantic recognition of station classification results to verify the suitable network search depth for metro travel knowledge graph. The results showed that, compared to the default 2 layers search depth, the node embedding features of this metro travel knowledge graph work optimally when the search depth is 3 layers. This study shows that the hyperparameter selection of the geographic knowledge graph representation is supposed to take into account the geographic features, and it is important to avoid the use of results from fields such as computer science that have not been distinguished. When the search depth is 3 layers, the metro station classification results are also more reasonable and explanatory, which can provide a basis for station planning and passenger flow prediction using knowledge graph and AI methods. © 2023 Cultura. International Journal of Philosophy of Culture and Axiology. All rights reserved."
"With the development of artificial intelligence and the rapid spread of the Internet, online teaching has become an increasingly popular method of education. However, in the context of the post-epidemic era of COVID-19, online teaching has become even more important, as many educational institutions have been forced to transition to this model to ensure continuity of learning. In this context, there is a growing need to develop innovative approaches to online teaching that can effectively address the challenges posed by the pandemic. Online teaching has become increasingly important for higher education institutions around the world, and it has been particularly crucial during the COVID-19 pandemic. The teaching of English at universities and colleges exhibited significant performance for online teaching. The ideology concept performs online teaching in English for politics and comprises of different strategies. English teaching, several strategies can be implemented. This research paper proposes a novel approach to integrate artificial intelligence (AI) and cloud computing technologies in the online English teaching model with a curriculum of ideological and political concern for colleges and universities. The proposed model, referred to as AIIOE, aims to enhance the quality and effectiveness of online English teaching while also providing a comprehensive education on ideological and political issues. The AIIOE model utilizes natural language processing (NLP), machine learning, and cloud computing technologies to provide a personalized and interactive learning experience to students. The proposed curriculum includes topics related to political ideology, history, and culture to enhance students' awareness and understanding of their social and political environment. The study adopts a mixed-methods approach, including a survey of English teachers, focus group interviews with students, and an analysis of students' performance in English language proficiency and ideological and political awareness. The results indicate that the AIIOE model significantly improves students' English language proficiency, knowledge of ideological and political issues, and overall learning experience. The examination is evaluated based on the ideological and political curriculum with an Internet-based online teaching mode in English teaching. With the investigation of the Internet online teaching model, the significant contribution is evaluated. Through analysis, it is concluded that the concept of the Internet Online teaching model significantly contributed to ideological and political factors. © 2023 Authors. All rights reserved."
"Background: Artificial intelligence (AI) tremendously influences our daily lives and the medical field, changing the scope of medicine. One of the fields where AI, and, in particular, predictive modeling, holds great promise is spinal oncology. An accurate patient prognosis is essential to determine the optimal treatment strategy for patients with spinal metastases. Multiple studies demonstrated that the physician’s survival predictions are inaccurate, which resulted in the development of numerous predictive models. However, difficulties arise when trying to interpret these models and, more importantly, assess their quality. Objective: To provide an overview of all stages and challenges in developing predictive models using the Skeletal Oncology Research Group machine learning algorithms as an example. Methods: A narrative review of all relevant articles known to the authors was conducted. Results: Building a predictive model consists of 6 stages: preparation, development, internal validation, presentation, external validation, and implementation. During validation, the following measures are essential to assess the model’s performance: calibration, discrimination, decision curve analysis, and the Brier score. The structured methodology in developing, validating, and reporting the model is vital when building predictive models. Two principal guidelines are the transparent reporting of a multivariable prediction model for individual prognosis or diagnosis checklist and the prediction model risk of bias assessment. To date, many predictive modeling studies lack the right validation measures or improperly report their methodology. Conclusions: A new health care age is being ushered in by the rapid advancement of AI and its applications in spinal oncology. A myriad of predictive models are being developed; however, the subsequent stages, quality of validation, transparent reporting, and implementation still need improvement. © International Society for the Advancement of Spine Surgery."
"Withers and Nadarajah (2022, to appear) gave solutions to univariate nonlinear recurrence equations. We consider the vector case. Let C denote the complex numbers. Let F(z) : Cq → Cq be any analytic function. Let w ∈ Cq be any fixed point of F(z), that is, F(w) = w. Set Ḟ(z) = dF(z)/dz′ ∈ Cq×q. Then for any eigenvalue r of Ḟ(w), the recurrence equation zn+1 = F(zn) ∈ Cq, for n = 0, 1, 2, . . . , has a solution of the form (Formula Presented), where α ∈ C is arbitrary and ai (w) ∈ Cq are given by recurrence. © Rocky Mountain Mathematics Consortium."
"Internet of Things based Automatic Attendance Management systems that use Artificial Intelligent cameras and deep learning algorithms can suggestively advance the accuracy and proficiency of class presence following in schools, colleges as well as universities. This technology involves the use of cameras that are placed in classrooms or other areas where attendance needs to be monitored.The cameras are equipped with advanced deep learning algorithms that can detect and recognize students based on their unique facial features. These algorithms use machine learning techniques to analyse images and identify individual faces, even in varying lighting conditions and different angles.The data collected by the cameras is then transmitted to an Intenet of Things based platform, which stores and approach the attendance data in real time. This platform can also be used to generate reports and analytics on attendance, helping teachers and administrators make data driven decisions to improve student performance. © 2023 Authors. All rights reserved."
"The current study examines how artificial intelligence (AI) enhances the performance of a company’s public accounting information systems (AIS). A quantitative approach was used to accomplish this goal and show how well it fits with reality within the scope of public accounting. Primary data was gathered through the use of a questionnaire and analysed with SPSS-22. To ensure the constructs' validity and trustworthiness, a chronbach's alpha and an exploratory factor analysis were performed. The hypothesis was examined via regression analysis. The research found that the use of AI techniques greatly contributed to the improvement of public accounting information system. The research suggests that organisations should coordinate the activities of intelligent systems with their financial targets. © 2023, Innovative Information Science and Technology Research Group. All rights reserved."
"Medical training for fitness nurses accommodates theoretical and practical sessions for understanding real-time patient care and handling. Some common errors such as needle injury, improper tool handling, etc. occur due to novice fitness nursing students. For preventing such errors and improving the training quality, this article introduces an Artificial Intelligence assimilated Preventive Training Measure (AI-PTM). The proposed method observes the different training sessions of fitness nursing students for error detection and training qualification. In this method, classification with recurrent learning is induced for identifying the error-causing feature in the mid of the training session. This error-causing feature is classified based on student characteristics (such as mishandling, lack of concentration, etc.) and objects (new equipment, precision handling, etc.). Based on the classification, the instance is modified in the recurrent training session, improving the student’s concentration. The identified error-causing features are stored, congruently matched, and used for training further nursing sessions. This method improves training accuracy, and precision handling, and reduces error. © 2023, Universidad Autonoma de Madrid y CV Ciencias del Deporte. All rights reserved."
"As a result of the Covid-19 pandemic, the field of Medical Sciences has been challenged with new challenges and benchmarks for development. Front line workers are overcoming the Covid-19 challenge with four steps: Screening and Diagnosis, Contact Tracing, Drug and Vaccine Development, and Prediction & Forecasting. Following the above segments carefully can save millions of lives. Artificial Intelligence has proven invaluable in predicting critical factors in many fields. With the ability of AI to process huge databases and conclude with high precision, we are motivated to use AI to screen and diagnose the Covid-19 pandemic. This paper examines the strategic use of Transfer Learning for screening and diagnosis of Covid-19 Patients. The Xception model is used to categorize Covid-19 infected patients. Our proposed Xception model has achieved better Accuracy, Sensitivity and Specificity as compared with state-of-the-art models. © 2023 by the Manish K. Assudani and Dr. Neeraj Sahu."
"With the rapid development of 6G wireless communication technology, the emergence of rich multimedia data for massive devices will lead to greater intensive computations and energy consumption. However, the requirements from both green communication and international low-carbon strategy can be challenging. In this article, we first systematically analyze the key challenges from the perspective of 6G networks for low-carbon smart city development. Then we propose an AI-driven visual end-edge-cloud architecture (E2C), which extends upon the conventional design from the perspective of human-machine fusion and carbon emission optimization. We provide systematical analysis and intelligent computing methods for carbon emission in visual end-edge-cloud architecture. This architecture can enable the provision of E2C AI intelligence for 6G networks through hybrid hierarchical optimization mechanisms. Finally, the experimental results demonstrate that our proposed architecture has better performance in smart cities, achieving lower carbon emissions compared to traditional methods. © 2002-2012 IEEE."
"Successful treatment of endogenous Cushing disease (CD) is often followed by a period of adrenal insufficiency (AI). We performed an exploratory study on genetic factors potentially involved in the hypothalamic-pituitary-adrenal (HPA) axis recovery in patients with CD after remission. We identified 90 patients who achieved remission after surgery and had a minimum of 3 months follow-up. Variants in a selected panel of genes that were rare in the general population and predicted as damaging in silico were retrieved from whole exome sequencing analysis. We did not identify any variant with significant correlation with recovery time after adjusting for multiple comparisons. On gene-specific analysis the BAG1 gene showed a correlation with shorter duration of postsurgical AI, but both patients with BAG1 variants later experienced a recurrence. After excluding patients with recurrence, no statistical association was recorded. To conclude, we did not identify a strong genetic modifier of HPA recovery in this exploratory study. © 2023 Oxford University Press. All rights reserved."
"The sixth-generation (6G) mobile networks are expected to feature the ubiquitous deployment of machine learning and artificial intelligence (AI) algorithms at the network edge. With rapid advancements in edge AI, the time has come to realize intelligence downloading onto edge devices (e.g., smartphones and sensors). To materialize this version, we propose a novel technology in this article called in-situ model downloading, which aims to achieve transparent and real-time replacement of on-device AI models by downloading from an AI library in the network. Its distinctive feature is the adaptation of downloading to time-varying situations (e.g., application, location, and time), devices' heterogeneous storage-and-computing capacities, and channel states. A key component of the presented framework is a set of techniques that dynamically compress a downloaded model at the depth-level, parameter-level, or bit-level to support adaptive model downloading. We further propose a virtualized 6G network architecture customized for deploying in-situ model downloading with the key feature of a three-tier (edge, local, and central) AI library. Furthermore, experiments are conducted to quantify 6G connectivity requirements and research opportunities pertaining to the proposed technology are discussed. © 2002-2012 IEEE."
"The developed concept for Artificial Intelligence (AI) introduction to the management system is concerned with a range of ethical, social, environmental and legal issues. Management system as a form of organising chaos and complexity becomes the only platform to design business and to make it sustainable irrespective of location and personalities engaged. At the time of the world economy demand for social actors, activism in the necessary transition of management oriented to reach the Sustainable Development Goals (SDG) is a crucial factor to form the new management digitized system. Environmental, Social and Governance (ESG) Investing managed assets as a part of all corporate assets. ESG initiative was a proposal of the UN to promote principles for a sustainable economy. Companies with better ESG performance can increase shareholders’ value by managing risks related to emerging ESG issues, namely bring the corporation to have energy transition experience. Different approaches of millennials to managing enterprises show their higher interest than that of predecessors to introduce ESG standards and tasks to the day-to-day management. Millennials are more interested in social values than in the investment return. Even the future of investments is dependent on the basic idea that investors are not short-termist but tend to be loyal to a project about which they have more relevant information. So, they may support new AI-based management in the case it becomes an efficient platform to design a human-oriented enterprise. This study aims at showing what the relationship of the management structure and process should be in order to manage the AI progress. Understanding of management content and work with the AI representation is of strategic importance. © Author(s) 2023."
"Continuous human monitoring has become increasingly important in various applications, including health, security, intelligent systems, and leisure activities. Human Activity Recognition (HAR) through the use of wearables, tagged objects, and device-free localization (DFL) has gained major attention from researchers. DFL approaches have been particularly recommended due to their non-intrusive nature and its applicability in diverse fields. The use of Artificial Intelligence (AI) has reinvented the utilization of deep concealed information for precise detection and interpretation. However, challenges which includes data collection, dealing with intra-class variability, and real-Time recognition in dynamic and instant changing scenarios still persists. This paper provides a review of the various techniques for HAR and their applications in different fields. A comprehensive analysis of methodologies and data from papers published from 2000 to 2023 has been conducted. The paper also discusses research problems and future opportunities in this field. © 2023 Lavoisier. All rights reserved."
"Artificial intelligence (AI) plays a crucial role in the intelligent development of China’s power system. It is also an important part of the digital development of the power grid. The development of AI determines whether the digital transformation of China’s power system can be successfully implemented. Therefore, this paper discusses the digital transformation of the power grid based on AI technologies. The author has established a digital evaluation index system to reflect the development of the power grid in one province. Both qualitative and quantitative methods have been adopted in the analysis, which delves into the economic effectiveness, quality, and coordination of power grid development in the province in a comprehensive way. Results show that, to meet the needs of the power grid’s digital transformation, the correlation coefficient between the power grid’s development and the province’s overall coordination has been increasing in recent years. © 2023 KIPS"
"Avian Influenza (AI) has become the largest animal epidemic in the world. So far hundreds of millions birds have died or been culled due to the disease. A novel highly-pathogenic avian influenza (HPAI) subtype H5N1 virus, which emerged in 1996 in domestic geese in China, passed from wild waterfowl as a low-pathogenic (LP) virus, and in the domestic birds was modified to a High Pathogenic (HP) virus due to mutational addition of basic amino acids to the cleavage site of the Hemagglutinin (HA) protein. This ""A/ goose/Guangdong/1/1996"" virus killed poultry but also infected at least 18 people of whom 6 died. In 2003-6 the virus spread to Asia, Europe, Africa and the Middle East. Mass mortality of migratory birds at the Salt Qinghai Lake in West China in 2005 was a turning point in the understanding of the virus co-circulation between poultry and wild birds. Israel is situated on migratory routes for over a billion birds each year in the autumn migration from Europe to Africa and in the northwards spring migration, thus the potential for virus transmission is high. The 1996's parental virus emerged in Israel in 2006, following the Qinghai Lake event, and included meat turkeys, heavy breeders and broilers. Since then, several focal outbreaks occurred in Israel every 1-3 years. In 2012, the same AIV-H5N1 was detected in meat turkeys and found also in alley cats following consumption of the bird carcasses, with 100% identity of the HA gene. A second multi-focal outbreak of AIV-H5N1 appeared in 2015 following the worldwide wave in 2014-15. This outbreak included meat turkeys, heavy breeders and egg layers. These viruses belonged phylogenetically to clade 2.2.1.2, emerged from previous clades 2.2 and 2.2.1.1. Reassortment of viral segments of a descendant-H5 AIV of the Chinese parental virus with AIV-N8 viruses created novel H5N8 viruses, first detected in China in 2010. An 2016's variant of this virus (group-B Gochang-like, clade 2.3.4.4), was discovered in May-June 2016 in wild swans at Lake Ubsu-Nur on the Russian-Mongolian border, and reached Central Asia, Europe, Africa, Middle East and Israel. The Ubsu-Nur 2016's AIV-H5N8 was genetically identical to an AIV-H5N8 that emerged in Israel six months later during the autumn migration. This was the first H5N8 outbreak in Israel, which included meat and breeder turkeys, heavy and light breeders, layers, breeder ducks and backyard poultry, but also many wild birds, not seen in the previous H5N1 outbreaks, of at least 16 species mostly waterfowl and raptors. The cleavage site of HA gene showed similarity between poultry and wild birds, as well as to grp B-2.3.4.4 European-Asian H5N8 viruses. Since October 2020, new H5 reassortants with LPAI viruses from wild birds contributed the Neuraminidase (NA) glycoprotein, have appeared in Europe and created new H5 strains. A second global outbreak of a new H5 variant occurred in 2020-21 in Russia and the Palearctic region. Following this global outbreak, a second AIV-H5N8 outbreak appeared in Israel in 2020 during the autumn migration. The outbreak included meat and breeder turkeys, heavy breeders, broilers, game birds mainly black swans (Cygnus atratus), and wild birds. The 2021–22's AIV-H5N1 epidemic season was the largest so far in Europe and Eurasia, with about 2500 outbreaks in poultry and culling of 50 million birds, and about 3600 detections in wild birds. That outbreak followed the 2020's wave of the new AIV-H5N8. Following this epidemic, a third multi-focal outbreak of AIV-H5N1 occurred in Israel in 2021, in meat and breeder turkeys, heavy breeders, organic egg layers and meat ducks. The 2021's outbreak was characterized also by affecting many wild birds, not seen before in H5N1's outbreaks in Israel, while the largest affected population was Common cranes (Grus grus) on their migration routes from Russia and Scandinavia to Ethiopia and Sudan through Israel, undergoing mass mortality of almost 10,000 birds. Other threatened species like Marbled teal (Marmaronetta angustirostris) have died also due to this virus, as well as hundreds of Great white pelicans (Pelecanus onocrotalus), and several species of waterfowl and raptors. Classification of the viruses by whole genome sequencing (WGS) revealed the same group B-2.3.4.4 clade as in the H5N8 events and similarity between poultry and wild birds. This virus probably came from Russia in the autumn migration. A new outbreak of AIV-H5N1 began emerging in Israel from November 2022, in meat turkeys, heavy and light breeders, broilers. Unlike the 2021's outbreak, this virus affected only a few wild birds. HA gene sequencing revealed the virus belonged to the same group B-2.3.4.4 clade as in previous outbreaks. In conclusion, all avian influenza subtype H5 outbreaks in Israel followed the European-Eurasian outbreaks. The virus origin was from South East Asia, reaching Israel through spillover with migrating birds via Eurasia and Europe. © 2023, Israel Veterinary Medical Association. All rights reserved."
"This study investigates the impact of artificial intelligence (AI) on decision-making, laziness, and privacy concerns among university students of Delhi NCR (India) and Fujairah (UAE). As AI technologies are increasingly adopted in various sectors, including education, to tackle contemporary challenges, there is a growing investment in AI, projected to reach USD 253.82 million between 2021 and 2025. However, while researchers and institutions worldwide praise the positive role of AI, this study sheds light on the concerns associated with its implementation. This study utilizes a qualitative methodology employing PLS-Smart for data analysis. The primary data was collected from 315 students representing various universities in Delhi NCR (India) and Fujairah (UAE). The sample was drawn using purposive sampling techniques from the population. The findings of the data analysis demonstrate that artificial intelligence (AI) has a significant impact on human decision-making, laziness, and security and privacy concerns. The results indicate that 68.9% of human laziness, 68.6% of personal privacy and security issues, and 27.7% of the loss of decision-making can be attributed to the influence of AI in Delhi and Fujairah. Notably, human laziness emerges as the most affected area due to artificial intelligence. © 2023, Journal of Content, Community and Communication. All Rights Reserved."
"The AAAI 2023 Spring Symposium on Challenges Requiring the Combination of Machine Learning and Knowledge Engineering was held at the Hyatt Regency, San Francisco Airport, California, USA, from March 27-29, 2023. The symposium gathered researchers and practitioners from machine learning and knowledge engineering to reflect on how combining the two fields can contribute to tackling future AI challenges. The symposium featured a joint keynote presentation by AI pioneers, over 25 presentations by contributors and authors who presented papers, datasets, ontologies, and research findings, and two novel challenges proposed to be solved by the community in a follow-up event. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence."
[No abstract available]
"This paper will discuss how current challenges in airport management can be addressed by means of increased operational control.The paper will show that the availability and correct usage of operational data can help solve many of the headaches airport managers face today.The paper starts with an overview of the key developments that led to today’s challenges, which will then be described. Next, how real-time and historical data can be used to address these challenges will be presented. Real-world examples will be used to illustrate that this is not just theory, but that tangible results are very much achievable. Finally, the paper will conclude with a summary of best practices that enable and facilitate the adoption of data-driven operations. © 2023, Henry Stewart Publications. All rights reserved."
"BIM technology comprises the digital representation of the physical and functional characteristics of the building or structures. It provides the architectural model, engineering, and professional construction with the collaboration of construction projects in an efficient and effective manner. BIM technology can be used to create 3D models of transportation hubs, which can help visualize and simulate different scenarios, optimize space utilization, and improve safety. With detailed 3D models and simulating different scenarios, BIM technology can help optimize space utilization, improve safety, and enhance the overall travel experience for passengers. Core architectural design refers to the fundamental design principles and elements that form the foundation of a building or structure. Hence, this research designed Artificial Intelligence (AI) integrated fuzzy set (AIF-BIM) model for the transportation hub construction. The design of the AIF-BIM model uses the associative rule-based model for the design of the transportation hub. With a designed AI model BIM technology is integrated for the examination of design, construction, and operation. Through the AIF-BIM model, the transportation hub engineering architects are evaluated for the different phases such as the design phase and construction phase. Simulation analysis stated that the application of BIM technology with AIF – BIM in transportation hubs can improve their design, construction, and operation. © 2023 International Journal on Recent and Innovation Trends in Computing and Communication. All rights reserved."
"AI is transforming the way we live and work, with the potential to improve our lives in many ways. However, there are risks associated with AI deployments including failures of model robustness and security, explainability and interpretability, bias and fairness, and privacy and ethics. While there are international efforts to define governance standards for responsible AI, these are currently only principles-based, leaving organizations uncertain as to how they can prepare for emerging regulations or evaluate their effectiveness. We propose the use of anticipatory thinking and a flexible model risk audit (MRA) framework to bridge this gap and enable organizations to take an advantage of the benefits of responsible AI. This approach enables organizations to characterize risk at the model level and to apply the anticipatory thinking employed by high reliability organizations to achieve responsible AI deployments. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence."
"SUMMARY. Radiomics can interpret radiological images with more detail and in less time compared to the human eye. Some challenges in managing esophageal cancer can be addressed by incorporating radiomics into image interpretation, treatment planning, and predicting response and survival. This systematic review and meta-analysis provides a summary of the evidence of radiomics in esophageal cancer. The systematic review was carried out using Pubmed, MEDLINE, and Ovid EMBASE databases—articles describing radiomics in esophageal cancer were included. A meta-analysis was also performed; 50 studies were included. For the assessment of treatment response using 18F-FDG PET/computed tomography (CT) scans, seven studies (443 patients) were included in the meta-analysis. The pooled sensitivity and specificity were 86.5% (81.1–90.6) and 87.1% (78.0–92.8). For the assessment of treatment response using CT scans, five studies (625 patients) were included in the meta-analysis, with a pooled sensitivity and specificity of 86.7% (81.4–90.7) and 76.1% (69.9–81.4). The remaining 37 studies formed the qualitative review, discussing radiomics in diagnosis, radiotherapy planning, and survival prediction. This review explores the wide-ranging possibilities of radiomics in esophageal cancer management. The sensitivities of 18F-FDG PET/CT scans and CT scans are comparable, but 18F-FDG PET/CT scans have improved specificity for AI-based prediction of treatment response. Models integrating clinical and radiomic features facilitate diagnosis and survival prediction. More research is required into comparing models and conducting large-scale studies to build a robust evidence base. © The Author(s) 2023. Published by Oxford University Press on behalf of International Society for Diseases of the Esophagus. All rights reserved."
[No abstract available]
"Persistence of imidacloprid, λ-cyhalothrin and spiromesifen on cabbage head and soil was studied. Two foliar applications of imidacloprid, λ-cyhalothrin and spiromesifen @ 25, 15 and 96 g ai ha-1 (single dose) and 50, 30 and 192 g ai ha-1 (double dose), respectively, were given at 10 days interval. Treated samples were processed by using modified QuEChERS method and the residue data was subjected to statistical analysis. Studies revealed that the initial deposits of imidacloprid, λ-cyhalothrin and spiromesifen were 0.39, 0.20 and 0.74 mg kg-1 for single dose and 0.80, 0.41 and 1.58 mg kg-1 for double dose, respectively. Among three insecticides, spiromesifen was the most persistent insecticide (10 days for both doses) followed by λ-cyhalothrin (7 and 10 days) and imidacloprid (5 and 7 days) on cabbage head. The safe waiting period for imdacloprid, λ-cyhalothrin and spiromesifen were 5, 6 and 9 days for single dose and 6, 7 and 11 days for double dose with the half life of 1.6, 2.5-3.1 and 1.8-1.9 days, respectively. © 2023, Society of Pesticide Science India. All rights reserved."
"An Integrated Campus Management Platform using Artificial Intelligence (AI) & Machine Learning (ML) is a platform that leverages advanced technologies to optimize the management of various operations in a campus environment. This platform can provide solutions for tasks such as student management, facility management, and academic planning among others. By utilizing AI and ML, the system can learn from historical data and adapt to changing conditions, providing real-time insights and analytics that can be used to make informed decisions. This abstract provides an overview of the Integrated Campus Management platform and the benefits it offers in terms of efficiency, accuracy, and scalability. Additionally, it highlights the role of AI and ML in enhancing the platform’s capabilities and improving the overall campus experience for students, faculty, and staff. © 2023 International Journal on Recent and Innovation Trends in Computing and Communication. All rights reserved."
"Background: The development of artificial intelligence (AI) techniques has provided a novel strategy for improving the performance of renal ultrasound. To reflect the development of AI methods in renal ultrasound, we aimed to clarify and analyze the state of AI-aided ultrasound research in renal diseases. Methods: PRISMA 2020 guidelines have been used to guide all processes and results. AI-aided renal ultrasound studies (for both image segmentation and disease diagnosis) published up to June 2022 were screened through the databases of PubMed and Web of Science. Accuracy/Dice similarity coefficient (DICE), the area under the curve (AUC), sensitivity/specificity, and other indications were applied as evaluation parameters. The PROBAST was used to assess the risk of bias in the studies screened. Results: Of 364 articles, 38 studies were analyzed, and could be divided into AI-aided diagnosis or prediction related studies (28/38) and image segmentation related studies (10/38). The output of these 28 studies involved differential diagnosis of local lesions, disease grading of, automatic diagnosis, and diseases prediction. The median values of accuracy and AUC were 0.88 and 0.96, respectively. Overall, 86% of the AI-aided diagnosis or prediction models were classified as high risk. An unclear source of data, inadequate sample size, inappropriate analysis methods, and lack of rigorous external validation were found to be the most frequent and critical risk factors in AI-aided renal ultrasound studies. Conclusions: AI is a potential technique in the ultrasound diagnosis of different types of renal diseases, but the reliability and availability need to be strengthened. The use of AI-aided ultrasound in chronic kidney disease and quantitative hydronephrosis diagnosis will be a promising possibility. The size and quality of sample data, rigorous external validation, and adherence to guidelines and standards should be considered in further studies. © 2023 AME Publishing Company. All rights reserved."
"Communications systems to date are primarily designed with the goal of reliable transfer of digital sequences (bits). Next generation (NextG) communication systems are beginning to explore shifting this design paradigm to reliably executing a given task, such as in task-oriented communications. In this article, wireless signal classification is considered as the task for the NextG Radio Access Network (RAN), where edge devices collect wireless signals for spectrum awareness and communicate with the NextG base station (gNodeB) that needs to identify the signal label. Edge devices may not have sufficient processing power and may not be trusted to perform the signal classification task, whereas the transfer of signals to the gNodeB may not be feasible due to stringent delay, rate, and energy restrictions. Task-oriented communications is considered by jointly training the transmitter, receiver, and classifier functionalities as an encoder-decoder pair for the edge device and the gNodeB. This approach improves the accuracy compared to the separated case of signal transfer followed by classification. Adversarial machine learning poses a major security threat to the use of deep learning for task-oriented communications. A major performance loss is shown when backdoor (Trojan) and adversarial (evasion) attacks target the training and test processes of task-oriented communications. © 2002-2012 IEEE."
"Who is the author of a work generated by AI? Can AI-generated works be protected by copyright law? This issue has attracted global attention. The vast majority of countries in the world have given a negative response to this question, but one Chinese court has given an affirmative answer, instead. Does this Chinese decision represent future thinking for the world in this area? It is necessary to investigate the reasons behind this decision, which are related to China's special interpretation of “human participation” and the criteria for judging originality. This judicial result was also related to China's current lack of a distinction between computer-assisted and AI-generated results. In the future, China may continue to uphold the existing determination; however, since China does not operate under case law, Chinese courts may still change their opinion. Moreover, China's choice may not have an impact on countries that are deeply influenced by natural law, but it may still impact some countries that are strongly influenced by utilitarianism. © 2023, Bucharest University of Economic Studies. All rights reserved."
"This study evaluated the efficacy of flonicamid 50%WG (50, 75 and 100 g ai ha-1) along with imidacloprid 17.8SL (25 g ai ha-1), thiamethoxam 25%WG (25 g ai ha-1), chlorpyriphos 19%ME (180 g ai ha-1) and fipronil 5% SC (75 g ai ha-1) against the rice ear head bug Leptocorisa acuta (Thunberg) in rice. The results revealed that flonicamid @ 100 g ai/ ha was the most effective (1.0 bugs hill-1) followed by flonicamid @ 75 g (1.13 bugs hill-1). Imidacloprid (1.22 bugs hill-1) was statistically on par with that of flonicamid @ 50 g (1.23 bugs hill-1) and thiamethoxam (1.24 bugs hill-1). The yield and cost-effectiveness were maximum in the flonicamid (48.93 q ha-1 @ 100 g) and imidacloprid (B: C; 2.43:1). © 2023, The Entomological Society of India. All rights reserved."
"The world’s current linear economic model is unsustainable. This model encourages improper use of limited natural resources and causes abundant waste production resulting in severe harm to the environment. A circular economy (CE) is a sustainable, restorative, and regenerative alternative to the current linear economy and is gaining popularity worldwide. Amongst various digital technologies, Artificial intelligence (AI) is a crucial enabler for CE and can aid significantly with the adoption and implementation of CE in real-world applications. In this paper, we describe the intersection of AI and CE and policies around implementing CE principles using AI. As a means of grounding the discussion, we discuss some initiatives taken by the Irish government to adopt circularity and explore the role AI plays in these. We present a number of practical examples of AI and CE from Ireland. We argue that digitalisation has potential in CE and it has a major role to play in the transition towards CE. We close the paper by reflecting on future steps around practical implementations of AI-based CE processes. © 2023 by the authors."
"As an emerging research area since generative artificial intelligence (represented by Chat Generative Pre-trained Transformer (ChatGPT)) has been accessible to the public, especially in education, appropriate AI application could bring numerous benefits to education; however, its abuse has the potential to be harmful. In this paper, we aimed to explore the potential of AI in the future of education with the analytical method of evolutionary game analysis (EGA). By studying the behavior of two agents, the school and the students, EGA can be used to identify strategies that can be used to improve the effectiveness of the education model in the context of the AI era. A stable evolutionary strategy for the school and students was devised under a variety of scenarios. Additionally, we conducted a numerical analysis to further explore the impact of several key factors on the stable strategy. The results indicated that schools should adopt positive supervision to standardize the use of AI in education, and students should be more active in becoming involved in AI technology. Based on this study, we believe that the school has the ability to provide effective suggestions and practical guidelines to help students succeed academically and embrace future trends in AI education. © 2023 by the authors."
"In this commentary, I review the articles in the IJAIED Special Issue on K-12 AI Education. The articles offer compelling motivation for early AI education and cover an impressive range of approaches, grade levels, and perspectives. Despite the differences, there is coherence across the articles in terms of the goals to address AI awareness, knowledge, skills, and ethics. Deep consideration has gone into creating developmentally appropriate AI content, which is arguably the greatest challenge for a complex topic like AI. However, as we find in many emerging topics in education, the demand for curricula and lessons has outpaced the capacity of the field to do sufficient empirical research on how kids learn about AI. Evidence for many of the design choices reflected in the proposals put forth in this special issue is still emerging. The authors have done an admirable job of organizing their ideas around principles from the learning sciences and connecting their efforts to more general curriculum design efforts, such as the K12 CS Framework (2016). The next step, which is promoted by all of the authors in the special issue, is to define a research agenda to provide an empirical basis for the design of early AI learning experiences and inform future iterations of the curricula and frameworks proposed. © 2023, The Author(s)."
"To better manage human resources (HR), companies are increasingly incorporating artificial intelligence (AI) and other AI-based tools into their HR management (HRM) strategies, at a universal scale. Companies on a global scale, highlight the employment prospects and use of resources, business judgment, and make predictions using machine learning approaches. This work aims at the situation that the human resource department faces high employee turnover in the company especially some experienced employees leave. The termination of an employee is predicted by using an enhanced ID3 decision tree with ABC rule miner. The best-classifying attributes are chosen by ID3 and association rules are mined to generate an enhanced decision tree to perform classification. It is then passed to the regressor model to make prediction. Gradient descent optimizer is used for optimizing the proposed machine learning model. Predictive analysis is done in HR dataset v-14 by visualizing and analyzing and exploiting the behavioral relationship among the attributes. The variables of employee termination are predicted by a data-driven predictive analysis from the performance measure metrics. © 2023"
"Background: There is considerable interest in the potential use of artificial intelligence (AI) systems in mammographic screening. However, it is essential to critically evaluate the performance of AI before it can become a modality used for independent mammographic interpretation. Purpose: To evaluate the reported standalone performances of AI for interpretation of digital mammography and digital breast tomosynthesis (DBT). Materials and Methods: A systematic search was conducted in PubMed, Google Scholar, Embase (Ovid), and Web of Science databases for studies published from January 2017 to June 2022. Sensitivity, specificity, and area under the receiver operating characteristic curve (AUC) values were reviewed. Study quality was assessed using the Quality Assessment of Diagnostic Accuracy Studies 2 and Comparative (QUADAS-2 and QUADAS-C, respectively). A random effects meta-Analysis and meta-regression analysis were performed for overall studies and for different study types (reader studies vs historic cohort studies) and imaging techniques (digital mammography vs DBT). Results: In total, 16 studies that include 1 108 328 examinations in 497 091 women were analyzed (six reader studies, seven historic cohort studies on digital mammography, and four studies on DBT). Pooled AUCs were significantly higher for standalone AI than radiologists in the six reader studies on digital mammography (0.87 vs 0.81, P = .002), but not for historic cohort studies (0.89 vs 0.96, P = .152). Four studies on DBT showed significantly higher AUCs in AI compared with radiologists (0.90 vs 0.79, P .001). Higher sensitivity and lower specificity were seen for standalone AI compared with radiologists. Conclusion: Standalone AI for screening digital mammography performed as well as or better than radiologists. Compared with digital mammography, there is an insufficient number of studies to assess the performance of AI systems in the interpretation of DBT screening examinations. © 2023 Radiological Society of North America Inc.. All rights reserved."
"In recent years, cardiovascular imaging examinations have experienced exponential growth due to technological innovation, and this trend is consistent with the most recent chest pain guidelines. Contrast media have a crucial role in cardiovascular magnetic resonance (CMR) imaging, allowing for more precise characterization of different cardiovascular diseases. However, contrast media have contraindications and side effects that limit their clinical application in determinant patients. The application of artificial intelligence (AI)-based techniques to CMR imaging has led to the development of non-contrast models. These AI models utilize non-contrast imaging data, either independently or in combination with clinical and demographic data, as input to generate diagnostic or prognostic algorithms. In this review, we provide an overview of the main concepts pertaining to AI, review the existing literature on non-contrast AI models in CMR, and finally, discuss the strengths and limitations of these AI models and their possible future development. © 2023 by the authors."
"Machine learning frameworks categorizing customer reviews on online products have significantly improved sales and product quality for major manufacturers. Manually scrutinizing extensive customer reviews is imprecise and time-consuming. Current product research techniques rely on text mining, neglecting audio, and image components, resulting in less productive outcomes for researchers and developers. AI-based machine learning frameworks that consider social media and online buyer reviews are essential for accurate recommendations in online e-commerce shops. This research paper proposes a novel machine-learning-based framework for categorizing customer reviews that uses a bag-of-features approach for feature extraction and a hybrid DNN framework for robust classification. We assess the performance of our machine learning framework using AliExpress and Amazon e-commerce product review data provided by customers, and we have achieved a classification accuracy of 91.5% with only 8.46% fallout. Moreover, when compared with state-of-the-art models, our proposed model shows superior performance in terms of sensitivity, specificity, precision, fallout, and accuracy. © 2023 by the author."
"Respiratory disorders, being one of the leading causes of disability worldwide, account for constant evolution in management technologies, resulting in the incorporation of artificial intelligence (AI) in the recording and analysis of lung sounds to aid diagnosis in clinical pulmonology practice. Although lung sound auscultation is a common clinical practice, its use in diagnosis is limited due to its high variability and subjectivity. We review the origin of lung sounds, various auscultation and processing methods over the years and their clinical applications to understand the potential for a lung sound auscultation and analysis device. Respiratory sounds result from the intra-pulmonary collision of molecules contained in the air, leading to turbulent flow and subsequent sound production. These sounds have been recorded via an electronic stethoscope and analyzed using back-propagation neural networks, wavelet transform models, Gaussian mixture models and recently with machine learning and deep learning models with possible use in asthma, COVID-19, asbestosis and interstitial lung disease. The purpose of this review was to summarize lung sound physiology, recording technologies and diagnostics methods using AI for digital pulmonology practice. Future research and development in recording and analyzing respiratory sounds in real time could revolutionize clinical practice for both the patients and the healthcare personnel. © 2023 by the authors."
"Analysis of dynamic differential speckle patterns, scattered from human tissues illuminated by a laser beam, has been found by many researchers to be applicable for noncontact sensing of various biomedical parameters. The COVID-19 global pandemic brought the need for massive rapid-remote detection of a fever in closed public spaces. The existing non-contact temperature measurement methods have a significant tradeoff between the measurement distance and accuracy. This paper aims to prove the feasibility of an accurate temperature measurement system based on speckle patterns analysis, enabling the sensing of human temperature from an extended distance greater than allowed by the existing methods. In this study, we used speckle patterns analysis combined with artificial intelligence (AI) methods for human temperature extraction, starting with fever/no fever binary classification and continuing with temperature measurement at higher resolution. © 2023 Optica Publishing Group under the terms of the Optica Open Access Publishing Agreement."
"SSD is a classical single-stage object detection algorithm, which predicts by generating different scales of feature maps on different convolutional layers. However, due to the problems of its insufficient non-linearity and the lack of semantic information in the shallow feature maps, as well as the fact that small objects contain few pixels, the detection accuracy of small objects is significantly worse than that of large- and medium-scale objects. Considering the above problems, we propose a novel object detector, self-attention combined feature fusion-based SSD for small object detection (SAFF-SSD), to boost the precision of small object detection. In this work, a novel self-attention module called the Local Lighted Transformer block (2L-Transformer) is proposed and is coupled with EfficientNetV2-S as our backbone for improved feature extraction. CSP-PAN topology is adopted as the detection neck to equip feature maps with both low-level object detail features and high-level semantic features, improving the accuracy of object detection and having a clear, noticeable and definitive effect on the detection of small targets. Simultaneously, we substitute the normalized Wasserstein distance (NWD) for the commonly used Intersection over Union (IoU), which alleviates the problem wherein the extensions of IoU-based metrics are very sensitive to the positional deviation of the small objects. The experiments illustrate the promising performance of our detector on many datasets, such as Pascal VOC 2007, TGRS-HRRSD and AI-TOD. © 2023 by the authors."
"In 2015, the United States put forward the concept of precision medicine, which changed medical treatment from ""one size fits all"" to personalization, and paid more attention to personalization and drug customization. In the same year, Spritam®, the world's first 3D printed tablet, was in the market, marking the emerging pharmaceutical 3D printing technology was recognized by regulatory authorities, and it also provided a new way for drug customization. 3D printing technology has strong interdisciplinary and high flexibility, which puts forward higher requirements for pharmaceutical staffs. With the development of artificial intelligence (AI), modern society can perform various tasks, such as disease diagnosis and robotic surgery, with superhuman speed and intelligence. As a major AI technology, machine learning (ML) has been widely used in many aspects of 3D printing drug, accelerating the research and development, production, and clinical application, and promoting the new process of global personalized medicine and industry 4.0. This paper introduces the basic concepts and main classifications of 3D printing drug, non-AI drug optimization technology and ML. It focuses on the analysis of the research progress of ML in 3D printing drug, and elucidates how AI can empower the intelligent level of 3D printing drug in pre-processing, printing, and post-processing process. It provides a new idea for accelerating the development of 3D printed drug. © 2023, Chinese Pharmaceutical Association. All rights reserved."
"The measurement of physiologic pressure helps diagnose and prevent associated health complications. From typical conventional methods to more complicated modalities, such as the estimation of intracranial pressures, numerous invasive and noninvasive tools that provide us with insight into daily physiology and aid in understanding pathology are within our grasp. Currently, our standards for estimating vital pressures, including continuous BP measurements, pulmonary capillary wedge pressures, and hepatic portal gradients, involve the use of invasive modalities. As an emerging field in medical technology, artificial intelligence (AI) has been incorporated into analyzing and predicting patterns of physiologic pressures. AI has been used to construct models that have clinical applicability both in hospital settings and at-home settings for ease of use for patients. Studies applying AI to each of these compartmental pressures were searched and shortlisted for thorough assessment and review. There are several AI-based innovations in noninvasive blood pressure estimation based on imaging, auscultation, oscillometry and wearable technology employing biosignals. The purpose of this review is to provide an in-depth assessment of the involved physiologies, prevailing methodologies and emerging technologies incorporating AI in clinical practice for each type of compartmental pressure measurement. We also bring to the forefront AI-based noninvasive estimation techniques for physiologic pressure based on microwave systems that have promising potential for clinical practice. © 2023 by the authors."
[No abstract available]
"Advances in machine learning and artificial intelligence (AI) techniques bring new opportunities to numerous intractable tasks for operation and control in modern electric distribution systems. Nevertheless, AI applications for such grids as cyber-physical systems encounter multifaceted challenges, e.g., high requirements for the quality and quantity of training data, data efficiency, physical inconsistency, interpretability, and privacy concerns. This paper provides a systematic overview of the state-of-the-art AI methodologies in the post-pandemic era, represented by transfer learning, deep attention mechanism, graph learning, and their combination with reinforcement learning and physics-guided neural networks. Dedicated research efforts on harnessing such recent advances, including power flow, state estimation, voltage control, topology identification, and line parameter calibration, are categorized and investigated in detail. Revolving around the characteristics of distribution system operation and integration of distributed energy resources, this paper also illuminates prospects and challenges typified by the privacy, explainability, and interpretability of such AI applications in smart grids. Finally, this paper attempts to shed light on the deeper and broader prospects in the realm of smart distribution grids by interoperating them with smart building and transportation electrification © 2023 by the authors."
"Female BRCA1/BRCA2 (=BRCA) pathogenic variants (PVs) carriers are at a substantially higher risk for developing breast cancer (BC) compared with the average risk population. Detection of BC at an early stage significantly improves prognosis. To facilitate early BC detection, a surveillance scheme is offered to BRCA PV carriers from age 25–30 years that includes annual MRI based breast imaging. Indeed, adherence to the recommended scheme has been shown to be associated with earlier disease stages at BC diagnosis, more in-situ pathology, smaller tumors, and less axillary involvement. While MRI is the most sensitive modality for BC detection in BRCA PV carriers, there are a significant number of overlooked or misinterpreted radiological lesions (mostly enhancing foci), leading to a delayed BC diagnosis at a more advanced stage. In this study we developed an artificial intelligence (AI)-network, aimed at a more accurate classification of enhancing foci, in MRIs of BRCA PV carriers, thus reducing false-negative interpretations. Retrospectively identified foci in prior MRIs that were either diagnosed as BC or benign/normal in a subsequent MRI were manually segmented and served as input for a convolutional network architecture. The model was successful in classification of 65% of the cancerous foci, most of them triple-negative BC. If validated, applying this scheme routinely may facilitate ‘earlier than early’ BC diagnosis in BRCA PV carriers. © 2023 by the authors."
"Histopathology research quickly evolves thanks to advances in whole slide imaging (WSI) and artificial intelligence (AI). However, existing WSI viewers are tailored either for clinical or research environments, but none suits both. This hinders the adoption of new methods and communication between the researchers and clinicians. The paper presents xOpat, an open-source, browser-based WSI viewer that addresses these problems. xOpat supports various data sources, such as tissue images, pathologists' annotations, or additional data produced by AI models. Furthermore, it provides efficient rendering of multiple data layers, their visual representations, and tools for annotating and presenting findings. Thanks to its modular, protocol-agnostic, and extensible architecture, xOpat can be easily integrated into different environments and thus helps to bridge the gap between research and clinical practice. To demonstrate the utility of xOpat, we present three case studies, one conducted with a developer of AI algorithms for image segmentation and two with a research pathologist. © 2023 The Authors. computer Graphics Forum published by Eurographics - The European Association for computer Graphics and John Wiley & Sons Ltd."
"It is generally accepted that the shear strength of Reinforced Concrete (RC) deep beams depends on the mechanical and geometrical parameters of the beam. The accurate estimation of shear strength is a substantial problem in engineering design. However, the prediction of shear strength in this type of beams is not very accurate. One of the relatively accurate methods for estimating shear strength of beams is Artificial Intelligence (AI) methods. Adaptive Neuro-Fuzzy Inference System (ANFIS) was presented as an AI method. In this study, the efficiency of ANFIS incorporating meta-heuristic algorithms for predicting shear strength of RC beams was investigated. Meta-heuristic algorithms were used to determine the optimum parameters of ANFIS for providing the efficient models of the prediction of the RC beam shear strength. To evaluate the accuracy of the proposed method, its results were compared with those of other methods. For this purpose, the parameters of concrete compressive strength, cross-section width, effective depth, beam length, shear span-to-depth beam ratio (a/d), as well as percentage of longitudinal and transverse reinforcement were selected as input data, and the shear strength of reinforced concrete deep beam as the output data. Here, K-fold validation method with k = 10 was used to train and test the algorithms. The results showed that the proposed model with second root mean square error of 25.968 and correlation coefficient of 0.914 is more accurate than other methods. Therefore, neural fuzzy inference system with meta-heuristic algorithms can be adopted as an efficient tool in the prediction of the shear strength of deep beams.  © University of Tehran 2022."
"Background: The factors affecting radiologists’ diagnostic determinations in artificial intelligence (AI)–assisted image reading remain underexplored. Purpose: To assess how AI diagnostic performance and reader characteristics influence detection of malignant lung nodules during AI-assisted reading of chest radiographs. Materials and Methods: This retrospective study consisted of two reading sessions from April 2021 to June 2021. Based on the first session without AI assistance, 30 readers were assigned into two groups with equivalent areas under the free-response receiver operating characteristic curve (AUFROCs). In the second session, each group reinterpreted radiographs assisted by either a high or low accuracy AI model (blinded to the fact that two different AI models were used). Reader performance for detecting lung cancer and reader susceptibility (changing the original reading following the AI suggestion) were compared. A generalized linear mixed model was used to identify the factors influencing AI-assisted detection performance, including readers’ attitudes and experiences of AI and Grit score. Results: Of the 120 chest radiographs assessed, 60 were obtained in patients with lung cancer (mean age, 67 years ± 12 [SD]; 32 male; 63 cancers) and 60 in controls (mean age, 67 years ± 12; 36 male). Readers included 20 thoracic radiologists (5–18 years of experience) and 10 radiology residents (2–3 years of experience). Use of the high accuracy AI model improved readers’ detection performance to a greater extent than use of the low accuracy AI model (area under the receiver operating characteristic curve, 0.77 to 0.82 vs 0.75 to 0.75; AUFROC, 0.71 to 0.79 vs 0.7 to 0.72). Readers who used the high accuracy AI showed a higher susceptibility (67%, 224 of 334 cases) to changing their diagnosis based on the AI suggestions than those using the low accuracy AI (59%, 229 of 386 cases). Accurate readings at the first session, correct AI suggestions, high accuracy Al, and diagnostic difficulty were associated with accurate AI-assisted readings, but readers’ characteristics were not. Conclusion: An AI model with high diagnostic accuracy led to improved performance of radiologists in detecting lung cancer on chest radiographs and increased radiologists’ susceptibility to AI suggestions. © RSNA, 2023."
"Artificial intelligence (AI) will play an important role in realizing maritime autonomous surface ships (MASSs). However, as a double-edged sword, this new technology brings forth new threats. The purpose of this study is to raise awareness among stakeholders regarding the potential security threats posed by AI in MASSs. To achieve this, we propose a hypothetical attack scenario in which a clean-label poisoning attack was executed on an object detection model, which resulted in boats being misclassified as ferries, thus preventing the detection of pirates approaching a boat. We used the poison frog algorithm to generate poisoning instances, and trained a YOLOv5 model with both clean and poisoned data. Despite the high accuracy of the model, it misclassified boats as ferries owing to the poisoning of the target instance. Although the experiment was conducted under limited conditions, we confirmed vulnerabilities in the object detection algorithm. This misclassification could lead to inaccurate AI decision making and accidents. The hypothetical scenario proposed in this study emphasizes the vulnerability of object detection models to clean-label poisoning attacks, and the need for mitigation strategies against security threats posed by AI in the maritime industry. © 2023 by the authors."
"In this article, the authors address some of the most pressing issues that stem from the relationship between the technological advancements of the twenty-first century and legal regulation. The development of neurotechnology and artificial intelligence (AI), while offering considerable opportunities for the betterment of social life, also poses unprecedented risks. These challenges manifest in a wide variety of topics. Areas such as human rights treaties, antitrust law, property law, and labor law are affected by these developments. The risks associated with the unregulated use of neurotechnology and AI do not cease at the sectorial stage. Some of the values upon which current democratic systems and governance models are built could be equally threatened. In anticipation of the harming potential of unmitigated technological advances, some governments and international institutions have enacted legal provisions to regulate the current digital landscape. These normative instruments, including the Chilean Constitutional Amendment and European Charts of Digital Rights, are also analyzed in the following pages. The purpose of this article is not purely descriptive © Indiana University Maurer School of Law."
"As internet traffic grows daily, so does the need to protect it. Network security protects data from unauthorized access and ensures their confidentiality and integrity. Steganography is the practice and study of concealing communications by inserting them into seemingly unrelated data streams (cover media). Investigating and adapting machine learning models in digital image steganalysis is becoming more popular. It has been demonstrated that steganography techniques used within such a framework perform more securely than do techniques using hand-crafted pieces. This work was carried out to investigate and examine machine learning methods’ critical contributions and beneficial roles. Machine learning is a field of artificial intelligence (AI) that provides the ability to learn without being explicitly programmed. Steganalysis is considered a classification problem that can be addressed by employing machine learning techniques and recent deep learning tools. The proposed ensemble model had four models (convolution neural networks (CNNs), Inception, AlexNet, and Resnet50), and after evaluating each model, the system voted on the best model for detecting stego images. Since active steganalysis is a classification problem that may be solved using active deep learning tools and modern machine learning methods, this paper’s major goal was to analyze deep learning algorithms’ vital roles and main contributions. The evaluation shows how to successfully detect images that contain a steganography algorithm that hides data in images. Thus, it suggests which algorithms work best, which need improvement, and which are easier to identify. © 2023 by the authors."
"Automatic natural language processing and, in particular, machine translation offer an enormous potential for Spanish and other languages spoken in Spain. For the last decade, public authorities have set policy objectives in artificial intelligence (AI) and languages, which are now endowed with over 1.1 billion euros in the Strategic Projects for Economic Recovery and Transformation (PERTE) ""New Language Economy"". A key factor is the (public) generation of infrastructures, resources, databases and, above all, language corpora that feed AI and other language technologies developed particularly by the private sector. EU Law tends towards flexibility and open use of these language resources. However, under intellectual property and data reuse regulations, there is no obligation to open these resources and make them available. On the other hand, the developers have the sui generis right to not allow data mining or other processing without their authorisation. Regulation is criticised for being insufficient. Given this situation, it is essential to choose permissive licences, such as those of the Meta-Share ecosystem, which has potential for extrapolation to Spanish corpora. Lastly, the key elements for planning and adopting models for language resource use and sustainability in Spain are discussed. © 2023 Escola d'Administracio Publica de Catalunya. All rights reserved."
"Background: Body composition data have been limited to adults with disease or older age. The prognostic impact in otherwise asymptomatic adults is unclear. Purpose: To use artificial intelligence–based body composition metrics from routine abdominal CT scans in asymptomatic adults to clarify the association between obesity, liver steatosis, myopenia, and myosteatosis and the risk of mortality. Materials and Methods: In this retrospective single-center study, consecutive adult outpatients undergoing routine colorectal cancer screening from April 2004 to December 2016 were included. Using a U-Net algorithm, the following body composition metrics were extracted from low-dose, noncontrast, supine multidetector abdominal CT scans: total muscle area, muscle density, subcutaneous and visceral fat area, and volumetric liver density. Abnormal body composition was defined by the presence of liver steatosis, obesity, muscle fatty infiltration (myosteatosis), and/or low muscle mass (myopenia). The incidence of death and major adverse cardiovascular events were recorded during a median follow-up of 8.8 years. Multivariable analyses were performed accounting for age, sex, smoking status, myosteatosis, liver steatosis, myopenia, type 2 diabetes, obesity, visceral fat, and history of cardiovascular events. Results: Overall, 8982 consecutive outpatients (mean age, 57 years ± 8 [SD]; 5008 female, 3974 male) were included. Abnormal body composition was found in 86% (434 of 507) of patients who died during follow-up. Myosteatosis was found in 278 of 507 patients (55%) who died (15.5% absolute risk at 10 years). Myosteatosis, obesity, liver steatosis, and myopenia were associated with increased mortality risk (hazard ratio [HR]: 4.33 [95% CI: 3.63, 5.16], 1.27 [95% CI: 1.06, 1.53], 1.86 [95% CI: 1.56, 2.21], and 1.75 [95% CI: 1.43, 2.14], respectively). In 8303 patients (excluding 679 patients without complete data), after multivariable adjustment, myosteatosis remained associated with increased mortality risk (HR, 1.89 [95% CI: 1.52, 2.35]; P < .001). Conclusion: Artificial intelligence–based profiling of body composition from routine abdominal CT scans identified myosteatosis as a key predictor of mortality risk in asymptomatic adults. © RSNA, 2023."
[No abstract available]
"A key aim of this paper is to explore how our professional tasks as geoscientists and petroleum engineers can be completed more effectively making use of tools powered by artificial intelligence (AI), offered in commercial platforms now readily available to individual users. This paper intends to provide some guidance, but at the same time does not claim to be comprehensive or conclusive in any way. The paper presents a utility assessment from the research and teaching vantage points of two professors and one student, from geosciences and petroleum engineering departments. After a brief overview of the new technologies, some key questions raised include: How can one assess originality of class papers by students and research papers by their professors? How will the contribution of intelligent devices be acknowledged? Will the presentation of conference papers by author avatars be accepted by the organising committee?. © 2023 EAGE Publishing BV. All rights reserved."
"One of the essential factors in maintaining environmental sustainability is to reduce the harmful effects of carbon dioxide (CO2) emissions. This can be performed either by reducing the emissions themselves or capturing and storing the emitted CO2. This work studies the solubility of carbon dioxide in the capturing solvent, which plays a crucial role in the effectiveness and cost-efficiency of carbon capture and storage (CCS). Therefore, the study aims to enhance the solubility of CO2 by integrating artificial intelligence (AI) and modern optimization. Accordingly, this study consists of two consecutive stages. In the first stage, an adaptive neuro-fuzzy inference system (ANFIS) model as an AI tool was developed based on experimental data. The mol fraction was targeted as the model’s output in terms of three operating parameters; the concentration of tetrabutylphosphonium methanesulfonate [TBP][MeSO3], temperature, and pressure of CO2. The operating ranges are (2–20 wt%), (30–60 °C), and (2–30 bar), respectively. Based on the statistical measures of the root mean squared error (RMSE) and the predicted R2, the ANFIS model outperforms the traditional analysis of variance (ANOVA) modeling technique, where the resulting values were found to be 0.126 and 0.9758 for the entire samples, respectively. In the second stage, an improved grey wolf optimizer (IGWO) was utilized to determine the optimal operating parameters that increase the solubility of CO2. The optimal values of the three operating parameters that improve the CO2 solubility were found to be 3.0933 wt%, 40.5 °C, and 30 bar, respectively. With these optimal values, the collaboration between the ANFIS and IGWO produced an increase of 13.4% in the mol fraction compared to the experimental data and the response surface methodology. To demonstrate the efficacy of IGWO, the obtained results were compared to the results of four competitive optimization techniques. The comparison showed that the IGWO demonstrates superior performance. Overall, this study provided a cost-efficient approach based on AI and modern optimization to enhance CO2 solubility in CCS. © 2023 by the author."
[No abstract available]
"The aim of this study was to evaluate the fertility response of dairy cows with anovulation type I on repeated low doses of GnRH agonist buserelin. The study was conducted on 83 anovulatory and 60 cyclic Polish Holstein Friesian cows. Anovulation type I was defined as small ovaries with follicles of ≤ 5 mm in diameter and without corpus luteum on two examinations in a 7-10 day interval between 50-60 days after parturition. Cows from the experimental group (n=58) received 0.4 μg of buserelin i.m. once a day for 5 consecutive days. Cows from the negative control group (n = 25) received saline. Sixty cyclic cows receiving no treatment served as positive controls. Intervals from calving to estrus and from calving to conception, pregnancy rate 30-35 days and 260 days after AI, and pregnancy loss were calculated. The anovulatory cows had a substantially prolonged calving to conception interval, decreased pregnancy rate and increased pregnancy loss and culling rate compared to cyclic herd mates. The average calving to conception interval was significantly (p⟨0.05) shorter in treated cows compared to non-treated anovulatory cows (153.7 days vs 209.3 days). In conclusion, repeated low doses of GnRH analogue buserelin led to a significant shortening of calving to conception interval. More clinical trials are needed to determine the practical usefulness of this method for the treatment of anovulation type I in dairy cows. Copyright© by the Polish Academy of Sciences."
"Coronaviruses are a well-established and deadly group of viruses that cause illness in both humans and animals. The novel type of this virus group, named COVID-19, was firstly reported in December 2019, and, with the passage of time, coronavirus has spread to almost all parts of the world. Coronavirus has been the cause of millions of deaths around the world. Furthermore, many countries are struggling with COVID-19 and have experimented with various kinds of vaccines to eliminate the deadly virus and its variants. This survey deals with COVID-19 data analysis and its impact on human social life. Data analysis and information related to coronavirus can greatly help scientists and governments in controlling the spread and symptoms of the deadly coronavirus. In this survey, we cover many areas of discussion related to COVID-19 data analysis, such as how artificial intelligence, along with machine learning, deep learning, and IoT, have worked together to fight against COVID-19. We also discuss artificial intelligence and IoT techniques used to forecast, detect, and diagnose patients of the novel coronavirus. Moreover, this survey also describes how fake news, doctored results, and conspiracy theories were spread over social media sites, such as Twitter, by applying various social network analysis and sentimental analysis techniques. A comprehensive comparative analysis of existing techniques has also been conducted. In the end, the Discussion section presents different data analysis techniques, provides future directions for research, and suggests general guidelines for handling coronavirus, as well as changing work and life conditions. © 2023 by the authors."
"Early in the history of the field of artificial intelligence (AI), a paradigm known as microworlds emerged in which researchers constructed computer simulations of aspects of the real world from which their nascent AI systems could learn. Although microworlds were ultimately abandoned, AI researchers have recently called for their return, this time borrowing from the literary genre of interactive fiction, whose forms and conventions they might use to represent the world in text for the purpose of teaching machines to speak. This confluence of literary form and scientific method invites a closer examination of the relationship between word and world in AI research. The author argues for a reading of microworlds research and of AI more broadly through the lens of literary realism and through the literary texts that comprise its data sets and from which researchers expect artificially intelligent machines to learn about the world. The question of what kind of knowledge literature represents lies at the heart of AI research and thus presents an opportunity for a deeper engagement between AI research and literary, game, and media studies. © 2023 Duke University Press. All rights reserved."
"Technological advancements, particularly in the field of artificial intelligence (AI) have played an increasingly important role in transforming education. More recently, ground-breaking AI applications like ChatGPT have demonstrated the potential to bring radical changes to the educational landscape due to their capability to understand complex questions, generate plausible responses and human-like writing, and assist with the completion of complex tasks. However, ChatGPT has limitations in the quality of its output, such as the inclusion of inaccurate, fabricated and biased information and the lack of critical thinking and in-depth understanding. The combinations of these capabilities and limitations along with external factors (e.g., the growing demand for personalized learning support, the irresponsible and unethical use of AI) presents a range of opportunities and challenges to the potential use of ChatGPT in education. This paper presents a thorough SWOT (strength, weakness, opportunity, threat) analysis of ChatGPT, based on which we propose how ChatGPT can be properly integrated into teaching and learning practice to harness its potential in education. © 2023 Hong Kong Bao Long Accounting And Secretarial Limited. All rights reserved."
"This article discusses the role of artificial intelligence (AI) in the design and engineering of porous inorganic nanomaterials, with a special focus on metal-organic frameworks (MOFs). MOFs are highly porous nanomaterials with a large surface area, making them ideal for various applications, including gas storage, catalysis, and drug/gene delivery. Machine learning algorithms can analyze large datasets of MOF structures and properties to identify trends and correlations, and this information can be used to predict the properties of new MOFs. AI can also optimize MOF properties for specific applications, predict the optimal synthesis conditions for a given MOF structure, and design new ligands and metal ions for MOF synthesis. Mathematical models and tools, such as molecular dynamics simulations and density functional theory calculations, can be used in conjunction with AI algorithms to improve the accuracy and efficiency of MOF synthesis. The article also explores whether AI can design a new MOF, highlighting the complex nature of the question and the different perspectives that need to be considered. © 2023 The Authors. Clinical and Translational Discovery published by John Wiley & Sons Australia, Ltd on behalf of Shanghai Institute of Clinical Bioinformatics."
"The emergence of increasingly powerful AI technologies calls for the design and development of K-12 AI literacy curricula that can support students who will be entering a profoundly changed labor market. However, developing, implementing, and scaling AI literacy curricula poses significant challenges. It will be essential to develop a robust, evidence-based AI education research foundation that can inform AI literacy curriculum development. Unlike K-12 science and mathematics education, there is not currently a research foundation for K-12 AI education. In this article we provide a component-based definition of AI literacy, present the need for implementing AI literacy education across all grade bands, and argue for the creation of research programs across four areas of AI education: (1) K-12 AI Learning & Technology; (2) K-12 AI Education Integration into STEM, Language Arts, and Social Science Education; (3) K-12 AI Professional Development for Teachers and Administrators; and (4) K-12 AI Assessment. © 2023, The Author(s)."
[No abstract available]
"We introduce an end-to-end computational framework that allows for hyperparameter optimization using the DeepHyper library, accelerated model training, and interpretable AI inference. The framework is based on state-of-the-art AI models including CGCNN, PhysNet, SchNet, MPNN, MPNN-transformer, and TorchMD-NET. We employ these AI models along with the benchmark QM9, hMOF, and MD17 datasets to showcase how the models can predict user-specified material properties within modern computing environments. We demonstrate transferable applications in the modeling of small molecules, inorganic crystals and nanoporous metal organic frameworks with a unified, standalone framework. We have deployed and tested this framework in the ThetaGPU supercomputer at the Argonne Leadership Computing Facility, and in the Delta supercomputer at the National Center for Supercomputing Applications to provide researchers with modern tools to conduct accelerated AI-driven discovery in leadership-class computing environments. We release these digital assets as open source scientific software in GitLab, and ready-to-use Jupyter notebooks in Google Colab. © 2023 The Author(s). Published by IOP Publishing Ltd."
"This review investigates the opportunities and challenges of interdisciplinary research in upper limb prosthetic (ULP) socket design and manufacturing, which is crucial for improving the lives of individuals with limb loss. By integrating various disciplines, such as engineering, materials science, biomechanics, and health care, with emerging technologies such as 3D printing, artificial intelligence (AI), and virtual reality (VR), interdisciplinary collaboration can foster innovative solutions tailored to users’ diverse needs. Despite the immense potential, interdisciplinary research faces challenges in effective communication, collaboration, and evaluation. This review analyses pertinent case studies and discusses the implications of interdisciplinary research, emphasizing the importance of fostering a shared understanding, open communication, and institutional innovation. By examining technological advancements, user satisfaction, and prosthetic device usage in various interdisciplinary research examples, invaluable insights and direction for researchers and professionals seeking to contribute to this transformative field are provided. Addressing the challenges and capitalizing on the opportunities offered by interdisciplinary research can significantly improve upper limb prosthetic socket design and manufacturing, ultimately enhancing the quality of life for users worldwide. © 2023 by the authors."
"The criticality of sustainable development to control the unprecedented consequences of climate change is clear. A vital element in launching sustainability projects is financing, especially for projects by small and medium enterprises. The first and crucial step to offering financing services for sustainable development is to identify and evaluate promising projects. The current practice to accomplish this step heavily depends on subject-matter expertise and professional networks. The current practice also involves extensive manual document reviews and subjective decisions. Therefore, existing methods are time-consuming, inefficient, and not scalable. This study proposes an automated system to identify potential sustainability projects for financing services using Artificial Intelligence (AI). The proposed method uses web crawlers and text mining solutions, including Natural Language Processing (NLP), to search the Internet, analyze text data, evaluate the information quantitatively, and identify potential sustainability projects for financing services. The proposed method was implemented and empirically assessed. The results indicate that the AI-enhanced system is able to identify and prioritize potential sustainability projects with 87% accuracy. The outcomes of this study will help financial experts and decision-makers take advantage of the information available on the Internet efficiently to improve the existing methods for identifying potential projects for financing services. © 2023 by the authors."
[No abstract available]
A status update on applying generative AI to synthetic data generation.
"The identification of a biomarker that is response predictive could offer a solution for the stratification of the treatment of head and neck cancers (HNC) in the context of high recurrence rates, especially those associated with loco-regional failure. Delta (Δ) radiomics, a concept based on the variation of parameters extracted from medical imaging using artificial intelligence (AI) algorithms, demonstrates its potential as a predictive biomarker of treatment response in HNC. The concept of image-guided radiotherapy (IGRT), including computer tomography simulation (CT) and position control imaging with cone-beam-computed tomography (CBCT), now offers new perspectives for radiomics applied in radiotherapy. The use of Δ features of texture, shape, and size, both from the primary tumor and from the tumor-involved lymph nodes, demonstrates the best predictive accuracy. If, in the case of treatment response, promising Δ radiomics results could be obtained, even after 24 h from the start of treatment, for radiation-induced xerostomia, the evaluation of Δ radiomics in the middle of treatment could be recommended. The fused models (clinical and Δ radiomics) seem to offer benefits, both in comparison to the clinical model and to the radiomic model. The selection of patients who benefit from induction chemotherapy is underestimated in Δ radiomic studies and may be an unexplored territory with major potential. The advantage offered by “in house” simulation CT and CBCT favors the rapid implementation of Δ radiomics studies in radiotherapy departments. Positron emission tomography (PET)-CT Δ radiomics could guide the new concepts of dose escalation on radio-resistant sub-volumes based on radiobiological criteria, but also guide the “next level” of HNC adaptive radiotherapy (ART). © 2023 by the authors."
"In recent years, artificial intelligence (AI) technology has promoted the development of electroencephalogram (EEG) emotion recognition. However, existing methods often overlook the computational cost of EEG emotion recognition, and there is still room for improvement in the accuracy of EEG emotion recognition. In this study, we propose a novel EEG emotion recognition algorithm called FCAN–XGBoost, which is a fusion of two algorithms, FCAN and XGBoost. The FCAN module is a feature attention network (FANet) that we have proposed for the first time, which processes the differential entropy (DE) and power spectral density (PSD) features extracted from the four frequency bands of the EEG signal and performs feature fusion and deep feature extraction. Finally, the deep features are fed into the eXtreme Gradient Boosting (XGBoost) algorithm to classify the four emotions. We evaluated the proposed method on the DEAP and DREAMER datasets and achieved a four-category emotion recognition accuracy of 95.26% and 94.05%, respectively. Additionally, our proposed method reduces the computational cost of EEG emotion recognition by at least 75.45% for computation time and 67.51% for memory occupation. The performance of FCAN–XGBoost outperforms the state-of-the-art four-category model and reduces computational costs without losing classification performance compared with other models. © 2023 by the authors."
"Recent advancements in Artificial Intelligence (AI), deep learning (DL), and computer vision have revolutionized various industrial processes through image classification and object detection. State-of-the-art Optical Character Recognition (OCR) and object detection (OD) technologies, such as YOLO and PaddleOCR, have emerged as powerful solutions for addressing challenges in recognizing textual and non-textual information on printed stickers. However, a well-established framework integrating these cutting-edge technologies for industrial applications still needs to be discovered. In this paper, we propose an innovative framework that combines advanced OCR and OD techniques to automate visual inspection processes in an industrial context. Our primary contribution is a comprehensive framework adept at detecting and recognizing textual and non-textual information on printed stickers within a company, harnessing the latest AI tools and technologies for sticker information recognition. Our experiments reveal an overall macro accuracy of 0.88 for sticker OCR across three distinct patterns. Furthermore, the proposed system goes beyond traditional Printed Character Recognition (PCR) by extracting supplementary information, such as barcodes and QR codes present in the image, significantly streamlining industrial workflows and minimizing manual labor demands. © 2023 by the authors."
"The aim was to assess the precision and accuracy of cephalometric analyses performed by artificial intelligence (AI) with and without human augmentation. Four dental professionals with varying experience levels identified 31 landmarks on 30 cephalometric radiographs twice. These landmarks were re-identified by all examiners with the aid of AI. Precision and accuracy were assessed by using intraclass correlation coefficients (ICCs) and mean absolute errors (MAEs). AI revealed the highest precision, with a mean ICC of 0.97, while the dental student had the lowest (mean ICC: 0.77). The AI/human augmentation method significantly improved the precision of the orthodontist, resident, dentist, and dental student by 3.26%, 2.17%, 19.75%, and 23.38%, respectively. The orthodontist demonstrated the highest accuracy with an MAE of 1.57 mm/°. The AI/human augmentation method improved the accuracy of the orthodontist, resident, dentist, and dental student by 12.74%, 19.10%, 35.69%, and 33.96%, respectively. AI demonstrated excellent precision and good accuracy in automated cephalometric analysis. The precision and accuracy of the examiners with the aid of AI improved by 10.47% and 27.27%, respectively. The AI/human augmentation method significantly improved the precision and accuracy of less experienced dental professionals to the level of an experienced orthodontist. © 2023 by the authors."
"Colposcopy is the gold standard diagnostic tool for identifying cervical lesions. However, the accuracy of colposcopies depends on the proficiency of the colposcopist. Machine learning algorithms using an artificial intelligence (AI) system can quickly process large amounts of data and have been successfully applied in several clinical situations. This study evaluated the feasibility of an AI system as an assistive tool for diagnosing high-grade cervical intraepithelial neoplasia lesions compared to the human interpretation of cervical images. This two-centered, crossover, double-blind, randomized controlled trial included 886 randomly selected images. Four colposcopists (two proficient and two inexperienced) independently evaluated cervical images, once with and the other time without the aid of the Cerviray AI® system (AIDOT, Seoul, Republic of Korea). The AI aid demonstrated improved areas under the curve on the localization receiver-operating characteristic curve compared with the colposcopy impressions of colposcopists (difference 0.12, 95% confidence interval, 0.10–0.14, p < 0.001). Sensitivity and specificity also improved when using the AI system (89.18% vs. 71.33%; p < 0.001, 96.68% vs. 92.16%; p < 0.001, respectively). Additionally, the classification accuracy rate improved with the aid of AI (86.40% vs. 75.45%; p < 0.001). Overall, the AI system could be used as an assistive diagnostic tool for both proficient and inexperienced colposcopists in cervical cancer screenings to estimate the impression and location of pathologic lesions. Further utilization of this system could help inexperienced colposcopists confirm where to perform a biopsy to diagnose high-grade lesions. © 2023 by the authors."
"Network security problems arise these days due to many challenges in cyberspace. The malicious attacks on installed wide networks are rapidly spreading due to their vulnerability. Therefore, the user and system information are at high risk due to network attacks. To protect networks against these attacks, Network Intrusion Detection and Prevention Systems (NIDPS) are installed on them. These NIDPS can detect malicious attacks by monitoring abnormal behavior and patterns in network traffic. These systems were mainly developed using Artificial Intelligence (AI) algorithms. These intelligent NIDPS are also able to detect the attack type while detecting network attacks. Previous studies have proposed many NIDPS for network security. However, many challenges exist so far such as limited available data for training AI algorithms, class imbalance problems, and automated selection of the most important features. These problems need to be solved first, which will lead to the precise detection of network attacks. Therefore, the proposed framework used the highly imbalanced UNSW-NB15 dataset for binary and multiclass classification of network attacks. In this framework, firstly dataset normalization is applied using standard deviation and the mean of feature columns; secondly, an Improved Salp Swarm Algorithm (ISSA) is applied for automated feature selection separately on binary and multiclass subsets. Thirdly, after applying feature selection, the SMOTE–Tomek class balancing method is applied where at least four different ML classifiers are used for binary and multiclass classification. The achieved results outperformed as compared to previous studies and improved the overall performance of NIDPS. © 2023 by the author."
"In recent years, electronic stethoscopes have been combined with artificial intelligence (AI) technology to digitally acquire heart sounds, intelligently identify valvular disease and congenital heart disease, and improve the accuracy of heart disease diagnosis. The research on AI-based intelligent stethoscopy technology mainly focuses on AI algorithms, and the commonly used methods are end-to-end deep learning algorithms and machine learning algorithms based on feature extraction, and the hot spot for future research is to establish a large standardized heart sound database and unify these algorithms for external validation; in addition, different electronic stethoscopes should also be extensively compared so that the algorithms can be compatible with different. In addition, there should be extensive comparison of different electronic stethoscopes so that the algorithms can be compatible with heart sounds collected by different stethoscopes; especially importantly, the deployment of algorithms in the cloud is a major trend in the future development of artificial intelligence. Finally, the research of artificial intelligence based on heart sounds is still in the preliminary stage, although there is great progress in identifying valve disease and congenital heart disease, they are all in the research of algorithm for disease diagnosis, and there is little research on disease severity, remote monitoring, prognosis, etc., which will be a hot spot for future research. Copyright: © 2023 The Author(s). Published by IMR Press."
"Stroke survivors often suffer from movement impairments that significantly affect their daily activities. The advancements in sensor technology and IoT have provided opportunities to automate the assessment and rehabilitation process for stroke survivors. This paper aims to provide a smart post-stroke severity assessment using AI-driven models. With the absence of labelled data and expert assessment, there is a research gap in providing virtual assessment, especially for unlabeled data. Inspired by the advances in consensus learning, in this paper, we propose a consensus clustering algorithm, PSA-NMF, that combines various clusterings into one united clustering, i.e., cluster consensus, to produce more stable and robust results compared to individual clustering. This paper is the first to investigate severity level using unsupervised learning and trunk displacement features in the frequency domain for post-stroke smart assessment. Two different methods of data collection from the U-limb datasets—the camera-based method (Vicon) and wearable sensor-based technology (Xsens)—were used. The trunk displacement method labelled each cluster based on the compensatory movements that stroke survivors employed for their daily activities. The proposed method uses the position and acceleration data in the frequency domain. Experimental results have demonstrated that the proposed clustering method that uses the post-stroke assessment approach increased the evaluation metrics such as accuracy and F-score. These findings can lead to a more effective and automated stroke rehabilitation process that is suitable for clinical settings, thus improving the quality of life for stroke survivors. © 2023 by the authors."
"We prove that the boundary of the Hall-Littlewood t-deformation of the Gelfand-Tsetlin graph is parametrized by infinite integer signatures, extending results of Gorin [23] and Cuenca [15] on boundaries of related deformed Gelfand-Tsetlin graphs. In the special case when 1/t is a prime p, we use this to recover results of Bufetov and Qiu [12] and Assiotis [1] on infinite p-adic random matrices, placing them in the general context of branching graphs derived from symmetric functions. Our methods rely on explicit formulas for certain skew Hall-Littlewood polynomials. As a separate corollary to these, we obtain a simple expression for the joint distribution of the cokernels of products A1,A2A1,A3A2A1, ⋯ of independent Haar-distributed Ai matrices overℤp, generalizing the explicit formula for the classical Cohen-Lenstra measure.  © 2022 The Author(s). Published by Oxford University Press. All rights reserved."
"To support physicians in clinical decision process on patients affected by Coronavirus Disease 2019 (COVID-19) in areas with a low vaccination rate, we devised and evaluated the performances of several machine learning (ML) classifiers fed with readily available clinical and laboratory data. Our observational retrospective study collected data from a cohort of 779 COVID-19 patients presenting to three hospitals of the Lazio-Abruzzo area (Italy). Based on a different selection of clinical and respiratory (ROX index and PaO2/FiO2 ratio) variables, we devised an AI-driven tool to predict safe discharge from ED, disease severity and mortality during hospitalization. To predict safe discharge our best classifier is an RF integrated with ROX index that reached AUC of 0.96. To predict disease severity the best classifier was an RF integrated with ROX index that reached an AUC of 0.91. For mortality prediction the best classifier was an RF integrated with ROX index, that reached an AUC of 0.91. The results obtained thanks to our algorithms are consistent with the scientific literature an accomplish significant performances to forecast safe discharge from ED and severe clinical course of COVID-19. © 2023 the author(s), published by De Gruyter, Berlin/Boston."
"Driven by technological advances from Industry 4.0, Healthcare 4.0 synthesizes medical sensors, artificial intelligence (AI), big data, the Internet of things (IoT), machine learning, and augmented reality (AR) to transform the healthcare sector. Healthcare 4.0 creates a smart health network by connecting patients, medical devices, hospitals, clinics, medical suppliers, and other healthcare-related components. Body chemical sensor and biosensor networks (BSNs) provide the necessary platform for Healthcare 4.0 to collect various medical data from patients. BSN is the foundation of Healthcare 4.0 in raw data detection and information collecting. This paper proposes a BSN architecture with chemical sensors and biosensors to detect and communicate physiological measurements of human bodies. These measurement data help healthcare professionals to monitor patient vital signs and other medical conditions. The collected data facilitates disease diagnosis and injury detection at an early stage. Our work further formulates the problem of sensor deployment in BSNs as a mathematical model. This model includes parameter and constraint sets to describe patient body characteristics, BSN sensor features, as well as biomedical readout requirements. The proposed model’s performance is evaluated by multiple sets of simulations on different parts of the human body. Simulations are designed to represent typical BSN applications in Healthcare 4.0. Simulation results demonstrate the impact of various biofactors and measurement time on sensor selections and readout performance. © 2023 by the authors."
"The applications of artificial intelligence (AI) in dementia research have garnered significant attention, prompting the planning of various research endeavors in current and future studies. The objective of this study is to provide a comprehensive overview of the research landscape regarding AI and dementia within scholarly publications and to suggest further studies for this emerging research field. A search was conducted in the Web of Science database to collect all relevant and highly cited articles on AI-related dementia research published in English until 16 May 2023. Utilizing bibliometric indicators, a search strategy was developed to assess the eligibility of titles, utilizing abstracts and full texts as necessary. The Bibliometrix tool, a statistical package in R, was used to produce and visualize networks depicting the co-occurrence of authors, research institutions, countries, citations, and keywords. We obtained a total of 1094 relevant articles published between 1997 and 2023. The number of annual publications demonstrated an increasing trend over the past 27 years. Journal of Alzheimer’s Disease (39/1094, 3.56%), Frontiers in Aging Neuroscience (38/1094, 3.47%), and Scientific Reports (26/1094, 2.37%) were the most common journals for this domain. The United States (283/1094, 25.86%), China (222/1094, 20.29%), India (150/1094, 13.71%), and England (96/1094, 8.77%) were the most productive countries of origin. In terms of institutions, Boston University, Columbia University, and the University of Granada demonstrated the highest productivity. As for author contributions, Gorriz JM, Ramirez J, and Salas-Gonzalez D were the most active researchers. While the initial period saw a relatively low number of articles focusing on AI applications for dementia, there has been a noticeable upsurge in research within this domain in recent years (2018–2023). The present analysis sheds light on the key contributors in terms of researchers, institutions, countries, and trending topics that have propelled the advancement of AI in dementia research. These findings collectively underscore that the integration of AI with conventional treatment approaches enhances the effectiveness of dementia diagnosis, prediction, classification, and monitoring of treatment progress. © 2023 by the authors."
"There are three common beliefs about the labor market: first, that increased use of artificial intelligence (AI) is going to cause large-scale unemployment in the future; second, that the postpandemic revitalization of mobility and migration would resupply the markets affected currently by the workforce shortage; and third, that job postings, under existing employment laws, do not use biased language and offer equal opportunities to job seekers. All of them are wrong.  © 1982-2012 IEEE."
[No abstract available]
"Regarding tools and systems from artificial intelligence (AI), chat-based ones from the area of generative AI have become a major focus regarding media coverage. ChatGPT and occasionally other systems (such as those from Microsoft and Google) are discussed with hundreds if not thousands of academic papers as well as newspaper articles. While various areas have considerably gone into this discussion, transportation and logistics has not yet come that far. In this paper, we explore the use of generative AI tools within this domain. More specifically, we focus on a topic related to sustainable passenger transportation, that is, the handling of disturbances in public transport when it comes to bus bunching and bus bridging. The first of these concepts is related to analyzing situations where we observe two or more buses of the same line following close to each other without being planned deliberately and the second is related to the case where buses are used to replace broken connections in other systems, such as subways. Generative AI tools seem to be able to provide meaningful entries and a lot of food for thought while the academic use may still be classified as limited. © 2023 by the author."
"Forests play an irreplaceable role in preserving soil and water, as well as realizing carbon neutrality. However, logging and urban expansion have caused widespread forest fragmentation globally, resulting in biodiversity loss and carbon emissions. Therefore, it is a prerequisite to develop a comprehensive index for evaluating the degree of forest fragmentation to propose effective policies for forest protection and restoration. In this study, a forest fragmentation comprehensive index (FFCI) was constructed through principal component analysis (PCA) based on land-use data from 2000 to 2020 in Fujian Province, composed of five commonly used landscape metrics: patch density (PD), largest patch index (LPI), mean patch area (MPA), aggregation index (AI), and division. Then, the semivariogram function and moving windows method were employed to explore the scale effect and spatiotemporal variations of FFCI. The spatial autocorrelation analysis was used to distinguish the spatial relationship of forest fragmentation, while the driving mechanisms were explored using the geographic detector (GD). The results show that the optimal scale to reflect forest fragmentation based on the semivariogram and moving window method was 3500 m. The proposed FFCI could explain more than 85% of the information for all landscape metrics, and the effectivity of FFCI was validated by urban–rural gradient and transect analysis. We also found that, despite having the highest forest coverage in China, Fujian Province has experienced severe forest fragmentation. High and medium fragmentation accounted for over 50% of all types of fragmentation, with decreasing trends in low and very low fragmentation and increasing trends in high fragmentation over time, indicating that the degree of forest fragmentation in the study area was aggravated over time. Moreover, the spatial distribution pattern of FFCI was mainly high–high clusters and low–low clusters, showing a decreasing trend year by year. The areas with high fragmentation were mainly distributed in the urban center of coastal cities, while the internal cities in western and central regions had a relatively low degree of fragmentation. Additionally, the spatial differentiation in the variation in FFCI was mainly influenced by elevation, slope, and nighttime light intensity. The superimposed impact of two factors on the variation in FFCI was greater than the impact of individual factors. These results provide an effective approach for assessing the degree of forest fragmentation and offer scientific support for mitigating forest fragmentation. © 2023 by the authors."
[No abstract available]
"While e-government (referring here to the first generation of e-government) was just the simple manner of delivering public services via electronic means, e-gov 2.0 refers to the use of social media and Web 2.0 technologies in government operations and public service delivery. However, the use of the term ‘e-government 2.0’ is becoming less common as the focus shifts towards broader digital transformation initiatives that may include AI technologies, among others, such as blockchain, virtual reality, and augmented reality. In this study, we present the relatively new concept of e-government 3.0, which is built upon the principles of e-government 2.0 but refers to the use of emerging technologies (e.g., artificial intelligence) to transform the delivery of public services and improve governance. The study objective is to explore the potential of e-government 3.0 to enhance citizen participation, improve public service delivery, and increase responsiveness and compliance of administrative systems in relation to citizens by integrating emerging technologies into government operations using as a background the evolution of e-government over time. The paper analyzes the challenges faced by municipalities in responding to citizen petitions, which are a core application of local democracies. The author starts by presenting an example of an e-petition system (as in use today) and analyses anonymized data of a text corpus of petitions directed to one of the Romania municipalities. He will propose an AI model able to deal faster and more accurately with the increased number of inputs, trying to promote it to municipalities who, for some reason, are still reluctant to implement AI in their operations. The conclusions will suggest that it may be more effective to focus on improving new algorithms rather than solely on ‘old’ technologies. © 2023 by the author."
"Objective: Echocardiography (ECG) is the most common method used to diagnose heart failure (HF). However, its accuracy relies on the experience of the operator. Additionally, the video format of the data makes it challenging for patients to bring them to referrals and reexaminations. Therefore, this study used a deep learning approach to assist physicians in assessing cardiac function to promote the standardization of echocardiographic findings and compatibility of dynamic and static ultrasound data. Methods: A deep spatio-temporal convolutional model r2plus1d-Pan (trained on dynamic data and applied to static data) was improved and trained using the idea of ""regression training combined with classification application,""which can be generalized to dynamic ECG and static cardiac ultrasound views to identify HF with a reduced ejection fraction (EF < 40%). Additionally, three independent datasets containing 8976 cardiac ultrasound views and 10085 cardiac ultrasound videos were established. Subsequently, a multinational, multi-center dataset of EF was labeled. Furthermore, model training and independent validation were performed. Finally, 15 registered ultrasonographers and cardiologists with different working years in three regional hospitals specialized in cardiovascular disease were recruited to compare the results. Results: The proposed deep spatio-temporal convolutional model achieved an area under the receiveroperating characteristic curve (AUC) value of 0.95 (95% confidence interval [CI]: 0.947 to 0.953) on the training set of dynamic ultrasound data and an AUC of 1 (95% CI, 1 to 1) on the independent validation set. Subsequently, the model was applied to the static cardiac ultrasound view (validation set) with simultaneous input of 1, 2, 4, and 8 images of the same heart, with classification accuracies of 85%, 81%, 93%, and 92%, respectively. On the static data, the classification accuracy of the artificial intelligence (AI) model was comparable with the best performance of ultrasonographers and cardiologists with more than 3 working years (P = 0.344), but significantly better than the median level (P = 0.0000008). Conclusion: A new deep spatio-temporal convolution model was constructed to identify patients with HF with reduced EF accurately (< 40%) using dynamic and static cardiac ultrasound images. The model outperformed the diagnostic performance of most senior specialists. This may be the first HF-related AI diagnostic model compatible with multi-dimensional cardiac ultrasound data, and may thereby contribute to the improvement of HF diagnosis. Additionally, the model enables patients to carry ""on-the-go""static ultrasound reports for referral and reexamination, thus saving healthcare resources.  © 2023 Zeye Liu, Yuan Huang, Hang Li, Wenchao Li, Fengwen Zhang, Wenbin Ouyang, Shouzheng Wang, Zhiling Luo, Jinduo Wang, Yan Chen, Ruibing Xia, Yakun Li, Xiangbin Pan, published by De Gruyter on behalf of the SMP."
[No abstract available]
"The wave energy sector has not reached a sufficient level of maturity for commercial competitiveness, thus requiring further efforts towards optimizing existing technologies and making wave energy a viable alternative to bolster energy mixes. Usually, these efforts are supported by physical and numerical modelling of complex physical phenomena, which require extensive resources and time to obtain reliable, yet limited results. To complement these approaches, artificial-intelligence-based techniques (AI) are gaining increasing interest, given their computational speed and capability of searching large solution spaces and/or identifying key study patterns. Under this scope, this paper presents a comprehensive review on the use of computational systems and AI-based techniques to wave climate and energy resource studies. The paper reviews different optimization methods, analyses their application to extreme events and examines their use in wave propagation and forecasting, which are pivotal towards ensuring survivability and assessing the local wave operational conditions, respectively. The use of AI has shown promising results in improving the efficiency, accuracy and reliability of wave predictions and can enable a more thorough and automated sweep of alternative design solutions, within a more reasonable timeframe and at a lower computational cost. However, the particularities of each case study still limit generalizations, although some application patterns have been identified—such as the frequent use of neural networks. © 2023 by the authors."
"Featured Application: Enhancing Clinical Diagnosis through the Integration of Deep Learning Techniques in Medical Image Recognition. This comprehensive review highlights the transformative potential of deep learning techniques in medical image recognition, with a focus on applications that can improve the accuracy and efficiency of clinical diagnosis. By examining a range of approaches, including image enhancement, multimodal medical image fusion, and intelligent image recognition tailored to specific anatomical structures, this study demonstrates the effectiveness of advanced neural network designs in extracting multilevel features from medical images. The featured application emphasizes the importance of addressing key challenges, such as data quality, model interpretability, generalizability, and computational resource requirements. By exploring future directions in data accessibility, active learning, explainable AI, model robustness, and computational efficiency, this study paves the way for the successful integration of AI in clinical practice, ultimately leading to enhanced patient care. Through this featured application, the potential of deep learning techniques to revolutionize medical imaging is brought to the forefront, demonstrating how these advanced methods can support clinicians in making more informed diagnostic decisions, ultimately improving patient outcomes and the overall quality of healthcare. The primary objective of this study is to provide an extensive review of deep learning techniques for medical image recognition, highlighting their potential for improving diagnostic accuracy and efficiency. We systematically organize the paper by first discussing the characteristics and challenges of medical imaging techniques, with a particular focus on magnetic resonance imaging (MRI) and computed tomography (CT). Subsequently, we delve into direct image processing methods, such as image enhancement and multimodal medical image fusion, followed by an examination of intelligent image recognition approaches tailored to specific anatomical structures. These approaches employ various deep learning models and techniques, including convolutional neural networks (CNNs), transfer learning, attention mechanisms, and cascading strategies, to overcome challenges related to unclear edges, overlapping regions, and structural distortions. Furthermore, we emphasize the significance of neural network design in medical imaging, concentrating on the extraction of multilevel features using U-shaped structures, dense connections, 3D convolution, and multimodal feature fusion. Finally, we identify and address the key challenges in medical image recognition, such as data quality, model interpretability, generalizability, and computational resource requirements. By proposing future directions in data accessibility, active learning, explainable AI, model robustness, and computational efficiency, this study paves the way for the successful integration of AI in clinical practice and enhanced patient care. © 2023 by the authors."
[No abstract available]
[No abstract available]
"Bicuspid Aortic Valves (BAV) are associated with an increased incidence of thoracic aortic aneurysms (TAA). TAA are a common aortic pathology characterized by enlargement of the aortic root and/or ascending aorta, and may become life threatening when left untreated. Typically occurring as the sole pathology in a patient, TAA are largely asymptomatic. However, in some instances, they are accompanied by aortic valve (AV) diseases: either congenital BAV or acquired in the form of Aortic Insufficiency (AI) or aortic stenosis (AS). When TAA are associated with aortic valve disease, determining an accurate and predictable prognosis becomes especially challenging. Patients with AV disease and concomitant TAA lack a widely accepted diagnostic approach, one that integrates our knowledge on aortic valve pathophysiology and encompasses multi-modality imaging approaches. This review summarizes the most recent scientific knowledge regarding the association between AV diseases (BAV, AI, AS) and ascending aortopathies (dilatation, aneurysm, and dissection). We aimed to pinpoint the gaps in monitoring practices and prediction of disease progression in TAA patients with concomitant AV disease. We propose that a morphological and functional analysis of the AV with multi-modality imaging should be included in aortic surveillance programs. This strategy would allow for improved risk stratification of these patients, and possibly new AV phenotypic-specific guidelines with more vigilant surveillance and earlier prophylactic surgery to improve patient outcomes. Copyright: © 2023 The Author(s). Published by IMR Press."
"Precipitation is a common natural phenomenon that plays an important role in climate regulation. At the same time, due to the spatial and temporal heterogeneity of precipitation, natural disasters such as floods and droughts often occur, which can seriously damage the economy and livelihood of humans. We use AI technology to study the attenuation caused by precipitation during the propagation of radio wave, trying to find the precipitation information from different attenuation, and then achieve the purpose of precipitation monitoring. The experimental results show that the neural network successfully learns the attenuation characteristics using the communication base station as the signal source and the standard meteorological precipitation information at the time of data collection as the label, and the experimental accuracy is around 95%. This suggests that the use of radio links combined with AI for rainfall monitoring is a novel approach to rainfall measurement and has some research implications. © 2023 Chinese Research Institute of Radiowave Propagation. All rights reserved."
"What the Houston Court qualified as “mysterious ‘black box’ impervious to challenge” was in practice a sophisticated software of many layers of calculations, which rated teachers’ effectiveness to make employment decisions. In the European Union, a system as such would fall under the Proposal for AI Regulation of 2021, which qualifies AI models in education and vocational training as “high-risk” systems. Automated decision-making systems (ADM systems), AI-driven or not, are being increasingly used by governments in public education for different purposes, such as handling applications for undergraduate admission or profiling students and teachers to assess their performance. Across cases and jurisdictions, there is growing evidence of how the use of ADM systems in the education sector is becoming quite problematic: arbitrary assignment of teaching posts in mobility procedures, undue barriers to access undergraduate studies, and frequent lack of transparency in their implementation and decisions. This Article discusses how Freedom of Information Act (FOIA) regimes may contribute to rendering governments’ ADM systems (AI-driven or not) accountable. The analysis of the FOIA cases (Parcoursoup saga in France, MIUR in Italy, and Ofqual in the United Kingdom) shows to what extent decisions granting access to the source code, functional and technical specifications, or third-party audits allow public scrutiny of ADM systems, detection of their pathologies, and better understanding of their adverse impacts on rights and freedoms, individual or collective. This Article also addresses the constitutional value of the right of access to public records (Parcoursup), and the importance of proactive and mandatory public dissemination to ensure traceability, transparency, and accountability of the ADM systems for FOIA purposes. In this sense, some legal initiatives across jurisdictions (Canada, France, Spain, United States, European Union) enhancing transparency and accountability of algorithmic systems will be examined. © Indiana University Maurer School of Law."
"The use of artificial intelligence (AI) is becoming more prevalent across industries such as healthcare, finance, and transportation. Artificial intelligence is based on the analysis of large datasets and requires a continuous supply of high-quality data. However, using data for AI is not without challenges. This paper comprehensively reviews and critically examines the challenges of using data for AI, including data quality, data volume, privacy and security, bias and fairness, interpretability and explainability, ethical concerns, and technical expertise and skills. This paper examines these challenges in detail and offers recommendations on how companies and organizations can address them. By understanding and addressing these challenges, organizations can harness the power of AI to make smarter decisions and gain competitive advantage in the digital age. It is expected, since this review article provides and discusses various strategies for data challenges for AI over the last decade, that it will be very helpful to the scientific research community to create new and novel ideas to rethink our approaches to data strategies for AI. © 2023 by the authors."
"OBJECTIVE: The aim of this study was to summarize the computed tomography (CT) chest scanning results of COVID-19 patients, and to assess the value of artificial intelligence (AI) dynamics and quantitative analysis of lesion volume change for the evaluation of the disease outcome. PATIENTS AND METHODS: First chest CT and reexamination imaging data of 84 patients diagnosed with COVID-19 who were treated at Jiangshan Hospital of Guiyang, Guizhou Province from February 4, 2020, to February 22, 2020, were retrospectively analyzed. Distribution, location, and nature of lesions were analyzed according to the characteristics of CT imaging and COVID-19 diagnosis and treatment guidelines. Based on the results of the analysis, patients were divided into the group without abnormal pulmonary imaging, the early group, the rapid progression group, and the dissipation group. AI software was used to dynamically measure the lesion volume in the first examination and in the cases with more than two reexaminations. RESULTS: There were statistically significant differences in the age of patients between the groups (p<0.01). The first chest CT examination of the lung without abnormal imaging findings mainly occurred in young adults. Early and rapid progression was more common in the elderly, with a median age of 56 years. The ratio of the lesion to the total lung volume was 3.7 (1.4, 5.3) ml 0.1%, 15.4 (4.5, 36.8) ml 0.3%, 115.0 (44.5, 183.3) ml 3.33%, 32.6 (8.7, 98.0) ml 1.22% in the non-imaging group, early group, rapid progression group, and dissipation group, respectively. Pairwise comparison between the four groups was statistically significant (p<0.001). AI measured the total volume of pneumonia lesions and the proportion of the total volume of pneumonia lesions to predict the receiver operating characteristic (ROC) curve from early development to rapid progression, with a sensitivity of 92.10%, 96.83%, specificity of 100%, 80.56%, and the area under the curve of 0.789. CONCLUSIONS: Accurate measurement of lesion volume and volume changes by AI technology is helpful in assessing the severity and development trend of the disease. The increase in the lesion volume proportion indicates that the disease has entered a rapid progression period and is aggravated."
"Radiologic tests often contain rich imaging data not relevant to the clinical indication. Opportunistic screening refers to the practice of systematically leveraging these incidental imaging findings. Although opportunistic screening can apply to imaging modalities such as conventional radiography, US, and MRI, most attention to date has focused on body CT by using artificial intelligence (AI)-assisted methods. Body CT represents an ideal high-volume modality whereby a quantitative assessment of tissue composition (eg, bone, muscle, fat, and vascular calcium) can provide valuable risk stratification and help detect unsuspected presymptomatic disease. The emergence of ""explainable"" AI algorithms that fully automate these measurements could eventually lead to their routine clinical use. Potential barriers to widespread implementation of opportunistic CT screening include the need for buy-in from radiologists, referring providers, and patients. Standardization of acquiring and reporting measures is needed, in addition to expanded normative data according to age, sex, and race and ethnicity. Regulatory and reimbursement hurdles are not insurmountable but pose substantial challenges to commercialization and clinical use. Through demonstration of improved population health outcomes and cost-effectiveness, these opportunistic CT-based measures should be attractive to both payers and health care systems as value-based reimbursement models mature. If highly successful, opportunistic screening could eventually justify a practice of standalone ""intended"" CT screening. © RSNA, 2023."
"Data-driven seismic inversion techniques are often used for estimation of subsurface properties. Employing the acoustic or elastic wave equation, inversion starts with approximate initial values of subsurface parameters, which are typically updated in iterative fashion. Here, we propose a two-stage unsupervised machine-learning (ML) methodology for efficient and accurate seismic impedance inversion. The first stage utilizes the generalization capability of convolutional neural networks (CNN) to produce realistic estimates of the acoustic impedance (AI), whereas the second stage incorporates physics information to generate synthetic data from the subsurface AI distribution. We also add Bayesian layers to the first stage of the network to evaluate the model errors. The proposed probabilistic approach to deep learning allows one to estimate the uncertainty of the inverted parameters, which enhances the interpretability of the model. We apply the algorithm to a poststack data set generated using the CGG Hampson-Russell software. After conducting network training with a sufficient number of data points, the network is applied to the rest of the data to estimate the model parameters. The developed approach has a significant advantage over more conventional ML strategies because it produces statistically justified uncertainty maps and eliminates the need to use labeled data for training. © 2023 Geophysical Press Ltd."
"Literature, poetry, and other forms of noncommercial creative expression challenge the techno-instrumentalist approaches to language, the predictive language generation, informing NLP (large natural language processing models) such as GPT-3 or -4 as well as, more generally, generative AI (text to image, video, audio). Claims that AI systems automate and expedite creativity reflect industry and research priorities of speed, scale, optimization, and frictionlessness driving much artificial intelligence design and application. But poetry will not optimize; the creative process cannot be reduced to a prompt. Some have noted that literary creations generated or augmented by artificial intelligence at best can offer form without meaning; using a GPT creation prompted by Maya Angelou’s poem “Still I Rise” as a case study, this essay argues that NLP’s predictive language generation and what I call algorithmic ahistoricity can also, more disturbingly, render meaning senseless. In doing so, GPT-3’s literary experiments are not “failed” because they do not meet some moving target of a literary standard, nor because of technological insufficiency, but because it can make it harder for people to name and navigate their realities. The coda explores an example of AI as literary interlocutor and creative engagement beyond optimization. © 2023 by Duke University Press."
"This article examines early Cold War attempts to generate poetry using computers. Set between the end of World War II and the rise of personal computing, computer-generated poetry from this period was shaped not only by artists but also the university lab, the defensecontactor, and the corporation. Computer-generated poetry from this era often participated in the larger project of fostering public conception of the power and prestige of computers. This ethos of “post-automation poetics” was also informed by computer science experiments with computation’s linguistic-processing powers—from machine translation to early AI. This article contextualizes the computer poetry of Alison Knowles, Nanni Balestrini, and others within the scientific concerns of mathematicians like Theo Lutz and linguists like Margaret Masterman. Framed by governmental power, university funding, and corporate ambition, “post-automation poetics” engages with computation’s relevance to literary production: from Cold War mainframes to contemporary large language models (LLMs) like GPT-3. © 2023 by Duke University Press."
"Alzheimer’s disease (AD) is a neurodegenerative disorder characterized primarily by a decline in cognitive function. However, the etiopathogenesis of AD is unclear. N6-methyladenosine (m6A) is abundant in the brain, and it is interesting to explore the relationship between m6A and AD causes. In this paper, the gene expression of METTL3 and NDUFA10 were found to correlate with the Mini-mental State Examination (MMSE), which is a clinical indicator of the degree of dementia. METTL3 is involved in post-transcriptional methylation and the formation of m6A. NDUFA10 encodes the protein with NADH dehydrogenase activity and oxidoreductase activity in the mitochondrial electron transport chain. The following three characteristics were observed in this paper: 1. The lower the expression level of NDUFA10, the smaller the MMSE, and the higher the degree of dementia. 2. If the expression level of METTL3 dropped below its threshold, the patient would have a risk of AD with a probability close to 100%, suggesting a basic necessity for m6A to protect mRNA. 3. The lower the expression levels of both METTL3 and NDUFA10, the more likely the patient would suffer from AD, implying the coherence between METTL3 and NDUFA10. Regarding the above discovery, the following hypothesis is presented: METTL3 expression level is downregulated, then the m6A modification level of NDUFA10 mRNA is also decreased, thereby reducing the expression level of NDUFA10-encoded protein. Furthermore, the abnormal expression of NDUFA10 contributes to the assembly disorder of mitochondrial complex I and affects the process of the electron respiratory chain, with the consequent development of AD. In addition, to confirm the above conclusions, the AI Ant Colony Algorithm was improved to be more suitable for discovering the characteristics of AD data, and the SVM diagnostic model was applied to mine the coherent effects on AD between METTL3 and NDUFA10. In conclusion, our findings suggest that dysregulated m6A leads to altered expression of its target genes, thereby affecting AD’s development. © 2023 by the authors."
"This paper argues for actual and legal regulation of artificial intelligence (AI) and facial recognition. These new technologies represent great opportunities to improve the welfare of societies. However, some of their uses can also enhance discrimination and, eventually, lead to violence. From a comparative approach (examining the European Union and Brazil), we address the current and future aspects of facial regulation, AI, and personal data. This paper shows that regulation is relevant to protect the rule of law, free markets, and individual freedoms. It also examines the looming risks unfolding from the unregulated uses of new technologies. Our concept of “Data Necropolitics” defines a predatory form of digital governance that exploits and discriminates against vulnerable populations. © Indiana University Maurer School of Law."
"As more and more AI agents are used in practice, it is time to think about how to make these agents fully autonomous so that they can (1) learn by themselves continually in a self-motivated and self-initiated manner rather than being retrained offline periodically on the initiation of human engineers and (2) accommodate or adapt to unexpected or novel circumstances. As the real-world is an open environment that is full of unknowns or novelties, the capabilities of detecting novelties, characterizing them, accommodating/adapting to them, and gathering ground-truth training data and incrementally learning the unknowns/novelties become critical in making the AI agent more and more knowledgeable, powerful and self-sustainable over time. The key challenge here is how to automate the process so that it is carried out continually on the agent's own initiative and through its own interactions with humans, other agents and the environment just like human on-the-job learning. This paper proposes a framework (called SOLA) for this learning paradigm to promote the research of building autonomous and continual learning enabled AI agents. To show feasibility, an implemented agent is also described. © 2023 The Authors. AI Magazine published by Wiley Periodicals LLC on behalf of the Association for the Advancement of Artificial Intelligence."
[No abstract available]
"Defect inspection is important to ensure consistent quality and efficiency in industrial manufacturing. Recently, machine vision systems integrating artificial intelligence (AI)-based inspection algorithms have exhibited promising performance in various applications, but practically, they often suffer from data imbalance. This paper proposes a defect inspection method using a one-class classification (OCC) model to deal with imbalanced datasets. A two-stream network architecture consisting of global and local feature extractor networks is presented, which can alleviate the representation collapse problem of OCC. By combining an object-oriented invariant feature vector with a training-data-oriented local feature vector, the proposed two-stream network model prevents the decision boundary from collapsing to the training dataset and obtains an appropriate decision boundary. The performance of the proposed model is demonstrated in the practical application of automotive-airbag bracket-welding defect inspection. The effects of the classification layer and two-stream network architecture on the overall inspection accuracy were clarified by using image samples collected in a controlled laboratory environment and from a production site. The results are compared with those of a previous classification model, demonstrating that the proposed model can improve the accuracy, precision, and F1 score by up to 8.19%, 10.74%, and 4.02%, respectively. © 2023 by the authors."
"Maintaining and rehabilitating pavement in a timely manner is essential for preserving or improving its condition, with roughness being a critical factor. Accurate prediction of road roughness is a vital component of sustainable transportation because it helps transportation planners to develop cost-effective and sustainable pavement maintenance and rehabilitation strategies. Traditional statistical methods can be less effective for this purpose due to their inherent assumptions, rendering them inaccurate. Therefore, this study employed explainable and supervised machine learning algorithms to predict the International Roughness Index (IRI) of asphalt concrete pavement in Sri Lankan arterial roads from 2013 to 2018. Two predictor variables, pavement age and cumulative traffic volume, were used in this study. Five machine learning models, namely Random Forest (RF), Decision Tree (DT), XGBoost (XGB), Support Vector Machine (SVM), and K-Nearest Neighbor (KNN), were utilized and compared with the statistical model. The study findings revealed that the machine learning algorithms’ predictions were superior to those of the regression model, with a coefficient of determination (R2) of more than 0.75, except for SVM. Moreover, RF provided the best prediction among the five machine learning algorithms due to its extrapolation and global optimization capabilities. Further, SHapley Additive exPlanations (SHAP) analysis showed that both explanatory variables had positive impacts on IRI progression, with pavement age having the most significant effect. Providing accurate explanations for the decision-making processes in black box models using SHAP analysis increases the trust of road users and domain experts in the predictions generated by machine learning models. Furthermore, this study demonstrates that the use of explainable AI-based methods was more effective than traditional regression analysis in IRI prediction. Overall, using this approach, road authorities can plan for timely maintenance to avoid costly and extensive rehabilitation. Therefore, sustainable transportation can be promoted by extending pavement life and reducing frequent reconstruction. © 2023 by the authors."
"We conducted this Systematic Review to create an overview of the currently existing Artificial Intelligence (AI) methods for Magnetic Resonance Diffusion-Weighted Imaging (DWI)/Fluid-Attenuated Inversion Recovery (FLAIR)—mismatch assessment and to determine how well DWI/FLAIR mismatch algorithms perform compared to domain experts. We searched PubMed Medline, Ovid Embase, Scopus, Web of Science, Cochrane, and IEEE Xplore literature databases for relevant studies published between 1 January 2017 and 20 November 2022, following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses guidelines. We assessed the included studies using the Quality Assessment of Diagnostic Accuracy Studies 2 tool. Five studies fit the scope of this review. The area under the curve ranged from 0.74 to 0.90. The sensitivity and specificity ranged from 0.70 to 0.85 and 0.74 to 0.84, respectively. Negative predictive value, positive predictive value, and accuracy ranged from 0.55 to 0.82, 0.74 to 0.91, and 0.73 to 0.83, respectively. In a binary classification of ±4.5 h from stroke onset, the surveyed AI methods performed equivalent to or even better than domain experts. However, using the relation between time since stroke onset (TSS) and increasing visibility of FLAIR hyperintensity lesions is not recommended for the determination of TSS within the first 4.5 h. An AI algorithm on DWI/FLAIR mismatch assessment focused on treatment eligibility, outcome prediction, and consideration of patient-specific data could potentially increase the proportion of stroke patients with unknown onset who could be treated with thrombolysis. © 2023 by the authors."
"Shrimp farming has traditionally served as a crucial source of seafood and revenue for coastal countries. However, with the rapid development of society, conventional small-scale manual shrimp farming can no longer meet the increasing demand for rapid growth. As a result, it is imperative to continuously develop automation technology for efficient large-scale shrimp farming. Smart shrimp farming represents an innovative application of advanced technologies and management practices in shrimp aquaculture to expand the scale of production. Nonetheless, the use of these new technologies is not without difficulties, including the scarcity of public datasets and the high cost of labeling. In this paper, we focus on the application of advanced computer vision techniques to shrimp farming. To achieve this objective, we first establish a high-quality shrimp dataset for training various deep learning models. Subsequently, we propose a method that combines unsupervised learning with downstream instance segmentation tasks to mitigate reliance on large training datasets. Our experiments demonstrate that the method involving contrastive learning outperforms the direct fine-tuning of an instance segmentation model for shrimp in instance segmentation tasks. Furthermore, the concepts presented in this paper can extend to other fields that utilize computer vision technologies. © 2023 by the authors."
"Brain tumor (BT) is a serious issue and potentially deadly disease that receives much attention. However, early detection and identification of tumor type and location are crucial for effective treatment and saving lives. Manual diagnoses are time-consuming and depend on radiologist experts; the increasing number of new cases of brain tumors makes it difficult to process massive and large amounts of data rapidly, as time is a critical factor in patients’ lives. Hence, artificial intelligence (AI) is vital for understanding disease and its various types. Several studies proposed different techniques for BT detection and classification. These studies are on machine learning (ML) and deep learning (DL). The ML-based method requires handcrafted or automatic feature extraction algorithms; however, DL becomes superior in self-learning and robust in classification and recognition tasks. This research focuses on classifying three types of tumors using MRI imaging: meningioma, glioma, and pituitary tumors. The proposed DCTN model depends on dual convolutional neural networks with VGG-16 architecture concatenated with custom CNN (convolutional neural networks) architecture. After conducting approximately 22 experiments with different architectures and models, our model reached 100% accuracy during training and 99% during testing. The proposed methodology obtained the highest possible improvement over existing research studies. The solution provides a revolution for healthcare providers that can be used as a different disease classification in the future and save human lives. © 2023 by the authors."
"Adaptive AI for context and activity recognition remains a relatively unexplored field due to difficulty in collecting sufficient information to develop supervised models. Additionally, building a dataset for human context activities “in the wild” demands time and human resources, which explains the lack of public datasets available. Some of the available datasets for activity recognition were collected using wearable sensors, since they are less invasive than images and precisely capture a user’s movements in time series. However, frequency series contain more information about sensors’ signals. In this paper, we investigate the use of feature engineering to improve the performance of a Deep Learning model. Thus, we propose using Fast Fourier Transform algorithms to extract features from frequency series instead of time series. We evaluated our approach on the ExtraSensory and WISDM datasets. The results show that using Fast Fourier Transform algorithms to extract features performed better than using statistics measures to extract features from temporal series. Additionally, we examined the impact of individual sensors on identifying specific labels and proved that incorporating more sensors enhances the model’s effectiveness. On the ExtraSensory dataset, the use of frequency features outperformed that of time-domain features by 8.9 p.p., 0.2 p.p., 39.5 p.p., and 0.4 p.p. in Standing, Sitting, Lying Down, and Walking activities, respectively, and on the WISDM dataset, the model performance improved by 1.7 p.p., just by using feature engineering. © 2023 by the authors."
"Recently, some facilities have utilized the dual-energy subtraction (DES) technique for chest radiography to increase pulmonary lesion detectability. However, the availability of the technique is limited to certain facilities, in addition to other limitations, such as increased noise in high-energy images and motion artifacts with the one-shot and two-shot methods, respectively. The aim of this study was to develop artificial intelligence-based DES (AI–DES) technology for chest radiography to overcome these limitations. Using a trained pix2pix model on clinically acquired chest radiograph pairs, we successfully converted 130 kV images into virtual 60 kV images that closely resemble the real images. The averaged peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) between virtual and real 60 kV images were 33.8 dB and 0.984, respectively. We also achieved the production of soft-tissue- and bone-enhanced images using a weighted image subtraction process with the virtual 60 kV images. The soft-tissue-enhanced images exhibited sufficient bone suppression, particularly within lung fields. Although the bone-enhanced images contained artifacts on and around the lower thoracic and lumbar spines, superior sharpness and noise characteristics were presented. The main contribution of our development is its ability to provide selectively enhanced images for specific tissues using only high-energy images obtained via routine chest radiography. This suggests the potential to improve the detectability of pulmonary lesions while addressing challenges associated with the existing DES technique. However, further improvements are necessary to improve the image quality. © 2023 by the authors."
"Assessing water quality is crucial for improving global water resource management, particularly in arid regions. This study aims to assess and monitor the status of groundwater quality based on hydrochemical parameters and by using artificial intelligence (AI) approaches. The irrigation water quality index (IWQI) is predicted by using support vector machine (SVM) and k-nearest neighbors (KNN) classifiers in Matlab’s classification learner toolbox. The classifiers are fed with the following hydrochemical input parameters: sodium adsorption ratio (SAR), electrical conductivity (EC), bicarbonate level (HCO3), chloride concentration (Cl), and sodium concentration (Na). The proposed methods were used to assess the quality of groundwater extracted from the desertic region of Adrar in Algeria. The collected groundwater samples showed that 9.64% of samples were of very good quality, 12.05% were of good quality, 21.08% were satisfactory, and 57.23% were considered unsuitable for irrigation. The IWQI prediction accuracies of the classifiers with the standardized, normalized, and raw data were 100%, 100%, and 90%, respectively. The cubic SVM with the normalized data develops the highest prediction accuracy for training and testing samples (94.2% and 100%, respectively). The findings of this work showed that the multiple regression model and machine learning could effectively assess water quality in desert zones for sustainable water management. © 2023 by the authors."
"Objective. The digital revolution in pathology represents an invaluable resource fto optimise costs, reduce the risk of error and improve patient care, even though it is still adopted in a minority of laboratories. Barriers include concerns about initial costs, lack of confidence in using whole slide images for primary diagnosis, and lack of guidance on transition. To address these challenges and develop a programme to facilitate the introduction of digital pathology (DP) in Italian pathology departments, a panel discussion was set up to identify the key points to be considered. Methods. On 21 July 2022, an initial conference call was held on Zoom to identify the main issues to be discussed during the face-To-face meeting. The final summit was divided into four different sessions: (I) the definition of DP, (II) practical applications of DP, (III) the use of AI in DP, (IV) DP and education. Results. Essential requirements for the implementation of DP are a fully tracked and automated workflow, selection of the appropriate scanner based on the specific needs of each department, and a strong commitment combined with coordinated teamwork (pathologists, technicians, biologists, IT service and industries). This could reduce human error, leading to the application of AI tools for diagnosis, prognosis and prediction. Open challenges are the lack of specific regulations for virtual slide storage and the optimal storage solution for large volumes of slides. Conclusion. Teamwork is key to DP transition, including close collaboration with industry. This will ease the transition and help bridge the gap that currently exists between many labs and full digitisation. The ultimate goal is to improve patient care. © 2023 Pacini Editore S.p.A.. All rights reserved."
"Uncertainty estimation methods using deep learning approaches strive against separating how uncertain the state of the world manifests to us via measurement (objective end) from the way this gets scrambled with the model specification and training procedure used to predict such state (subjective means) - e.g., number of neurons, depth, connections, priors (if the model is bayesian), weight initialization, etc. This poses the question of the extent to which one can eliminate the degrees of freedom associated with these specifications and still being able to capture the objective end. Here, a novel non-parametric quantile estimation method for continuous random variables is introduced, based on the simplest neural network architecture with one degree of freedom: a single neuron. Its advantage is first shown in synthetic experiments comparing with the quantile estimation achieved from ranking the order statistics (specifically for small sample size) and with quantile regression. In real-world applications, the method can be used to quantify predictive uncertainty under the split conformal prediction setting, whereby prediction intervals are estimated from the residuals of a pre-trained model on a held-out validation set and then used to quantify the uncertainty in future predictions - the single neuron used here as a structureless ""thermometer""that measures how uncertain the pre-trained model is. Benchmarking regression and classification experiments demonstrate that the method is competitive in quality and coverage with state-of-the-art solutions, with the added benefit of being more computationally efficient. © 2023 World Scientific Publishing Company."
"This study aims to give a comprehensive analysis of customers' acceptance and use of AI gadgets and its relevant ethical issues in the tourism and hospitality business in the era of the Internet of Things. Adopting a PRISMA methodology for Systematic Reviews and Meta- Analyses, the present research reviews how tourism and hospitality scholars have conducted research on AI technology in the field of tourism and the hospitality industry. Most of the journal articles related to AI issues published in Web of Science, ScienceDirect.com and the journal websites were considered in this review. The results of this research offer a better understanding of AI implementation with roboethics to investigate AI-related issues in the tourism and hospitality industry. In addition, it provides decision-makers in the hotel industry with practical references on service innovation, participation in the design of AI devices and AI device applications, meeting customer needs, and optimising customer experience. The theoretical implications and practical interpretations are further identified.  © 2023 Zhu et al."
"This study aimed to investigate the clinical implications and prognostic value of artificial intelligence (AI)-based results for chest radiographs (CXR) in coronavirus disease 2019 (COVID-19) patients. Patients who were admitted due to COVID-19 from September 2021 to March 2022 were retrospectively included. A commercial AI-based software was used to assess CXR data for consolidation and pleural effusion scores. Clinical data, including laboratory results, were analyzed for possible prognostic factors. Total O2 supply period, the last SpO2 result, and deterioration were evaluated as prognostic indicators of treatment outcome. Generalized linear mixed model and regression tests were used to examine the prognostic value of CXR results. Among a total of 228 patients (mean 59.9 ± 18.8 years old), consolidation scores had a significant association with erythrocyte sedimentation rate and C-reactive protein changes, and initial consolidation scores were associated with the last SpO2 result (estimate −0.018, p = 0.024). All consolidation scores during admission showed significant association with the total O2 supply period and the last SpO2 result. Early changing degree of consolidation score showed an association with deterioration (odds ratio 1.017, 95% confidence interval 1.005–1.03). In conclusion, AI-based CXR results for consolidation have potential prognostic value for predicting treatment outcomes in COVID-19 patients. © 2023 by the authors."
"Objectives This study aimed to explore experiences of women with anal incontinence following a childbirth injury, and to identify areas of missed opportunities within care they received. Design This is a qualitative study involving semi-structured interviews. Setting Participants were recruited via five hospitals in the UK, and via social media adverts and communication from charity organisations. Participants Women who have experienced anal incontinence following a childbirth injury, either within 7 years of sustaining the injury, or if they identified new, or worsening symptoms of AI at the time of menopause. Main outcome measures Main outcomes are experiences of women with anal incontinence following childbirth injury, and missed opportunities within the care they received. Results The following main themes were identified: opportunities for diagnosis missed, missed opportunities for information sharing and continuity and timeliness of care. Conclusions Anal Incontinence following a childbirth injury has a profound impact on women. Lack of information and awareness both amongst women and healthcare professionals contributes to delays in accurate diagnosis and appropriate treatment. © 2023 Parsons et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"The Internet of Things (IoT) has experienced significant growth and is now a fundamental part of the next-generation Internet. Alongside improving daily life, IoT devices generate and collect vast amounts of data that can be leveraged by AI-enabled big data analytics for diverse applications. However, due to the machine-to-machine communication inherent in IoT, ensuring data security and privacy is crucial to mitigate various malicious cyber attacks, including man-in-the-middle, impersonation, and data poisoning attacks. Nevertheless, designing an efficient and adaptable IoT security framework poses challenges due to the limited computational and communication power of IoT devices, as well as their wide-ranging variety. To address these challenges, this paper proposes an Access Key Agreement (AKA) scheme called the “Reliable Device-Access Framework for the Industrial IoT (RDAF-IIoT)”. RDAF-IIoT verifies the user’s authenticity before granting access to real-time information from IIoT devices deployed in an industrial plant. Once authenticated at the gateway node, the user and IIoT device establish a session key for future encrypted communication. The security of the proposed RDAF-IIoT is validated using a random oracle model, while the Scyther tool is employed to assess its resilience against various security attacks. Performance evaluations demonstrate that the proposed scheme requires lower computational and communication costs compared to related security frameworks while providing enhanced security features. © 2023 by the author."
"PURPOSE: To implement the technical feasibility of an AI-based software prototype optimized for the detection of COVID-19 pneumonia in CT datasets of the lung and the differentiation between other etiologies of pneumonia. METHODS: This single-center retrospective case–control-study consecutively yielded 144 patients (58 female, mean age 57.72 ± 18.25 y) with CT datasets of the lung. Subgroups including confirmed bacterial (n = 24, 16.6%), viral (n = 52, 36.1%), or fungal (n = 25, 16.6%) pneumonia and (n = 43, 30.7%) patients without detected pneumonia (comparison group) were evaluated using the AI-based Pneumonia Analysis prototype. Scoring (extent, etiology) was compared to reader assessment. RESULTS: The software achieved an optimal sensitivity of 80.8% with a specificity of 50% for the detection of COVID-19; however, the human radiologist achieved optimal sensitivity of 80.8% and a specificity of 97.2%. The mean postprocessing time was 7.61 ± 4.22 min. The use of a contrast agent did not influence the results of the software (p = 0.81). The mean evaluated COVID-19 probability is 0.80 ± 0.36 significantly higher in COVID-19 patients than in patients with fungal pneumonia (p < 0.05) and bacterial pneumonia (p < 0.001). The mean percentage of opacity (PO) and percentage of high opacity (PHO ≥ −200 HU) were significantly higher in COVID-19 patients than in healthy patients. However, the total mean HU in COVID-19 patients was −679.57 ± 112.72, which is significantly higher than in the healthy control group (p < 0.001). CONCLUSION: The detection and quantification of pneumonia beyond the primarily trained COVID-19 datasets is possible and shows comparable results for COVID-19 pneumonia to an experienced reader. The advantages are the fast, automated segmentation and quantification of the pneumonia foci. © 2023 by the authors."
"Artificial intelligence (AI) plays a more and more important role in our everyday life due to the advantages that it brings when used, such as 24/7 availability, a very low percentage of errors, ability to provide real time insights, or performing a fast analysis. AI is increasingly being used in clinical medical and dental healthcare analyses, with valuable applications, which include disease diagnosis, risk assessment, treatment planning, and drug discovery. This paper presents a narrative literature review of AI use in healthcare from a multi-disciplinary perspective, specifically in the cardiology, allergology, endocrinology, and dental fields. The paper highlights data from recent research and development efforts in AI for healthcare, as well as challenges and limitations associated with AI implementation, such as data privacy and security considerations, along with ethical and legal concerns. The regulation of responsible design, development, and use of AI in healthcare is still in early stages due to the rapid evolution of the field. However, it is our duty to carefully consider the ethical implications of implementing AI and to respond appropriately. With the potential to reshape healthcare delivery and enhance patient outcomes, AI systems continue to reveal their capabilities. © 2023 by the authors."
"The increasing role of artificial intelligence (AI) in daily life has led to the emergence of numerous skill sets associated with AI. AI literacy, which is becoming an important citizenship competence, has become a skill that has gained importance in recent years. It is thought that the aim of social studies to prepare students as active citizens of today and the future has made AI literacy a direct subject of social studies education. However, the literature on the relationship between social studies and AI literacy is still scarce. This study aims to explore the relationship between social studies and AI literacy and to discuss the potential role of social studies in teaching AI literacy. In this direction, the concept of AI is defined, and the teaching of AI literacy in education is examined; the teaching of this literacy within the scope of social studies is discussed, and an example of a hands-on activity-supported lesson plan, which can be used in social studies classes, is presented. This study is important in terms of pointing out the potential role of social studies in AI literacy teaching. © 2023, Duzce University, Faculty of Education. All rights reserved."
"The successful implementation of Human–Robot Collaboration (HRC) has become a prominent feature of smart manufacturing environments. Key industrial requirements, such as flexibility, efficiency, collaboration, consistency, and sustainability, present pressing HRC needs in the manufacturing sector. This paper provides a systemic review and an in-depth discussion of the key technologies currently being employed in smart manufacturing with HRC systems. The work presented here focuses on the design of HRC systems, with particular attention given to the various levels of Human–Robot Interaction (HRI) observed in the industry. The paper also examines the key technologies being implemented in smart manufacturing, including Artificial Intelligence (AI), Collaborative Robots (Cobots), Augmented Reality (AR), and Digital Twin (DT), and discusses their applications in HRC systems. The benefits and practical instances of deploying these technologies are showcased, emphasizing the substantial prospects for growth and improvement in sectors such as automotive and food. However, the paper also addresses the limitations of HRC utilization and implementation and provides some insights into how the design of these systems should be approached in future work and research. Overall, this paper provides new insights into the current state of HRC in smart manufacturing and serves as a useful resource for those interested in the ongoing development of HRC systems in the industry. © 2023 by the authors."
"Echocardiography is an essential tool in diagnostic cardiology and is fundamental to clinical care. Artificial intelligence (AI) can help health care providers serving as a valuable diagnostic tool for physicians in the field of echocardiography specially on the automation of measurements and interpretation of results. In addition, it can help expand the capabilities of research and discover alternative pathways in medical management specially on prognostication. In this review article, we describe the current role and future perspectives of AI in echocardiography. ©The Author(s) 2023. Published by Baishideng Publishing Group Inc. All rights reserved."
"The research process has always involved collaboration, from brainstorming sessions to writing groups. Researchers rely on their peers for direction, encouragement, and insight. What if the collaborator was not human? What if the partner were a machine? This was the question I pondered as I started my most recent research project. As an academic librarian, I’ve always been interested in discovering innovative ways to assist researchers. As soon as I learned that AI assistants could help researchers in their work, I knew I had to give it a shot. And so, I began working with ChatGPT, a powerful generative AI tool. © 2023, Association of College and Research Libraries. All rights reserved."
"This paper reports on advances in the state-of-the-art deep learning disruption prediction models based on the Fusion Recurrent Neural Network (FRNN) originally introduced in a 2019 NATURE publication [https://doi.org/10.1038/s41586-019-1116-4]. In particular, the predictor now features not only the “disruption score,” as an indicator of the probability of an imminent disruption, but also a “sensitivity score” in real time to indicate the underlying reasons for the imminent disruption. This adds valuable physics interpretability for the deep learning model and can provide helpful guidance for control actuators now implemented into a modern plasma control system (PCS). The advance is a significant step forward in moving from modern deep learning disruption prediction to real-time control and brings novel AI-enabled capabilities relevant for application to the future burning plasma ITER system. Our analyses use large amounts of data from JET and DIII-D vetted in the earlier NATURE publication. In addition to “when” a shot is predicted to disrupt, this paper addresses reasons “why” by carrying out sensitivity studies. FRNN is accordingly extended to use more channels of information, including measured DIII-D signals such as (i) the “n1rms” signal that is correlated with the n = 1 modes with finite frequency, including neoclassical tearing mode and sawtooth dynamics; (ii) the bolometer data indicative of plasma impurity control; and (iii) “q-min”—the minimum value of the safety factor relevant to the key physics of kink modes. The additional channels and interpretability features expand the ability of the deep learning FRNN software to provide information about disruption subcategories as well as more precise and direct guidance for the actuators in a PCS. © 2023 The Authors. Contributions to Plasma Physics published by Wiley-VCH GmbH."
"With the increasing volume of collected Earth observation (EO) data, artificial intelligence (AI) methods have become state-of-the-art in processing and analyzing them. However, there is still a lack of high-quality, large-scale EO datasets for training robust networks. This paper presents AgriSen-COG, a large-scale benchmark dataset for crop type mapping based on Sentinel-2 data. AgriSen-COG deals with the challenges of remote sensing (RS) datasets. First, it includes data from five different European countries (Austria, Belgium, Spain, Denmark, and the Netherlands), targeting the problem of domain adaptation. Second, it is multitemporal and multiyear (2019–2020), therefore enabling analysis based on the growth of crops in time and yearly variability. Third, AgriSen-COG includes an anomaly detection preprocessing step, which reduces the amount of mislabeled information. AgriSen-COG comprises 6,972,485 parcels, making it the most extensive available dataset for crop type mapping. It includes two types of data: pixel-level data and parcel aggregated information. By carrying this out, we target two computer vision (CV) problems: semantic segmentation and classification. To establish the validity of the proposed dataset, we conducted several experiments using state-of-the-art deep-learning models for temporal semantic segmentation with pixel-level data (U-Net and ConvStar networks) and time-series classification with parcel aggregated information (LSTM, Transformer, TempCNN networks). The most popular models (U-Net and LSTM) achieve the best performance in the Belgium region, with a weighted F1 score of 0.956 (U-Net) and 0.918 (LSTM).The proposed data are distributed as a cloud-optimized GeoTIFF (COG), together with a SpatioTemporal Asset Catalog (STAC), which makes AgriSen-COG a findable, accessible, interoperable, and reusable (FAIR) dataset. © 2023 by the author."
"This paper provides a comprehensive and systematic review of fault localization methods based on artificial intelligence (AI) in power distribution networks described in the literature. The review is organized into several sections that cover different aspects of the methods proposed. It first discusses the advantages and disadvantages of various techniques used, including neural networks, fuzzy logic, and reinforcement learning. The paper then compares the types of input and output data generated by these algorithms. The review also analyzes the data-gathering systems, including the sensors and measurement equipment used to collect data for fault diagnosis. In addition, it discusses fault type and DG considerations, which, together with the data-gathering systems, determine the applicability range of the methods. Finally, the paper concludes with a discussion of future trends and research gaps in the field of AI-based fault location methods. Highlighting the advantages, limitations, and requirements of current AI-based methods, this review can serve the researchers working in the field of fault location in power systems to select the most appropriate method based on their distribution system and requirements, and to identify the key areas for future research. © 2023 by the authors."
"The milling machine serves an important role in manufacturing because of its versatility in machining. The cutting tool is a critical component of machining because it is responsible for machining accuracy and surface finishing, impacting industrial productivity. Monitoring the cutting tool’s life is essential to avoid machining downtime caused due to tool wear. To prevent the unplanned downtime of the machine and to utilize the maximum life of the cutting tool, the accurate prediction of the remaining useful life (RUL) cutting tool is essential. Different artificial intelligence (AI) techniques estimate the RUL of cutting tools in milling operations with improved prediction accuracy. The IEEE NUAA Ideahouse dataset has been used in this paper for the RUL estimation of the milling cutter. The accuracy of the prediction is based on the quality of feature engineering performed on the unprocessed data. Feature extraction is a crucial phase in RUL prediction. In this work, the authors considers the time–frequency domain (TFD) features such as short-time Fourier-transform (STFT) and different wavelet transforms (WT) along with deep learning (DL) models such as long short-term memory (LSTM), different variants of LSTN, convolutional neural network (CNN), and hybrid models that are a combination of CCN with LSTM variants for RUL estimation. The TFD feature extraction with LSTM variants and hybrid models performs well for the milling cutting tool RUL estimation. © 2023 by the authors."
"The present study thoroughly evaluates the most common blocking challenges faced by the federated learning (FL) ecosystem and analyzes existing state-of-the-art solutions. A system adaptation pipeline is designed to enable the integration of different AI-based tools in the FL system, while FL training is conducted under realistic conditions using a distributed hardware infrastructure. The suggested pipeline and FL system’s robustness are tested against challenges related to tool deployment, data heterogeneity, and privacy attacks for multiple tasks and data types. A representative set of AI-based tools and related datasets have been selected to cover several validation cases and distributed to each edge device to closely reflect real-world scenarios. The study presents significant outcomes of the experiments and analyzes the models’ performance under different realistic FL conditions, while highlighting potential limitations and issues that occurred during the FL process. © 2023 by the authors."
Breast cancer remains the leading cause of cancer-related deaths in women worldwide. Current screening regimens and clinical breast cancer risk assessment models use risk factors such as demographics and patient history to guide policy and assess risk. Applications of artificial intelligence methods (AI) such as deep learning (DL) and convolutional neural networks (CNNs) to evaluate individual patient information and imaging showed promise as personalized risk models. We reviewed the current literature for studies related to deep learning and convolutional neural networks with digital mammography for assessing breast cancer risk. We discussed the literature and examined the ongoing and future applications of deep learning techniques in breast cancer risk modeling. © 2023 by the authors.
"Amaranth, a pseudocereal crop which is rich in nutrients and climate resistant, can provide an opportunity to increase food security and nutritional content for the growing population. Farmers rely mainly on synthetic fertilizers to improve the quality and yield of the crop; however, this overuse harms the ecosystem. Understanding the mechanism causing this environmental deterioration is crucial for crop production and ecological sustainability. In recent years, high-throughput phenotyping using Artificial Intelligence (AI) has been thriving and can provide an effective solution for the identification of fertilizer overuse. Influenced by the strength of deep learning paradigms and IoT sensors, a novel multimodal fusion network (M2F-Net) is proposed for high-throughput phenotyping to diagnose overabundance of fertilizers. In this paper, we developed and analyzed three strategies that fuse agrometeorological and image data by assessing fusion at various stages. Initially two unimodal baseline networks were trained: Multi-Layer Perceptron (MLP) on agrometeorological data and a pre-trained Convolutional Neural Network (CNN) model DenseNet-121 on image data. With these baselines, the multimodal fusion network is developed, capable of adeptly learning from image and non-image data and the model’s performance is evaluated in terms of accuracy and Area Under Curve (AUC). Moreover, the fusion approaches that are considered outperformed the unimodal networks remarkably with 91% accuracy. From the experimental result, it is proven that incorporating agrometeorological information and images can substantially boost the classification performance for the overabundance of fertilizer. © 2023 by the authors."
"This study aimed to observe the impact of eight explainable AI (XAI) explanation techniques on user trust and satisfaction in the context of XAI-enhanced learning analytics while comparing two groups of STEM college students based on their Bologna study level, using various established feature relevance techniques, certainty, and comparison explanations. Overall, the students reported the highest trust in local feature explanation in the form of a bar graph. Additionally, master’s students presented with global feature explanations also reported high trust in this form of explanation. The highest measured explanation satisfaction was observed with the local feature explanation technique in the group of bachelor’s and master’s students, with master’s students additionally expressing high satisfaction with the global feature importance explanation. A detailed overview shows that the two observed groups of students displayed consensus in favored explanation techniques when evaluating trust and explanation satisfaction. Certainty explanation techniques were perceived with lower trust and satisfaction than were local feature relevance explanation techniques. The correlation between itemized results was documented and measured with the Trust in Automation questionnaire and Explanation Satisfaction Scale questionnaire. Master’s-level students self-reported an overall higher understanding of the explanations and higher overall satisfaction with explanations and perceived the explanations as less harmful. © 2023 by the authors."
"Although many industries have already implemented technologies based on artificial intelligence (AI) in their business, the effects of new digital solutions on customer satisfaction are not yet fully known. This study aimed to evaluate the AI-based advice implemented by an Italian start-up operating in food supplements to support customer choices. The evaluation utilized the Delphi method and a questionnaire survey. This research aimed to provide companies wishing to use AI with a preliminary evaluation criterion for improving customer satisfaction through digital approaches. Research findings indicate that AI-based advice can improve customer perspectives, such as customer satisfaction and loyalty, by providing a value-added business service, diversified for each product category. However, some mistakes have emerged, which may still be a limitation in the use of AI-based advice. Therefore, this study presents an innovative approach to evaluate the performance of digital advice in traditional sectors such as the food industry. © 2023 by the authors."
"OBJECTIVES: The aim of this retrospective study was to assess the long-term results of root remodelling with tricuspid aortic valves and the effects of concomitant cusp repair and annuloplasty. METHODS: Between October 1995 and December 2021, 684 patients with root aneurysm and regurgitant tricuspid valves were treated by root remodelling. The mean age was 56.5 [standard deviation (SD): 14] years, and 538 (77.6%) were male. Relevant aortic regurgitation was present in 68.3%. Concomitant procedures were performed in 374 patients. The long-term results were analysed. The mean follow-up of 7.2 (SD: 5.3) years (median 6.6 years); it was 95% complete (4934.4 patient-years). RESULTS: Cusp prolapse was repaired in 83%, and an annuloplasty was added in 353 instances (51.6%). Hospital mortality was 2.3%, and survival was 81.7% (SD: 1.2) and 55.7% (SD: 5.8) at 10 and 20 years; age and measurement of effective height were independent predictors for death. Freedom from Aortic insufficiency (AI) II was 90.5 (SD: 1.9) at 10 years and 76.7 (SD: 4.5) at 20 years. Cusp repair of all cusps showed a lower freedom from recurrent AI >_II at 10 years (P < 0.001). Suture annuloplasty showed a lower freedom from recurrent AI II at 10 years (P = 0.07). Freedom from reoperation was 95.5 (SD: 1.1) and 92.8 (SD: 2.8) at 10 and 20 years. The addition of an annuloplasty showed no difference (P = 0.236). Cusp repair had no effect on valve durability (P = 0.390). CONCLUSIONS: Root remodelling leads to good long-term stability. The addition of cusp repair improves the valve stability over time. The addition of suture annuloplasty improves early valve competency; it showed no effect on freedom from reoperation up to 10 years. VC The Author(s) 2023. Published by Oxford University Press on behalf of the European Association for Cardio-Thoracic Surgery. All rights reserved."
"The Valley of Death confronts industrial biotechnology with a significant challenge to the commercialization of products. Fortunately, with the integration of computation, automation and artificial intelligence (AI) technology, the industrial biotechnology accelerates to cross the Valley of Death. The Fourth Industrial Revolution (Industry 4.0) has spurred advanced development of intelligent biomanufacturing, which has evolved the industrial structures in line with the worldwide trend. To achieve this, intelligent biomanufacturing can be structured into three main parts that comprise digitalization, modeling and intellectualization, with modeling forming a crucial link between the other two components. This paper provides an overview of mechanistic models, data-driven models and their applications in bioprocess development. We provide a detailed elaboration of the hybrid model and its applications in bioprocess engineering, including strain design, process control and optimization, as well as bioreactor scale-up. Finally, the challenges and opportunities of biomanufacturing towards Industry 4.0 are also discussed. © 2023 by the authors."
"The purpose of the current investigation was to examine the effects of exercise intensity on asymmetry in pedal forces when the accumulation of fatigue is controlled for, and to assess the reliability of asymmetry outcomes during cycling. Participants completed an incremental cycling test to determine maximal oxygen consumption and the power that elicited maximal oxygen consumption (pVO2max). Participants were allotted 30 min of recovery before then cycling at 60%, 70%, 80%, and 90% of pVO2max for 3 min each, with 5 min of active recovery between each intensity. Participants returned to the laboratory on separate days to repeat all measures. A two-way repeated measures analysis of variance (ANOVA) was utilized to detect differences in power production AI at each of the submaximal exercise intensities and between Trials 1 and 2. Intraclass correlations were utilized to assess the test–retest reliability for the power production asymmetry index (AI). An ANOVA revealed no significant intensity–visit interactions for the power production AI (f = 0.835, p = 0.485, η2 = 0.077), with no significant main effects present. ICC indicated excellent reliability in the power production AI at all intensities. Exercise intensity did not appear to affect asymmetry in pedal forces, while excellent reliability was observed in asymmetry outcomes. © 2023 by the authors."
"In recent years, most educational institutions have integrated digital technologies into their teaching–learning processes. Learning Management Systems (LMS) have gained increasing popularity, particularly in higher education, due to their ability to manage teacher–student interactions. These systems store valuable information which describes students’ behaviour throughout a course. These data can be utilised to construct statistical models that represent learner behaviour within an online LMS platform. In this study, we aim to compare different sources of information and, more ambitiously, to provide insights into which source of information is most valuable for inferring student performance. The considered sets of information come from (i) the Moodle LMS; (ii) socio-economic data about students acquired from a survey; and (iii) subject marks achieved throughout the course. To determine the relevance of the incorporated information, we use artificial intelligence (AI) methods, and we report the importance measures of four state-of-the-art methods. Our findings indicate that the selected methodology is suitable for making inferences about student performance while also shedding light on model decisions through explainability. © 2023 by the authors."
"Although the principal aim of the rockfall management is to prevent rock boulders from reaching the buildings instead of the buildings resisting the boulder impacts, there usually exists a residual risk that has to be assessed, even when structural protection measurements are taken. The evaluation of the expected damage of buildings due to rockfalls using empirical data from past events is not always possible, as transferring and applying damage observations from one area to another can be unrealistic. In order to simulate potential rockfall scenarios and their damage on buildings, numerical methods can be an alternative. However due to their increased requirements in expertise and computational costs, their integration into the risk analysis is limited, and simpler tools to assess the rockfall vulnerability of buildings are needed. This paper focuses on the application of artificial intelligence AI methods for providing the expected damage of masonry walls which are subjected to rockfall impacts. First, a damage database with 672 datasets was created numerically using the particle finite element method and the finite element method. The input variables are the rock volume (VR), the rock velocity (RV), the masonry wall (t) and the masonry tensile strength (Formula presented.). The output variable is a damage index (DI) equal to the percentage of the damaged wall area. Different AI algorithms were investigated and the ANN LM 4-21-1 model was selected to optimally assess the expected wall damage. The optimum model is provided here (a) as an analytical equation and (b) in the form of contour graphs, mapping the DI value. Known the VR and the RV, the DI can be directly used as an input for the vulnerability of masonry walls into the quantitative rockfall risk assessment equation. © 2023 by the authors."
"Background: The aim of the study was to evaluate the independent prognostic role of PIK3CA activating mutations and an association between PIK3CA activating mutations and efficacy of adjuvant endocrine therapy (ET) in patients with operable invasive lobular carcinoma (ILC). Patients and methods: A single institution study of patients with early-stage ILC treated between 2003 and 2008 was performed. Clinicopathological parameters, systemic therapy exposure and outcomes (distant metastasis-free survival [DMFS] and overall survival [OS]) were collected based on presence or absence of PIK3CA activating mutation in the primary tumor determined using a quantitative polymerase chain reaction (PCR)-based assay. An association between PIK3CA mutation status and prognosis in all patient cohort was analyzed by Kaplan-Meier survival analysis, whereas an association between PIK3CA mutation and ET was analyzed in estrogen receptors (ER) and/or progesterone receptors (PR)-positive group of our patients by the Cox proportional hazards model. Results: Median age at diagnosis of all patients was 62.8 years and median follow-up time was 10.8 years. Among 365 patients, PIK3CA activating mutations were identified in 45%. PIK3CA activating mutations were not associated with differential DMFS and OS (p = 0.36 and p = 0.42, respectively). In patients with PIK3CA mutation each year of tamoxifen (TAM) or aromatase inhibitor (AI) decreased the risk of death by 27% and 21% in comparison to no ET, respectively. The type and duration of ET did not have significant impact on DMFS, however longer duration of ET had a favourable impact on OS. Conclusions: PIK3CA activating mutations are not associated with an impact on DMFS and OS in early-stage ILC. Patients with PIK3CA mutation had a statistically significantly decreased risk of death irrespective of whether they received TAM or an AI. © 2023 Domen Ribnikar et al., published by Sciendo."
"The demand for online education is gradually growing. Most universities and other institutions are faced with the fact that it is almost impossible to track how honestly test takers take exams remotely. In online formats, there are many simple opportunities that allow for cheating and using the use of outside help. Online proctoring based on artificial intelligence technologies in distance education is an effective technological solution to prevent academic dishonesty. This article explores the development and implementation of an online control proctoring system using artificial intelligence technology for conducting online exams. The article discusses the proctoring systems used in Kazakhstan, compares the functional features of the selected proctoring systems, and describes the architecture of Proctor SU. A prototype of the Proctor SU proctoring system has been developed. As a pilot program, the authors used this system during an online university exam and examined the results of the test. According to the author’s examination, students have a positive attitude towards the use of Proctor SU online proctoring. The proposed proctor system includes features of face detection, face tracking, audio capture, and the active capture of system windows. Models CNN, R-CNN, and YOLOv3 were used in the development process. The YOLOv3 model processed images in real time at 45 frames per second, and CNN and R-CNN processed images in real time at 30 and 38 frames per second. The YOLOv3 model showed better results in terms of real-time face recognition. Therefore, the YOLOv3 model was implemented into the Proctor SU proctoring system. © 2023 by the authors."
"The treatment of low-temperature and low-turbidity water, together with the control of operating parameters, is a big problem in water treatment. In this study, the daily monitoring data of one water supply plant from 2021 to 2022 was used to predict the effluent chemical oxygen demand (COD) during low temperature and turbid periods by using black box artificial intelligence models (AI), such as Support Vector Regression (SVR), Decision Tree (DT), Random Forest (RF) and Backpropagation Neural Network (BP). The results of a single model show that the DT model has better results than the other single models. In ensemble modeling, the performance of single artificial intelligence models can be improved by using neural network integration. In the validation phase, the ensemble model can improve the prediction accuracy by about 15%. At the same time, the model also obtained a reliable prediction effect in the same region, water source, and the process of the water supply plant. © 2023 IOP Publishing Ltd."
"Recent times have seen a significant rise in interest from mobile operators, vendors, and research projects toward achieving more energy-efficient and sustainable networks. Not surprisingly, it comes at a time when higher traffic demand and more stringent and diverse network requirements result in diminishing benefits for operators using complex AI-driven network optimization solutions. In this paper, we propose the idea of tower companies that facilitate radio access network (RAN) infrastructure sharing between operators and evaluate the additional energy savings obtained in this process. In particular, we focus on the RAN-as-a-Service (RANaaS) implementation, wherein each operator leases and controls an independent logical RAN instance running on the shared infrastructure. We show how an AI system can assist operators in optimizing their share of resources under multiple constraints. This paper aims to provide a vision, a quantitative and qualitative analysis of the RANaaS paradigm, and its benefits in terms of energy efficiency. Through simulations, we show the possibility to achieve up to 75 percent energy savings per operator over 24 h compared to the scenario where none of the energy-saving features are activated. This is an additional 55 percent energy savings from sharing the RAN infrastructure compared to the baseline scenario where the operators use independent hardware. © 2023 by the authors."
"Attempts to use computers to aid in the detection of breast malignancies date back more than 20 years. Despite significant interest and investment, this has historically led to minimal or no significant improvement in performance and outcomes with traditional computer-aided detection. However, recent advances in artificial intelligence and machine learning are now starting to deliver on the promise of improved performance. There are at present more than 20 FDA-approved AI applications for breast imaging, but adoption and utilization are widely variable and low overall. Breast imaging is unique and has aspects that create both opportunities and challenges for AI development and implementation. Breast cancer screening programs worldwide rely on screening mammography to reduce the morbidity and mortality of breast cancer, and many of the most exciting research projects and available AI applications focus on cancer detection for mammography. There are, however, multiple additional potential applications for AI in breast imaging, including decision support, risk assessment, breast density quantitation, workflow and triage, quality evaluation, response to neoadjuvant chemotherapy assessment, and image enhancement. In this review the current status, availability, and future directions of investigation of these applications are discussed, as well as the opportunities and barriers to more widespread utilization. © 2023 by the authors."
"Throughout human history, agriculture has undergone a series of progressive transformations based on ever-evolving technologies in an effort to increase productivity and profitability. Over the years, farming methods have evolved significantly, progressing from Agriculture 1.0, which relied on primitive tools, to Agriculture 2.0, which incorporated machinery and advanced farming practices, and subsequently to Agriculture 3.0, which emphasized mechanization and employed intelligent machinery and technology to enhance productivity levels. To further automate and increase agricultural productivity while minimizing agricultural inputs and pollutants, a new approach to agricultural management based on the concepts of the fourth industrial revolution is being embraced gradually. This approach is referred to as “Agriculture 4.0” and is mainly implemented through the use of Internet of Things (IoT) technologies, enabling the remote control of sensors and actuators and the efficient collection and transfer of data. In addition, fueled by technologies such as robotics, artificial intelligence, quantum sensing, and four-dimensional communication, a new form of smart agriculture, called “Agriculture 5.0,” is now emerging. Agriculture 5.0 can exploit the growing 5G network infrastructure as a basis. However, only 6G-IoT networks will be able to offer the technological advances that will allow the full expansion of Agriculture 5.0, as can be inferred from the relevant scientific literature and research. In this article, we first introduce the scope of Agriculture 5.0 as well as the key features and technologies that will be leveraged in the much-anticipated 6G-IoT communication systems. We then highlight the importance and influence of these developing technologies in the further advancement of smart agriculture and conclude with a discussion of future challenges and opportunities. © 2023 by the authors."
"Although optimizing deep neural networks is becoming crucial for deploying the networks on edge AI devices, it faces increasing challenges due to scarce hardware resources in modern IoT and mobile devices. This study proposes a quantization method that can quantize all internal computations and parameters in the memory modification. Unlike most previous methods that primarily focused on relatively simple CNN models for image classification, the proposed method, Unified Scaling-Based Pure-Integer Quantization (USPIQ), can handle more complex CNN models for object detection. USPIQ aims to provide a systematic approach to convert all floating-point operations to pure-integer operations in every model layer. It can significantly reduce the computational overhead and make it more suitable for low-power neural network accelerator hardware consisting of pure-integer datapaths and small memory aimed at low-power consumption and small chip size. The proposed method optimally calibrates the scale parameters for each layer using a subset of unlabeled representative images. Furthermore, we introduce a notion of the Unified Scale Factor (USF), which combines the conventional two-step scaling processes (quantization and dequantization) into a single process for each layer. As a result, it improves the inference speed and the accuracy of the resulting quantized model. Our experiment on YOLOv5 models demonstrates that USPIQ can significantly reduce the on-chip memory for parameters and activation data by ~75% and 43.68%, respectively, compared with the floating-point model. These reductions have been achieved with a minimal loss in mAP@0.5—at most 0.61%. In addition, our proposed USPIQ exhibits a significant improvement in the inference speed compared to ONNX Run-Time quantization, achieving a speedup of 1.64 to 2.84 times. We also demonstrate that USPIQ outperforms the previous methods in terms of accuracy and hardware reduction for 8-bit quantization of all YOLOv5 versions. © 2023 by the authors."
"Objectives: To provide patient-centred care, psychological knowledge, and skills are necessary for pharmaceutical communication. Acquisition of these communication skills is closely related to patient comprehension. Therefore, to improve pharmacist's communication skills, pharmacist need to learn the characteristics of their medication instructions, such as posture, facial expressions, eye contact, nodding, and more. For the analysis of medical communication, there is a rating scale, functional analysis, and others. However, these methods may not match the actual emotions due to their analysis skills and the psychological stress of the patients. In this study, we examined the methods to evaluate patient-pharmacist communication using emotion recognition AI software, which recognises emotions from facial expressions. Methods: With the cooperation of six simulated patients (SP) and eight pharmacists, we recorded the SP's facial expressions during medication instruction. The facial expression video was analysed using emotion recognition AI, which can obtain emotion values (anger, contempt, disgust, fear, joy, sadness, surprise, and engagement). We compared the emotion of the extracted peaks with the feedback and calculated the emotion match rate. Key findings: As a result, 33% of the emotions matched in the peak and feedback. This result indicates that emotion recognition AI cannot capture every feedback emotion. However, in joy, the result was not affected by engagement, and the match rate between peak and feedback was high. Conclusions: In the future, emotion recognition AI will allow us to assess the effects of communication skills of the pharmacists on the psychological state of the patients more objectively and noninvasively.  © 2023 The Author(s). Published by Oxford University Press on behalf of the Royal Pharmaceutical Society. All rights reserved."
"The development of artificial intelligence (AI) allows for the construction of technologies capable of implementing functions that represent the human mind, senses, and problem-solving skills, leading to automation, rapid data analysis, and acceleration of tasks. These solutions has been initially implemented in medical fields relying on image analysis; however, technological development and interdisciplinary collaboration allows for the introduction of AI-based enhancements to further medical specialties. During the COVID-19 pandemic, novel technologies established on big data analysis experienced a rapid expansion. Yet, despite the possibilities of advancements with these AI technologies, there are number of shortcomings that need to be resolved to assert the highest and the safest level of performance, especially in the setting of the intensive care unit (ICU). Within the ICU, numerous factors and data affect clinical decision making and work management that could be managed by AI-based technologies. Early detection of a patient’s deterioration, identification of unknown prognostic parameters, or even improvement of work organization are a few of many areas where patients and medical personnel can benefit from solutions developed with AI. © 2023 by the authors."
"In recent years, digital computing in memory (CIM) has been an efficient and high-performance solution in artificial intelligence (AI) edge inference. Nevertheless, digital CIM based on non-volatile memory (NVM) is less discussed for the sophisticated intrinsic physical and electrical behavior of non-volatile devices. In this paper, we propose a fully digital non-volatile CIM (DNV-CIM) macro with compressed coding look-up table (LUT) multiplier (CCLUTM) using the 40 nm technology, which is highly compatible with the standard commodity NOR Flash memory. We also provide a continuous accumulation scheme for machine learning applications. When applied to a modified ResNet18 network trained under the CIFAR-10 dataset, the simulations indicate that the proposed CCLUTM-based DNV-CIM can achieve a peak energy efficiency of 75.18 TOPS/W with 4-bit multiplication and accumulation (MAC) operations. © 2023 by the authors."
"Turbulence control involves fluid dynamics and control theory, and is of great importance to many fields such as aeronautics and astronautics, vehicle, wind power generation, etc. Due to the complexity of turbulence, traditional control methods face many bottlenecks in the field of turbulence control. The development of artificial intelligence (AI) technology provides a tool to break through these bottlenecks. This paper briefly summarizes the applications of AI in turbulence control reported in the literature, focusing on AI control systems, algorithms, and the outstanding achievements achieved in different turbulence control applications, as well as the first attempt by the author's team to analyze the big data generated by the AI control system to discover important information and even the control scaling law. The challenges and future prospects are also analyzed. © 2023 Chinese Academy of Mechanics. All rights reserved."
"The increase in the concentration of geological gas emissions in the atmosphere and particularly the increase of methane is considered by the majority of the scientific community as the main cause of global climate change. The main reasons that place methane at the center of interest, lie in its high global warming potential (GWP) and its lifetime in the atmosphere. Anthropogenic processes, like engineering geology ones, highly affect the daily profile of gasses in the atmosphere. Should direct measures be taken to reduce emissions of methane, immediate global warming mitigation could be achieved. Due to its significance, methane has been monitored by many space missions over the years and as of 2017 by the Sentinel-5P mission. Considering the above, we conclude that monitoring and predicting future methane concentration based on past data is of vital importance for the course of climate change over the next decades. To that end, we introduce a method exploiting state-of-the-art recurrent neural networks (RNNs), which have been proven particularly effective in regression problems, such as time-series forecasting. Aligned with the green artificial intelligence (AI) initiative, the paper at hand investigates the ability of different RNN architectures to predict future methane concentration in the most active regions of Texas, Pennsylvania and West Virginia, by using Sentinel-5P methane data and focusing on computational and complexity efficiency. We conduct several empirical studies and utilize the obtained results to conclude the most effective architecture for the specific use case, establishing a competitive prediction performance that reaches up to a 0.7578 mean squared error on the evaluation set. Yet, taking into consideration the overall efficiency of the investigated models, we conclude that the exploitation of RNN architectures with less number of layers and a restricted number of units, i.e., one recurrent layer with 8 neurons, is able to better compensate for competitive prediction performance, meanwhile sustaining lower computational complexity and execution time. Finally, we compare RNN models against deep neural networks along with the well-established support vector regression, clearly highlighting the supremacy of the recurrent ones, as well as discuss future extensions of the introduced work. © 2023 by the authors."
"An international reader study was conducted to gauge an average diagnostic accuracy of radiologists interpreting chest X-ray images, including those from fluorography and mammography, and establish requirements for stand-alone radiological artificial intelligence (AI) models. The retrospective studies in the datasets were labelled as containing or not containing target pathological findings based on a consensus of two experienced radiologists, and the results of a laboratory test and follow-up examination, where applicable. A total of 204 radiologists from 11 countries with various experience performed an assessment of the dataset with a 5-point Likert scale via a web platform. Eight commercial radiological AI models analyzed the same dataset. The AI AUROC was 0.87 (95% CI:0.83–0.9) versus 0.96 (95% CI 0.94–0.97) for radiologists. The sensitivity and specificity of AI versus radiologists were 0.71 (95% CI 0.64–0.78) versus 0.91 (95% CI 0.86–0.95) and 0.93 (95% CI 0.89–0.96) versus 0.9 (95% CI 0.85–0.94) for AI. The overall diagnostic accuracy of radiologists was superior to AI for chest X-ray and mammography. However, the accuracy of AI was noninferior to the least experienced radiologists for mammography and fluorography, and to all radiologists for chest X-ray. Therefore, an AI-based first reading could be recommended to reduce the workload burden of radiologists for the most common radiological studies such as chest X-ray and mammography. © 2023 by the authors."
"With the enhanced interoperability of information among vehicles, the demand for collaborative sharing among vehicles increases. Based on blockchain, the classical consensus algorithms in collaborative IoV (Internet of Vehicle), such as PoW (Proof of Work), PoS (Proof of Stake), and DPoS (Delegated Proof of Stake), only consider the node features, which is hard to adapt to the immediacy and flexibility of vehicles. On the other hand, classical consensus algorithms often require mass computing, which undoubtedly increases the communication overhead, resulting in the inability to achieve collaborative IoV under asymmetric networks. Therefore, proposing a low failure rate consensus algorithm that takes into account running time and energy consumption becomes a major challenge in IoV applications. This paper proposes an AI-enabled consensus algorithm with vehicle features, combining vehicle-based metrics and neural networks. First, we introduce vehicle-based metrics such as vehicle online time, performance, and behavior. Then, we propose an integral model and a hierarchical classification method, which combine with a BP neural network to obtain the optimal solution for interconnection. Among them, we also use Informer to predict the future online duration of vehicles, which effectively solves the situation that the primary node vehicle drops off in collaborative IoV. Finally, the experimentations show that the vehicle-based metrics eliminate the problem of the primary node vehicle being offline, which realizes the collaborative IoV considering vehicle features. Meanwhile, it reduces the vehicle network system delay and energy consumption. © 2023 by the authors."
"The objective of the study was to establish the effect of appropriate supplementation days (days -21 to +7) using four isonitrogenous (14.7% CP) diets balanced to provide low (Lo-ME) or high (Hi-ME) metabolizable energy on the body condition score (BCS), body weight (BW) change, and reproductive performances of sheep. Thirty-five Doyogena ewes (27.71 ± 2.87 kg, 2–5 years of age, BCS of 2.0–2.5) grazing on natural pasture were randomly assigned to supplementary treatments consisting of combinations of enset leaf (EL) and commercial concentrate (CC): T0 (control), T1 (250 g EL + 500 g CC: Lo-ME), T2 (400 g EL + 500 g CC; Hi-ME), T3 (500 g EL + 400 g CC; Hi-ME), and T4 (500 g EL + 250 g CC; Lo-ME). The estrous cycle was synchronized with one intramuscular injection, 5 mg PGF2α, prior to artificial insemination. The dry matter (DM) from the pasture provided 1.10–1.46 kg/day, which corresponds to the DM requirements of the ewes until late gestation. However, the pasture provided a protein content of 9.52%, which was insufficient for breeding, mid-gestation, and gestation, requiring minimums of 16.1%, 13.1%, and 14.8%, respectively. The pasture could only provide enough energy for breeding ewes with a BW of up to 30 kg. The energy provided by pasture was insufficient for ewes weighing > 30 kg at mid-gestation and gestation, providing 6.9–9.2 MJ/day, below the requirement of 11.92–16.32 MJ/day required for mid-gestation and gestation. The energy was not sufficient for large ewes weighing > 40 kg. Supplementary diets T1–T4 provided DM in the range of 1.7–2.29 kg/day. This was sufficient for AI, mid-gestation, and gestation phases. Dietary supplements increased (p < 0.01) BW during breeding and mid-gestation. During lambing, T2 and T3 increased BW (p < 0.05) compared to T4 and T1. T4 had a similar effect (p > 0.05) on BW during lambing. T1, T2, and T3 significantly increased BCS (p < 0.05). T2 and T3 increased (p < 0.05) BCS at mid-gestation, but only T2 significantly increased BCD (p < 0.05) during lambing. All dietary supplements resulted in a shorter (p < 0.05) time to the resumption of estrous and the length of estrous (p < 0.05). T1, T2, and T3 resulted in a stronger estrous response (p < 0.05). Dietary supplements enhanced (p < 0.05) the conception rate and fecundity rate. The conception rate was highest in T2 and T3 at 85.7% and 83.3%, respectively. T2 had the highest fecundity rate at 151.7% (p < 0.05). Dietary supplementation increased the rate of lambing (LR), litter size (LS), and weight of lambs at birth (LBW). The LR for treatments T2, T3, and T4 was 100% versus 66.7% in the control. T1 and T2 significantly increased (p < 0.05) LS, but T4 had a similar LS to the control. Dietary supplements T1, T3, and T4 tended to increase (p < 0.05) LBW, but T2 increased LBW significantly (p < 0.05). Supplementation (T2, T3) with 400 g enset + 500 g CC and 500 g enset + 400 g CC are promising feed supplements to increase the reproductive capacities of Doyogena ewes in Ethiopia. Energy is as important to ewe flushing as protein. © 2023 by the authors."
"The increasing complexity of Multi-Agent Systems (MASs), coupled with the emergence of Artificial Intelligence (AI) and Large Language Models (LLMs), have highlighted significant gaps in our understanding of the behavior and interactions of diverse entities within dynamic environments. Traditional game theory approaches have often been employed in this context, but their utility is limited by the static and homogenous nature of their models. With the transformative influence of AI and LLMs on business and society, a more dynamic and nuanced theoretical framework is necessary to guide the design and management of MASs. In response to this pressing need, we propose an Extended Coevolutionary (EC) Theory in this paper. This alternative framework incorporates key aspects of coevolutionary dynamics, adaptive learning, and LLM-based strategy recommendations to model and analyze the strategic interactions among heterogeneous agents in MASs. It goes beyond game theory by acknowledging and addressing the diverse interactions (economic transactions, social relationships, information exchange) and the variability in risk aversion, social preferences, and learning capabilities among entities. To validate the effectiveness of the EC framework, we developed a simulation environment that enabled us to explore the emergence of cooperation and defection patterns in MASs. The results demonstrated the potential of our framework to promote cooperative behavior and maintain robustness in the face of disruptions. The dynamics and evolution of the Multi-Agent System over time were also visualized using advanced techniques. Our findings underscore the potential of harnessing LLMs to facilitate cooperation, enhance social welfare, and promote resilient strategies in multi-agent environments. Moreover, the proposed EC framework offers valuable insights into the interplay between strategic decision making, adaptive learning, and LLM-informed guidance in complex, evolving systems. This research not only responds to the current challenges faced in modeling MASs, but also paves the way for future research in this rapidly developing field. © 2023 by the authors."
"Oral cancer (OC) is one of the most common forms of head and neck cancer and continues to have the lowest survival rates worldwide, even with advancements in research and therapy. The prognosis of OC has not significantly improved in recent years, presenting a persistent challenge in the biomedical field. In the field of oncology, artificial intelligence (AI) has seen rapid development, with notable successes being reported in recent times. This systematic review aimed to critically appraise the available evidence regarding the utilization of AI in the diagnosis, classification, and prediction of oral cancer (OC) using histopathological images. An electronic search of several databases, including PubMed, Scopus, Embase, the Cochrane Library, Web of Science, Google Scholar, and the Saudi Digital Library, was conducted for articles published between January 2000 and January 2023. Nineteen articles that met the inclusion criteria were then subjected to critical analysis utilizing QUADAS-2, and the certainty of the evidence was assessed using the GRADE approach. AI models have been widely applied in diagnosing oral cancer, differentiating normal and malignant regions, predicting the survival of OC patients, and grading OC. The AI models used in these studies displayed an accuracy in a range from 89.47% to 100%, sensitivity from 97.76% to 99.26%, and specificity ranging from 92% to 99.42%. The models’ abilities to diagnose, classify, and predict the occurrence of OC outperform existing clinical approaches. This demonstrates the potential for AI to deliver a superior level of precision and accuracy, helping pathologists significantly improve their diagnostic outcomes and reduce the probability of errors. Considering these advantages, regulatory bodies and policymakers should expedite the process of approval and marketing of these products for application in clinical scenarios. © 2023 by the authors."
"Artificial intelligence (AI) systems have many applications with tremendous current and future value to human society. As AI systems penetrate the aspects of everyday life, a pressing need arises to explain their decision-making processes to build trust and familiarity among end users. In high-stakes fields such as healthcare and self-driving cars, AI systems are required to have a minimum standard for accuracy and to provide well-designed explanations for their output, especially when they impact human life. Although many techniques have been developed to make algorithms explainable in human terms, no design methodologies that will allow software teams to systematically draw out and address explainability-related issues during AI design and conception have been established. In response to this gap, we proposed the explainability in design (EID) methodological framework for addressing explainability problems in AI systems. We explored the literature on AI explainability to narrow down the field into six major explainability principles that will aid designers in brainstorming around the metrics and guide the critical thinking process. EID is a step-by-step guide to AI design that has been refined over a series of user studies and interviews with experts in AI explainability. It is devised for software design teams to uncover and resolve potential issues in their AI products and to simply refine and explore the explainability of their products and systems. The EID methodology is a novel framework that aids in the design and conception stages of the AI pipeline and can be integrated into the form of a step-by-step card game. Empirical studies involving AI system designers have shown that EID can decrease the barrier of entry and the time and experience required to effectively make well-informed decisions for integrating explainability into their AI solutions. © The author(s) 2023."
"Artificial intelligence-powered recommendation systems have gained popularity as a tool to enhance user experience and boost sales. Platforms often need to make decisions about which seller to recommend and the strength of the recommendation when conducting recommendations. Therefore, it is necessary to explore the recommendation strategy of the platform in the case of duopoly competition. We develop a game model where two competing manufacturers sell products through an agency contract on a common platform, and they can decide whether or not to provide recommendations to the manufacturers. Our highlight lies in the endogenous recommendation strength of the platform. The findings suggest that it is optimal for the platform to offer recommendation services when the commission rate is high. The platform also prefers to only recommend one manufacturer in the market with low or high competition, but it prefers to recommend both manufacturers in moderately competitive markets. From the view of manufacturers, they can benefit from the recommendation service as long as the commission rate is not too low. Moreover, recommending only one manufacturer consistently yields stronger recommendations compared to recommending multiple manufacturers. However, the impact of recommendation on prices is influenced by the commission rate and product substitutability. These results have significant implications for platform decision making and provide valuable insights into the trade-offs involved in the development of recommendation systems. © 2023 by the authors."
"Recently, machine learning (ML) and deep learning (DL) models based on artificial intelligence (AI) have emerged as fast and reliable tools for predicting water quality index (WQI) in various regions worldwide. In this study, we propose a novel stacking framework based on DL models for WQI prediction, employing a convolutional neural network (CNN) model. Additionally, we introduce explainable AI (XAI) through XGBoost-based SHAP (SHapley Additive exPlanations) values to gain valuable insights that can enhance decision-making strategies in water management. Our findings demonstrate that the stacking model achieves the highest accuracy in WQI prediction (R2: 0.99, MAPE: 15.99%), outperforming the CNN model (R2: 0.90, MAPE: 58.97%). Although the CNN model shows a relatively high R2 value, other statistical measures indicate that it is actually the worst-performing model among the five tested. This discrepancy may be attributed to the limited training data available for the CNN model. Furthermore, the application of explainable AI (XAI) techniques, specifically XGBoost-based SHAP values, allows us to gain deep insights into the models and extract valuable information for water management purposes. The SHAP values and interaction plot reveal that elevated levels of total dissolved solids (TDS), zinc, and electrical conductivity (EC) are the primary drivers of poor water quality. These parameters exhibit a nonlinear relationship with the water quality index, implying that even minor increases in their concentrations can significantly impact water quality. Overall, this study presents a comprehensive and integrated approach to water management, emphasizing the need for collaborative efforts among all stakeholders to mitigate pollution levels and uphold water quality. By leveraging AI and XAI, our proposed framework not only provides a powerful tool for accurate WQI prediction but also offers deep insights into the models, enabling informed decision-making in water management strategies. © 2023 by the authors."
"Positive energy districts (PEDs) are urban areas which seek to take an integral approach to climate neutrality by including technological, spatial, regulatory, financial, legal, social, and economic perspectives. It is still a new concept and approach for many stakeholders. ChatGPT, a generative pre-trained transformer, is an advanced artificial intelligence (AI) chatbot based on a complex network structure and trained by the company OpenAI. It has the potential for the fast learning of PED. This paper reports a trial test in which ChatGPT is used to provide written formulations of PEDs within three frameworks: challenge, impact, and communication and dissemination. The results are compared with the formulations derived from over 80 PED experts who took part in a two-day workshop discussing many aspects of PED research and development. The proposed methodology involves querying ChatGPT with specific questions and recording its responses. Subsequently, expert opinions on the same questions are provided to ChatGPT, aiming to elicit a comparison between the two sources of information. This approach enables an evaluation of ChatGPT’s answers in relation to the insights shared by domain experts. By juxtaposing the outputs, a comprehensive assessment can be made regarding the reliability, accuracy, and alignment of ChatGPT’s responses with expert viewpoints. It is found that ChatGPT can be a useful tool for the rapid formulation of basic information about PEDs that could be used for its wider dissemination amongst the general public. The model is also noted as having a number of limitations, such as providing pre-set single answers, a sensitivity to the phrasing of questions, a tendency to repeat non-important (or general) information, and an inability to assess inputs negatively or provide diverse answers to context-based questions. Its answers were not always based on up-to-date information. Other limitations and some of the ethical–social issues related to the use of ChatGPT are also discussed. This study not only validated the possibility of using ChatGPT to rapid study PEDs but also trained ChatGPT by feeding back the experts’ discussion into the tool. It is recommended that ChatGPT can be involved in real-time PED meetings or workshops so that it can be trained both iteratively and dynamically. © 2023 by the authors."
"Damage to bridge expansion joints arises from a variety of causes such as increasingly deteriorated bridges, abnormal temperatures, and increased traffic. To detect anomalies in the expansion joints, this study proposes an Artificial Intelligence (AI)-model-based diagnosis method of analyzing the vibration of the bridge bearing that supports the upper structure of a bridge. The proposed system establishes big data with the measured displacement of a bridge bearing and makes an AI-based prediction about the risk of bridge expansion joints. Replacing a bridge bearing makes it possible to manage the bridge displacement before and after construction and helps improve safety inspections and diagnosis methods. It is necessary to prepare a bridge with anomalies for the AI model training. For this reason, a bridge with a bridge bearing was simulated. In addition, a vehicle suitable for the bridge was simulated. The displacement data in normal and abnormal situations were collected, cleaned, and applied to the AI analysis model. The system was found to have over 90% accuracy of prediction about expansion joint faulting and damage. © 2023 by the authors."
"The expanding integration of artificial intelligence (AI) in various aspects of society makes the infosphere around us increasingly complex. Humanity already faces many obstacles trying to have a better understanding of our own minds, but now we have to continue finding ways to make sense of the minds of AI. The issue of AI’s capability to have independent thinking is of special attention. When dealing with such an unfamiliar concept, people may rely on existing human properties, such as survival desire, to make assessments. Employing information-processing-based Bayesian Mindsponge Framework (BMF) analytics on a dataset of 266 residents in the United States, we found that the more people believe that an AI agent seeks continued functioning, the more they believe in that AI agent’s capability of having a mind of its own. Moreover, we also found that the above association becomes stronger if a person is more familiar with personally interacting with AI. This suggests a directional pattern of value reinforcement in perceptions of AI. As the information processing of AI becomes even more sophisticated in the future, it will be much harder to set clear boundaries about what it means to have an autonomous mind. © 2023 by the authors."
"The importance of and need for cyber security have increased in the last decade. The critical infrastructure of the country, modeled with cyber-physical systems (CPS), is becoming vulnerable because of a lack of efficient safety measures. Attackers are becoming more innovative, and attacks are becoming undetectable, thereby causing huge risks to these systems. In this scenario, intelligent and evolving detection methods should be introduced to replace basic and outworn methods. The ability of artificial intelligence (AI) to analyze data and predict outcomes has created an opportunity for researchers to explore the power of AI in cyber security. This article discusses new-age intelligence and smart techniques such as pattern recognition models, deep neural networks, generative adversarial networks, and reinforcement learning for cyber security in CPS. The differences between the traditional security methods used in information technology and the security methods used in CPS are analyzed, and the need for a transition into intelligent methods is discussed in detail. A deep neural network-based controller that detects and mitigates cyber attacks is designed for microgrid systems. As a case study, a stealthy local covert attack that overcomes the existing microgrid protection is modeled. The ability of the DNN controller to detect and mitigate the SLCA is observed. The experiment is performed in a simulation and also in real-time to analyze the effectiveness of AI in cyber security. © 2023 by the authors."
"Drawing on the notion of compensatory behavior, this paper studies how students compensate for learning loss during a pandemic and what role artificial intelligence (AI) plays in this regard. We further probe into a difference in compensatory behavior for learning loss in terms of quantity, pattern, and pace (i.e., tripartite aspect of learning behavior) of AI-powered learning app usage depending on the level of pandemic threat and the proximity of a goal to students. Results show that the pandemic threat affects student learning behavior differently. Immediately following the COVID-19 outbreak, students who live in the epicenter of the outbreak (versus those who do not) use the app less at first, but with time, they use it more (quantity), on a more regular basis (pattern), and rebound to a curriculum path (pace) comparable to students who do not live in the outbreak’s epicenter. These findings collectively explain behavior that is consistent with compensation for learning loss. The results also partially corroborate the goal-proximity effect, revealing that proximity to a goal (e.g., the degree to which the national university admission exam is approaching) has a moderating role in explaining the tripartite perspective of student learning behavior. Overall, these findings have important theoretical and practical implications for understanding how innovative education technologies can not only facilitate student learning during adversity, but also support learning recovery after adversity. © 2022 INFORMS."
"The Internet of Things (IoT) plays a critical role in remotely monitoring a wide variety of different application sectors, including agriculture, building, and energy. The wind turbine energy generator (WTEG) is a real-world application that can take advantage of IoT technologies, such as a low-cost weather station, where human activities can be significantly affected by enhancing the production of clean energy based on the known direction of the wind. Meanwhile, common weather stations are neither affordable nor customizable for specific applications. Moreover, due to weather forecast changes over time and location within the same city, it is not efficient to rely on a limited number of weather stations that may be located far away from a recipient’s location. Therefore, in this paper, we focus on presenting a low-cost weather station that relies on an artificial intelligence (AI) algorithm that can be distributed across a WTEG area with minimal cost. The proposed study measures multiple weather parameters, such as wind direction, wind velocity (WV), temperature, pressure, mean sea level, and relative humidity to provide current measurements to recipients and AI-based forecasts. In addition, the proposed study consists of several heterogeneous nodes and a controller for each station in a target area. The collected data can be transmitted through Bluetooth low energy (BLE). The experimental results reveal that the proposed study matches the standard of the National Meteorological Center (NMC), with a nowcast measurement of 95% accuracy for WV and 92% for wind direction (WD). © 2023 by the authors."
"Challenges faced in network security have significantly steered the deployment timeline of Fifth Generation (5G) communication at a global level; therefore, research in Sixth Generation (6G) security analysis is profoundly necessitated. The prerogative of this paper is to present a survey on the emerging 6G cellular communication paradigm to highlight symmetry with legacy security concepts along with asymmetric innovative aspects such Artificial Intelligence (AI), Quantum Computing, Federated Learning, etc. We present a taxonomy of the threat model in 6G communication in five security legacy concepts, including Confidentiality, Integrity, Availability, Authentication and Access control (CIA3). We also suggest categorization of threat-countering techniques specific to 6G communication into three types: cryptographic methods, entity attributes and Intrusion Detection System (IDS). Thus, with this premise, we distributed the authentication techniques in eight types, including handover authentication, mutual authentication, physical layer authentication, deniable authentication, token-based authentication, certificate-based authentication, key agreement-based authentication and multi-factor authentication. We specifically suggested a series of future research directions at the conclusive edge of this survey. © 2023 by the authors."
"In the past couple of years, the world has come to realize the importance of renewable sources of energy and the disadvantages of excessive use of fossil fuels. Numerous studies have been conducted to implicate the benefits of artificial intelligence in areas of green energy production. Artificial intelligence (AI) and machine learning algorithms are believed to be the driving forces behind the fourth industrial revolution and possess capabilities for interpreting non-linear relationships that exist in complex problems. Sustainable biofuels are derived from renewable resources such as plants, crops, and waste materials other than food crops. Unlike traditional fossil fuels such as coal and oil, biofuels are considered to be more sustainable and environmentally friendly. The work discusses the transesterification of jatropha oil into biodiesel using KOH and NaOH as alkaline catalysts. This research aims to examine and optimize the nonlinear relationship between transesterification process parameters (molar ratio, temperature, reaction time, and catalyst concentration) and biodiesel properties. The methodology employed in this study utilizes AI and machine learning algorithms to predict biodiesel properties and improve the yield and quality of biodiesel. Deep neural networks, linear regression, polynomial regression, and K-nearest neighbors are the algorithms implemented for prediction purposes. The research comprehensively examines the impact of individual transesterification process parameters on biodiesel properties, including yield, viscosity, and density. Furthermore, this research introduces the use of genetic algorithms for optimizing biodiesel production. The genetic algorithm (GA) generates optimal values for transesterification process parameters based on the desired biodiesel properties, such as yield, viscosity, and density. The results section presents the transesterification process parameters required for obtaining 72%, 85%, and 98% biodiesel yields. By leveraging AI and machine learning, this research aims to enhance the efficiency and sustainability of biodiesel production processes. © 2023 by the authors."
"The advancement of technology associated with the field, especially the use of unmanned aerial vehicles (UAV) coupled with multispectral cameras, allows us to monitor the condition of crops in real time and contribute to the field of machine learning. The objective of this study was to estimate both productivity and above-ground biomass (AGB) for the corn crop by applying different vegetation indices (VIs) via high-resolution aerial imagery. Among the indices tested, strong correlations were obtained between productivity and the normalized difference vegetation index (NDVI) with a significance level of p < 0.05 (0.719), as well as for the normalized difference red edge (NDRE), or green normalized difference vegetation index (GNDVI) with crop productivity (p < 0.01), respectively 0.809 and 0.859. The AGB results align with those obtained previously; GNDVI and NDRE showed high correlations, but now with a significance level of p < 0.05 (0.758 and 0.695). Both GNDVI and NDRE indices showed coefficients of determination for productivity and AGB estimation with 0.738 and 0.654, and 0.701 and 0.632, respectively. The use of the GNDVI and NDRE indices shows excellent results for estimating productivity as well as AGB for the corn crop, both at the spatial and numerical levels. The possibility of predicting crop productivity is an essential tool for producers, since it allows them to make timely decisions to correct any deficit present in their agricultural plots, and further contributes to AI integration for drone digital optimization. © 2023 by the authors."
"At present, considering the novelty of Immersive Technologies (ImTs) associated with Digital Twin (DT), Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) in the context of the metaverse and its rapid and ongoing development in Building Information Modeling (BIM), knowledge of specific possibilities and methods for integrating ImTs into building process workflows remains fragmented and scarce. Therefore, this paper aims to explore the research progress and trends of immersive technology-driven BIM applications, providing a helpful reference for understanding the current knowledge system and stimulating future research. To the best of the authors’ knowledge, this is the first attempt to use macro-quantitative bibliometric analysis and micro-qualitative analysis methods to explore the research topic of ImTs-driven BIM. This study obtains 758 related studies in the past decade, year 2013 to 2022, through a series of keywords from the Web of Science Core Collection database and uses VOSviewer software to conduct keywords co-occurrence analysis and overlay visualisation to visualise the relationship between ImTs and BIM, which contains six clusters, namely VR, Internet of Things (IoT), DT, 3D model, design, and AR. The macro-quantitative analysis on ImTs-driven BIM applications throughout all the stages of the building lifecycle reveals the themes, content, and characteristics of the applications across the stages, which tend to be integrated with emerging advanced technology and tools, such as Artificial Intelligence (AI), blockchain, and deep learning. © 2023 by the authors."
"Due to the high prevalence and rates of disability associated with musculoskeletal system diseases, more thorough research into diagnosis, pathogenesis, and treatments is required. One of the key contributors to the emergence of diseases of the musculoskeletal system is thought to be changes in the biomechanics of the human musculoskeletal system. However, there are some defects concerning personal analysis or dynamic responses in current biomechanical research methodologies. Digital twin (DT) was initially an engineering concept that reflected the mirror image of a physical entity. With the application of medical image analysis and artificial intelligence (AI), it entered our lives and showed its potential to be further applied in the medical field. Consequently, we believe that DT can take a step towards personalized healthcare by guiding the design of industrial personalized healthcare systems. In this perspective article, we discuss the limitations of traditional biomechanical methods and the initial exploration of DT in musculoskeletal system diseases. We provide a new opinion that DT could be an effective solution for musculoskeletal system diseases in the future, which will help us analyze the real-time biomechanical properties of the musculoskeletal system and achieve personalized medicine. © 2023 by the authors."
"Background: Stroke is a significant public health problem and a leading cause of death and long-term disability worldwide. Several treatments for ischemic stroke have been developed, but these treatments have limited effectiveness. One potential treatment for this condition is Actovegin®/AODEJIN, a calf blood deproteinized hemodialysate/ultrafiltrate that has been shown to have pleiotropic/multifactorial and possibly multimodal effects. The actual actions of this medicine are thought to be mediated by its ability to reduce oxidative stress, inflammation, and apoptosis and to enhance neuronal survival and plasticity. Methods: To obtain the most up-to-date information on the effects of Actovegin®/AODEJIN in ischemic stroke, we systematically reviewed the literature published in the last two years. This review builds upon our previous systematic literature review published in 2020, which used the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) method to search for and select related articles over almost two decades, between 1 January 2001 and 31 December 2019. Additionally, we compared the results of our PRISMA search (human intelligence-based) with those obtained from an interrogation of a GPT-based chatbot (ChatGPT) in order to ensure comprehensive coverage of potentially relevant studies. Results: Our updated review found limited new evidence on the use of Actovegin®/AODEJIN in ischemic stroke, although the number of articles on this subject consistently increased compared to that from our initial systematic literature review. Specifically, we found five articles up to 2020 and eight more until December 2022. While these studies suggest that Actovegin®/AODEJIN may have neuroprotective effects in ischemic stroke, further clinical trials are needed to confirm these findings. Consequently, we performed a funnel analysis to evaluate the potential for publication bias. Discussion: Our funnel analysis showed no evidence of publication bias, suggesting that the limited number of studies identified was not due to publication bias but rather due to a lack of research in this area. However, there are limitations when using ChatGPT, particularly in distinguishing between truth and falsehood and determining the appropriateness of interpolation. Nevertheless, AI can provide valuable support in conducting PRISMA-type systematic literature reviews, including meta-analyses. Conclusions: The limited number of studies identified in our review highlights the need for additional research in this area, especially as no available therapeutic agents are capable of curing central nervous system lesions. Any contribution, including that of Actovegin (with consideration of a positive balance between benefits and risks), is worthy of further study and periodic reappraisal. The evolving advancements in AI may play a role in the near future. © 2023 by the authors."
"(1) Background: Mastery of auscultation can be challenging for many healthcare providers. Artificial intelligence (AI)-powered digital support is emerging as an aid to assist with the interpretation of auscultated sounds. A few AI-augmented digital stethoscopes exist but none are dedicated to pediatrics. Our goal was to develop a digital auscultation platform for pediatric medicine. (2) Methods: We developed StethAid—a digital platform for artificial intelligence-assisted auscultation and telehealth in pediatrics—that consists of a wireless digital stethoscope, mobile applications, customized patient-provider portals, and deep learning algorithms. To validate the StethAid platform, we characterized our stethoscope and used the platform in two clinical applications: (1) Still’s murmur identification and (2) wheeze detection. The platform has been deployed in four children’s medical centers to build the first and largest pediatric cardiopulmonary datasets, to our knowledge. We have trained and tested deep-learning models using these datasets. (3) Results: The frequency response of the StethAid stethoscope was comparable to those of the commercially available Eko Core, Thinklabs One, and Littman 3200 stethoscopes. The labels provided by our expert physician offline were in concordance with the labels of providers at the bedside using their acoustic stethoscopes for 79.3% of lungs cases and 98.3% of heart cases. Our deep learning algorithms achieved high sensitivity and specificity for both Still’s murmur identification (sensitivity of 91.9% and specificity of 92.6%) and wheeze detection (sensitivity of 83.7% and specificity of 84.4%). (4) Conclusions: Our team has created a technically and clinically validated pediatric digital AI-enabled auscultation platform. Use of our platform could improve efficacy and efficiency of clinical care for pediatric patients, reduce parental anxiety, and result in cost savings. © 2023 by the authors."
"The abilities to form concepts and abstractions, and to make analogies, are key to human intelligence, but AI systems have a long way to go before they can match the abilities of humans in these areas. To develop machines that can abstract and analogize, researchers typically focus on idealized problem domains that are meant to capture the essence of human abstraction abilities without having to deal with the complexity of real-world situations. This commentary describes why solving problems in these domains remains difficult for AI systems, and discusses how AI researches can make progress on imbuing machines with these essential abilities. © 2023 New York Academy of Sciences."
"Recent advances in artificial intelligence (AI) technology have led to increasing interest in the development of AI-based tool wear condition monitoring methods, heavily relying on large training samples. However, the high cost of tool wear experiment and the uncertainty of tool wear change in the machining process lead to the problems of sample missing and insufficiency in the model training stage, which seriously affects the identification accuracy of many AI models. In this paper, a novel identification method based on finite-element modeling (FEM) and the synthetic minority oversampling technique (SMOTE) is proposed to overcome the problem of sample missing and sample insufficiency. Firstly, a few tool wear monitoring experiments are carried out to obtain experimental samples with low cost. Then, a FEM model based on the Johnson–Cook constitutive model was established and verified according to the experimental samples. Based on the verified FEM model, the simulated missing sample in the experiments can be supplemented to compose a complete training set. Finally, the SMOTE is employed to expand the sample size to construct a perfect training set to train the SVM classification model. End milling tool wear monitoring experiments demonstrate that the proposed FEM-SMOTE method can obtain 98.7% identification accuracy, which is 30% higher than that based on experimental samples. The proposed method provides an effective approach for tool wear condition monitoring with low experimental cost. © 2023 by the authors."
"Background: Accurate breast cancer risk assessment after a negative screening result could enable better strategies for early detection. Purpose: To evaluate a deep learning algorithm for risk assessment based on digital mammograms. Materials and Methods: A retrospective observational matched case-control study was designed using the OPTIMAM Mammography Image Database from the National Health Service Breast Screening Programme in the United Kingdom from February 2010 to September 2019. Patients with breast cancer (cases) were diagnosed following a mammographic screening or between two triannual screening rounds. Controls were matched based on mammography device, screening site, and age. The artificial intelligence (AI) model only used mammograms at screening before diagnosis. The primary objective was to assess model performance, with a secondary objective to assess heterogeneity and calibration slope. The area under the receiver operating characteristic curve (AUC) was estimated for 3-year risk. Heterogeneity according to cancer subtype was assessed using a likelihood ratio interaction test. Statistical significance was set at P < .05. Results: Analysis included patients with screen-detected (median age, 60 years [IQR, 55-65 years]; 2044 female, including 1528 with invasive cancer and 503 with ductal carcinoma in situ [DCIS]) or interval (median age, 59 years [IQR, 53-65 years]; 696 female, including 636 with invasive cancer and 54 with DCIS) breast cancer and 1:1 matched controls, each with a complete set of mammograms at the screening preceding diagnosis. The AI model had an overall AUC of 0.68 (95% CI: 0.66, 0.70), with no evidence of a significant difference between interval and screen-detected (AUC, 0.69 vs 0.67; P = .085) cancer. The calibration slope was 1.13 (95% CI: 1.01, 1.26). There was similar performance for the detection of invasive cancer versus DCIS (AUC, 0.68 vs 0.66; P = .057). The model had higher performance for advanced cancer risk (AUC, 0.72 ≥stage II vs 0.66 <stage II; P = .037). The AUC for detecting breast cancer in mammograms at diagnosis was 0.89 (95% CI: 0.88, 0.91). Conclusion: The AI model was found to be a strong predictor of breast cancer risk for 3-6 years following a negative mammographic screening.  © RSNA, 2023."
"Objective: To assess the prevalence of chronic pain, its physical and psychosocial impact on daily life, and the various therapies adopted to alleviate pain. Method: The cross-sectional population-based telephonic survey was conducted from May to July 2021 at Shaukat Khanum Memorial Cancer Hospital, Lahore, Pakistan, and comprised patients of either gender aged at least 18 years suffering from chronic pain who visited the institutional laboratory collection centres. In the first phase, people who were suffering from chronic pain were screened, while in the second phase, data was collected using a detailed questionnaire exploring pain history, treatment and its effects. The data was compiled and analysed using Antlere’s AI based software. Results: Of the 4,801 patients contacted, 757(15.75%) were suffering from chronic pain. A total of 201(20%) subjects reported that their pain score was 5/10 on the numerical rating scale. Back pain was the major complaint (183, 18%) among the subjects. Of the total, 335(44.25%) were having active treatment, and 226 (67%) of them said the medication was effective. Overall, 706 (93%) patients had never visited a pain management specialist. Furthermore, 252 (33%) participants were diagnosed with depression, and 106 (14%) patients said that they were suicidal at some point in life. Conclusion: The survey observed that a high percentage of unawareness existed on pain management among the Pakistani citizens. © 2023 Pakistan Medical Association. All rights reserved."
"One of the three most serious and deadly cancers in the world is colorectal cancer. The most crucial stage, like with any cancer, is early diagnosis. In the medical industry, artificial intelligence (AI) has recently made tremendous strides and showing promise for clinical applications. Machine learning (ML) and deep learning (DL) applications have recently gained popularity in the analysis of medical texts and images due to the benefits and achievements they have made in the early diagnosis of cancerous tissues and organs. In this paper, we intend to systematically review the state-of-the-art research on AI-based ML and DL techniques applied to the modeling of colorectal cancer. All research papers in the field of colorectal cancer are collected based on ML and DL techniques, and they are then classified into three categories: the aim of the prediction, the method of the prediction, and data samples. Following that, a thorough summary and a list of the studies gathered under each topic are provided. We conclude our study with a critical discussion of the challenges and opportunities in colorectal cancer prediction using ML and DL techniques by concentrating on the technical and medical points of view. Finally, we believe that our study will be helpful to scientists who are considering employing ML and DL methods to diagnose colorectal cancer. © 2023 by the authors."
"Background: Image-derived artificial intelligence (AI) risk models have shown promise in identifying high-risk women in the short term. The long-term performance of image-derived risk models expanded with clinical factors has not been investigated. Methods: We performed a case–cohort study of 8110 women aged 40–74 randomly selected from a Swedish mammography screening cohort initiated in 2010 together with 1661 incident BCs diagnosed before January 2022. The imaging-only AI risk model extracted mammographic features and age at screening. Additional lifestyle/familial risk factors were incorporated into the lifestyle/familial-expanded AI model. Absolute risks were calculated using the two models and the clinical Tyrer–Cuzick v8 model. Age-adjusted model performances were compared across the 10-year follow-up. Results: The AUCs of the lifestyle/familial-expanded AI risk model ranged from 0.75 (95%CI: 0.70–0.80) to 0.68 (95%CI: 0.66–0.69) 1–10 years after study entry. Corresponding AUCs were 0.72 (95%CI: 0.66–0.78) to 0.65 (95%CI: 0.63–0.66) for the imaging-only model and 0.62 (95%CI: 0.55–0.68) to 0.60 (95%CI: 0.58–0.61) for Tyrer–Cuzick v8. The increased performances were observed in multiple risk subgroups and cancer subtypes. Among the 5% of women at highest risk, the PPV was 5.8% using the lifestyle/familial-expanded model compared with 5.3% using the imaging-only model, p < 0.01, and 4.6% for Tyrer–Cuzick, p < 0.01. Conclusions: The lifestyle/familial-expanded AI risk model showed higher performance for both long-term and short-term risk assessment compared with imaging-only and Tyrer–Cuzick models. © 2023 by the authors."
"On 28 February–2 March 2023, the 2023 States General of Artificial Intelligence (AI) event was held in Italy under the sponsorship of several multinational companies. The purpose of this event was mainly to create a venue for allowing international protagonists of AI to discuss and confront on the recent trends in AI. The aim of this paper is to report on the state of the art of the literature on the most recent control engineering and artificial intelligence methods for managing and controlling energy networks with improved efficiency and effectiveness. More in detail, to the best of the authors’ knowledge, the scope of the literature review considered in this paper is specifically limited to recent trends in EV charging, cyber-physical security, and predictive maintenance. These application scenarios were identified in the above-mentioned event as responsible for triggering most of the business needs currently expressed by energy companies. A critical discussion of the most relevant methodological approaches and experimental setups is provided, together with an overview of the future research directions. © 2023 by the authors."
"Skin cancer is the most common cancer diagnosis in the United States, with approximately one in five Americans expected to be diagnosed within their lifetime. Non-melanoma skin cancer is the most prevalent type of skin cancer, and as cases rise globally, physicians need reliable tools for early detection. Artificial intelligence has gained substantial interest as a decision support tool in medicine, particularly in image analysis, where deep learning has proven to be an effective tool. Because specialties such as dermatology rely primarily on visual diagnoses, deep learning could have many diagnostic applications, including the diagnosis of skin cancer. Furthermore, with the advancement of mobile smartphones and their increasingly powerful cameras, deep learning technology could also be utilized in remote skin cancer screening applications. Ultimately, the available data for the detection and diagnosis of skin cancer using deep learning technology are promising, revealing sensitivity and specificity that are not inferior to those of trained dermatologists. Work is still needed to increase the clinical use of AI-based tools, but based on the current data and the attitudes of patients and physicians, deep learning technology could be used effectively as a clinical decision-making tool in collaboration with physicians to improve diagnostic efficiency and accuracy. © 2023 by the authors."
"Recently, novel applications in the space of artificial intelligence (AI) such as solving constraint optimization problems, probabilistic inferencing, contextual adaptation, and continual learning from noisy data are gaining momentum to address relevant real-world problems. A majority of these tasks are compute and/or memory intensive. While traditional deep learning has been fueled by the utilization of graphic processing units (GPUs) to accelerate algorithms primarily in the cloud, today we see a surge in the development of application/domain-specific integrated circuits and systems that aim at providing an order of magnitude improvement over traditional GPU-based approaches in terms of energy efficiency and latency. This growing branch of research taps into the realms of neuronal dynamics, collective computing using dynamical systems, harnessing stochasticity to enable probabilistic computing, and even draws inspiration from quantum computing. We envision such specialized application/domain-specific systems to perform complex tasks such as solving NP-hard optimization problems, performing reasoning and cognition in the presence of uncertainty with superior energy-efficiency (and/or area and latency improvements) compared to conventional GPU-based approaches and von Neumann computing using traditional silicon-based devices, circuits, and architectures. Of special interest is to utilize such nontraditional computing approaches to reduce the time to obtain solutions for computationally challenging problems that otherwise tend to grow exponentially with problem size. To support this vision, there needs to be fundamental advances in both nontraditional devices and circuits/architectures. Recent works have shown that novel circuit topologies and architectures involving non-Boolean, oscillatory, spiking, probabilistic, or quantum-inspired computing are more suited toward tackling applications such as solving constraint optimization problems, performing energy-based learning, performing Bayesian learning and inference, lifelong continual learning, and solving quantum-inspired applications such as Quantum Monte Carlo. A flurry of current research highlights that compared to traditional silicon-based devices, emerging nanodevices utilizing novel quantum materials such as complex oxides, ferroelectric materials, and spintronic materials can allow the realization of these novel circuits and architectures with lower foot-print area, higher energy efficiency, and lower latency.  © 2014 IEEE."
"In recent years, a complex set of dynamic developments driven by both the economy and the emergence of digital technologies has put pressure on manufacturing companies to adapt. The concept of servitization, i.e., the shift from a product-centric to a service-centric value creation logic, can help manufacturing companies stabilize their business in such volatile times. Existing academic literature investigates the potential and challenges of servitization and the associated development of data-based services, so-called smart services, with a view to external market performance. However, with the increasing use of digital technologies in manufacturing and the development of internal smart services based on them, we argue that the existing insights on external servitization are also of interest for internal transformation. In this paper, we identify key findings from service literature, apply them to digital factory transformation, and structure them into six fields of action along the dimensions of people, technology, and organization. As a result, recommendations for designing digital factory transformation in manufacturing companies are derived from the perspective of servitization and developing internal smart services. © 2023 by the authors."
"Flood management and media production planning are both tasks that require timely and sound decision making, as well as effective collaboration between professionals in a team split between remote headquarter operators and in situ actors. This paper presents an extended reality (XR) platform that utilizes interactive and immersive technologies and integrates artificial intelligence (AI) algorithms to support the professionals and the public involved in such incidents and events. The developed XR tools address various specialized end-user needs of different target groups and are fueled by modules that intelligently collect, analyze, and link data from heterogeneous sources while considering user-generated content. This platform was tested in a flood-prone area and in a documentary planning scenario, where it was used to create immersive and interactive experiences. The findings demonstrate that it increases situation awareness and improves the overall performance of the professionals involved. The proposed XR system represents an innovative technological approach for tackling the challenges of flood management and media production, one that also has the potential to be applied in other fields. © 2023 by the authors."
"Advanced artificial intelligence (AI) techniques have led to significant developments in optical character recognition (OCR) technologies. OCR applications, using AI techniques for transforming images of typed text, handwritten text, or other forms of text into machine-encoded text, provide a fair degree of accuracy for general text. However, even after decades of intensive research, creating OCR with human-like abilities has remained evasive. One of the challenges has been that OCR models trained on general text do not perform well on localized or personalized handwritten text due to differences in the writing style of alphabets and digits. This study aims to discuss the steps needed to create an adaptive framework for OCR models, with the intent of exploring a reasonable method to customize an OCR solution for a unique dataset of English language numerical digits were developed for this study. We develop a digit recognizer by training our model on the MNIST dataset with a convolutional neural network and contrast it with multiple models trained on combinations of the MNIST and custom digits. Using our methods, we observed results comparable with the baseline and provided recommendations for improving OCR accuracy for localized or personalized handwritten text. This study also provides an alternative perspective to generating data using conventional methods, which can serve as a gold standard for custom data augmentation to help address the challenges of scarce data and data imbalance. © 2023 by the authors."
"Crack development is a clear indicator of the durability of concrete bridges. Traditional bridge inspections that rely inspectors to climb on bridges with lift cars are unsafe for inspectors and also time- and labor-consuming. Therefore, this research proposes a solution that applies unmanned aerial vehicles (UAV) and high-resolution digital cameras to measure concrete bridge cracks. An experiment was conducted on an Ai-He concrete bridge located in Yangmei District, Taoyuan City, Taiwan. Two types of images were taken. Close-up images observed cracks more clearly, and long-range images covered the ground control points. We registered these two types of images to establish the absolute coordinate system with ground control points and tie points through block triangulation. This research examines three approaches of generating tie points: (1) manually select tie points with features on the bridge such as nails and dots, (2) randomly input tie points generated from Scale-Invariant Feature Transform (SIFT), and (3) randomly input tie points generated from SIFT as the initial tie points and perform automatic tie generation with the ERDAS Leica Photogrammetry Suite (LPS) image matching module (automatic tie generation). Afterwards, close-up images were processed into orthorectified images with 0.1 mm pixel size for crack size measurements. Crack sizes were determined by a manual measurement approach and an inflection point approach for comparison. This research established a workflow for UAV bridge inspection that locates and measures cracks in concrete bridges, which consequently provides a safe and cost-efficient concrete bridge crack monitoring solution with acceptable accuracies. © 2023 by the authors."
"Chest X-ray (CXR) is the most important technique for performing chest imaging, despite its well-known limitations in terms of scope and sensitivity. These intrinsic limitations of CXR have prompted the development of several artificial intelligence (AI)-based software packages dedicated to CXR interpretation. The online database “AI for radiology” was queried to identify CE-marked AI-based software available for CXR interpretation. The returned studies were divided according to the targeted disease. AI-powered computer-aided detection software is already widely adopted in screening and triage for pulmonary tuberculosis, especially in countries with few resources and suffering from high a burden of this disease. AI-based software has also been demonstrated to be valuable for the detection of lung nodules detection, automated flagging of positive cases, and post-processing through the development of digital bone suppression software able to produce digital bone suppressed images. Finally, the majority of available CE-marked software packages for CXR are designed to recognize several findings, with potential differences in sensitivity and specificity for each of the recognized findings. © 2023 by the authors."
"Advancements in wearable medical devices using the IoT technology are shaping the modern healthcare system. With the emergence of the Internet of Healthcare Things (IoHT), efficient healthcare services can be provided to patients. Healthcare professionals have effectively used AI-based models to analyze the data collected from IoHT devices to treat various diseases. Data must be processed and analyzed while avoiding privacy breaches, in compliance with legal rules and regulations, such as the HIPAA and GDPR. Federated learning (FL) is a machine learning-based approach allowing multiple entities to train an ML model collaboratively without sharing their data. It is particularly beneficial in healthcare, where data privacy and security are substantial concerns. Even though FL addresses some privacy concerns, there is still no formal proof of privacy guarantees for IoHT data. Privacy-enhancing technologies (PETs) are tools and techniques designed to enhance the privacy and security of online communications and data sharing. PETs provide a range of features that help protect users’ personal information and sensitive data from unauthorized access and tracking. This paper comprehensively reviews PETs concerning FL in the IoHT scenario and identifies several key challenges for future research. © 2023 by the authors."
"A full connected world is expected to be introduced in the sixth generation mobile network (6G). As a typical fully connected scenario, the internet of vehicle (IoV) enables intelligent vehicle operations via artificial intelligence (AI) and edge computing technologies. Thus, integrating intelligence into edge computing is, no doubt, a promising development trend. In the future of vehicular networks, a massive variety of services need powerful computing resources and higher quality of service (QoS). Existing computing resources are insufficient to match those increasing requirements. Most works on this problem focused on finding the power-delay’s trade-off, ignoring QoS and stable load balance. In this study, we found that the computing power and redundancy of vehicles’ in IoV is increasing. So, those redundant computing resources are possible to be used to solve the shortage of computing resource. CNN is a typical AI technique. This technology is very suitable for solving the problems in this article. So, we adopted CNN technique of AI to design and algorithm of convolutional long short-term memory (CN_LSTM) based traffic prediction (ACLBTP). ACLBTP was designed to gain the predicted number of vehicles belonging to the edge node. Secondly, according to the problem of insufficient computing resources on remote servers, we found that a large amount of redundant computing resources exist in edge nodes. So, we used edge computing technique to solve the problem of insufficient computing resources on remote servers. ASOBCL was designed to distribute computing tasks to edge nodes. Meanwhile, an intelligent service offloading framework was provided in this article. Based on the framework, an algorithm of improved gradient descent (AIGD) was created to accelerate the speed of iteration. So, the ACLBTP’s convergence of convolutional neural network (CNN) based on AIGD was able to be accelerated too. In ASOBCL, a sorting technique was adopted to speed up the offloading work. Simulation results demonstrated the fact that the prediction strategy designed in this paper had high accuracy. The low offloading time and maintaining stable load balance were gained via running ASOBCL. Low offloading time means short response time. Additionally, the QoS was guaranteed. So, these strategies designed in this paper were effective and valuable. © 2023 by the authors."
"Artificial intelligence (AI) has the potential to revolutionize the drug discovery process, offering improved efficiency, accuracy, and speed. However, the successful application of AI is dependent on the availability of high-quality data, the addressing of ethical concerns, and the recognition of the limitations of AI-based approaches. In this article, the benefits, challenges, and drawbacks of AI in this field are reviewed, and possible strategies and approaches for overcoming the present obstacles are proposed. The use of data augmentation, explainable AI, and the integration of AI with traditional experimental methods, as well as the potential advantages of AI in pharmaceutical research, are also discussed. Overall, this review highlights the potential of AI in drug discovery and provides insights into the challenges and opportunities for realizing its potential in this field. Note from the human authors: This article was created to test the ability of ChatGPT, a chatbot based on the GPT-3.5 language model, in terms of assisting human authors in writing review articles. The text generated by the AI following our instructions (see Supporting Information) was used as a starting point, and its ability to automatically generate content was evaluated. After conducting a thorough review, the human authors practically rewrote the manuscript, striving to maintain a balance between the original proposal and the scientific criteria. The advantages and limitations of using AI for this purpose are discussed in the last section. © 2023 by the authors."
"The domestication of animals and the cultivation of crops have been essential to human development throughout history, with the agricultural sector playing a pivotal role. Insufficient nutrition often leads to plant diseases, such as those affecting rice crops, resulting in yield losses of 20–40% of total production. These losses carry significant global economic consequences. Timely disease diagnosis is critical for implementing effective treatments and mitigating financial losses. However, despite technological advancements, rice disease diagnosis primarily depends on manual methods. In this study, we present a novel self-attention network (SANET) based on the ResNet50 architecture, incorporating a kernel attention mechanism for accurate AI-assisted rice disease classification. We employ attention modules to extract contextual dependencies within images, focusing on essential features for disease identification. Using a publicly available rice disease dataset comprising four classes (three disease types and healthy leaves), we conducted cross-validated classification experiments to evaluate our proposed model. The results reveal that the attention-based mechanism effectively guides the convolutional neural network (CNN) in learning valuable features, resulting in accurate image classification and reduced performance variation compared to state-of-the-art methods. Our SANET model achieved a test set accuracy of 98.71%, surpassing that of current leading models. These findings highlight the potential for widespread AI adoption in agricultural disease diagnosis and management, ultimately enhancing efficiency and effectiveness within the sector. © 2023 by the authors."
"Background: This study aimed to explore possible differences of the whole-brain functional connectivity of the anterior cingulate cortex (ACC) and anterior insula (AI), in a sample of depressed patients with major depressive disorder (MDD), bipolar disorder (BD) and healthy controls (HC). Methods: A hundred and three subjects (nMDD = 35, nBD = 25, and nHC = 43) between the ages of eighteen and sixty-five years old underwent functional magnetic resonance imaging. The CONN Toolbox was used to process and analyze the functional connectivity of the ACC and AI. Results: The comparison between the patients (MDD/BD) and HC yielded increased resting-state functional connectivity (rsFC) between the ACC and the motor and somatosensory cortices (SSC), superior parietal lobule (SPL), precuneus, and lateral occipital cortex, which was driven by the BD group. In addition, hyperconnectivity between the right AI and the motor and SSC was found in BD, as compared to HC. In MDD, as compared to HC, hyperconnectivity between ACC and SPL and the lateral occipital cortex was found, with no statistical rsFC differences for the AI seed. Compared to BD, the MDD group showed ACC–cerebellum hyperconnectivity and a trend for increased rsFC between the right AI and the bilateral superior frontal cortex. Conclusions: Considering the observed hyperconnectivity between the ACC/somatosensory cortex in the patient group, we suggest depression may be related to an impairment of the sensory-discriminative function of the SSC, which results in the phenomenological signature of mental pain in both MDD and BD. These findings suggest that future research should investigate this particular network with respect to motor functions and executive control, as a potential differential diagnostic biomarker for MDD and BD. © 2023 by the authors."
"Various machine learning algorithms have been applied to network intrusion classification problems, including both binary and multi-class classifications. Despite the existence of numerous studies involving unbalanced network intrusion datasets, such as CIC-IDS2017, a prevalent approach is to address the issue by either merging the classes to optimize their numbers or retaining only the most dominant ones. However, there is no consistent trend showing that accuracy always decreases as the number of classes increases. Furthermore, it is essential for cybersecurity practitioners to recognize the specific type of attack and comprehend the causal factors that contribute to the resulting outcomes. This study focuses on tackling the challenges associated with evaluating the performance of multi-class classification for network intrusions using highly imbalanced raw data that encompasses the CIC-IDS2017 and CSE-CIC-IDS2018 datasets. The research concentrates on investigating diverse machine learning (ML) models, including Logistic Regression, Random Forest, Decision Trees, CNNs, and Artificial Neural Networks. Additionally, it explores the utilization of explainable AI (XAI) methods to interpret the obtained results. The results obtained indicated that decision trees using the CART algorithm performed best on the 28-class classification task, with an average macro F1-score of 0.96878. © 2023 by the authors."
"Background: Artificial intelligence (AI) models have improved US assessment of thyroid nodules; however, the lack of generalizability limits the application of these models. Purpose: To develop AI models for segmentation and classification of thyroid nodules in US using diverse data sets from nationwide hospitals and multiple vendors, and to measure the impact of the AI models on diagnostic performance. Materials and Methods: This retrospective study included consecutive patients with pathologically confirmed thyroid nodules who underwent US using equipment from 12 vendors at 208 hospitals across China from November 2017 to January 2019. The detection, segmentation, and classification models were developed based on the subset or complete set of images. Model performance was evaluated by precision and recall, Dice coefficient, and area under the receiver operating characteristic curve (AUC) analyses. Three scenarios (diagnosis without AI assistance, with freestyle AI assistance, and with rule-based AI assistance) were compared with three senior and three junior radiologists to optimize incorporation of AI into clinical practice. Results: A total of 10 023 patients (median age, 46 years [IQR 37–55 years]; 7669 female) were included. The detection, segmentation, and classification models had an average precision, Dice coefficient, and AUC of 0.98 (95% CI: 0.96, 0.99), 0.86 (95% CI: 0.86, 0.87), and 0.90 (95% CI: 0.88, 0.92), respectively. The segmentation model trained on the nationwide data and classification model trained on the mixed vendor data exhibited the best performance, with a Dice coefficient of 0.91 (95% CI: 0.90, 0.91) and AUC of 0.98 (95% CI: 0.97, 1.00), respectively. The AI model outperformed all senior and junior radiologists (P < .05 for all comparisons), and the diagnostic accuracies of all radiologists were improved (P < .05 for all comparisons) with rule-based AI assistance. Conclusion: Thyroid US AI models developed from diverse data sets had high diagnostic performance among the Chinese population. Rule-based AI assistance improved the performance of radiologists in thyroid cancer diagnosis. © RSNA, 2023."
"Electrocardiograms (ECGs) provide crucial information for evaluating a patient’s cardiovascular health; however, they are not always easily accessible. Photoplethysmography (PPG), a technology commonly used in wearable devices such as smartwatches, has shown promise for constructing ECGs. Several methods have been proposed for ECG reconstruction using PPG signals, but some require signal alignment during the training phase, which is not feasible in real-life settings where ECG signals are not collected at the same time as PPG signals. To address this challenge, we introduce PPG2ECGps, an end-to-end, patient-specific deep-learning neural network utilizing the W-Net architecture. This novel approach enables direct ECG signal reconstruction from PPG signals, eliminating the need for signal alignment. Our experiments show that the proposed model achieves mean values of 0.977 mV for Pearson’s correlation coefficient, 0.037 mV for the root mean square error, and 0.010 mV for the normalized dynamic time-warped distance when comparing reconstructed ECGs to reference ECGs from a dataset of 500 records. As PPG signals are more accessible than ECG signals, our proposed model has significant potential to improve patient monitoring and diagnosis in healthcare settings via wearable devices. © 2023 by the authors."
"AI fairness is tasked with evaluating and mitigating bias in algorithms that may discriminate towards protected groups. This paper examines if bias exists in AI algorithms used in disaster management and in what manner. We consider the 2017 Hurricane Harvey when flood victims in Houston resorted to social media to request for rescue. We evaluate a Random Forest regression model trained to predict Twitter rescue request rates from social-environmental data using three fairness criteria (independence, separation, and sufficiency). The Social Vulnerability Index (SVI), its four sub-indices, and four variables representing digital divide were considered sensitive attributes. The Random Forest regression model extracted seven significant predictors of rescue request rates, and from high to low importance they were percent of renter occupied housing units, percent of roads in flood zone, percent of flood zone area, percent of wetland cover, percent of herbaceous, forested and shrub cover, mean elevation, and percent of households with no computer or device. Partial Dependence plots of rescue request rates against each of the seven predictors show the non-linear nature of their relationships. Results of the fairness evaluation of the Random Forest model using the three criteria show no obvious biases for the nine sensitive attributes, except that a minor imperfect sufficiency was found with the SVI Housing and Transportation sub-index. Future AI modeling in disaster research could apply the same methodology used in this paper to evaluate fairness and help reduce unfair resource allocation and other social and geographical disparities. © 2023 The Author(s). Published by IOP Publishing Ltd."
"Thermal energy recovery systems have different candidates to mitigate CO2 emissions as recommended by the UN in its list of SDGs. One of these promising systems is thermal absorption transformers, which generally use lithium-water bromide as the working fluid. A Double Stage Heat Transformer (DSHT) is a thermal machine that allows the recovery of thermal energy at a higher temperature than it is supplied through the effect of steam absorption in a concentrated solution of lithium bromide. There are very precise thermodynamic models which allow us to calculate all the possible operating conditions of the DSHT. To perform the control of these systems, the use of Artificial Intelligence (AI) is proposed with two computational techniques—Fuzzy Logic (FL) and Artificial Neural Network (ANN)—to calculate in real-time the set of variables that maximize the product’s Gross Temperature Lift (GTL) and Coefficient of Performance (COP) in a DSHT. The values for Coefficient of Determination (R2), Mean Square Error Root (MRSE), and Mean Error Bias (MBE) for the two types of computational techniques were analyzed and compared with the purpose of identifying which of them may be more accurate to calculate the operating conditions (temperatures, pressures, concentration and flows) with the highest COP for an interval of the value of the temperature absorption entered by the user. The result of the analysis of the evaluated techniques concluded that the control strategy of a DSHT in real-time will be based on the precise calculation of the refrigerant flow in the second evaporator with a Neural Network of 30 neurons, 300 weights and 40 bias, as it is more accurate than the Fuzzy Logic technique. The goodness-of-fit for two computational techniques was evaluated as having an R2 higher than 0.98 for the provided data. Future AI controllers must be based on evaporator flow values with evaporator power at 3.9−04 kg/KJ. © 2023 by the authors."
"Biologging refers to the use of animal-borne recording devices to study wildlife behavior. In the case of audio recording, such devices generate large amounts of data over several months, and thus require some level of processing automation for the raw data collected. Academics have widely adopted offline deep-learning-classification algorithms to extract meaningful information from large datasets, mainly using time-frequency signal representations such as spectrograms. Because of the high deployment costs of animal-borne devices, the autonomy/weight ratio remains by far the fundamental concern. Basically, power consumption is addressed using onboard mass storage (no wireless transmission), yet the energy cost associated with data storage activity is far from negligible. In this paper, we evaluate various strategies to reduce the amount of stored data, making the fair assumption that audio will be categorized using a deep-learning classifier at some point of the process. This assumption opens up several scenarios, from straightforward raw audio storage paired with further offline classification on one side, to a fully embedded AI engine on the other side, with embedded audio compression or feature extraction in between. This paper investigates three approaches focusing on data-dimension reduction: (i) traditional inline audio compression, namely ADPCM and MP3, (ii) full deep-learning classification at the edge, and (iii) embedded pre-processing that only computes and stores spectrograms for later offline classification. We characterized each approach in terms of total (sensor + CPU + mass-storage) edge power consumption (i.e., recorder autonomy) and classification accuracy. Our results demonstrate that ADPCM encoding brings 17.6% energy savings compared to the baseline system (i.e., uncompressed raw audio samples). Using such compressed data, a state-of-the-art spectrogram-based classification model still achieves 91.25% accuracy on open speech datasets. Performing inline data-preparation can significantly reduce the amount of stored data allowing for a 19.8% energy saving compared to the baseline system, while still achieving 89% accuracy during classification. These results show that while massive data reduction can be achieved through the use of inline computation of spectrograms, it translates to little benefit on device autonomy when compared to ADPCM encoding, with the added downside of losing original audio information. © 2023 by the authors."
"As Industry 4.0 networks continue to evolve at a rapid pace, they are becoming increasingly complex and distributed. These networks incorporate a range of technologies that are integrated into smart manufacturing systems, requiring adaptability, security, and resilience. However, managing the complexity of Industry 4.0 networks presents significant challenges, particularly in terms of security and the integration of diverse technologies into a functioning and efficient infrastructure. To address these challenges, emerging digital twin standards are enabling the connection of various systems by linking individual digital twins, creating a system of systems. The objective is to develop a “universal translator” that can interpret inputs from both the real and digital worlds, merging them into a seamless cyber-physical reality. It will be demonstrated how the myriad of technologies and systems in Industry 4.0 networks can be connected through the use of digital twins to create a seamless “system of systems”. This will improve interoperability, resilience, and security in smart manufacturing systems. The paper will also outline the potential benefits and limitations of digital twins in addressing the challenges of Industry 4.0 networks. © 2023 by the authors."
"Air pollution is a pressing concern in urban areas, necessitating the critical monitoring of air quality to understand its implications for public health. Internet of Things (IoT) devices are widely utilized in air pollution monitoring due to their sensor capabilities and seamless data transmission over the Internet. Artificial intelligence (AI) and machine learning techniques play a crucial role in classifying patterns derived from sensor data. Environmental stations offer a multitude of parameters that can be obtained to uncover hidden patterns showcasing the impact of pollution on the surrounding environment. This paper focuses on utilizing the CO parameter as an indicator of pollution in two datasets collected from wireless environmental monitoring devices in the greater Port area and the Town Hall of Igoumenitsa City in Greece. The datasets are normalized to facilitate their utilization in classification algorithms. The k-means algorithm is applied, and the elbow method is used to determine the optimal number of clusters. Subsequently, the datasets are introduced to the grammatical evolution algorithm to calculate the percentage fault. This method constructs classification programs in a human-readable format, making it suitable for analysis. Finally, the proposed method is compared against four state-of-the-art models: the Adam optimizer for optimizing artificial neural network parameters, a genetic algorithm for training an artificial neural network, the Bayes model, and the limited-memory BFGS method applied to a neural network. The comparison reveals that the GenClass method outperforms the other approaches in terms of classification error. © 2023 by the authors."
"The biomedical literature is a vast and invaluable resource for biomedical research. Integrating knowledge from the literature with biomedical data can help biological studies and the clinical decision-making process. Efforts have been made to gather information from the biomedical literature and create biomedical knowledge bases, such as KEGG and Reactome. However, manual curation remains the primary method to retrieve accurate biomedical entities and relationships. Manual curation becomes increasingly challenging and costly as the volume of biomedical publications quickly grows. Fortunately, recent advancements in Artificial Intelligence (AI) technologies offer the potential to automate the process of curating, updating, and integrating knowledge from the literature. Herein, we highlight the AI capabilities to aid in mining knowledge and building the knowledge base from the biomedical literature.  © 2023 the author(s), published by De Gruyter, Berlin/Boston."
"The purpose of this article is to identify the differences in various aspects of the perception of artificial intelligence by students of economics and business studies at different levels of study and, on this basis, to formulate recommendations both to the higher education institutions themselves, which educate in the field of economic and business sciences, as well as to curriculum designers. First, we utilized descriptive statistics to analyze the responses for each construct among undergraduate and postgraduate students. In the second part, we employed the Kolmogorov-Smirnov and Shapiro-Wilk tests to assess the normality of data distribution. Finally, in the third part, we employed the non-parametric Mann-Whitney U test to identify the differences between undergraduate and postgraduate students. The results show that statistically significant differences can be identified especially in how students of both study levels see and understand the importance of AI. Although we did not identify significant differences between students of both levels in how they see their role in the future labor market, which will be (or already is) characterized by artificial intelligence, we must emphasize that students of both levels evaluate their roles modestly in this respect. Therefore, on this basis, we have made recommendations for more active development and integration of AI in the study process; the article presents important suggestions for improving education to prepare students for the business world of artificial intelligence. © 2023 by the authors."
"OBJECTIVE: The goal of this study was to compare the effect of different artificial intelligence (AI) machine learning and conventional therapy (CT) on upper limb impairments in patients with stroke. MATERIALS AND METHODS: PubMed, PubMed Central, Google Scholar, MEDLINE, Cochrane Library, Web of Science, Research Gate, and Wiley Online Library were searched. Descriptive statistics about variables were reported to calculate standardized mean differences in outcomes of motor control (the primary outcome), functional independence, upper extremity performance, and muscle tone. The Physiotherapy Evidence Database (PEDro) Scale was used to assess qualitative papers. The primary outcomes of AI and CT have been included in the meta-analyses. RESULTS: Ten papers with a total of 481 stroke patients were included and upper limb rehabilitation, upper limb functioning, and basic manual dexterity were examined. The heterogeneity test of the whole included measures (I2=45%) was medium. There were significant differences between the included measures (p-value=0.03) with a total SMD of 0.10 [0.01, 0.19]. According to the test for subgroup difference, it was found that there was a highly significant difference between the subgroups of the included measures (p-value=0.01) and the heterogeneity test (I2=59.8%). CONCLUSIONS: AI is a feasible and safe method in post-stroke rehabilitation and improves upper-extremity function compared to CT. Significant AI post-treatment effects on upper-limb impairments have been observed. The findings showed that higher-quality evidence was detected in six assessment scales. However, a lower quality of evidence was detected in other scales. This indicated large or very large and consistent estimates of the treatment effects, and researchers were confident about the results. Therefore, the included observational studies are likely to provide an overestimate of the true effect."
"Spatially resolved sequencing technologies help us dissect how cells are organized in space. Several available computational approaches focus on the identification of spatially variable genes (SVGs), genes whose expression patterns vary in space. The detection of SVGs is analogous to the identification of differentially expressed genes and permits us to understand how genes and associated molecular processes are spatially distributed within cellular niches. However, the expression activities of SVGs fail to encode all information inherent in the spatial distribution of cells. Here, we devised a deep learning model, Spatially Informed Artificial Intelligence (SPIN-AI), to identify spatially predictive genes (SPGs), whose expression can predict how cells are organized in space. We used SPIN-AI on spatial transcriptomic data from squamous cell carcinoma (SCC) as a proof of concept. Our results demonstrate that SPGs not only recapitulate the biology of SCC but also identify genes distinct from SVGs. Moreover, we found a substantial number of ribosomal genes that were SPGs but not SVGs. Since SPGs possess the capability to predict spatial cellular organization, we reason that SPGs capture more biologically relevant information for a given cellular niche than SVGs. Thus, SPIN-AI has broad applications for detecting SPGs and uncovering which biological processes play important roles in governing cellular organization. © 2023 by the authors."
"Twin pregnancies are highly undesirable in dairy cattle; they compromise the health and wellbeing of a cow and dramatically impair the farm economy. Recently, a genomic prediction for twin pregnancies has been developed. The objective of this study was to assess environmental and management risk factors affecting the incidence of twin pregnancies in high-producing dairy cows in their first lactation, with a special emphasis placed on the genomic prediction values of twin pregnancy. Our study population of primiparous cows proved valuable in identifying factors other than genomic predictive values that influence twin pregnancy rates. The odds ratio for twin pregnancies was 0.85 (p < 0.0001) for each unit of a prediction value increase, 3.5 (p = 0.023) for cows becoming pregnant during the negative photoperiod, and 0.33 (p = 0.016) for cows producing ≥42 kg of milk at AI, compared with the remaining cows who produced <42 kg of milk. As a general conclusion, the practical implication of our findings is that genomic prediction values can identify the risk of twin pregnancy at a herd level. Given the cumulative effect of genomic selection, selecting animals with a reduced genetic risk of twin pregnancies can contribute to reducing the incidence of twin pregnancies in dairy herds. © 2023 by the authors."
"Self-location plays a crucial role in a framework of autonomous navigation, especially in a GNSS/radio-denied environment. At the current time, self-location for artificial agents still has to resort to the visual and laser technologies in the framework of deep neural networks, which cannot model the environments effectively, especially in some dynamic and complex scenes. Instead, researchers have attempted to transplant the navigation principle of mammals into artificial intelligence (AI) fields. As a kind of mammalian neuron, the grid cells are believed to provide a context-independent spatial metric and update the representation of self-location. By exploiting the mechanism of grid cells, we adopt the oscillatory interference model for location encoding. Furthermore, in the process of location decoding, the capacity of autonomous navigation is extended to a significantly wide range without the phase ambiguity, based on a multi-scale periodic representation mechanism supported by a step-wise phase unwrapping algorithm. Compared with the previous methods, the proposed grid-like self-location can achieve a much wider spatial range without the limitation imposed by the spatial scales of grid cells. It is also able to suppress the phase noise efficiently. The proposed method is validated by simulation results. © 2023 by the authors."
[No abstract available]
"Invasiveness status, histological grade, lymph node stage, and tumour size are important prognostic factors for breast cancer survival. This evaluation aims to compare these features for cancers detected by AI and human readers using digital mammography. Women diagnosed with breast cancer between 2009 and 2019 from three UK double-reading sites were included in this retrospective cohort evaluation. Differences in prognostic features of cancers detected by AI and the first human reader (R1) were assessed using chi-square tests, with significance at p < 0.05. From 1718 screen-detected cancers (SDCs) and 293 interval cancers (ICs), AI flagged 85.9% and 31.7%, respectively. R1 detected 90.8% of SDCs and 7.2% of ICs. Of the screen-detected cancers detected by the AI, 82.5% had an invasive component, compared to 81.1% for R1 (p-0.374). For the ICs, this was 91.5% and 93.8% for AI and R1, respectively (p = 0.829). For the invasive tumours, no differences were found for histological grade, tumour size, or lymph node stage. The AI detected more ICs. In summary, no differences in prognostic factors were found comparing SDC and ICs identified by AI or human readers. These findings support a potential role for AI in the double-reading workflow. © 2023 by the authors."
"This descriptive article explores the use of smart devices for health and wellness in the context of telehealth, highlighting rapidly evolving technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI). Key innovations, benefits, challenges, and opportunities related to the adoption of these technologies are outlined. The article provides a descriptive and accessible approach to understanding the evolution and impact of smart devices in the tele-exercise reality. Nowadays, technological advances provide solutions that were unthinkable just a few years ago. The habits of the general population have also changed over the past few years. Hence, there is a need to investigate this issue and draw the attention of the scientific community to this topic by describing the benefits and challenges associated with each topic. If individuals no longer go to exercise, the exercise must go to their homes instead. © 2023 by the authors."
[No abstract available]
"The field of anesthesia has always been at the forefront of innovation and technology, and the integration of Artificial Intelligence (AI) represents the next frontier in anesthesia care. The use of AI and its subtypes, such as machine learning, has the potential to improve efficiency, reduce costs, and ameliorate patient outcomes. AI can assist with decision making, but its primary advantage lies in empowering anesthesiologists to adopt a proactive approach to address clinical issues. The potential uses of AI in anesthesia can be schematically grouped into clinical decision support and pharmacologic and mechanical robotic applications. Tele-anesthesia includes strategies of telemedicine, as well as device networking, for improving logistics in the operating room, and augmented reality approaches for training and assistance. Despite the growing scientific interest, further research and validation are needed to fully understand the benefits and limitations of these applications in clinical practice. Moreover, the ethical implications of AI in anesthesia must also be considered to ensure that patient safety and privacy are not compromised. This paper aims to provide a comprehensive overview of AI in anesthesia, including its current and potential applications, and the ethical considerations that must be considered to ensure the safe and effective use of the technology. © 2023 by the authors."
"The estimation of crop yield is a compelling and highly relevant task in the scenario of the challenging climate change we are facing. With this aim, a reinterpretation and a simplification of the Food and Agriculture Organization (FAO) fundamentals are presented to calculate the fresh biomass of forage crops. A normalized difference vegetation index (NDVI) series observed from a multispectral camera on board an unmanned aircraft system (UAS) was the basis for the estimation. Eight fields in Spain of different rainfed intercropping forages were flown over simultaneously, with eight field measurements from February to June 2020. The second derivative applied to the NDVI time series determined the key points of the growing cycle, whereas the NDVI values themselves were integrated and multiplied by a standardized value of the normalized water productivity (WP*). The scalability of the method was tested using two scales of the NDVI values: the point scale (at the precise field measurement location) and the plot scale (mean of 400 m2). The resulting fresh biomass and, therefore, the proposal were validated against a dataset of field-observed benchmarks during the field campaign. The agreement between the estimated and the observed fresh biomass afforded a very good prediction in terms of the determination coefficient (R2, that ranged from 0.17 to 0.85) and the agreement index (AI, that ranged from 0.55 to 0.90), with acceptable estimation errors between 10 and 30%. The best period to estimate fresh biomass was found to be between the second fortnight of April and the first fortnight of May. © 2023 by the authors."
"The past few decades have witnessed remarkable progress in the application of artificial intelligence (AI) and machine learning (ML) in medicine, notably in medical imaging. The application of ML to dental and oral imaging has also been developed, powered by the availability of clinical dental images. The present work aims to investigate recent progress concerning the application of ML in the diagnosis of oral diseases using oral X-ray imaging, namely the quality and outcome of such methods. The specific research question was developed using the PICOT methodology. The review was conducted in the Web of Science, Science Direct, and IEEE Xplore databases, for articles reporting the use of ML and AI for diagnostic purposes in X-ray-based oral imaging. Imaging types included panoramic, periapical, bitewing X-ray images, and oral cone beam computed tomography (CBCT). The search was limited to papers published in the English language from 2018 to 2022. The initial search included 104 papers that were assessed for eligibility. Of these, 22 were included for a final appraisal. The full text of the articles was carefully analyzed and the relevant data such as the clinical application, the ML models, the metrics used to assess their performance, and the characteristics of the datasets, were registered for further analysis. The paper discusses the opportunities, challenges, and limitations found. © 2023 by the authors."
"Background: The Tibetan group is one of the oldest Sino-Tibetan ethnic groups. The origin, migration as well as the genetic background of Tibetans have become the research hotspots in the field of forensic genetics. The use of ancestry informative markers (AIMs) allows the investigation of the genetic background of the Gannan Tibetan group. Methods: In this study, the 165 ancestry informative single nucleotide polymorphism (AI-SNP) loci included in the Precision ID Ancestry Panel were used to genotype 101 Gannan Tibetans using the Ion S5 XL system. The forensic statistical parameters of 165 AI-SNP in the Gannan Tibetan group were calculated. Population genetic analyses including Nei's genetic distances, phylogenetic analyses, pairwise fixation index, principal component analyses and population ancestry composition analyses were also conducted to evaluate the genetic relationships between the Gannan Tibetan group and other reference populations. Results: Forensic parameters of the 165 AI-SNP loci indicated that not all of the SNPs showed high genetic polymorphisms in the Gannan Tibetan group. Population genetic analyses indicated that the Gannan Tibetan group had close genetic affinities with East Asian populations, especially with the groups residing in its neighboring geographical regions. Conclusions: The 165 AI-SNP loci in the Precision ID Ancestry Panel showed high ancestral prediction powers for different continental populations. When trying to predict the ancestral information of East Asian subpopulations using this panel, the prediction results are not particularly accurate. The 165 AI-SNP loci showed varying degrees of genetic polymorphisms in the Gannan Tibetan group, and the combined use of these loci could be an effective tool in the forensic individual identification and parentage testing of this group. The Gannan Tibetan group has close genetic affinities with East Asian populations compared with other reference populations, especially tighter genetic relationships with the groups residing in its neighboring geographical regions.  © The author(s) 2023."
"Artificial intelligence (AI) development across the health sector has recently been the most crucial. Early medical information, identification, diagnosis, classification, then analysis, along with viable remedies, are always beneficial developments. Precise and consistent image classification has critical in diagnosing and tactical decisions for healthcare. The core issue with image classification has become the semantic gap. Conventional machine learning algorithms for classification rely mainly on low-level but rather high-level characteristics, employ some handmade features to close the gap, but force intense feature extraction as well as classification approaches. Deep learning is a powerful tool with considerable advances in recent years, with deep convolution neural networks (CNNs) succeeding in image classification. The main goal is to bridge the semantic gap and enhance the classification performance of multi-modal medical images based on the deep learning-based model ResNet50. The data set included 28378 multi-modal medical images to train and validate the model. Overall accuracy, precision, recall, and F1-score evaluation parameters have been calculated. The proposed model classifies medical images more accurately than other state-of-the-art methods. The intended research experiment attained an accuracy level of 98.61%. The suggested study directly benefits the health service. © 2023 Abid et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited."
"According to numerous reports, ChatGPT represents a significant breakthrough in the field of artificial intelligence. ChatGPT is a pre-trained AI model designed to engage in natural language conversations, utilizing sophisticated techniques from Natural Language Processing (NLP), Supervised Learning, and Reinforcement Learning to comprehend and generate text comparable to human-generated text. This article provides an overview of the training process and fundamental functionality of ChatGPT, accompanied by a preliminary review of the relevant literature. Notably, this article presents the first comprehensive literature review of this technology at the time of publication, aiming to aggregate all the available pertinent articles to facilitate further developments in the field. Ultimately, the authors aim to offer an appraisal of the technology’s potential implications on existing knowledge and technology, along with potential challenges that must be addressed. © 2023 by the authors."
"With the aim to enhance automation in conflict detection and resolution (CD&R) tasks in the air traffic management (ATM) domain, this article studies the use of artificial intelligence and machine learning (AI/ML) methods to learn air traffic controllers’ (ATCOs) policy in resolving conflicts among aircraft assessed to violate separation minimum constraints during the en route phase of flights, in the tactical phase of operations. The objective is to model (Formula presented.) conflicts are being resolved by ATCOs. Towards this goal, the article formulates the ATCO policy learning problem for conflict resolution, addresses the challenging issue of an inherent lack of information in real-world data, and presents AI/ML methods that learn models of ATCOs’ behavior. The methods are evaluated using real-world datasets. The results show that AI/ML methods can achieve good accuracy on predicting ATCOs’ actions given specific conflicts, revealing the preferences of ATCOs for resolution actions in specific circumstances. However, the high accuracy of predictions is hindered by real-world data-inherent limitations. © 2023 by the authors."
"Black spot identification, a spatiotemporal phenomenon, involves analysing the geographical location and time-based occurrence of road accidents. Typically, this analysis examines specific locations on road networks during set time periods to pinpoint areas with a higher concentration of accidents, known as black spots. By evaluating these problem areas, researchers can uncover the underlying causes and reasons for increased collision rates, such as road design, traffic volume, driver behaviour, weather, and infrastructure. However, challenges in identifying black spots include limited data availability, data quality, and assessing contributing factors. Additionally, evolving road design, infrastructure, and vehicle safety technology can affect black spot analysis and determination. This study focused on traffic accidents in Greek road networks to recognize black spots, utilizing data from police and government-issued car crash reports. The study produced a publicly available dataset called Black Spots of North Greece (BSNG) and a highly accurate identification method. Dataset: https://github.com/iokarama/BSNG-dataset (accessed on 15 June 2023). Dataset License: CC-BY-NC. © 2023 by the authors."
"Artificial Intelligence (AI) is a disruptive technology that nowadays has countless applications in many day-to-day and professional domains. Higher education institutions need to adapt both to changes in their processes and to changes in curricula brought on by AI. Studying students’ attitudes toward AI can be useful for analyzing what changes in AI teaching need to be implemented. This article uses an electronic survey to study the attitudes of Spanish students in the fields of economics and business management and education. A learning experience was also implemented with a small subset of students as a hands-on introduction to AI, where students were prompted to reflect on their experiences as users of AI. The results show that students are aware of AI’s impact and are willing to further their education in AI, although their current knowledge is limited due to a lack of training. We believe that AI education should be expanded and improved, especially by presenting realistic use cases and the real limitations of the technology, so that students are able to use AI confidently and responsibly in their professional future. © 2023 by the authors."
"In this paper, we study distributed inference and learning over networks which can be modeled by a directed graph. A subset of the nodes observes different features, which are all relevant/required for the inference task that needs to be performed at some distant end (fusion) node. We develop a learning algorithm and an architecture that can combine the information from the observed distributed features, using the processing units available across the networks. In particular, we employ information-theoretic tools to analyze how inference propagates and fuses across a network. Based on the insights gained from this analysis, we derive a loss function that effectively balances the model’s performance with the amount of information transmitted across the network. We study the design criterion of our proposed architecture and its bandwidth requirements. Furthermore, we discuss implementation aspects using neural networks in typical wireless radio access and provide experiments that illustrate benefits over state-of-the-art techniques. © 2023 by the authors."
"Developing applicable clinical machine learning models is a difficult task when the data includes spatial information, for example, radiation dose distributions across adjacent organs at risk. We describe the co-design of a modeling system, DASS, to support the hybrid human-machine development and validation of predictive models for estimating long-term toxicities related to radiotherapy doses in head and neck cancer patients. Developed in collaboration with domain experts in oncology and data mining, DASS incorporates human-in-the-loop visual steering, spatial data, and explainable AI to augment domain knowledge with automatic data mining. We demonstrate DASS with the development of two practical clinical stratification models and report feedback from domain experts. Finally, we describe the design lessons learned from this collaborative experience. © 2023 The Authors. Computer Graphics Forum published by Eurographics - The European Association for Computer Graphics and John Wiley & Sons Ltd."
"Video-recorded robotic-assisted surgeries allow the use of automated computer vision and artificial intelligence/deep learning methods for quality assessment and workflow analysis in surgical phase recognition. We considered a dataset of 209 videos of robotic-assisted laparoscopic inguinal hernia repair (RALIHR) collected from 8 surgeons, defined rigorous ground-truth annotation rules, then pre-processed and annotated the videos. We deployed seven deep learning models to establish the baseline accuracy for surgical phase recognition and explored four advanced architectures. For rapid execution of the studies, we initially engaged three dozen MS-level engineering students in a competitive classroom setting, followed by focused research. We unified the data processing pipeline in a confirmatory study, and explored a number of scenarios which differ in how the DL networks were trained and evaluated. For the scenario with 21 validation videos of all surgeons, the Video Swin Transformer model achieved ~0.85 validation accuracy, and the Perceiver IO model achieved ~0.84. Our studies affirm the necessity of close collaborative research between medical experts and engineers for developing automated surgical phase recognition models deployable in clinical settings. © 2023 by the authors."
"Background: The recent release of large language models for public use, such as ChatGPT and Google Bard, has opened up a multitude of potential benefits as well as challenges. Purpose: To evaluate and compare the accuracy and consistency of responses generated by publicly available ChatGPT and Google Bard to nonexpert questions related to lung cancer prevention, screening, and terminology commonly used in radiology reports based on the recommendation of Lung Imaging Reporting and Data System (Lung-RADS) version 2022 from the American College of Radiology and the Fleischner Society. Materials and Methods: Forty of the exact same questions were created and presented to ChatGPT-3.5 and Google Bard experimental version as well as Bing and Google search engines by three different authors of this article. Each answer was reviewed by two radiologists for accuracy. Responses were scored as correct, partially correct, incorrect, or unanswered. Consistency was also evaluated among the answers. Here, consistency was defined as the agreement between the three answers provided by ChatGPT-3.5, Google Bard experimental version, and Bing and Google search engines regardless of whether the concept conveyed was correct or incorrect. The accuracy among different tools was evaluated using statistical software. Results: ChatGPT-3.5 answered 120 questions, with 85 (70.8%) correct answers, 14 (11.7%) partially correct answers, and 21 (17.5%) incorrect answers. Google Bard did not answer 23 of the 120 questions (19.2%), while it provided an answer for 97 questions (80.8%). Google Bard’s answers were rated 62 (51.7%) correct, 11 (9.2%) partially correct, and 24 (20%) incorrect. Bing answered 120 questions, with 74 (61.7%) correct answers, 13 (10.8%) partially correct answers, and 33 (27.5%) incorrect answers. Google search engine answered 120 questions, with 66 (55%) correct answers, 27 (22.5%) partially correct answers, and 27 (22.5%) incorrect answers. ChatGPT-3.5 was more likely to provide correct or partially correct answers than Google Bard, by approximately 1.5-fold (odds ratio [OR] = 1.55, P = .004). ChatGPT-3.5 and the Google search engine were more likely to be consistent than Google Bard by approximately seven- and 29-fold (OR = 6.65 [P = .002] for ChatGPT-3.5 and 28.83 [P = .002] for the Google search engine). Conclusion: Although ChatGPT-3.5 had a higher accuracy in comparison with the other tools, neither ChatGPT nor Google Bard or the Bing and Google search engines answered all questions correctly and with 100% consistency. © RSNA, 2023."
"The demand for explainable and transparent models increases with the continued success of reinforcement learning. In this article, we explore the potential of generating shallow decision trees (DTs) as simple and transparent surrogate models for opaque deep reinforcement learning (DRL) agents. We investigate three algorithms for generating training data for axis-parallel and oblique DTs with the help of DRL agents (“oracles”) and evaluate these methods on classic control problems from OpenAI Gym. The results show that one of our newly developed algorithms, the iterative training, outperforms traditional sampling algorithms, resulting in well-performing DTs that often even surpass the oracle from which they were trained. Even higher dimensional problems can be solved with surprisingly shallow DTs. We discuss the advantages and disadvantages of different sampling methods and insights into the decision-making process made possible by the transparent nature of DTs. Our work contributes to the development of not only powerful but also explainable RL agents and highlights the potential of DTs as a simple and effective alternative to complex DRL models. © 2023 by the authors."
"The credibility of online examinations in Higher Education is hardened by numerous factors and use-case scenarios. This paper reports on a longitudinal study, that spanned over eighteen months, in which various stakeholders from three European Higher Education Institutions (HEIs) participated, aiming to identify core threat scenarios experienced during online examinations, and to, accordingly, propose threat models, data metrics and countermeasure features that HEI learning management systems can embrace to address the identified threat scenarios. We also report on a feasibility study of an open-source intelligent and continuous student identity management system, namely TRUSTID, which implements the identified data metrics and countermeasures. A user evaluation with HEI students (n = 133) revealed that the TRUSTID system is resilient and effective against impersonation attacks, based on intelligent face and voice identification mechanisms, and scored well in usability and user experience. Aspects concerning the preservation of privacy in storing, retrieving and processing sensitive personal data are also discussed. © 2023 by the authors."
"AI-based models have shown promising results in diagnosing eye diseases based on multi-sources of data collected from medical IOT systems. However, there are concerns regarding their generalization and robustness, as these methods are prone to overfitting specific datasets. The development of Explainable Artificial Intelligence (XAI) techniques has addressed the black-box problem of machine learning and deep learning models, which can enhance interpretability and trustworthiness and optimize their performance in the real world. Age-related macular degeneration (AMD) is currently the primary cause of vision loss among elderly individuals. In this study, XAI methods were applied to detect AMD using various ophthalmic imaging modalities collected from medical IOT systems, such as colorful fundus photography (CFP), optical coherence tomography (OCT), ultra-wide fundus (UWF) images, and fluorescein angiography fundus (FAF). An optimized deep learning (DL) model and novel AMD identification systems were proposed based on the insights extracted by XAI. The findings of this study demonstrate that XAI not only has the potential to improve the transparency, reliability, and trustworthiness of AI models for ophthalmic applications, but it also has significant advantages for enhancing the robustness performance of these models. XAI could play a crucial role in promoting intelligent ophthalmology and be one of the most important techniques for evaluating and enhancing ophthalmic AI systems. © 2023 by the authors."
"Autoimmune (AI) diseases, which present in a multitude of systemic manifestations, have been connected to many underlying factors. These factors include the environment, genetics, individual microbiomes, and diet. An individual’s gut microbiota is an integral aspect of human functioning, as it is intimately integrated into the metabolic, mechanical, immunological, and neurologic pathways of the body. The microbiota dynamically changes throughout our lifetimes and is individually unique. While the gut microbiome is ever-adaptive, gut dysbiosis can exert a significant influence on physical and mental health. Gut dysbiosis is a common factor in various AI, and diets with elevated fat and sugar content have been linked to gut microbiome alterations, contributing to increased systemic inflammation. Additionally, multiple AI’s have increased levels of certain inflammatory markers such as TNF-a, IL-6, and IL-17 that have been shown to contribute to arthropathy and are also linked to increased levels of gut dysbiosis. While chronic inflammation has been shown to affect many physiologic systems, this review explores the connection between gut microbiota, bone metabolism, and the skeletal and joint destruction associated with various AI, including psoriatic arthritis, systemic lupus erythematosus, irritable bowel disease, and rheumatoid arthritis. This review aims to define the mechanisms of microbiome crosstalk between the cells of bone and cartilage, as well as to investigate the potential bidirectional connections between AI, bony and cartilaginous tissue, and the gut microbiome. By doing this, the review also introduces the concept of altering an individual’s specific gut microbiota as a form of regenerative medicine and potential tailored therapy for joint destruction seen in AI. We hope to show multiple, specific ways to target the microbiome through diet changes, rebalancing microbial diversity, or decreasing specific microbes associated with increased gut permeability, leading to reduced systemic inflammation contributing to joint pathology. Additionally, we plan to show that diet alterations can promote beneficial changes in the gut microbiota, supporting the body’s own endogenous processes to decrease inflammation and increase healing. This concept of microbial alteration falls under the definition of regenerative medicine and should be included accordingly. By implementing microbial alterations in regenerative medicine, this current study could lend increasing support to the current research on the associations of the gut microbiota, bone metabolism, and AI-related musculoskeletal pathology. © 2023 by the authors."
"This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI’s Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming capabilities to Apple’s Xcode for mobile software development. This paper also presents the challenges of and opportunities for incorporating NLP techniques with software naturalness, empowering developers with advanced coding assistance and streamlining the software development process. © 2023 by the authors."
"An intelligent conversational agent for the legal domain is an AI-powered system that can communicate with users in natural language and provide legal advice or assistance. In this paper, we present CREA2, an agent designed to process legal concepts and be able to guide users on legal matters. The conversational agent can help users navigate legal procedures, understand legal jargon, and provide recommendations for legal action. The agent can also give suggestions helpful in drafting legal documents, such as contracts, leases, and notices. Additionally, conversational agents can help reduce the workload of legal professionals by handling routine legal tasks. CREA2, in particular, will guide the user in resolving disputes between people residing within the European Union, proposing solutions in controversies between two or more people who are contending over assets in a divorce, an inheritance, or the division of a company. The conversational agent can later be accessed through various channels, including messaging platforms, websites, and mobile applications. This paper presents a retrieval system that evaluates the similarity between a user’s query and a given question. The system uses natural language processing (NLP) algorithms to interpret user input and associate responses by addressing the problem as a semantic search similar question retrieval. Although a common approach to question and answer (Q&A) retrieval is to create labelled Q&A pairs for training, we exploit an unsupervised information retrieval system in order to evaluate the similarity degree between a given query and a set of questions contained in the knowledge base. We used the recently proposed SBERT model for the evaluation of relevance. In the paper, we illustrate the effective design principles, the implemented details and the results of the conversational system and describe the experimental campaign carried out on it. © 2023 by the authors."
"As a concept that is somewhat under emergence, the notion of the Metaverse varies across different academic articles. Yet there is a shared view on the benefits to its ongoing implementation, particularly for digital art, where the technology can provide a new metric for artists to showcase and sell their artwork to a global audience with minimal barriers, and for consumers to have an unbounded experience not limited by physical space or museum entry fees. In this article, a contribution is provided to a broader conversation about the future of the digital art and the Metaverse and its role in shaping our online culture. We discuss the concept of the Metaverse, its structure, the role of artificial intelligence and the benefits (and limitations) the technology holds for digital art. For a case study, we develop a 3D art gallery housing an art collection generated using artificial-intelligence-based techniques such as diffusion models. A total of 67 individuals are surveyed from three pools (two in-person and one online-based), with questions relating to the future of digital art, the Metaverse and artificial intelligence. Findings include that the majority of participants were familiar with the concept of the Metaverse and overall, they had a predominately optimistic view of both the use artificial intelligence for art, and the use of the Metaverse to support digital art, with 85.3% of the participants having already seen artificial-intelligence-based artwork. The identification of consumer segments further highlights the importance of finding customised solutions, considering consumers’ heterogenous preferences for AI-generated art. Research presented in this article will be beneficial for those looking to explore the Metaverse for artwork and develop virtual galleries, and the findings further highlight the Metaverse as a potential democratising force in the art world. © 2023 by the authors."
"Optimizing water distribution both from an energy-saving perspective and from a quality of service perspective is a challenging task since it involves a complex system with many nodes, many hidden variables and many operational constraints. For this reason, water distribution systems need to handle a delicate trade-off between the effectiveness and computational time of the solution. In this paper, we propose a new computationally efficient method, named rule-based control, to optimize water distribution networks without the need for a rigorous formulation of the optimization problem. As a matter of fact, since it is based on a machine learning approach, the proposed method employs only a set of historical data, where the configuration can be labeled according to a quality criterion. Since it is a data-driven approach, it could be applied to any complex network where historical labeled data are available. In particular, rule-based control exploits a rule-based classification method that allows us to retrieve the rules leading to good or bad performances of the system, even without any information about its physical laws. The evaluation of the results on some simulated scenarios shows that the proposed approach is able to reduce energy consumption while ensuring a good quality of the service. The proposed approach is currently used in the water distribution system of the Milan (Italy) water main. © 2023 by the authors."
"The purpose of the paper is to present a model of factors affecting the successful project implementation by introducing agility and artificial intelligence to increase the company’s competitiveness. In the model, the multidimensional constructs describing the implementation of an agile work environment and artificial intelligence technologies and tools were developed. These multidimensional constructs are agile work environment, agile leadership, agile team skills and capabilities, improving the work of the leader in the project, adopting AI technologies in the project, and using AI solutions in a project. Their impact on successful project implementation and on the company competitiveness was tested. The fundamental reason for conducting this research and developing the model is to enhance the understanding of factors that contribute to the successful implementation of projects and to increase a company’s competitiveness. Our developed model encompasses multidimensional constructs that describe the agile work environment and the utilization of AI technologies. By examining the impact of these constructs on both successful project implementation and company competitiveness, we aimed to establish a comprehensive framework that captures the relationship between agility, AI, and successful project implementation. This model serves as a valuable tool for companies seeking to improve their project implementation processes and gain a competitive edge in the market. The research was based on a sample of 473 managers/owners in medium-sized and large companies. Structural equation modeling was used to test the hypotheses. In today’s turbulent environment, the results will help develop guidelines for a successful combination of agile business practices and artificial intelligence to achieve successful project implementation, increasing a company’s competitiveness. © 2023 by the authors."
"This study provides a structured literature review of the recent COllaborative roBOT (COBOT) applications in industrial and service contexts. Several papers and research studies were selected and analyzed, observing the collaborative robot interactions, the control technologies and the market impact. This review focuses on stationary COBOTs that may guarantee flexible applications, resource efficiency, and worker safety from a fixed location. COBOTs offer new opportunities to develop and integrate control techniques, environmental recognition of time-variant object location, and user-friendly programming to interact safely with humans. Artificial Intelligence (AI) and machine learning systems enable and boost the COBOT’s ability to perceive its surroundings. A deep analysis of different applications of COBOTs and their properties, from industrial assembly, material handling, service personal assistance, security and inspection, Medicare, and supernumerary tasks, was carried out. Among the observations, the analysis outlined the importance and the dependencies of the control interfaces, the intention recognition, the programming techniques, and virtual reality solutions. A market analysis of 195 models was developed, focusing on the physical characteristics and key features to demonstrate the relevance and growing interest in this field, highlighting the potential of COBOT adoption based on (i) degrees of freedom, (ii) reach and payload, (iii) accuracy, and (iv) energy consumption vs. tool center point velocity. Finally, a discussion on the advantages and limits is summarized, considering anthropomorphic robot applications for further investigations. © 2023 by the authors."
"Background: Deep learning is an important means to realize the automatic detection, segmentation, and classification of pulmonary nodules in computed tomography (CT) images. An entire CT scan cannot directly be used by deep learning models due to image size, image format, image dimensionality, and other factors. Between the acquisition of the CT scan and feeding the data into the deep learning model, there are several steps including data use permission, data access and download, data annotation, and data preprocessing. This paper aims to recommend a complete and detailed guide for researchers who want to engage in interdisciplinary lung nodule research of CT images and Artificial Intelligence (AI) engineering. Methods: The data preparation pipeline used the following four popular large-scale datasets: LIDC-IDRI (Lung Image Database Consortium image collection), LUNA16 (Lung Nodule Analysis 2016), NLST (National Lung Screening Trial) and NELSON (The Dutch-Belgian Randomized Lung Cancer Screening Trial). The dataset preparation is presented in chronological order. Findings: The different data preparation steps before deep learning were identified. These include both more generic steps and steps dedicated to lung nodule research. For each of these steps, the required process, necessity, and example code or tools for actual implementation are provided. Discussion and conclusion: Depending on the specific research question, researchers should be aware of the various preparation steps required and carefully select datasets, data annotation methods, and image preprocessing methods. Moreover, it is vital to acknowledge that each auxiliary tool or code has its specific scope of use and limitations. This paper proposes a standardized data preparation process while clearly demonstrating the principles and sequence of different steps. A data preparation pipeline can be quickly realized by following these proposed steps and implementing the suggested example codes and tools. © 2023"
"Background: Artificial intelligence (AI)-based mobile phone apps (mHealth) have the potential to streamline care for suspicious skin lesions in primary care. This study aims to investigate the conditions and feasibility of a study that incorporates an AI-based app in primary care and evaluates its potential impact. Methods: We conducted a pilot feasibility study from November 22nd, 2021 to June 9th, 2022 with a mixed-methods design on implementation of an AI-based mHealth app for skin cancer detection in three primary care practices in the Netherlands (Rotterdam, Leiden and Katwijk). The primary outcome was the inclusion and successful participation rate of patients and general practitioners (GPs). Secondary outcomes were the reasons, facilitators and barriers for successful participation and the potential impact in both pathways for future sample size calculations. Patients were offered use of an AI-based mHealth app before consulting their GP. GPs assessed the patients blinded and then unblinded to the app. Qualitative data included observations and audio-diaries from patients and GPs and focus-groups and interviews with GPs and GP assistants. Findings: Fifty patients were included with a median age of 52 years (IQR 33.5–60.3), 64% were female, and 90% had a light skin type. The average patient inclusion rate was 4–6 per GP practice per month and 84% (n = 42) successfully participated. Similarly, in 90% (n = 45 patients) the GPs also successfully completed the study. GPs never changed their working diagnosis, but did change their treatment plan (n = 5) based on the app's assessments. Notably, 54% of patients with a benign skin lesion and low risk rating, indicated that they would be reassured and cancel their GP visit with these results (p < 0.001). Interpretation: Our findings suggest that studying implementation of an AI-based mHealth app for detection of skin cancer in the hands of patients or as a diagnostic tool used by GPs in primary care appears feasible. Preliminary results indicate potential to further investigate both intended use settings. Funding: SkinVision B.V. © 2023 The Author(s)"
"• AIM: To investigate the changes in the macular microvasculature in eyes with central retinal artery occlusion (CRAO) and paracentral acute middle maculopathy (PAMM) . •METHODS: Retrospective study. A total of 27 cases (27 eyes) who diagnosed with CRAO - PAMM and 29 patients (29 eyes) diagnosed as CRAO but with no PAMM were hospitalized in our hospital from January 2020 to December 2021. There were 33 normal people (33 eyes) who underwent physical examination in our hospital selected as control group. Optical coherence tomography angiography (OCTA) was used to measure retinal blood flow and thickness parameters in the 3 mm× 3 mm area of the macula. The correlation among macular retinal blood flow density, retinal thickness, foveal avascular zone (FAZ) area, FAZ perimeter, acircularity index (AI), flow density in a 300 - μ m - wide region around the FAZ (FD - 300) and lesion area, best corrected visual acuity (BCVA) in the CRAO- PAMM group was analyzed. • RESULTS: Among the three groups, there were significant differences in the overall and parafoveal blood flow density of superficial capillary layer (SCP) and deep capillary layer (DCP), foveal thickness, FAZ area, FAZ perimeter, AI and FD - 300 (all P < 0. 05) . In the CRAO - PAMM group, the lesion area was negatively correlated with DCP overall and parafoveal blood flow density (r = - 0.569, P = 0. 002; r = - 0. 543, P = 0. 004), and positively correlated with the parafoveal thickness (r = 0. 606, P = 0.001); BCVA (LogMAR) was negatively correlated with DCP foveal and parafoveal blood flow density (r = - 0.433, P = 0. 024; r = - 0. 515, P = 0. 006), and positively correlated with FAZ area, perimeter and lesion area (r = 0. 484, P = 0.011; r = 0.531, P = 0.004; r = 0.417, P = 0.030) . • CONCLUSION: Patients with CRAO and PAMM have lower macular blood flow density, heavier macular edema and poorer visual acuity, and BCVA may be influenced by both lesion area and FAZ area. © 2023 International Journal of Ophthalmology (c/o Editorial Office). All rights reserved."
"Artificial intelligence and emerging data science techniques are being leveraged to interpret medical image scans. Traditional image analysis relies on visual interpretation by a trained radiologist, which is time-consuming and can, to some degree, be subjective. The development of reliable, automated diagnostic tools is a key goal of radiomics, a fast-growing research field which combines medical imaging with personalized medicine. Radiomic studies have demonstrated potential for accurate lung cancer diagnoses and prognostications. The practice of delineating the tumor region of interest, known as segmentation, is a key bottleneck in the development of generalized classification models. In this study, the incremental multiple resolution residual network (iMRRN), a publicly available and trained deep learning segmentation model, was applied to automatically segment CT images collected from 355 lung cancer patients included in the dataset “Lung-PET-CT-Dx”, obtained from The Cancer Imaging Archive (TCIA), an open-access source for radiological images. We report a failure rate of 4.35% when using the iMRRN to segment tumor lesions within plain CT images in the lung cancer CT dataset. Seven classification algorithms were trained on the extracted radiomic features and tested for their ability to classify different lung cancer subtypes. Over-sampling was used to handle unbalanced data. Chi-square tests revealed the higher order texture features to be the most predictive when classifying lung cancers by subtype. The support vector machine showed the highest accuracy, 92.7% (0.97 AUC), when classifying three histological subtypes of lung cancer: adenocarcinoma, small cell carcinoma, and squamous cell carcinoma. The results demonstrate the potential of AI-based computer-aided diagnostic tools to automatically diagnose subtypes of lung cancer by coupling deep learning image segmentation with supervised classification. Our study demonstrated the integrated application of existing AI techniques in the non-invasive and effective diagnosis of lung cancer subtypes, and also shed light on several practical issues concerning the application of AI in biomedicine. © 2023 by the authors."
"In this article, we advocate for the design of ad hoc artificial intelligence (AI)/machine learning (ML) models to facilitate their usage in future smart infrastructures based on communication networks. To motivate this, we first review key operations identified by the 3GPP for transferring AI/ML models through 5G networks and the main existing techniques to reduce their communication overheads. We also present a novel communication-aware ML framework, which we refer to as Accordion, that enables an efficient AI/ML model transfer thanks to an overhauled model training and communication protocol. We demonstrate the communication-related benefits of Accordion, analyse key performance trade-offs, and discuss potential research directions within this realm. © 1979-2012 IEEE."
"Background: Although several clinical breast cancer risk models are used to guide screening and prevention, they have only moderate discrimination. Purpose: To compare selected existing mammography artificial intelligence (AI) algorithms and the Breast Cancer Surveillance Consortium (BCSC) risk model for prediction of 5-year risk. Materials and Methods: This retrospective case-cohort study included data in women with a negative screening mammographic examination (no visible evidence of cancer) in 2016, who were followed until 2021 at Kaiser Permanente Northern California. Women with prior breast cancer or a highly penetrant gene mutation were excluded. Of the 324 009 eligible women, a random subcohort was selected, regardless of cancer status, to which all additional patients with breast cancer were added. The index screening mammographic examination was used as input for five AI algorithms to generate continuous scores that were compared with the BCSC clinical risk score. Risk estimates for incident breast cancer 0 to 5 years after the initial mammographic examination were calculated using a time-dependent area under the receiver operating characteristic curve (AUC). Results: The subcohort included 13 628 patients, of whom 193 had incident cancer. Incident cancers in eligible patients (additional 4391 of 324 009) were also included. For incident cancers at 0 to 5 years, the time-dependent AUC for BCSC was 0.61 (95% CI: 0.60, 0.62). AI algorithms had higher time-dependent AUCs than did BCSC, ranging from 0.63 to 0.67 (Bonferroni-adjusted P < .0016). Time-dependent AUCs for combined BCSC and AI models were slightly higher than AI alone (AI with BCSC time-dependent AUC range, 0.66-0.68; Bonferroni-adjusted P < .0016). Conclusion: When using a negative screening examination, AI algorithms performed better than the BCSC risk model for predicting breast cancer risk at 0 to 5 years. Combined AI and BCSC models further improved prediction.  © RSNA, 2023."
"Internet of Things (IoT) technologies, including drones, can efficiently capture industrial data, promoting the fourth industrial revolution, Industry 4.0. Moreover, as the 5G technologies evolve, Edge AI can push the AI programs from the remote cloud to the network edges close to end devices, enabling reliable and low-latency intelligent services. Compared with traditional applications, Industry 4.0 applications require more accuracy and lower latency. Most importantly, the robustness of Edge AI system is also critical for Industry 4.0 applications. In this work, we propose a robust Edge AI system for real-time industry 4.0 applications. Our proposed robust AI system can conduct model combination design and model deployment design based on the demands of applications, for example, application accuracy and application latency. Our system is also robust to physical system failures and resumes running intermediately when physical system failures occur.  © 2017 IEEE."
"Transition metal oxide materials are of great utility, with a diversity of topical applications ranging from catalysis to electronic devices. Because of their widespread importance in materials science, there is increasing interest in developing computational tools capable of reliable prediction of transition metal oxide phase behavior and properties. The workhorse of materials theory is density functional theory (DFT). Accordingly, we have investigated the impact of various correlation and exchange approximations on their ability to predict the properties of NiO using DFT. We have chosen NiO as a particularly challenging representative of transition metal oxides in general. In so doing, we have provided validation for the use of the r2SCAN density functional for predicting the materials properties of oxides. r2SCAN yields accurate structural properties of NiO and a local spin moment that notably persists under pressure, consistent with experiment. The outcome of our study is a pragmatic scheme for providing electronic structure data to enable the parameterization of interatomic potentials using state-of-the-art artificial intelligence (AI) and machine learning (ML) methodologies. The latter is essential to allow large scale molecular dynamics simulations of bulk and surface materials phase behavior and properties with ab initio accuracy. © 2023 Author(s)."
[No abstract available]
[No abstract available]
"Multi-criteria ABC classification is a useful model for automatic inventory management and optimization. This model enables a rapid classification of inventory items into three groups, having varying managerial levels. Several methods, based on different criteria and principles, were proposed to build the ABC classes. However, existing ABC classification methods operate as black-box AI processes that only provide assignments of the items to the different ABC classes without providing further managerial explanations. The multi-criteria nature of the inventory classification problem makes the utilization and the interpretation of item classes difficult, without further information. Decision makers usually need additional information regarding important characteristics that were crucial in determining the managerial classes of the items because such information can help managers better understand the inventory groups and make inventory management decisions more transparent. To address this issue, we propose a two-phased explainable approach based on eXplainable Artificial Intelligence (XAI) capabilities. The proposed approach provides both local and global explanations of the built ABC classes at the item and class levels, respectively. Application of the proposed approach in inventory classification of a firm, specialized in retail sales, demonstrated its effectiveness in generating accurate and interpretable ABC classes. Assignments of the items to the different ABC classes were well-explained based on the item’s criteria. The results in this particular application have shown a significant impact of the sales, profit, and customer priority as criteria that had an impact on determining the item classes. © 2023 by the authors."
"Advanced Raman spectroscopy (RS) systems have gained new interest in the field of medicine as an emerging tool for in vivo tissue discrimination. The coupling of RS with artificial intelligence (AI) algorithms has given a boost to RS to analyze spectral data in real time with high specificity and sensitivity. However, limitations are still encountered due to the large amount of clinical data which are required for the pre-training process of AI algorithms. In this study, human healthy and cancerous colon specimens were surgically resected from different sites of the ascending colon and analyzed by RS. Two transfer learning models, the one-dimensional convolutional neural network (1D-CNN) and the 1D–ResNet transfer learning (1D-ResNet) network, were developed and evaluated using a Raman open database for the pre-training process which consisted of spectra of pathogen bacteria. According to the results, both models achieved high accuracy of 88% for healthy/cancerous tissue discrimination by overcoming the limitation of the collection of a large number of spectra for the pre-training process. This gives a boost to RS as an adjuvant tool for real-time biopsy and surgery guidance. © 2023 by the authors."
"The maritime industry plays a crucial role in the global economy, with roughly 90% of world trade being conducted through the use of merchant ships and more than a million seafarers. Despite recent efforts to improve reliability and ship structure, the heavy dependence on human performance has led to a high number of casualties in the industry. Decision errors are the primary cause of maritime accidents, with factors such as lack of situational awareness and attention deficit contributing to these errors. To address this issue, the study proposes an Ant Colony Optimization (ACO) based algorithm to design and validate a verified set of instructions for performing each daily operational task in a standardised manner. This AI‐based approach can optimise the path for complex tasks, provide clear and sequential instructions, improve efficiency, and reduce the likelihood of human error by minimising personal preference and false assumptions. The proposed solution can be transformed into a globally accessible, standardised instructions manual, which can significantly contribute to minimising human error during daily operational tasks on ships. © 2023, Faculty of Navigation, Gdynia Maritime University. All rights reserved."
"Since the start of 5G work in 3GPP in early 2016, tremendous progress has been made in both standardization and commercial deployments. 3GPP is now entering the second phase of 5G standardization, known as 5G-Advanced, built on the 5G baseline in 3GPP Releases 15, 16, and 17. 3GPP Release 18, the start of 5G-Advanced, includes a diverse set of features that cover both device and network evolutions, providing balanced mobile broadband evolution and further vertical domain expansion and accommodating both immediate and long-term commercial needs. 5G-Advanced will significantly expand 5G capabilities, address many new use cases, transform connectivity experiences, and serve as an essential step in developing mobile communications towards 6G. This paper provides a comprehensive overview of the 3GPP 5G-Advanced development, introducing the prominent state-of-the-art technologies investigated in 3GPP and identifying key evolution directions for future research and standardization.  © 1983-2012 IEEE."
"Most of the power electronic converters based on the devices such as Silicon Controlled Rectifiers (SCRs) have been broadly utilized in home, business, and modern use in recent years. Despite their many benefits, these power electronic converters have major issues such as pulling harmonic current and the reactive part of the current from the supply, as well as having a highly nonlinear characteristic. The harmonics produced by the current supplied by these nonlinear elements cause voltage distortion at the common coupling point, which is causing problems for the functioning of number of sensitive instruments and other consumer appliances. Artificial Neural Networks (ANN) are a type of Artificial Intelligence (AI) approach that has been applied to improve the efficiency and regulation of the converter. In order to avoid the need for a Digital Signal Processors (DSP) by avoiding the online timing computations for various voltage space vectors in various regions and sectors and produce higher pulse resolution, an ANN-based space vector pulse width modulation (SVPWM) technique is proposed in this paper. The analysis of a 3-layered feedforward back propagation ANN algorithm based SVPWM control for NPC converter used to integrate PV source to grid has been evaluated and found to be better as compared to traditional techniques.  © 2023 IOP Publishing Ltd."
"Artificial intelligence (AI) has revolutionized many fields, including bioimage analysis. Powerful AI-based tools enable the analysis of large image datasets with unprecedented accuracy. However, easy-to-use tools are needed to allow experimentalists without specialised technical knowledge to leverage AI to its full potential. Here, we discuss our efforts to develop an intuitive and open-source bioimage analysis software that enables scientists to easily access state-of-the-art AI models. © 2023, Die Autoren."
"An important precondition for the development of artificial intelligence (AI) in network industries is the access to big data and the attendant necessities of data sharing and data portability. The goal of this paper is to analyze the changing needs of entrepreneurial decision-making to exhaust the innovation potential of AI-driven big data value chains taking into account AI-specific ethical, security and privacy regulations. The analytical concept of AI-powered big data virtual networks is investigated with a focus on the governance of 5G-based big data value chains required for Internet of Things (IoT) applications in particular smart networks. Although several actors may be involved—such as broadband providers, cloud service providers, geopositioning service providers, or sensor network service providers—the final responsibility for bundling these different service components lies in the hands of the AI-powered big data virtual network providers. In addition to the required data privacy and security regulations, the exploration of new liability rules for AI interacting with traditional technologies is becoming relevant, taking into account AI-specific ethical and transparency obligations. Firstly, the complementary roles of the EU data regulatory framework and European AI regulatory framework are examined. Secondly, the network economic concept of AI-powered big data virtual networks is elaborated taking into account the required regulations. Thirdly, the heterogeneity of AI systems—required for a variety of IoT applications—is considered, with a particular focus on the application of AI within the transportation sector. © The Author(s) 2023."
"Large-scale language-image (LLI) models have the potential to open new forms of critical practice through architectural research. Their success enables designers to research within discourses that are profoundly connected to the built environment but did not previously have the resources to engage in spatial research. Although LLI models do not generate coherent building ensembles, they offer an esthetic experience of an AI infused design practice. This paper contextualizes diffusion models architecturally. Through a comparison of approaches to diffusion models in architecture, this paper outlines data-centric methods that allow architects to design critically using computation. The design of text-driven latent spaces extends the histories of typological design to synthetic environments including non-building data into an architectural space. More than synthesizing quantic ratios in various arrangements, the architect contributes by assessing new categorical differences into generated work. The architects’ creativity can elevate LLI models with a synthetic architecture, nonexistent in the data sets the models learned from. © The Author(s) 2023."
"International students face unique challenges in pursuing higher education in a foreign country. To address these challenges and enhance their academic experience, higher education institutions are increasingly exploring the use of artificial intelligence (AI) applications. This research essay aims to investigate the impact of AI on the education of international students. Instead of a traditional literature review, it employs a research approach to examine the potential applications of AI and discuss associated concerns. The research paper explores various AI applications, such as personalized learning experiences, adaptive testing, predictive analytics, and chatbots for learning and research. By analyzing the role of AI in education for international students, this research paper sheds light on how AI can improve learning efficiency and provide customized educational support. Additionally, it identifies significant risks and limitations, including privacy concerns, cultural differences, language proficiency, and ethical implications, which must be effectively addressed. The findings contribute to a better understanding of the potential impact of AI on international students’ educational experiences and offer insights into the integration of AI into educational administration and learning processes. © 2023 by the authors."
"The art-as-research described in this article-a project named The Drowned World, after J.G. Ballard’s dystopian novel of the same name-is a reflection on environmental collapse and its technical representation in the Anthropocene. Is imagination being subsumed by the artificial? Are the sociotechnic hyperobjects of artificial intelligence and global warming chimera of the imagination, or are they certainties emergent from a desire for virtuality? In The Drowned World project, the author proposes to employ text-driven image synthesis as an aesthetic apparatus obscuring the distinction between imagination (potentiality) and virtuality (artificiality) to offer a computational poetics of a world drowning in data, an imaginative ecology of the virtual sublime. © 2023, MIT Press Journals. All rights reserved."
"In this article, the authors assesses the scope of the use of artificial intelligence technology in the management of the US federal public finance system in the years 2019-2022, with particular emphasis on public finance control. This assessment is indispensable in answering the question of whether artificial intelligence, which is a relatively new technology, is used in the area in question, and to what extent, and most importantly whether, it enables the US federal administration to control public finances. In order to answer this question, a holistic review was carried out of American law standards from the last decade and government documents that directly regulate the issue of artificial intelligence, including the definition of artificial intelligence adopted for the first time in American law, but also a number of other hard law and soft law standards. Law files that are directly or indirectly related to AI. The above allowed the determination of the scope of the political and legal approach to artificial intelligence in the United States of America, as well as the extraction of many conclusions about the use of artificial intelligence and the challenges facing its development. In the opinion of the author, the added value of this publication is the preliminary answer to the question of whether artificial intelligence can change the paradigm of the essence of managing the public finance system. © 2023 Sebastian Skuza et al., published by Sciendo."
[No abstract available]
"In future decades, the demand for poultry meat and eggs is predicted to considerably increase in pace with human population growth. Although this expansion clearly represents a remarkable opportunity for the sector, it conceals a multitude of challenges. Pollution and land erosion, competition for limited resources between animal and human nutrition, animal welfare concerns, limitations on the use of growth promoters and antimicrobial agents, and increasing risks and effects of animal infectious diseases and zoonoses are several topics that have received attention from authorities and the public. The increase in poultry production must be achieved mainly through optimization and increased efficiency. The increasing ability to generate large amounts of data (“big data”) is pervasive in both modern society and the farming industry. Information accessibility—coupled with the availability of tools and computational power to store, share, integrate, and analyze data with automatic and flexible algorithms—offers an unprecedented opportunity to develop tools to maximize farm profitability, reduce socio-environmental impacts, and increase animal and human health and welfare. A detailed description of all topics and applications of big data analysis in poultry farming would be infeasible. Therefore, the present work briefly reviews the application of sensor technologies, such as optical, acoustic, and wearable sensors, as well as infrared thermal imaging and optical flow, to poultry farming. The principles and benefits of advanced statistical techniques, such as machine learning and deep learning, and their use in developing effective and reliable classification and prediction models to benefit the farming system, are also discussed. Finally, recent progress in pathogen genome sequencing and analysis is discussed, highlighting practical applications in epidemiological tracking, and reconstruction of microorganisms’ population dynamics, evolution, and spread. The benefits of the objective evaluation of the effectiveness of applied control strategies are also considered. Although human-artificial intelligence collaborations in the livestock sector can be frightening because they require farmers and employees in the sector to adapt to new roles, challenges, and competencies—and because several unknowns, limitations, and open-ended questions are inevitable—their overall benefits appear to be far greater than their drawbacks. As more farms and companies connect to technology, artificial intelligence (AI) and sensing technologies will begin to play a greater role in identifying patterns and solutions to pressing problems in modern animal farming, thus providing remarkable production-based and commercial advantages. Moreover, the combination of diverse sources and types of data will also become fundamental for the development of predictive models able to anticipate, rather than merely detect, disease occurrence. The increasing availability of sensors, infrastructures, and tools for big data collection, storage, sharing, and analysis—together with the use of open standards and integration with pathogen molecular epidemiology—have the potential to address the major challenge of producing higher-quality, more healthful food on a larger scale in a more sustainable manner, thereby protecting ecosystems, preserving natural resources, and improving animal and human welfare and health. © 2023 by the authors."
[No abstract available]
"In this study, the key areas and current trends in the field of big data applications in the insurance industry are identified, along with suggestions for future research initiatives. We identified the most prominent authors, journals, organizations, and countries based on their total publications and citations, showing their significance within the network, using bibliometric analysis on a sample of 191 articles retrieved from Scopus from 1976 to 2021. VOSviewer and R-Biblioshiny tools were used to generate the bibliometric output on these retrieved papers. The findings showed that although while a good number of writers from other parts of the world contributed to the literature on big data applications in the insurance industry, during this time, most research papers have listed the United States, India, and China as their affiliated countries. The yearly publication was either one or two, with some discontinuity, from 1976 to 2011, but since 2012, it has increased, exhibiting an exponential growth tendency. The three journals ""Risks,""""Applied Stochastic Models in Business and Industry,""and ""Expert Systems with Applications""are the most popular for including a sizable number of papers in the field of big data technologies in the insurance sector. Each of the top 10 authors in this field published two research papers during these 46 years. Seven areas, including fraud detection and prevention, risk assessment, pricing & rate making, technology utilization, risk management, claim processing & prediction, and finally digitalization, were the major focus of research papers on bigdata applications in the insurance business. The human-centered AI system development, adoption of wearable technology, personalization, and other topics were found to have received very little attention in this study. As a result, the researchers may now direct future research in this area. This study is completely new of its kind in the domain of insurance though few documents are available on the broad concept of finance. © 2023 The Author(s)."
"Magnetic resonance imaging (MRI) nowadays plays an important role in the identification of brain underpinnings in a wide range of neuropsychiatric disorders, including Autism Spectrum Disorders (ASD). Characterizing the hallmarks in these pathologies is not a straightforward task and machine learning (ML) is certainly one of the most promising tools for addressing complex and non-linear problems. ML algorithms and, in particular, deep neural networks (DNNs), need large datasets in order to be properly trained and thus ensure generalization capabilities on new data. Large datasets can be obtained by collecting images from different centers, thus bringing unavoidable biases in the analysis due to differences in hardware and scanning protocols between different centers. In this work, we dealt with the issue of multicenter MRI data harmonization by comparing two different approaches: the analytical ComBat-GAM procedure, whose effectiveness is already documented in the literature, and an originally developed site-adversarial deep neural network (ad-DNN). The latter aims to perform a classification task while simultaneously searching for site-relevant patterns in order to make predictions free from site-related biases. As a case study, we implemented DNN and ad-DNN classifiers to distinguish subjects with ASD with respect to typical developing controls based on functional connectivity measures derived from data of the multicenter ABIDE collection. The classification performance of the proposed ad-DNN, measured in terms of the area under the ROC curve (AUC), achieved the value of AUC = (Formula presented.), which is comparable to that obtained by a DNN on data harmonized according to the analytical procedure (AUC = (Formula presented.)). The relevant functional connectivity alterations identified by both procedures showed an agreement between each other and with the patterns of neuroanatomical alterations previously detected in the same cohort of subjects. © 2023 by the authors."
"Motivation: Utilizing AI-driven approaches for drug-target interaction (DTI) prediction require large volumes of training data which are not available for the majority of target proteins. In this study, we investigate the use of deep transfer learning for the prediction of interactions between drug candidate compounds and understudied target proteins with scarce training data. The idea here is to first train a deep neural network classifier with a generalized source training dataset of large size and then to reuse this pre-trained neural network as an initial configuration for re-training/fine-tuning purposes with a small-sized specialized target training dataset. To explore this idea, we selected six protein families that have critical importance in biomedicine: kinases, G-protein-coupled receptors (GPCRs), ion channels, nuclear receptors, proteases, and transporters. In two independent experiments, the protein families of transporters and nuclear receptors were individually set as the target datasets, while the remaining five families were used as the source datasets. Several size-based target family training datasets were formed in a controlled manner to assess the benefit provided by the transfer learning approach. Results: Here, we present a systematic evaluation of our approach by pre-training a feed-forward neural network with source training datasets and applying different modes of transfer learning from the pre-trained source network to a target dataset. The performance of deep transfer learning is evaluated and compared with that of training the same deep neural network from scratch. We found that when the training dataset contains fewer than 100 compounds, transfer learning outperforms the conventional strategy of training the system from scratch, suggesting that transfer learning is advantageous for predicting binders to under-studied targets.  © 2023 The Author(s). Published by Oxford University Press."
"Digital innovations in the field of border control, migration and asylum have increasingly become a hot topic in migration studies due to the exponential growth of tech-based tools used in these fields. This article focuses on a new aspect of this phenomenon, reflected by the growing interest, in and outside the EU, in implementing artificial intelligence (AI) systems, as new tools tested and used to support decision-making processes in migration-related issues. Specifically, the article focuses on the iBorderCtrl project, which has been used to test a particular type of AI dubbed “emotional artificial intelligence”, giving rise to unprecedented problems in the field of protecting the fundamental rights of subjects and, particularly, foreigners. Then, after reviewing the rapid spread of AI in the field of migration and looking at the concept of emotional AI, as a subsystem worthy of specific analysis, it examines the iBorderCtrl project as a case study, with special emphasis on the judgement of the General Court of the European Union of 15 December 2021 (T-158/19), and contextualising it in the light of the proposed European Commission Regulation on AI, assessing the repercussions of emotional AI in the field of fundamental rights. © 2023, Escola d'Administracio Publica de Catalunya. All rights reserved."
"Pathology is the science of how a tissue changes during the process of the disease. The pathology is of important knowledge for understanding subsequent treatment concepts of a disease. In the cariology field, pathological features of caries are often presented using tooth sections, whereby the sequence and spread can be monitored. It is optimal to describe such changes using thin undecalcified tooth sections as an overview can be given of both enamel demineralization and pulp-dentine reactions. Also, an optimal understanding is achieved if the clinical status of carious lesion activity is known. Different studies using human teeth have shown the principle changes in progressive stages of carious lesions; the growth of the enamel lesion reflects the growth condition of the cariogenic biofilm. Surprisingly, the pulp (the odontoblast) is aware of the cariogenic stimuli even before mineral alteration has taken place within the dentine. The microorganisms mainly invade the dentine during enamel cavitation. In this chapter, the current improvement of knowledge on advanced carious lesions has been assessed in detail both histologically and radiographically. From a radiographic point of view, well-defined deep and extremely deep carious lesions and their difference are presented. Recent advances in artificial intelligence (AI) in medicine have raised the possibility of increasing the accuracy and speed of histopathological examination techniques. However, the literature involving AI-based histopathological features of hard and soft dentinal tissue pathologic changes is still scarce. © 2023 S. Karger AG. All rights reserved."
"Objective. Hydrocephalus is the leading indication for pediatric neurosurgical care worldwide. Identification of postinfectious hydrocephalus (PIH) verses non-postinfectious hydrocephalus, as well as the pathogen involved in PIH is crucial for developing an appropriate treatment plan. Accurate identification requires clinical diagnosis by neuroscientists and microbiological analysis, which are time-consuming and expensive. In this study, we develop a domain enriched AI method for computerized tomography (CT)-based infection diagnosis in hydrocephalic imagery. State-of-the-art (SOTA) convolutional neural network (CNN) approaches form an attractive neural engineering solution for addressing this problem as pathogen-specific features need discovery. Yet black-box deep networks often need unrealistic abundant training data and are not easily interpreted. Approach. In this paper, a novel brain attention regularizer is proposed, which encourages the CNN to put more focus inside brain regions in its feature extraction and decision making. Our approach is then extended to a hybrid 2D/3D network that mines inter-slice information. A new strategy of regularization is also designed for enabling collaboration between 2D and 3D branches. Main results. Our proposed method achieves SOTA results on a CURE Children’s Hospital of Uganda dataset with an accuracy of 95.8% in hydrocephalus classification and 84% in pathogen classification. Statistical analysis is performed to demonstrate that our proposed methods obtain significant improvements over the existing SOTA alternatives. Significance. Such attention regularized learning has particularly pronounced benefits in regimes where training data may be limited, thereby enhancing generalizability. To the best of our knowledge, our findings are unique among early efforts in interpretable AI-based models for classification of hydrocephalus and underlying pathogen using CT scans. © 2023 The Author(s). Published by IOP Publishing Ltd."
"Several malware variants have attacked systems and data over time. Ransomware is among the most harmful malware since it causes huge losses. In order to get a ransom, ransomware is software that locks the victim’s machine or encrypts his personal information. Numerous research has been conducted to stop and quickly recognize ransomware attacks. For proactive forecasting, artificial intelligence (AI) techniques are used. Traditional machine learning/deep learning (ML/DL) techniques, however, take a lot of time and decrease the accuracy and latency performance of network monitoring. In this study, we utilized the Hoeffding trees classifier as one of the stream data mining classification techniques to detect and prevent ransomware attacks. Three Hoeffding trees classifier algorithms are selected to be applied to the Resilient Information Systems Security (RISS) research group dataset. After configuration, Massive Online Analysis (MOA) software is utilized as a testing framework. The results of Hoeffding tree classifier algorithms are then assessed to choose the enhanced model with the highest accuracy and latency performance. In conclusion, the 99.41% classification accuracy was the highest result achieved by the EFDT algorithm in 66 ms. © 2023 by the authors."
"Wound healing is a dynamic process with multiple phases. Rapid profiling and quantitative characterization of inflammation and infection remain challenging. We report a paper-like battery-free in situ AI-enabled multiplexed (PETAL) sensor for holistic wound assessment by leveraging deep learning algorithms. This sensor consists of a wax-printed paper panel with five colorimetric sensors for temperature, pH, trimethylamine, uric acid, and moisture. Sensor images captured by a mobile phone were analyzed by neural network-based machine learning algorithms to determine healing status. For ex situ detection via exudates collected from rat perturbed wounds and burn wounds, the PETAL sensor can classify healing versus nonhealing status with an accuracy as high as 97%. With the sensor patches attached on rat burn wound models, in situ monitoring of wound progression or severity is demonstrated. This PETAL sensor allows early warning of adverse events, which could trigger immediate clinical intervention to facilitate wound care management. © 2023 The Authors."
"In June 2022, six Boeing 737s - fully loaded with tents, food, satellite Internet equipment, drones, geophysical survey gear, drilling equipment, and a team of experienced geologists - flew to a remote airstrip in northern Quebec. The geologists were hunting for major deposits of the minerals needed to power a clean-energy future. Given the mix of cutting-edge scientific computing and old-school bravado, it was as though they were channeling Alan Turing and Indiana Jones simultaneously. • Our startup, KoBold Metals, acquired an 800-square-kilometer mineral claim in this region of Canada based in part on predictions from our artificial intelligence systems. According to the AI, there was good reason to believe we'd find valuable deposits of nickel and cobalt buried below the surface. Summer snowmelts in this near-arctic area created a brief window to bring in a small village's worth of equipment and personnel to test our predictions.  © 1964-2012 IEEE."
"A market for mental health apps, designed to help millions of refugees manage symptoms of Post Traumatic Syndrome Disorder and other mental health issues, has proliferated since the outbreak of the so-called refugee crisis in 2015. These bite-size, on-the-go, mindfulness-based apps have emerged at the intersection of new investment models, state-of-the-art AI and surveillance and border control regimes. Conceived of as a more cost-effective approach to refugee mental health care, mental health apps are part of a larger endeavour to create the 'smart' refugee. Self-monitoring, agile, entrepreneurial and resilient in the face of adversity, the smart refugee is expected to emerge as a node in a network of information flow, constantly connected to digital technology, at once receiving and providing real-time data. Biometric and data markets, some of the fastest growing in the world, have already been eagerly collecting refugee fingerprints, iris scans, facial images and other genomic information. To add to this arsenal of data, the new apps are harvesting, storing and selling what I call the mental prints of refugee trauma, turning the human experience of loss, grief and suffering into quantifiable and marketable commodities. © The Author(s) 2023."
"Mendelian disorders are prevalent in neonatal and pediatric intensive care units and are a leading cause of morbidity and mortality in these settings. Current diagnostic pipelines that integrate phenotypic and genotypic data are expert-dependent and time-intensive. Artificial intelligence (AI) tools may help address these challenges. Dx29 is an open-source AI tool designed for use by clinicians. It analyzes the patient’s phenotype and genotype to generate a ranked differential diagnosis. We used Dx29 to retrospectively analyze 25 acutely ill infants who had been diagnosed with a Mendelian disorder, using a targeted panel of ~5000 genes. For each case, a trio (proband and both parents) file containing gene variant information was analyzed, alongside patient phenotype, which was provided to Dx29 by three approaches: (1) AI extraction from medical records, (2) AI extraction with manual review/editing, and (3) manual entry. We then identified the rank of the correct diagnosis in Dx29’s differential diagnosis. With these three approaches, Dx29 ranked the correct diagnosis in the top 10 in 92–96% of cases. These results suggest that non-expert use of Dx29’s automated phenotyping and subsequent data analysis may compare favorably to standard workflows utilized by bioinformatics experts to analyze genomic data and diagnose Mendelian diseases. © 2023 by the authors."
"Artificial intelligence (AI)-based medical technologies are rapidly evolving into actionable solutions for clinical practice. Machine learning (ML) algorithms can process increasing amounts of laboratory data such as gene expression immunophenotyping data and biomarkers. In recent years, the analysis of ML has become particularly useful for the study of complex chronic diseases, such as rheumatic diseases, heterogenous conditions with multiple triggers. Numerous studies have used ML to classify patients and improve diagnosis, to stratify the risk and determine disease subtypes, as well as to discover biomarkers and gene signatures. This review aims to provide examples of ML models for specific rheumatic diseases using laboratory data and some insights into relevant strengths and limitations. A better understanding and future application of these analytical strategies could facilitate the development of precision medicine for rheumatic patients. © 2023 Elsevier B.V."
"We argue that cognition (information processing) and internal phenomenological sensations, including emotions, are intimately related and are not separable. We aver that phenomenological sensations are dynamical “modes” of firing behaviour that (i) exist over time and over large parts of the cortex’s neuron-to-neuron network and (ii) are consequences of the network-of-networks architecture, coupling the individual neuronal dynamics and the necessary time delay incurred by neuron-to-neuron transmission: if you possess those system properties, then you will have the dynamical modes and, thus, the phenomenological sensations. These modes are consequences of incoming external stimuli and are competitive within the system, suppressing and locking-out one another. On the other hand, the presence of any such mode acts as a preconditioner for the immediate (dynamic) cognitive processing of information. Thus, internal phenomenological sensations, including emotions, reduce the immediate decision set (of feasible interpretations) and hence the cognitive load. For organisms with such a mental inner life, there would clearly be a large cognitive evolutionary advantage, resulting in the well-known “thinking fast, thinking slow” phenomena. We call this the entwinement hypothesis: how latent conscious phenomena arise from the dynamics of the cognitive processing load, and how these precondition the cognitive tasks immediately following. We discuss how internal dynamical modes, which are candidates for emotions down to single qualia, can be observed by reverse engineering large sets of simulations of system’s stimulated responses, either using vast supercomputers (with full 10B neuronal network analyses) or else using laptops to do the same for appropriately generalised Kuramoto models (networks of k-dimensional clocks, each representing the 10,000 neurons within a single neural column). We explain why such simplifications are appropriate. We also discuss the consequent cognitive advantages for information-processing systems exhibiting internal sensations and the exciting implications for next-generation (non-binary) computation and for AI. © 2023 by the authors."
"This paper presents a benchmarking survey on query expansion techniques for social media information retrieval, with a focus on comparing the performance of methods using semantic web technologies. The study evaluated query expansion techniques such as generative AI models and semantic matching algorithms and how they are integrated in a semantic framework. The evaluation was based on cosine similarity metrics, including the Discounted Cumulative Gain (DCG), Ideal Discounted Cumulative Gain (IDCG), and normalized Discounted Cumulative Gain (nDCG), as well as the Mean Average Precision (MAP). Additionally, the paper discusses the use of semantic web technologies as a component in a pipeline for building thematic knowledge graphs from retrieved social media data with extended ontologies integrated for the refugee crisis. The paper begins by introducing the importance of query expansion in information retrieval and the potential benefits of incorporating semantic web technologies. The study then presents the methodologies and outlines the specific procedures for each query expansion technique. The results of the evaluation are presented, as well as the rest semantic framework, and the best-performing technique was identified, which was the curie-001 generative AI model. Finally, the paper summarizes the main findings and suggests future research directions. © 2023 by the authors."
"Introduction Geriatric co-management is known to improve treatment of older adults in various clinical settings, however, widespread application of the concept is limited due to restricted resources. Digitalization may offer options to overcome these shortages by providing structured, relevant information and decision support tools for medical professionals. We present the SURGE-Ahead project (Supporting SURgery with GEriatric co-management and Artificial Intelligence) addressing this challenge. Methods A digital application with a dashboard-style user interface will be developed, displaying 1) evidence-based recommendations for geriatric co-management and 2) artificial intelligence- enhanced suggestions for continuity of care (COC) decisions. The development and implementation of the SURGE-Ahead application (SAA) will follow the Medical research council framework for complex medical interventions. In the development phase a minimum geriatric data set (MGDS) will be defined that combines parametrized information from the hospital information system with a concise assessment battery and sensor data. Two literature reviews will be conducted to create an evidence base for co-management and COC suggestions that will be used to display guideline-compliant recommendations. Principles of machine learning will be used for further data processing and COC proposals for the postoperative course. In an observational and AI-development study, data will be collected in three surgical departments of a University Hospital (trauma surgery, general and visceral surgery, urology) for AI-training, feasibility testing of the MGDS and identification of co-management needs. Usability will be tested in a workshop with potential users. During a subsequent project phase, the SAA will be tested and evaluated in clinical routine, allowing its further improvement through an iterative process. Discussion The outline offers insights into a novel and comprehensive project that combines geriatric co-management with digital support tools to improve inpatient surgical care and continuity of care of older adults. © 2023 Leinert et al."
"Background: Head and neck cancer (HNC) is characterized by complex-shaped tumors and numerous organs at risk (OARs), inducing challenging radiotherapy (RT) planning, optimization, and delivery. In this review, we provided a thorough description of the applications of artificial intelligence (AI) tools in the HNC RT process. Methods: The PubMed database was queried, and a total of 168 articles (2016–2022) were screened by a group of experts in radiation oncology. The group selected 62 articles, which were subdivided into three categories, representing the whole RT workflow: (i) target and OAR contouring, (ii) planning, and (iii) delivery. Results: The majority of the selected studies focused on the OARs segmentation process. Overall, the performance of AI models was evaluated using standard metrics, while limited research was found on how the introduction of AI could impact clinical outcomes. Additionally, papers usually lacked information about the confidence level associated with the predictions made by the AI models. Conclusions: AI represents a promising tool to automate the RT workflow for the complex field of HNC treatment. To ensure that the development of AI technologies in RT is effectively aligned with clinical needs, we suggest conducting future studies within interdisciplinary groups, including clinicians and computer scientists. © 2023 by the authors."
"The preoperative differentiation of breast phyllodes tumors (PTs) from fibroadenomas (FAs) plays a critical role in identifying an appropriate surgical treatment. Although several imaging modalities are available, reliable differentiation between PT and FA remains a great challenge for radiologists in clinical work. Artificial intelligence (AI)-assisted diagnosis has shown promise in distinguishing PT from FA. However, a very small sample size was adopted in previous studies. In this work, we retrospectively enrolled 656 breast tumors (372 FAs and 284 PTs) with 1945 ultrasound images in total. Two experienced ultrasound physicians independently evaluated the ultrasound images. Meanwhile, three deep-learning models (i.e., ResNet, VGG, and GoogLeNet) were applied to classify FAs and PTs. The robustness of the models was evaluated by fivefold cross validation. The performance of each model was assessed by using the receiver operating characteristic (ROC) curve. The area under the curve (AUC), accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) were also calculated. Among the three models, the ResNet model yielded the highest AUC value, of 0.91, with an accuracy value of 95.3%, a sensitivity value of 96.2%, and a specificity value of 94.7% in the testing data set. In contrast, the two physicians yielded an average AUC value of 0.69, an accuracy value of 70.7%, a sensitivity value of 54.4%, and a specificity value of 53.2%. Our findings indicate that the diagnostic performance of deep learning is better than that of physicians in the distinction of PTs from FAs. This further suggests that AI is a valuable tool for aiding clinical diagnosis, thereby advancing precision therapy. © 2023 by the authors."
"The outbreak of COVID-19 posed a significant challenge to the emergency management system for public health emergencies, especially in China, where the epidemic began. As intelligent technology has injected new vitality into emergency management, applying intelligent technology to optimize emergency resource allocation (ERA) has become a focus of research in the post-epidemic era. Based on China’s experience in preventing and controlling COVID-19, this paper first analyzes the characteristics and process of ERA in public health emergencies, and then synthesizes the relevant Chinese studies in recent years to identify the intelligent technologies affecting ERA in China using word frequency analysis technology. We also construct an intelligent emergency resource allocation mechanism in four areas: medical intelligence, management intelligence, decision-making intelligence, and supervision intelligence. Finally, we use the entropy-TOPSIS method to evaluate the impact of intelligent technologies on ERA, and we rank the criticality of intelligent technologies. The experimental results show that (i.) medical intelligence and management intelligence are the keys to developing intelligent ERA, and (ii.) among the identified essential intelligent technologies, artificial intelligence (AI), and big data technology have a more significant and critical role in emergency resource intelligence allocation. © 2023 by the authors."
"A psychiatric nurse since 2013, who became a clinical psychologist in 2022, I have had the opportunity, on numerous occasions, to use isolation and therapeutic restraint as part of my nursing practice, mainly in a closed psychiatric admissions service. These therapeutic tools, specific to psychiatry, are used in a very specific theoretical and legislative framework. Their use always leads to reflection, both individually and as a team. Indeed, their use must remain the last therapeutic bulwark to be used because it can be experienced with difficulty or even in a traumatic way by the patient, which can damage the relationship of trust with the carers. Thus, it is important that this practice be supervised and discussed with the patient and the team in order to be as appropriate as possible. © 2023; Infirmière en psychiatrie depuis 2013, devenue psychologue clinicienne en 2022, j'ai eu l'occasion, à de nombreuses reprises, d'utiliser l'isolement et la contention thérapeutique dans le cadre de ma pratique soignante, essentiellement en service fermé d'admissions psychiatriques. Ces outils thérapeutiques, spécifiques à la psychiatrie, sont utilisés dans un cadre théorique et législatif bien spécifique. Leur utilisation amène toujours à réflexion, à la fois individuelle mais aussi en équipe. En effet, leur recours doit rester le dernier rempart thérapeutique à utiliser car il peut être vécu difficilement voire de façon traumatique par le patient pouvant mettre à mal la relation de confiance avec les soignants. Ainsi, il est important que cette pratique soit encadrée et discutée avec le patient et en équipe afin d’être la plus adaptée possible. © 2023"
"ChatGPT is everywhere. From the weekend party table to the weekday university seminars, people have been talking about ChatGPT. Since its public launch at the end of last November, it reached 100 million users in January and has become the fastest-growing Internet service ever. GPT is short for generative pretrained transformer, and the chatbot is the large language model pretrained with text databases from the Internet and human feedback. The hype around ChatGPT may be well justified, as the tech innovation could transform some fields, such as how humans interact with machines. Will it be long before truly intelligent machines are developed to transform industrial sectors such as manufacturing?  © 1994-2011 IEEE."
"Artificial intelligence (AI) applications have transformed healthcare. This study is based on a general literature review uncovering the role of AI in healthcare and focuses on the following key aspects: (i) medical imaging and diagnostics, (ii) virtual patient care, (iii) medical research and drug discovery, (iv) patient engagement and compliance, (v) rehabilitation, and (vi) other administrative applications. The impact of AI is observed in detecting clinical conditions in medical imaging and diagnostic services, controlling the outbreak of coronavirus disease 2019 (COVID-19) with early diagnosis, providing virtual patient care using AI-powered tools, managing electronic health records, augmenting patient engagement and compliance with the treatment plan, reducing the administrative workload of healthcare professionals (HCPs), discovering new drugs and vaccines, spotting medical prescription errors, extensive data storage and analysis, and technology-assisted rehabilitation. Nevertheless, this science pitch meets several technical, ethical, and social challenges, including privacy, safety, the right to decide and try, costs, information and consent, access, and efficacy, while integrating AI into healthcare. The governance of AI applications is crucial for patient safety and accountability and for raising HCPs’ belief in enhancing acceptance and boosting significant health consequences. Effective governance is a prerequisite to precisely address regulatory, ethical, and trust issues while advancing the acceptance and implementation of AI. Since COVID-19 hit the global health system, the concept of AI has created a revolution in healthcare, and such an uprising could be another step forward to meet future healthcare needs. © 2023 by the authors."
"The goal of this study was to find out how business intelligence systems, AI, and digital leadership affect how satisfied employees are with their jobs and how much value they add to companies in the Greater Amman Municipality. After the study samples were taken and looked at, a total of 246 samples were approved to be used in the PLS software-based analysis. The results of this study showed that putting in place business intelligence tools, artificial intelligence, and digital leadership all made employees happier with their jobs and gave businesses more value. The research showed that there are four key parts to digital leadership: commander, communicator, collaborator, and co-creator. The main parts of business intelligence are Data Warehouse, Data Mining, Business Process Management, and Competitive Intelligence. Findings show that digital transformation is made up of three key parts: changing processes, developing business models, and changing domains. The results also show that an employee's level of job satisfaction, which includes things like business success, work commitment, and job thinking, is linked to how much value they add to the company. Intriguingly, the current results go against those of earlier studies, which said that the variables of interest have no effect on how happy employees are with their jobs or how much value companies add for their customers. When the results of this study are looked at as a whole, they say that businesses should start doing things that make employees happier at work and increase the value of the business. The current study is innovative because it focuses on the most important parts of business intelligence, artificial intelligence, and digital leadership in order to improve employee satisfaction at work and the quality of business learning with added value in Greater Amman Municipality. © 2023 by the authors; licensee Growing Science, Canada."
"With various emerging technologies and integration possibilities, smart facility management has gained wide interest in recent years. Several technologies were introduced to support facilities management and improve decision-making, such as Building Information Modeling (BIM), Internet of Things (IoT), Digital Twin (DT), artificial intelligence (AI), and blockchain. Yet, facility managers still face challenges related to data handling and the actual implementation of these technologies. Thus, this paper explores the trends and integration possibilities of smart facilities management technologies to provide a deeper understanding of the current research state and the areas for future exploration. The Scopus database is utilized to collect literature data, and a bibliometric analysis is conducted on 7236 publications of different types, including conference publications, articles, reviews, and book chapters, using VOSviewer software. The results revealed a noticeable growth in the annual number of publications related to this field after 2018. BIM, IoT, and DT were seen to share the greatest research attention, with BIM being the dominant technology. With recent wide attention, blockchain technology is noticed to be introducing many integration possibilities. In addition, the prominent contributing authors, countries, and sources to this research area are also identified. © 2023 by the authors."
"The Fourth Industrial Revolution, also known as Industry 4.0, marks the technological shift from traditional manufacturing systems to smart cyberphysical systems. It leads to an improvement in overall productivity and a reduction in environmental impact and promotes sustainable economic development. Industry 4.0 has been driven by emerging technologies such as the Internet of Things (IoT), also called the Industrial Internet of Things; digital twins; artificial intelligence; cloud computing; and edge/fog computing [1], [2], [3], [4], [5]. It is a hot topic in both academia and industry. The implementation of IoT connects physical assets to cybernetworks and captures a significant amount of data. These data, often 'big', are then fed to AI-based mission-critical systems to perform production monitoring, quality inspection, fault root cause analysis, quality prediction, and process control. The proper adoption of relevant Industry 4.0 technologies should lead to significant efficiency improvements and cost reductions in various industrial sectors.  © 1994-2011 IEEE."
"The use of genomic selection significantly reduces the age of dairy bulls entering semen production compared to progeny testing. The study aimed to identify early indicators that could be used for screening bulls during their performance testing period and could give us insight into their future semen production performance, acceptance for the AI station, and prediction of their future fertility. The study population consisted of 142 young Norwegian Red bulls enrolled at the performance test station, followed until we received semen production data, semen doses, and, subsequently, non-return rates (NR56) from the AI station. A range of semen quality parameters were measured with computer-assisted sperm analysis and flow cytometry from ejaculates collected from 65 bulls (9–13 months). The population morphometry of normal spermatozoa was examined, showing that Norwegian Red bulls at 10 months of age have homogenous sperm morphometry. Norwegian Red bulls could be separated into 3 clusters according to their sperm's reaction patterns to stress test and cryopreservation. Results of semi-automated morphology assessment of young Norwegian Red bulls showed that 42% of bulls rejected for the AI station and 18% of bulls accepted had ejaculates with abnormal morphology scores. For the youngest age group at 10 months, the mean (SD) proportion of spermatozoa with normal morphology was 77.5% (10.6). Using novel interpretation of sperm stress test combined with sperm morphology analysis and consecutive cryopreservation at a young age allowed identification of the candidate's sperm quality status. This could help breeding companies introduce young bulls earlier to the AI stations. © 2023 The Authors"
"The integration of renewable energy resources into smart grids has become increasingly important to address the challenges of managing and forecasting energy production in the fourth energy revolution. To this end, artificial intelligence (AI) has emerged as a powerful tool for improving energy production control and management. This study investigates the application of machine learning techniques, specifically ARIMA (auto-regressive integrated moving average) and Bi-LSTM (bidirectional long short-term memory) models, for predicting solar power production for the next year. Using one year of real-time solar power production data, this study trains and tests these models on performance measures such as mean absolute error (MAE) and root mean squared error (RMSE). The results demonstrate that the Bi-LSTM (bidirectional long short-term memory) model outperforms the ARIMA (auto-regressive integrated moving average) model in terms of accuracy and is able to successfully identify intricate patterns and long-term relationships in the real-time-series data. The findings suggest that machine learning techniques can optimize the integration of renewable energy resources into smart grids, leading to more efficient and sustainable power systems. © 2023 by the authors."
"The level of scientific and technical progress is determined in many respects by inorganic materials - phosphors, sorbents, catalysts, aluminates, chromates, ferrite. The luminescing materials are widely used in medicine, criminalistics, agriculture, in various industries. It creates prerequisites for improvement of methods of receiving phosphors and further research of influence of features of structure of firm phases of materials on their spectral properties. Initial substances for synthesis, the leading role belongs to oxides, hydroxides and systems on their basis. Red phosphors which part compounds of europium are of particular interest. The systems of in the common besieged hydroxides (CBH) received continuous sedimentation of water solutions of nitrates of Eu (III) and AI(III), Fe(III), Cr(III) alkali. The SOG systems were studied by methods of the physical and chemical analysis: differential and thermal, IR-spectroscopic, X-ray phase, X-ray fluorescent, power dispersive. Registration, the emitted luminescent radiation of Eu3+ in a system, in the field of 741.4-746.9 nanometers were carried out at a stage of crystallization of amorphous products. The dependence of intensity of a luminescence of systems on synthesis conditions was established. © The Authors, published by EDP Sciences, 2023."
[No abstract available]
"Background: IntraUterine Growth Restriction (IUGR) is a global public health concern and has major implications for neonatal health. The early diagnosis of this condition is crucial for obtaining positive outcomes for the newborn. In recent years Artificial intelligence (AI) and machine learning (ML) techniques are being used to identify risk factors and provide early prediction of IUGR. We performed a systematic review (SR) and meta-analysis (MA) aimed to evaluate the use and performance of AI/ML models in detecting fetuses at risk of IUGR. Methods: We conducted a systematic review according to the PRISMA checklist. We searched for studies in all the principal medical databases (MEDLINE, EMBASE, CINAHL, Scopus, Web of Science, and Cochrane). To assess the quality of the studies we used the JBI and CASP tools. We performed a meta-analysis of the diagnostic test accuracy, along with the calculation of the pooled principal measures. Results: We included 20 studies reporting the use of AI/ML models for the prediction of IUGR. Out of these, 10 studies were used for the quantitative meta-analysis. The most common input variable to predict IUGR was the fetal heart rate variability (n = 8, 40%), followed by the biochemical or biological markers (n = 5, 25%), DNA profiling data (n = 2, 10%), Doppler indices (n = 3, 15%), MRI data (n = 1, 5%), and physiological, clinical, or socioeconomic data (n = 1, 5%). Overall, we found that AI/ML techniques could be effective in predicting and identifying fetuses at risk for IUGR during pregnancy with the following pooled overall diagnostic performance: sensitivity = 0.84 (95% CI 0.80–0.88), specificity = 0.87 (95% CI 0.83–0.90), positive predictive value = 0.78 (95% CI 0.68–0.86), negative predictive value = 0.91 (95% CI 0.86–0.94) and diagnostic odds ratio = 30.97 (95% CI 19.34–49.59). In detail, the RF-SVM (Random Forest–Support Vector Machine) model (with 97% accuracy) showed the best results in predicting IUGR from FHR parameters derived from CTG. Conclusions: our findings showed that AI/ML could be part of a more accurate and cost-effective screening method for IUGR and be of help in optimizing pregnancy outcomes. However, before the introduction into clinical daily practice, an appropriate algorithmic improvement and refinement is needed, and the importance of quality assessment and uniform diagnostic criteria should be further emphasized. © 2023 by the authors."
"Artificial intelligence (AI) is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having marked effects on global health. Here, we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. The results of our preregistered study, including 697 participants, show that GPT-3 is a doubleedge sword: In comparison with humans, it can produce accurate information that is easier to understand, but it can also produce more compelling disinformation. We also show that humans cannot distinguish between tweets generated by GPT-3 and written by real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation and on how information campaigns can be improved to benefit global health. © 2023 The Authors."
"Porcine respiratory disease complex is an economically important disease in the swine industry. Early detection of the disease is crucial for immediate response to the disease at the farm level to prevent and minimize the potential damage that it may cause. In this paper, recent studies on the application of artificial intelligence (AI) in the early detection and monitoring of respiratory disease in swine have been reviewed. Most of the studies used coughing sounds as a feature of respiratory disease. The performance of different models and the methodologies used for cough recognition using AI were reviewed and compared. An AI technology available in the market was also reviewed. The device uses audio technology that can monitor and evaluate the herd’s respiratory health status through cough-sound recognition and quantification. The device also has temperature and humidity sensors to monitor environmental conditions. It has an alarm system based on variations in coughing patterns and abrupt temperature changes. However, some limitations of the existing technology were identified. Substantial effort must be exerted to surmount the limitations to have a smarter AI technology for monitoring respiratory health status in swine. © 2023 by the authors."
"Indaziflam and oxadiazon are efficacious preemergence herbicides used in warm-season turfgrass because of their persistence and residual activity. It is beneficial to quantify effective concentrations for preemergence control of summer annual weeds and determine whether these concentrations are maintained throughout weed emergence periods. Therefore, greenhouse bioassays were conducted with barnyardgrass, broadleaf signalgrass, doveweed, large crabgrass, and purple nutsedge. Treatments included indaziflam at 0, 4, 8, 12, 17, 21, 25, 29, 33, and 37 g ai ha-1 or oxadiazon at 0, 420, 841, 1,260, 1,681, 2,102, 2,354, 2,942, 3,363, and 3,783 g ha-1. Although preemergence herbicides are not used to control perennial weeds, purple nutsedge was included to investigate the effect of selected herbicides on its growth. Herbicide EC50, EC80, and EC90 for seedling emergence inhibition and shoot and root mass reduction were quantified from log-logistic dose-response curves. Herbicide concentration that remains from a preemergence application during the regional species-specific periodicity of emergence was predicted using first-order kinetics equations. Indaziflam and oxadiazon controlled seedling emergence 14 d after treatment (DAT) in the evaluated annual weeds and shoot and root mass in all species 84 DAT. Indaziflam applied in mid-March at 33 g ha-1 may provide up to 90% seedling emergence inhibition in large crabgrass and signalgrass; up to 80% in barnyardgrass; and up to 50% in doveweed. Oxadiazon applied in mid-March at 3,363 g ha-1 may provide up to 80% seedling emergence inhibition in all species. Indaziflam and oxadiazon may control up to 80% shoot mass and up to 50% root mass, respectively, in purple nutsedge and 80% to 90% shoot or root mass in other species. Such information is useful in evaluating adequacy of herbicide management practices for season-long weed control, and it aids turfgrass managers in applying preemergence herbicides at optimal timing based on target weed species. © North Carolina State University, 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
[No abstract available]
[No abstract available]
"This article addresses the directions of change in the area of control of the collection and distribution of public funds, in the context of the possibility of using new technologies, with a view to compliance with the legal standards of a democratic law-governed state and the challenges facing law-makers in properly legislating the use of AI. Issues related to the risks and threats posed by the implementation of new technologies in the public sector regarding technical (e.g. algorithmic bias) and legal (e.g. inadequate regulation of the possibility of using technology by a public administration) aspects are raised, bearing in mind that incorrect regulation of these issues is not without impact on trust in the state and ensuring the protection of democratic values. Analysing the issues related to the implementation of technological solutions to control public finances, it also indicates new areas for control in the public sector, for example the control of algorithm-based applications. © 2023 Anna Zalcewicz, published by Sciendo."
"Background: In the past vicennium, several artificial intelligence (AI) and machine learning (ML) models have been developed to assist in medical diagnosis, decision making, and design of treatment protocols. The number of active pathologists in Poland is low, prolonging tumor patients’ diagnosis and treatment journey. Hence, applying AI and ML may aid in this process. Therefore, our study aims to investigate the knowledge of using AI and ML methods in the clinical field in pathologists in Poland. To our knowledge, no similar study has been conducted. Methods: We conducted a cross-sectional study targeting pathologists in Poland from June to July 2022. The questionnaire included self-reported information on AI or ML knowledge, experience, specialization, personal thoughts, and level of agreement with different aspects of AI and ML in medical diagnosis. Data were analyzed using IBM® SPSS® Statistics v.26, PQStat Software v.1.8.2.238, and RStudio Build 351. Results: Overall, 68 pathologists in Poland participated in our study. Their average age and years of experience were 38.92 ± 8.88 and 12.78 ± 9.48 years, respectively. Approximately 42% used AI or ML methods, which showed a significant difference in the knowledge gap between those who never used it (OR = 17.9, 95% CI = 3.57–89.79, p < 0.001). Additionally, users of AI had higher odds of reporting satisfaction with the speed of AI in the medical diagnosis process (OR = 4.66, 95% CI = 1.05–20.78, p = 0.043). Finally, significant differences (p = 0.003) were observed in determining the liability for legal issues used by AI and ML methods. Conclusion: Most pathologists in this study did not use AI or ML models, highlighting the importance of increasing awareness and educational programs regarding applying AI and ML in medical diagnosis. © 2023 by the authors."
"(1) Background: The prevalence of uncontrolled hypertension is rising all across the world, making it a concern for public health. The usage of mobile health applications has resulted in a number of positive outcomes for the management and control of hypertension. (2) Objective: The study’s primary goal is to explain the steps to create a hypertension application (app) that considers cultural and social standards in Saudi Arabia, motivational features, and the needs of male and female Saudi citizens. (3) Methods: This study reports the emerged features and content needed to be adapted or developed in health apps for hypertension patients during an interactive qualitative analysis focus group activity with (n = 5) experts from the Saudi Ministry of Health. A gap analysis was conducted to develop an app based on a deep understanding of user needs with a patient-centred approach. (4) Results: Based on the participant’s reviews in this study, the app was easy to use and can help Saudi patients to control their hypertension, the design was interactive, motivational features are user-friendly, and there is a need to consider other platforms such as Android and Blackberry in a future version. (5) Conclusions: Mobile health apps can help Saudis change their unhealthy lifestyles. Target users, usability, motivational features, and social and cultural standards must be considered to meet the app’s aim. © 2023 by the authors."
"Gait analysis is a powerful technique that detects and identifies foot disorders and walking irregularities, including pronation, supination, and unstable foot movements. Early detection can help prevent injuries, correct walking posture, and avoid the need for surgery or cortisone injections. Traditional gait analysis methods are expensive and only available in laboratory settings, but new wearable technologies such as AI and IoT-based devices, smart shoes, and insoles have the potential to make gait analysis more accessible, especially for people who cannot easily access specialized facilities. This research proposes a novel approach using IoT, edge computing, and tiny machine learning (TinyML) to predict gait patterns using a microcontroller-based device worn on a shoe. The device uses an inertial measurement unit (IMU) sensor and a TinyML model on an advanced RISC machines (ARM) chip to classify and predict abnormal gait patterns, providing a more accessible, cost-effective, and portable way to conduct gait analysis. © 2023 by the authors."
"This study has examined factors, such as technology and employee influence on artificial intelligence (AI) adoption of e-innovative projects in the United Arab Emirates. The present study revealed the success or failure of e-innovation adoption in the public sector of the UAE and hinted at potential e-innovative projects to consider essential factors before adopting it. The study's sample covered the government sector, and the data collection method was a survey questionnaire with a sample size of 1037 responses made up of government employees. This paper was mainly built upon the diffusion of innovation and technology acceptance theories. The analysis findings showed that technology (an external factor) significantly and positively contributed to adopting AI e-innovation technology. Further analysis revealed that employee (internal factor) proxies directly influenced the adoption of AI e-innovation technology. Overall, internal and external factors contributed to adopting e-innovation technology in the United Arab Emirates. For future directions, additional factors related to the market should be considered to explore their contribution. © 2023 by the authors; licensee Growing Science, Canada."
"The challenge of the expansion of millions of data-intensive Internet of Things (IoT) devices has led to more restriction data rates in the 5G wireless communication network. A web server can make use of network features and functions in a variety of capacities by detecting digital records of human and object behaviors from the Internet of Everything (IoE) for autonomous networks and devices. While web server appears to be a potential option when used in conjunction with next-generation wireless communications, such as 5G technology, it introduces new issues at the edge of the network. In this article, we discuss the progression in the development of wireless technologies beyond IoT (i.e., IoE for autonomous networks), while explaining the key enabling technologies beyond 5G networks. A web server-based edge architecture has been proposed for managing a large-scale of IoE devices based on 6G-enabled technology for autonomous networks and a smart resource distribution approach. The proposed system allocates receiving work-loads from IoE devices based on their flexible service requirements using the Boltzmann machines approach designed for energy-efficient communications. In addition, at the edge network, an Artificial Intelligence (AI)-driven method, namely the Support Vector Machines (SVM) retrieval model, is used to assess the data and obtain accurate results. The proposed system has been simulated and compared with some of the existing algorithms considering different use case scenarios. An overview of the emerging challenges of the proposed architecture has been discussed.  © 2017 IEEE."
"Surgery plays a central role in the treatment of benign and malignant pancreatic diseases. Artificial intelligence (AI) is an important upcoming technology to support surgeons in pre-, intra-, and postoperative diagnosis, decision-making and training toward an optimized patient care. Current AI applications show a promising role in the evaluation of preoperative images for prediction of malignancy and resectability, intraoperative decision support, surgical training as well as a postoperative risk stratification to personalize the management of complications. This scoping review summarizes the most up to date developments of AI in pancreatic surgery with the highest available level of evidence. © 2023 The Author(s)."
"AI-mediated communication is designed to help us do our work more quickly and efficiently. But does it come at a cost? This study uses smart replies to show how AI influences humans without any intent on the part of the developer—the very use of AI is sufficient. I propose a loss of agency theory as a viable approach for studying the impact of AI on human agency. This theory focusses on the transfer of agency that is forced by circumstances (such as time pressure), human weaknesses (such as complacency), and conceptual priming. Mixed methods involving a crowdsourced experiment test that theory. The quantitative results reveal that machine agency affects the content we author and the behavior we generate. But the transfers between human and machine agency are fluid; they complement, replace, and reinforce each other at one and the same time. © 2023 The Author(s)"
"Objective: Although educating radiology trainees about artificial intelligence (AI) has become increasingly emphasized, the types of AI educational curricula are not well understood. We performed a systematic review of original studies describing curricula used to teach AI concepts and practical applications for radiology residents and fellows. Materials and methods: We performed a PubMed search for original studies published as of July 22, 2022, describing AI curricula geared toward radiology residents or fellows. Studies meeting inclusion criteria were evaluated for curricula design, implementation details, and outcomes. Descriptive statistics were used to summarize these curricula. Results: Five studies were included describing an AI curriculum, all geared toward radiology residents. All five curricula were led by radiologists, mostly by individual academic radiology departments (4; 80%) with one led by the ACR Resident and Fellow Section. Curricula design included didactic sessions (5; 100%), assigned readings (4; 80%), hands-on learning (3; 60%), and journal clubs (3; 60%); only one had individualized learning plans. All four studies that evaluated the impact of the curricula on participants’ knowledge or attitudes showed positive effects. Discussion: Amid increasing recognition of the importance of AI education for radiologists-in-training, several AI curricula for radiology residents have been implemented. Although curricula designs varied and it is unclear if one type is superior, they have had a positive impact on residents’ knowledge and attitudes toward AI. As AI becomes increasingly adopted in radiology, these curricula serve as models for other departments and programs to develop AI educational initiatives to prepare the next generation of radiologists for the AI era. © 2023 American College of Radiology"
"The purpose of this study is to explore how machine learning technologies can improve healthcare operations management. A machine learning-based model to solve a specific medical problem is developed to achieve this research purpose. Specifically, this study presents an AI solution for malaria infection diagnosis by applying the CNN (convolutional neural network) algorithm. Based on malaria microscopy image data from the NIH National Library of Medicine, a total of 24,958 images were used for deep learning training, and 2600 images were selected for final testing of the proposed diagnostic architecture. The empirical results indicate that the CNN diagnostic model correctly classified most malaria-infected and non-infected cases with minimal misclassification, with performance metrics of precision (0.97), recall (0.99), and f1-score (0.98) for uninfected cells, and precision (0.99), recall (0.97), and f1-score (0.98) for parasite cells. The CNN diagnostic solution rapidly processed a large number of cases with a high reliable accuracy of 97.81%. The performance of this CNN model was further validated through the k-fold cross-validation test. These results suggest the advantage of machine learning-based diagnostic methods over conventional manual diagnostic methods in improving healthcare operational capabilities in terms of diagnostic quality, processing costs, lead time, and productivity. In addition, a machine learning diagnosis system is more likely to enhance the financial profitability of healthcare operations by reducing the risk of unnecessary medical disputes related to diagnostic errors. As an extension for future research, propositions with a research framework are presented to examine the impacts of machine learning on healthcare operations management for safety and quality of life in global communities. © 2023 by the authors."
"Characteristics like self-managing, self-adaptation, and self-organization are the main objectives of intelligent network operation. AI and Machine Learning (ML) algorithms will enable future networks to operate entirely autonomously. However, current network architectures are not fully prepared to include and properly handle the promised Network Intelligence (NI). This article looks at scaling, one key Management and Orchestration (MANO) operation that allows the network to adapt to unexpected changes. We show how different scaling methods can fit the Monitor-Analyze-Plan-Execute over a shared Knowledge (MAPE-K), a well-established framework that allows architectural-based adaptation. Using a cloud-based scenario, we compare and highlight architectural differences between two prominent scaling methods, one based on Reinforcement Learning (RL) and the other based on classical control theory, showing that only the data-driven approach is adaptable enough to achieve automation. We conclude the article by pointing toward future research in autonomous, adaptive networks.  © 1979-2012 IEEE."
"The performance of optimization algorithms, and consequently of AI/machine learning solutions, is strongly influenced by the setting of their hyperparameters. Over the last decades, a rich literature has developed proposing methods to automatically determine the parameter setting for a problem of interest, aiming at either robust or instance-specific settings. Robust setting optimization is already a mature area of research, while instance-level setting is still in its infancy, with contributions mainly dealing with algorithm selection. The work reported in this paper belongs to the latter category, exploiting the learning and generalization capabilities of artificial neural networks to adapt a general setting generated by state-of-the-art automatic configurators. Our approach differs significantly from analogous ones in the literature, both because we rely on neural systems to suggest the settings, and because we propose a novel learning scheme in which different outputs are proposed for each input, in order to support generalization from examples. The approach was validated on two different algorithms that optimized instances of two different problems. We used an algorithm that is very sensitive to parameter settings, applied to generalized assignment problem instances, and a robust tabu search that is purportedly little sensitive to its settings, applied to quadratic assignment problem instances. The computational results in both cases attest to the effectiveness of the approach, especially when applied to instances that are structurally very different from those previously encountered. © 2023 by the authors."
"In the above article [1], the reference numbers [10], [11], [12], and [13] in Table II were not correct and should be changed to [11], [12], [13], and [14]. The correct Table II is illustrated as follows. (Table Presented). © 1993-2012 IEEE."
Weed control efficacy and ornamental plants tolerance to dimethenamid–p (0.75%) þ pendimethalin (1%) granular herbicide was evaluated in flat tray-grown weeds and container-grown ornamental plants. Dimethenamid – p þ pendimethalin at 2.94 kg ai. ha–1 (2.62 lb ai. A–1) controlled the tested broadleaf and grassy weeds .80% for up to 8 wk following herbicide application. The higher dimethenamid–p þ pendimethalin rates of 5.88 kg ai. ha–1 (5.25 lb ai. A–1) provided 94% to 99% control of the tested weed species but caused commercially unacceptable injury to pygmyweed [Crassula radicans (Haw.) D. Dietr. ‘Red carpet’]. Chocolate flower (Berlandiara lyrata Benth.) tolerated dimethenamid–p þ pendimethalin at 2.94 kg ai. ha–1 (2.62 lb ai. A–1) and 5.88 kg ai. ha–1 (5.25 lb ai.A–1) but the 11.77 kg ai.ha–1 (10.5 lb ai.A–1) rate was injurious in one of the two study years. Leucothoe [Leucothe fontanesiana (Steudel) Sleumer ‘Rainbow’] showed excellent tolerance to dimethenamid–p þ pendimethalin at rates up to 11.77 kg ai.ha–1 (10.5 lb ai. A–1). © 2023 Horticultural Research Institute.
"Due to natural processes of the earth, some major adverse events can occur like earthquakes, tsunamis, storms, fire storms, dust storms, floods, tornadoes, volcanic eruptions hurricanes etc. which can result into financial, environmental or human losses. An earthquake in a populated area may result into not only extensive damage to property but also to huge casualties and injuries. Thus prediction of earthquake is very necessary to avoid these losses. Convolution Neural Network (CNN) applied to earthquake prediction has been reviewed here. © 2023, World Research Association. All rights reserved."
"Structural integrity is a crucial aspect of engineering components, particularly in the field of additive manufacturing (AM). Surface roughness is a vital parameter that significantly influences the structural integrity of additively manufactured parts. This research work focuses on the prediction of the surface roughness of additive-manufactured polylactic acid (PLA) specimens using eight different supervised machine learning regression-based algorithms. For the first time, explainable AI techniques are employed to enhance the interpretability of the machine learning models. The nine algorithms used in this study are Support Vector Regression, Random Forest, XGBoost, AdaBoost, CatBoost, Decision Tree, the Extra Tree Regressor, the Explainable Boosting Model (EBM), and the Gradient Boosting Regressor. This study analyzes the performance of these algorithms to predict the surface roughness of PLA specimens, while also investigating the impacts of individual input parameters through explainable AI methods. The experimental results indicate that the XGBoost algorithm outperforms the other algorithms with the highest coefficient of determination value of 0.9634. This value demonstrates that the XGBoost algorithm provides the most accurate predictions for surface roughness compared with other algorithms. This study also provides a comparative analysis of the performance of all the algorithms used in this study, along with insights derived from explainable AI techniques. © 2023 by the authors."
"There is a growing interest in finding new ways to address the difficult task of introducing programming to secondary students for the first time to improve students’ computational thinking (CT) skills. Therefore, extensive research is required in this field. Worldwide, new ways to address this difficult task have been developed: visual execution environments and approaches by text programming or visual programming are among the most popular. This paper addresses the complex task by using a visual execution environment (VEE) to introduce the first programming concepts that should be covered in any introductory programming course. These concepts include variables, input and output, conditionals, loops, arrays, functions, and files. This study explores two approaches to achieve this goal: visual programming (using Scratch) and text programming (using Java) to improve CT. Additionally, it proposes an AI recommendation model into the VEE to further improve the effectiveness of developing CT among secondary education students. This integrated model combines the capabilities of an AI learning system module and a personalized learning module to better address the task at hand. To pursue this task, an experiment has been carried out among 23 preservice secondary teachers’ students in two universities, one in Madrid, Spain, and the other in Galway, Ireland. The overall results showed a significant improvement in the Scratch group. However, when analyzing the results based on specific programming concepts, significance was observed only in the Scratch group, specifically for the Loop concept. © 2023 by the authors."
"Giardia duodenalis is a zoonotic protozoan infecting various vertebrates such as humans and domestic animals. The aim of this study was to determine the frequency and genotypes of G. duodenalis using polymerase chain reaction-restriction fragment length polymorphism (PCR-RFLP) in dogs of Urmia, Iran. Overall, 246 stool specimens were collected from 100 pet, 49 stray, and 97 shelter dogs in the Urmia, Iran. Totally, seven samples (2.48%) were microscopically positive in terms of Giardia cyst. The PCR-RFLP analysis revealed that three (1.21%) and two (0.83%) samples have the C and D genotypes, respectively. In addition, two samples (0.83%) were belonged to the AI sub-group. A significant association was determined between the frequency of Giardia infection and life style, age, and stool form of dogs. The findings of the study showed the high frequency of Giardia infection in stray dogs and the dogs under one-year-old. Furthermore, the C and D genotypes of G. duodenalis were predominant in dogs of Urmia, Iran. © 2023 Urmia University. All rights reserved."
"Contemporary security information and event management (SIEM) solutions struggle to identify critical security incidents effectively due to the overwhelming number of false alerts generated by disparate security products, which results in significant alert fatigue and hinders effective incident response. To overcome this challenge, we propose a next-generation SIEM framework that integrates security orchestration automation and response capabilities and utilizes a divide-and-conquer strategy to mitigate the impact of low-quality IDS alerts. The proposed framework leverages advanced machine learning and data visualization tools—including a cost-sensitive learning method and an event segmenting algorithm—to filter and correlate alerts plus an augmented visualization tool to expedite the triage process. The proposed framework was evaluated experimentally on a dataset collected from a real-world enterprise network, and we report highly convincing results. The alert screening scheme demonstrates significant potential for real-world security operations. We believe that our findings will contributing to the development of a next-generation SIEM system that effectively addresses alert fatigue and lays the foundation for future research in this field. © 2023 by the authors."
"With the qualitative development of DC microgrids, the usage of different loads with unique conditions and features is now possible in electric power grids. Due to the negative impedance features of some loads, which are called constant power loads (CPLs), the control of DC power converters faces huge challenges from a stability point of view. Despite the significant advances in semiconductors, there is no upgrade in the control of gate drivers to exploit all potential of power electronic systems. In this paper, quantum computations are incorporated into artificial intelligence (AI) to stabilize a full-bridge (FB) DC-DC boost converter feeding CPL. Aiming to improve the bus voltage stabilization of the FB DC-DC boost converter, a quantum deep reinforcement learning (QDRL) control methodology is developed. By defining a reward function according to the specification of the FB power converter, the desired performance and control objectives are fulfilled. The main task of QDRL is to adjust the control coefficients embedded in the feedback controller to suppress the negative impedance effect resulting from deploying the CPLs. By deploying the potential advantages of quantum fundamentals, the deep reinforcement learning improved by quantum specifications will not only enhance the performance of the DRL algorithm on conventional processes but also advance related research areas such as quantum computing and AI. Unlike the basic quantum theory, which requires real quantum hardware, QDRL can be executed on classic computers. To examine the feasibility of the QDRL scheme, hardware-in-the-loop (HiL) examinations are conducted using the OPAL-RT. The comparison of the proposed controller with the classic state-of-the-art methodologies reveals the superiority and feasibility of QDRL-based control schemes in both the transient and steady-state conditions to other schemes. Analysis using various performance criteria, including the integral absolute error (IAE), integral time absolute error (ITAE), mean absolute error (MAE), and root mean square error (RMSE), demonstrates the dynamic improvement of the proposed scheme over sliding mode control (approximately 50%) and proportional integral control (approximately 100%). © 2023 by the authors."
"Tackling the most pressing problems for humanity, such as the climate crisis and the threat of global pandemics, requires accelerating the pace of scientific discovery. While science has traditionally relied on trial and error and even serendipity to a large extent, the last few decades have seen a surge of data-driven scientific discoveries. However, in order to truly leverage large-scale data sets and high-throughput experimental setups, machine learning methods will need to be further improved and better integrated in the scientific discovery pipeline. A key challenge for current machine learning methods in this context is the efficient exploration of very large search spaces, which requires techniques for estimating reducible (epistemic) uncertainty and generating sets of diverse and informative experiments to perform. This motivated a new probabilistic machine learning framework called GFlowNets, which can be applied in the modeling, hypotheses generation and experimental design stages of the experimental science loop. GFlowNets learn to sample from a distribution given indirectly by a reward function corresponding to an unnormalized probability, which enables sampling diverse, high-reward candidates. GFlowNets can also be used to form efficient and amortized Bayesian posterior estimators for causal models conditioned on the already acquired experimental data. Having such posterior models can then provide estimators of epistemic uncertainty and information gain that can drive an experimental design policy. Altogether, here we will argue that GFlowNets can become a valuable tool for AI-driven scientific discovery, especially in scenarios of very large candidate spaces where we have access to cheap but inaccurate measurements or too expensive but accurate measurements. This is a common setting in the context of drug and material discovery, which we use as examples throughout the paper. © 2023 The Author(s). Published by the Royal Society of Chemistry."
[No abstract available]
"In this report, we explore the role of carboxylic acid deblockers in an oxygen tolerant, room temperature, and alkylborane initiated reversible addition-fragmentation chain transfer (AI-RAFT) polymerization. Our studies employ an air-stable and coordinated trialkylborane-amine complex as the initiator which upon exposure to deblocker liberates free alkylborane that consumes oxygen and generates radicals to drive RAFT polymerization. Kinetic experiments reveal an enhancement in the polymerization rate when using higher concentrations of propionic acid deblocker and that the carboxylic acid has a partial order of one, aligning with the homolytic substitution mechanism for the autoxidation of alkylboranes. Different carboxylic acid deblockers were also investigated by their relative pKa values, showing that lower pKa acids can be employed at greatly diminished concentrations while still providing high monomer conversions, predetermined molecular weights, and narrow dispersity polymers. The findings disclosed in this report provide important kinetic parameters, mechanistic insights, and formulation details which are instrumental in understanding and improving AI-RAFT. © 2023 The Royal Society of Chemistry."
"Beyond-5G, towards the next generation (i.e., 6G), inspiring plenty of emerging use cases and business models, demands new evaluation for enormously evolving performance requirements of networks. Since the trend of artificial intelligent IoT (AI-IoT) development in 6G tends to leverage more autonomous management with even less human participation, the conventional human-oriented key performance indicators, e.g., quality of service (QoS) and quality of experience (QoE), will not fulfill the demand of things-driven 6G AI-IoT deployment. In this article, we discuss the potential vital emerging performance merit in 6G AI-IoT development, and further propose a novel new measure of 6G AI-IoT performance, referred to as quality of things' experience (QoTE), to facilitate the evaluation of the performance of AI-IoT-based technologies and applications for inter-things network deployment in 6G. Moreover, by defining things-oriented criteria, the proposed QoTE is considered and quantified with the emerging IoT standard (IEEE P2668, i.e., IDex), towards a new era to enable IDex for things-oriented, effective, and objective indexing for inter-things 6G AI-IoT systems/applications.  © 1979-2012 IEEE."
"Predicting floor water inrush has become increasingly challenging as coal is being mined at greater depths. We established a practical predictive method using a hybrid artificial intelligence (AI) model and geographic information system (GIS) techniques. The hybrid model is a classifier that mixes a back propagation neural network (BPNN) with an adaptive boosting algorithm (AdaBoost). To assess the effectiveness of the model, 33 borehole data points with known water inrush results in the Yangcheng coal mine were used as data samples to train and test the model. The outcomes demonstrated a predictive accuracy of 100%, far exceeding the accuracy and stability of the BPNN classifier alone using the same parameters. Then, GIS techniques were used to extend the approach throughout the mining region; the greatest risk was shown to be in the middle of the area. Given the limited data set, errors may exist in extending the risk predictions for the entire mining area, so more data needs to be collected to ensure the accuracy of subsequent predictions. Still, we believe that the methods and steps adopted in this study can be used to create practical predictive models in different mining regions. © 2023, The Author(s) under exclusive licence to International Mine Water Association."
"Artificial intelligence (AI) and machine learning (ML) have revolutionized the way health organizations approach social media. The sheer volume of data generated through social media can be overwhelming, but AI and ML can help organizations effectively manage this information to improve telehealth, remote patient monitoring, and the well-being of individuals and communities. Previous research has revealed several trends in AI–ML adoption: First, AI can be used to enhance social media marketing. Drawing on sentiment analysis and related tools, social media is an effective way to increase brand awareness and customer engagement. Second, social media can become a very useful data collection tool when integrated with new AI–ML technologies. Using this function well requires researchers and practitioners to protect users’ privacy carefully, such as through the deployment of privacy-enhancing technologies (PETs). Third, AI–ML enables organizations to maintain a long-term relationship with stakeholders. Chatbots and related tools can increase users’ ability to receive personalized content. The review in this paper identifies research gaps in the literature. In view of these gaps, the paper proposes a conceptual framework that highlights essential components for better utilizing AI and ML. Additionally, it enables researchers and practitioners to better design social media platforms that minimize the spread of misinformation and address ethical concerns more readily. It also provides insights into the adoption of AI and ML in the context of remote patient monitoring and telehealth within social media platforms. © 2023 by the author."
"Through a case study of images generated by Swedish artist Steph Maj Swanson using an AI text-to-image (T2I) model, this article explores the strategy of negative weight prompting in T2I models as a phenomenon of apophasis. Apophasis is a linguistic strategy commonly deployed in texts of mystical theology to express the ineffability of God through negative concepts. In this article, a comparison of apophatic strategies in mystical texts and T2I models is engaged to highlight the mutual benefit of theorising AI with the help of religious theory and concepts. With this, the article builds on previous work on the New Visibility of Religion, enchantment, and post-secularism—especially the research of Beth Singler on religious continuities in representations of AI. Recent work on AI prompt engineering, computational linguistics, and computational geometry is invoked to explain the linguistic processes of T2I models. Poststructuralist semiotics is then employed to theorise the search for the Transcendental Signified in apophatic theology. The article concludes that linguistic theology can help to elucidate technological use cases, subsequently arguing for further dialogue between scholars in artificial intelligence and religious studies, and for a revaluation of religion in the technological sphere. © 2023 by the author."
"In this article, a general introduction to the area of sensor array and multichannel signal processing is provided, including associated activities of the IEEE Signal Processing Society (SPS) Sensor Array and Multichannel (SAM) Technical Committee (TC). The main technological advances in five SAM subareas made in the past 25 years are then presented in detail, including beamforming, direction-of-arrival (DOA) estimation, sensor location optimization, target/source localization based on sensor arrays, and multiple-input multiple-output (MIMO) arrays. Six recent developments are also provided at the end to indicate possible promising directions for future SAM research, which are graph signal processing (GSP) for sensor networks; tensor-based array signal processing, quaternion-valued array signal processing, 1-bit and noncoherent sensor array signal processing, machine learning and artificial intelligence (AI) for sensor arrays; and array signal processing for next-generation communication systems.  © 1991-2012 IEEE."
"This paper presents a machine learning approach to automatically classifying post-harvest vegetal species. Color images of vegetal species were applied to convolutional neural networks (CNNs) and support vector machine (SVM) classifiers. We focused on okra as the target vegetal species and classified it into two quality types. However, our approach could also be applied to other species. The machine learning solution consists of several components, and each design process and its combinations are essential for classification quality. Therefore, we carefully investigated their effects on classification accuracy. Through our experimental evaluation, we confirmed the following: (1) in color space selection, HLG (hue, lightness, and green) and HSL (hue, saturation, and lightness) are essential for vegetal species; (2) suitable preprocessing techniques are required owing to the complexity of the data and noise load; and (3) the diversity extension of learning image data by mixing different datasets obtained under different conditions is quite effective in reducing the overfitting possibility. The results of this study will assist AI practitioners in the design and development of post-harvest classifications based on machine learning. © 2023 by the authors."
"Artificial intelligence (AI) is an emerging technological system that provides a platform to manage and analyze data by emulating human cognitive functions with greater accuracy, revolutionizing patient care and introducing a paradigm shift to the healthcare industry. The purpose of this study is to identify the underlying factors that affect the adoption of artificial intelligence in healthcare (AIH) by healthcare providers and to understand “What are the factors that influence healthcare providers’ behavioral intentions to adopt AIH in their routine practice?” An integrated survey was conducted among healthcare providers, including consultants, residents/students, and nurses. The survey included items related to performance expectancy, effort expectancy, initial trust, personal innovativeness, task complexity, and technology characteristics. The collected data were analyzed using structural equation modeling. A total of 392 healthcare professionals participated in the survey, with 72.4% being male and 50.7% being 30 years old or younger. The results showed that performance expectancy, effort expectancy, and initial trust have a positive influence on the behavioral intentions of healthcare providers to use AIH. Personal innovativeness was found to have a positive influence on effort expectancy, while task complexity and technology characteristics have a positive influence on effort expectancy for AIH. The study’s empirically validated model sheds light on healthcare providers’ intention to adopt AIH, while the study’s findings can be used to develop strategies to encourage this adoption. However, further investigation is necessary to understand the individual factors affecting the adoption of AIH by healthcare providers. © 2023 by the authors."
"The advent of artificial intelligence (AI), especially the state-of-the-art deep learning frameworks, has begun a silent revolution in all medical subfields, including ophthalmology. Due to their specific microvascular and neural structures, the eyes are anatomically associated with the rest of the body. Hence, ocular image-based AI technology may be a useful alternative or additional screening strategy for systemic diseases, especially where resources are scarce. This review summarizes the current applications of AI related to the prediction of systemic diseases from multimodal ocular images, including cardiovascular diseases, dementia, chronic kidney diseases, and anemia. Finally, we also discuss the current predicaments and future directions of these applications. © 2023 by the authors."
"In this conceptual paper, we explore how artificial intelligence (AI) holds promise as an important and additional tool for combating corruption in public procurement in emerging economies. We aim to extend the understanding of how the emergent technology of AI may be another important tool in the fight against corruption in emerging economies. We discuss how AI and related emergent technologies can help build greater accountability and transparency regimes in public procurement, a key source of public corruption. We argue that there are infrastructural, social, ethical, and political challenges to the successful use of emerging technologies of AI in the fight against public corruption. We make inferences from the discussion and provide some tentative guidelines for policymakers. We suggest that using AI as part of a program of institutional reforms in the public sector would increase technology's role and contributions to the fight against systemic corruption in public procurement in emerging economies. &copy; 2022 The Author(s). Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com. © 2023 Oxford University Press. All rights reserved."
"Aims Pulmonary vein isolation (PVI) guided by the Ablation Index (AI) has shown high acute and mid-term efficacy in the treatment of paroxysmal atrial fibrillation (AF). Previous data before the AI-era had suggested that wide-area circumferential ablation (WACA) was preferable to ostial ablation. However, with the use of AI, we hypothesize that ostial circumferential ablation is non-inferior to WACA and can improve outcomes in paroxysmal AF. Methods Prospective, multicentre, non-randomized, non-inferiority study of consecutive patients were referred for paroxysmal AF and results ablation from January 2020 to September 2021. All procedures were performed using the AI software, and patients were separated into two different groups: WACA vs. ostial circumferential ablation. Acute reconnection, procedural data, and 1-year arrhythmia recurrence were assessed. During the enrolment period, 162 patients (64% males, mean age of 60 ± 11 years) fulfilled the study inclusion criteria—81 patients [304 pulmonary vein (PV)] in the WACA group and 81 patients (301 PV) in the ostial group. Acute PV reconnection was identified in 7.9% [95% confidence interval (CI), 4.9–11.1%] of PVs in the WACA group compared with 3.3% (95% CI, 1.8–6.1%) of PVs in the ostial group [P < 0.001 for non-inferiority; adjusted odds ratio 0.51 (95% CI, 0.23–0.83), P = 0.05]. Patients in the WACA group had longer ablation (35 vs. 29 min, P = 0.001) and procedure (121 vs. 102 min, P < 0.001) times. No significant difference in arrhythmia recurrence was seen at 1-year of follow-up [11.1% in WACA vs. 9.9% in ostial, hazard ratio 1.13 (95% CI, 0.44–1.94), P = 0.80 for superiority]. Conclusion In paroxysmal AF patients treated with tailored AI-guided PVI, ostial circumferential ablation is not inferior to WACA with regard to acute PV reconnection, while allowing quicker procedures with less ablation time. © 2023 Oxford University Press. All rights reserved."
"Building from the authors’ plan to conceptualize an artificial intelligence (AI) solution that allows SME owners to make more sustainable choices in foreign-market-entry decisions, this systematic literature review (SLR) researches the state-of-the-art in SME internationalization from 1920 to 2023 (since 2014 in more depth). The authors gather all articles in Scopus, tagged with the keyword internationalization (25,303 as of January 2023), order them by citations, and download the top 2000 papers’ metadata for analysis and debate, then narrow it to reviews and SMEs, and use bibliometric visualization and qualitative data analysis software (VOSviewer and NVivo) to identify the key players and determinants of export performance/intensity, and finally draw conclusions. The results reveal key internationalization theories, top authors, reviews, and sources and expand Werner´s determinants via several tables and figures. The findings reveal the rise of relevance regarding theories related to social narratives and corporate activism, but also show that there is still much to do in SME internationalization, namely on what makes a small firm well established in their native market and have success in other countries. The contribution to science is an update on the topic and the pinpointing of several trends and gaps, such as a focus on services, theory integration, longitudinal studies between antecedents and performance, strategic fit versus opportunism, network theory on niche marketing, born-“glocal” strategies, disruptive technologies, and discourse variables, for the future of SME export success. © 2023 by the authors."
"The reliability and accuracy of welding image recognition (WIR) is critical, which can largely improve domain experts’ insight of the welding system. To ensure its performance, deep learning (DL), as the cutting-edge artificial intelligence technique, has been prevailingly studied and adopted to empower intelligent WIR in various industry implementations. However, to date, there still lacks a comprehensive review of the DL-based WIR (DLBWIR) in literature. Aiming to address this issue, and to better understand its development and application, this paper undertakes a state-of-the-art survey of the existing DLBWIR research holistically, including the key technologies, the main applications and tasks, and the public datasets. Moreover, possible research directions are also highlighted at last, to offer insightful knowledge to both academics and industrial practitioners in their research and development work in WIR. © 2023 The Society of Manufacturing Engineers"
"The use of data mining to predict early employment readiness of students is gaining importance due to the expansion of data production in various industries. This study aims to address the employability issue in Middle Eastern nations by utilizing an Adaptive Neuro-Fuzzy Inference System (ANFIS) data mining technology. The experimental investigation used data from tracer studies conducted by three Jordanian universities, consisting of 22 parameters. Results showed that despite achieving an accuracy of 94% for the graduate dataset, ANFIS exhibited high complexity due to the large number of attributes used. The study has implications for selecting relevant variables and investigating multiple aspects. Data mining has various applications, including classification, clustering, regression, association rule development, and outlier analysis. As data production continues to expand, this study provides insights into the potential use of ANFIS in predicting early employment readiness of students in Middle Eastern nations. © 2023 by the authors; licensee Growing Science, Canada."
"In healthcare, the development and deployment of insufficiently fair systems of artificial intelligence (AI) can undermine the delivery of equitable care. Assessments of AI models stratified across subpopulations have revealed inequalities in how patients are diagnosed, treated and billed. In this Perspective, we outline fairness in machine learning through the lens of healthcare, and discuss how algorithmic biases (in data acquisition, genetic variation and intra-observer labelling variability, in particular) arise in clinical workflows and the resulting healthcare disparities. We also review emerging technology for mitigating biases via disentanglement, federated learning and model explainability, and their role in the development of AI-based software as a medical device. © 2023. Springer Nature Limited."
"This special issue addresses a largely neglected area of Information Systems (IS) research – the regulation of and through Information Technology (IT). As with other human technologies, IT artefacts present risks and can harm individuals, groups, organisations, economies, and society: However, this remains a largely unexplored topic in IS research. Nevertheless, regulators, social commentators, the media, and the public have voiced their concerns about such risks, particularly those related to artificial intelligence (AI), cybersecurity, privacy, digital assets (e.g. cryptocurrencies), and the market dominance of digital platforms. Accordingly, regulations have been instituted or proposed to regulate IT artefacts in and across several business sectors. Additionally, in response to the informational challenges posed by a complex web of laws and regulations, regulators and business organisations have implemented IT artefacts to transform regulatory and supervisory processes or to enhance organisational risk management and compliance reporting capabilities. This special issue addresses the research challenges related to emergent issues surrounding the regulation of and through IT. In this editorial, we take stock of where the field currently stands. We advance a conceptual ontology of IT regulation to guide future research by specifying several taken-for-granted core concepts. By rendering the concepts, categories, and their relationships explicit, the model and its related research questions provide a firm foundation to generate a cumulative body of research on the regulation of and through IT. © Association for Information Technology Trust 2023."
"BACKGROUND/OBJECTIVES: The purpose of this study was to establish a database (DB) of foods containing vitamin D that were investigated in the Korea National Health and Nutrition Examination Survey (KNHANES), to estimate the dietary vitamin D intake, to evaluate the dietary adequacy of this intake, and to identify the major food sources of Koreans that contain vitamin D. SUBJECTS/METHODS: This study used data from the KNHANES 2016–2019. Individuals aged ≥ 1 year who participated in the nutrition survey (n = 28,418) were included. The dietary intake was assessed by the 24-h dietary recall method and individual dietary vitamin D intake was estimated using a newly established vitamin D DB. Dietary adequacy was evaluated by comparing the dietary intake of the participants with adequate intake (AI) as defined by Dietary Reference Intakes for Koreans (KDRIs) 2020. RESULTS: The average dietary vitamin D intake for all the subjects was 3.13 μg/d, which was 33.1% of AI. Dietary vitamin D intake was lower in rural residents, the elderly, and those with low income. The major food groups that contributed to the total dietary vitamin D intake were fish and shellfish (61.59%), eggs (17.75%), meat (8.03%), milk (4.25%), legumes (3.93%), and grains (3.84%). The top 10 individual food items that contributed to the total vitamin D intake were eggs (17.44%), squid (8.5%), eels (7.44%), salmon (5.35%), mackerel (5.27%), anchovies (4.65%), yellow croakers (4.58%), pork meat (4.47%), soymilk (4.46%), and skipjack tuna (3.80%). CONCLUSION: These results show that the mean dietary vitamin D intake of Koreans is lower than the reference AI level. Nutritional policies need to be put in place to increase the vitamin D intake of Koreans in the future. In addition, comprehensive research on all the sources of vitamin D, including intake of supplements and biosynthesis in the skin, is required. © 2023 The Korean Nutrition Society and the Korean Society of Community Nutrition."
"The banking industry has been employing artificial intelligence (AI) technologies to enhance the quality of its services. More recently, AI algorithms, such as natural language understanding (NLU), have been integrated into chatbots to improve banking applications. These chatbots are typically designed to cater to customers’ needs. However, research in the development of troubleshooting chatbots for technical purposes remains scarce, especially in the banking sector. Although a company may possess a knowledge database, a standard methodology is essential to guiding an AI developer in building a chatbot, making the modeling of technical needs into a specialized chatbot a challenging task. This paper presents a novel methodology for developing troubleshooting chatbots. We apply this methodology to create an AI-powered chatbot capable of performing technical ATM maintenance tasks. We propose the TroubleshootingBot, an experimental protocol to obtain data for evaluating the chatbot through two scenarios. The first scenario detects user intent, and the second recognizes desired values in a user’s phrase (e.g., three beeps or two beeps). For these scenarios, we achieved accuracies of 0.93 and 0.88, respectively. This work represents a significant advancement in virtual assistants for banking applications and holds potential for other technical problem-solving applications. © 2023 by the authors."
"The emergence of the Internet of Things (IoT) and its subsequent evolution into the Internet of Everything (IoE) is a result of the rapid growth of information and communication technologies (ICT). However, implementing these technologies comes with certain obstacles, such as the limited availability of energy resources and processing power. Consequently, there is a need for energy-efficient and intelligent load-balancing models, particularly in healthcare, where real-time applications generate large volumes of data. This paper proposes a novel, energy-aware artificial intelligence (AI)-based load balancing model that employs the Chaotic Horse Ride Optimization Algorithm (CHROA) and big data analytics (BDA) for cloud-enabled IoT environments. The CHROA technique enhances the optimization capacity of the Horse Ride Optimization Algorithm (HROA) using chaotic principles. The proposed CHROA model balances the load, optimizes available energy resources using AI techniques, and is evaluated using various metrics. Experimental results show that the CHROA model outperforms existing models. For instance, while the Artificial Bee Colony (ABC), Gravitational Search Algorithm (GSA), and Whale Defense Algorithm with Firefly Algorithm (WD-FA) techniques attain average throughputs of 58.247 Kbps, 59.957 Kbps, and 60.819 Kbps, respectively, the CHROA model achieves an average throughput of 70.122 Kbps. The proposed CHROA-based model presents an innovative approach to intelligent load balancing and energy optimization in cloud-enabled IoT environments. The results highlight its potential to address critical challenges and contribute to developing efficient and sustainable IoT/IoE solutions. © 2023 by the authors."
"Global warming and climate change are responsible for many disasters. Floods pose a serious risk and require immediate management and strategies for optimal response times. Technology can respond in place of humans in emergencies by providing information. As one of these emerging artificial intelligence (AI) technologies, drones are controlled in their amended systems by unmanned aerial vehicles (UAVs). In this study, we propose a secure method of flood detection in Saudi Arabia using a Flood Detection Secure System (FDSS) based on deep active learning (DeepAL) based classification model in federated learning to minimize communication costs and maximize global learning accuracy. We use blockchain-based federated learning and partially homomorphic encryption (PHE) for privacy protection and stochastic gradient descent (SGD) to share optimal solutions. InterPlanetary File System (IPFS) addresses issues with limited block storage and issues posed by high gradients of information transmitted in blockchains. In addition to enhancing security, FDSS can prevent malicious users from compromising or altering data. Utilizing images and IoT data, FDSS can train local models that detect and monitor floods. A homomorphic encryption technique is used to encrypt each locally trained model and gradient to achieve ciphertext-level model aggregation and model filtering, which ensures that the local models can be verified while maintaining privacy. The proposed FDSS enabled us to estimate the flooded areas and track the rapid changes in dam water levels to gauge the flood threat. The proposed methodology is straightforward, easily adaptable, and offers recommendations for Saudi Arabian decision-makers and local administrators to address the growing danger of flooding. This study concludes with a discussion of the proposed method and its challenges in managing floods in remote regions using artificial intelligence and blockchain technology. © 2023 by the authors."
"Recent years have seen the emergence and application of artificial intelligence (AI) in diagnostic decision support systems. There are approximately 80 etiologies that can underly uveitis, some very rare, and AI may lend itself to their detection. This synthesis of the literature selected articles that focused on the use of AI in determining the diagnosis, classification, and underlying etiology of uveitis. The AI-based systems demonstrated relatively good performance, with a classification accuracy of 93–99% and a sensitivity of at least 80% for identifying the two most probable etiologies underlying uveitis. However, there were limitations to the evidence. Firstly, most data were collected retrospectively with missing data. Secondly, ophthalmic, demographic, clinical, and ancillary tests were not reliably integrated into the algorithms’ dataset. Thirdly, patient numbers were small, which is problematic when aiming to discriminate rare and complex diagnoses. In conclusion, the data indicate that AI has potential as a diagnostic decision support system, but clinical applicability is not yet established. Future studies and technologies need to incorporate more comprehensive clinical data and larger patient populations. In time, these should improve AI-based diagnostic tools and help clinicians diagnose, classify, and manage patients with uveitis. © 2023 by the authors."
"The share of electricity generation from Variable Renewable Energy Sources (VRES) has increased over the last 20 years. Despite promoting the decarbonization of the energy mix, these sources bring negative characteristics to the energy mix, such as power ramps, load mismatch, unpredictability, and fluctuation. One of the ways to mitigate these characteristics is the hybridization of power plants. This paper evaluates the benefits of hybridizing a plant using an AI-based methodology for optimizing the wind–solar ratio based on the Brazilian regulatory system. For this study, the hybrid plant was modeled using data collected over a period of 10 months. The measurements were obtained using two wind profilers (LIDAR and SODAR) and a sun tracker (Solys 2) as part of the EOSOLAR R&D project conducted in the state of Maranhão, Brazil. After the power plant modeling, a Genetic Algorithm (GA) was used to determine the optimal wind–solar ratio, considering costs with transmission systems. The algorithm achieved a monthly profit increase of more than 39% with an energy curtailment inferior to 1%, which indicates economic complementarity. Later, the same methodology was also applied to verify the wind–solar ratio’s sensitivity to solar energy pricing. The results show that a price increase of 15% would change the power plant’s optimal configuration. © 2023 by the authors."
"Artificial Intelligence (AI) applications in different fields are developing rapidly, among which AI painting technology, as an emerging technology, has received wide attention from users for its creativity and efficiency. This study aimed to investigate the factors that influence user acceptance of the use of AIBPS by proposing an extended model that combines the Extended Technology Acceptance Model (ETAM) with an AI-based Painting System (AIBPS). A questionnaire was administered to 528 Chinese participants, and validated factor analysis data and Structural Equation Modeling (SEM) were used to test our hypotheses. The findings showed that Hedonic Motivation (HM) and Perceived Trust (PE) had a positive effect (+) on users’ Perceived Usefulness (PU) and Perceived Ease of Use (PEOU), while Previous Experience (PE) and Technical Features (TF) had no effect (−) on users’ Perceived Usefulness (PU). This study provides an important contribution to the literature on AIBPS and the evaluation of systems of the same type, which helps to promote the sustainable development of AI in different domains and provides a possible space for the further extension of TAM, thus helping to improve the user experience of AIBPS. The results of this study provide insights for system developers and enterprises to better motivate users to use AIBPS. © 2023 by the authors."
[No abstract available]
"This study compares the accuracy and effectiveness of our novel 3D-printed titanium cutting guides with intraoperative surgical navigation for performing intraoral condylectomy in patients with mandibular condylar osteochondroma (OC). A total of 21 patients with mandibular condylar OC underwent intraoral condylectomy with either 3D-printed cutting guides (cutting guide group) or with surgical navigation (navigation group). The condylectomy accuracy in the cutting guide group and navigation group was determined by analyzing the three-dimensional (3D) discrepancies between the postoperative computed tomography (CT) images and the preoperative virtual surgical plan (VSP). Moreover, the improvement of the mandibular symmetry in both groups was determined by evaluating the chin deviation, chin rotation and mandibular asymmetry index (AI). The superimposition of the condylar osteotomy area showed that the postoperative results were very close to the VSP in both groups. The mean 3D deviation and maximum 3D deviation between the planned condylectomy and the actual result were 1.20 ± 0.60 mm and 2.36 ± 0.51 mm in the cutting guide group, and 1.33 ± 0.76 mm and 4.27 ± 1.99 mm in the navigation group. Moreover, the facial symmetry was greatly improved in both groups, indicated by significantly decreased chin deviation, chin rotation and AI. In conclusion, our results show that both 3D-printed cutting-guide-assisted and surgical-navigation-assisted methods of intraoral condylectomy have high accuracy and efficiency, while using a cutting guide can generate a relatively higher surgical accuracy. Moreover, our cutting guides exhibit user-friendly features and simplicity, which represents a promising prospect in everyday clinical practice. © 2023 by the authors."
"Peritoneal carcinosis is a condition characterized by the spread of cancer cells to the peritoneum, which is the thin membrane that lines the abdominal cavity. It is a serious condition that can result from many different types of cancer, including ovarian, colon, stomach, pancreatic, and appendix cancer. The diagnosis and quantification of lesions in peritoneal carcinosis are critical in the management of patients with the condition, and imaging plays a central role in this process. Radiologists play a vital role in the multidisciplinary management of patients with peritoneal carcinosis. They need to have a thorough understanding of the pathophysiology of the condition, the underlying neoplasms, and the typical imaging findings. In addition, they need to be aware of the differential diagnoses and the advantages and disadvantages of the various imaging methods available. Imaging plays a central role in the diagnosis and quantification of lesions, and radiologists play a critical role in this process. Ultrasound, computed tomography, magnetic resonance, and PET/CT scans are used to diagnose peritoneal carcinosis. Each imaging procedure has advantages and disadvantages, and particular imaging techniques are recommended based on patient conditions. Our aim is to provide knowledge to radiologists regarding appropriate techniques, imaging findings, differential diagnoses, and treatment options. With the advent of AI in oncology, the future of precision medicine appears promising, and the interconnection between structured reporting and AI is likely to improve diagnostic accuracy and treatment outcomes for patients with peritoneal carcinosis. © 2023 by the authors."
"Artificial intelligence (AI) is being proposed for a range of subfields that deal with photovoltaic (PV) systems as a result of improvements in computer power, tool accessibility, and data generation. The methods employed at present in the PV industry for a variety of tasks, including the outcomes of design, forecasting, control, and maintenance, have been found to be relatively inaccurate. Additionally, the use of AI to carry out these tasks has improved in terms of accuracy and precision, which has made the topic itself highly interesting. In light of this, the goal of this article is to examine the effect AI approaches have on the solar value chain. The article involves creating a map of all currently accessible AI technologies, identifying potential future uses for AI, and weighing the advantages and disadvantages of these technologies’ relative to more conventional approaches. This article lays special emphasis on discussing AI techniques for improving the power quality in grid systems involving space vector pulse width modulated inverters interfacing the photovoltaic to the grid along with power converter defect monitoring, filter flaw detection, and battery monitoring. © 2023, Intelektual Pustaka Media Utama. All rights reserved."
"Liver cancer is a leading cause of cancer-related death worldwide, and its early detection and treatment are crucial for improving morbidity and mortality. Biomarkers have the potential to facilitate the early diagnosis and management of liver cancer, but identifying and implementing effective biomarkers remains a major challenge. In recent years, artificial intelligence has emerged as a promising tool in the cancer sphere, and recent literature suggests that it is very promising in facilitating biomarker use in liver cancer. This review provides an overview of the status of AI-based biomarker research in liver cancer, with a focus on the detection and implementation of biomarkers for risk prediction, diagnosis, staging, prognostication, prediction of treatment response, and recurrence of liver cancers. © 2023 by the authors."
"AI techniques have recently been put under the spotlight for analyzing electrocardiograms (ECGs). However, the performance of AI-based models relies on the accumulation of large-scale labeled datasets, which is challenging. To increase the performance of AI-based models, data augmentation (DA) strategies have been developed recently. The study presented a comprehensive systematic literature review of DA for ECG signals. We conducted a systematic search and categorized the selected documents by AI application, number of leads involved, DA method, classifier, performance improvements after DA, and datasets employed. With such information, this study provided a better understanding of the potential of ECG augmentation in enhancing the performance of AI-based ECG applications. This study adhered to the rigorous PRISMA guidelines for systematic reviews. To ensure comprehensive coverage, publications between 2013 and 2023 were searched across multiple databases, including IEEE Explore, PubMed, and Web of Science. The records were meticulously reviewed to determine their relevance to the study’s objective, and those that met the inclusion criteria were selected for further analysis. Consequently, 119 papers were deemed relevant for further review. Overall, this study shed light on the potential of DA to advance the field of ECG diagnosis and monitoring. © 2023 by the authors."
"Background: Sarcopenia is a well know prognostic factor in oncology, influencing patients’ quality of life and survival. We aimed to investigate the role of sarcopenia, assessed by a Computed Tomography (CT)-based artificial intelligence (AI)-powered-software, as a predictor of objective clinical benefit in advanced urothelial tumors and its correlations with oncological outcomes. Methods: We retrospectively searched patients with advanced urothelial tumors, treated with systemic platinum-based chemotherapy and an available total body CT, performed before and after therapy. An AI-powered software was applied to CT to obtain the Skeletal Muscle Index (SMI-L3), derived from the area of the psoas, long spine, and abdominal muscles, at the level of L3 on CT axial images. Logistic and Cox-regression modeling was implemented to explore the association of sarcopenic status and anthropometric features to the clinical benefit rate and survival endpoints. Results: 97 patients were included, 66 with bladder cancer and 31 with upper-tract urothelial carcinoma. Clinical benefit outcomes showed a linear positive association with all the observed body composition variables variations. The chances of not experiencing disease progression were positively associated with ∆_SMI-L3, ∆_psoas, and ∆_long spine muscle when they ranged from ~10–20% up to ~45–55%. Greater survival chances were matched by patients achieving a wider ∆_SMI-L3, ∆_abdominal and ∆_long spine muscle. Conclusions: A CT-based AI-powered software body composition and sarcopenia analysis provide prognostic assessments for objective clinical benefits and oncological outcomes. © 2023 by the authors."
"This comprehensive analysis dives deep into the intricate interplay between artificial intelligence (AI) and human agency, examining the remarkable capabilities and inherent limitations of large language models (LLMs) such as GPT-3 and ChatGPT. The paper traces the complex trajectory of AI's evolution, highlighting its operation based on statistical pattern recognition, devoid of self-consciousness or innate comprehension. As AI permeates multiple spheres of human life, it raises substantial ethical, legal, and societal concerns that demand immediate attention and deliberation. The metaphorical illustration of humans as “shepherds” of AI, coupled with a thought-provoking examination of “agency without intelligence,” engenders deep reflections on the future of human-AI interplay and the associated philosophical, ethical, and practical ramifications. The paper further elucidates critical challenges associated with AI, such as ethical dilemmas, “agency without intelligence,” the role of human oversight, and the environmental impact of AI technologies. In the pursuit of solutions, the paper maps prospective directions, emphasizing the need for robust AI policies, improved AI literacy, sustainable AI practices, and a continued dialogue on the multifaceted issues surrounding AI. Through a holistic exploration of AI, this paper underlines the necessity for a multidisciplinary, collaborative approach to chart an ethical, beneficial, and sustainable AI future, enabling society to harness the potential of AI responsibly while mitigating the associated risks. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"This study explores the reasons (for and against) the adoption of AI in the hospitality industry among Pakistani customers. The hypothesis was tested using the sample obtained from Pakistani hospitality customers. The data is collected via an online survey and analyzed with the structural equation modeling and PROCESS macro. The study results indicate that cultural values positively associate reasons for, attitudes, and intentions to adopt. Moreover, the results prove that attitude mediates the relationship between reasons (for and against) and intention to adopt AI services in hospitality. Furthermore, the finding shows that customer emotional intelligence has no moderation effect on attitude and intention to adopt AI services. This study highlights technological complexity and safety concerns are the most significant barriers in the studied context. Further, addressing reason against (technological complexity and safety concerns) may allow policymakers to lessen the present customer attitude-intention gap by tackling the factors that cause customers to resist adopting AI services in hospitality. In this respect, marketers should develop a marketing campaign strategy focusing on the benefits of the adoption of AI services in comparison to employee service. This is the first empirical study examining cultural values, reasons (for and against) attitudes, their relationship with intentions, and moderating role of an attractive, important, yet ignored variable, customer emotional intelligence. This study confirms that the behavioral reasoning theory can better describe the customers' adoption behavior of AI services in the hospitality sector. © 2023 The Authors"
"In recent years, there has been a dramatic rise in interest in retrosynthesis prediction with artificial intelligence (AI) techniques. Unlike conventional retrosynthesis prediction performed by chemists and by rule-based expert systems, AI-driven retrosynthesis prediction automatically learns chemistry knowledge from off-the-shelf experimental datasets to predict reactions and retrosynthesis routes. This provides an opportunity to address many conventional challenges, including heavy reliance on extensive expertise, the sub-optimality of routes, and prohibitive computational cost. This review describes the current landscape of AI-driven retrosynthesis prediction. We first discuss formal definitions of the retrosynthesis problem and review the outstanding research challenges therein. We then review the related AI techniques and recent progress that enable retrosynthesis prediction. Moreover, we propose a novel landscape that provides a comprehensive categorization of different retrosynthesis prediction components and survey how AI reshapes each component. We conclude by discussing promising areas for future research. © 2022 THE AUTHORS"
"Background: Identifying patients with clinically significant prostate cancer (csPCa) before biopsy helps reduce unnecessary biopsies and improve patient prognosis. The diagnostic performance of traditional transrectal ultrasound (TRUS) for csPCa is relatively limited. This study was aimed to develop a high-performance convolutional neural network (CNN) model (P-Net) based on a TRUS video of the entire prostate and investigate its efficacy in identifying csPCa. Methods: Between January 2021 and December 2022, this study prospectively evaluated 832 patients from four centres who underwent prostate biopsy and/or radical prostatectomy. All patients had a standardised TRUS video of the whole prostate. A two-dimensional CNN (2D P-Net) and three-dimensional CNN (3D P-Net) were constructed using the training cohort (559 patients) and tested on the internal validation cohort (140 patients) as well as on the external validation cohort (133 patients). The performance of 2D P-Net and 3D P-Net in predicting csPCa was assessed in terms of the area under the receiver operating characteristic curve (AUC), biopsy rate, and unnecessary biopsy rate, and compared with the TRUS 5-point Likert score system as well as multiparametric magnetic resonance imaging (mp-MRI) prostate imaging reporting and data system (PI-RADS) v2.1. Decision curve analyses (DCAs) were used to determine the net benefits associated with their use. The study is registered at https://www.chictr.org.cn with the unique identifier ChiCTR2200064545. Findings: The diagnostic performance of 3D P-Net (AUC: 0.85–0.89) was superior to TRUS 5-point Likert score system (AUC: 0.71–0.78, P = 0.003–0.040), and similar to mp-MRI PI-RADS v2.1 score system interpreted by experienced radiologists (AUC: 0.83–0.86, P = 0.460–0.732) and 2D P-Net (AUC: 0.79–0.86, P = 0.066–0.678) in the internal and external validation cohorts. The biopsy rate decreased from 40.3% (TRUS 5-point Likert score system) and 47.6% (mp-MRI PI-RADS v2.1 score system) to 35.5% (2D P-Net) and 34.0% (3D P-Net). The unnecessary biopsy rate decreased from 38.1% (TRUS 5-point Likert score system) and 35.2% (mp-MRI PI-RADS v2.1 score system) to 32.0% (2D P-Net) and 25.8% (3D P-Net). 3D P-Net yielded the highest net benefit according to the DCAs. Interpretation: 3D P-Net based on a prostate grayscale TRUS video achieved satisfactory performance in identifying csPCa and potentially reducing unnecessary biopsies. More studies to determine how AI models better integrate into routine practice and randomized controlled trials to show the values of these models in real clinical applications are warranted. Funding: The National Natural Science Foundation of China (Grants 82202174 and 82202153), the Science and Technology Commission of Shanghai Municipality (Grants 18441905500 and 19DZ2251100), Shanghai Municipal Health Commission (Grants 2019LJ21 and SHSLCZDZK03502), Shanghai Science and Technology Innovation Action Plan ( 21Y11911200), and Fundamental Research Funds for the Central Universities ( ZD-11-202151), Scientific Research and Development Fund of Zhongshan Hospital of Fudan University (Grant 2022ZSQD07). © 2023 The Author(s)"
"Hormone receptor-positive breast cancer (HR+ BC) cells depend on estrogen and its receptor, ER. Due to this dependence, endocrine therapy (ET) such as aromatase inhibitor (AI) treatment is now possible. However, ET resistance (ET-R) occurs frequently and is a priority in HR+ BC research. The effects of estrogen have typically been determined under a special culture condition, i.e., phenol red-free media supplemented with dextran-coated charcoal-stripped fetal bovine serum (CS-FBS). However, CS-FBS has some limitations, such as not being fully defined or ordinary. Therefore, we attempted to find new experimental conditions and related mechanisms to improve cellular estrogen responsiveness based on the standard culture medium supplemented with normal FBS and phenol red. The hypothesis of pleiotropic estrogen effects led to the discovery that T47D cells respond well to estrogen under low cell density and medium replacement. These conditions made ET less effective there. The fact that several BC cell culture supernatants reversed these findings implies that housekeeping autocrine factors regulate estrogen and ET responsiveness. Results reproduced in T47D subclone and MCF-7 cells highlight that these phenomena are general among HR+ BC cells. Our findings offer not only new insights into ET-R but also a new experimental model for future ET-R studies. © 2023 by the authors."
"Humans may deprive each other of human qualities if the social context encourages it. But what about the opposite: do people attribute human traits to non-human entities without a mind, such as Artificial Intelligence (AI)? Perceived humanness is based on the assumption that the other can act (has agency) and has experiences (thoughts and feelings). This review shows that AI fails to fully elicit these two dimensions of mind perception. Embodied AI may trigger agency attribution, but only humans trigger the attribution of experience. Importantly, people are more likely to attribute mind in general and agency specifically to AI that resembles the human form. Lastly, people's pre-dispositions and the social context affect people's tendency to attribute human traits to AI. © 2023 Elsevier Ltd"
"This study aims to investigate the influence of emerging digital technologies, such as artificial intelligence (AI), the Internet of Things (IoT), and cloud computing, on the digital intensity index (DII). The research method employed involves quantitative analysis of the indicators regarding DII and emerging digital technologies, conducted based on data published by Eurostat for EU members in 2021. During our research, we formulated and tested hypotheses about the relationship between the DII and emerging digital technologies, and the effect on the DII of using AI-based technologies in various economic processes. The formulated hypotheses were validated via four regression models designed during this study, using the most relevant factors. Our research results demonstrate that the DII is positively influenced by emerging IoT and cloud computing digital technologies, as well as the use of AI technologies based on machine learning and AI-based robotic process automation (RPA) software. Furthermore, the same positive influence was identified in human resource management and recruitment processes compared to the intensity with which these technologies are used in other economic processes. Based on these findings, this study offers persuasive arguments for implementing emerging digital technologies at the EU organizational level to achieve significant increases in digitalization levels. © 2023 by the authors."
"Medical image analysis plays an important role in clinical diagnosis. In this paper, we examine the recent Segment Anything Model (SAM) on medical images, and report both quantitative and qualitative zero-shot segmentation results on nine medical image segmentation benchmarks, covering various imaging modalities, such as optical coherence tomography (OCT), magnetic resonance imaging (MRI), and computed tomography (CT), as well as different applications including dermatology, ophthalmology, and radiology. Those benchmarks are representative and commonly used in model development. Our experimental results indicate that while SAM presents remarkable segmentation performance on images from the general domain, its zero-shot segmentation ability remains restricted for out-of-distribution images, e.g., medical images. In addition, SAM exhibits inconsistent zero-shot segmentation performance across different unseen medical domains. For certain structured targets, e.g., blood vessels, the zero-shot segmentation of SAM completely failed. In contrast, a simple fine-tuning of it with a small amount of data could lead to remarkable improvement of the segmentation quality, showing the great potential and feasibility of using fine-tuned SAM to achieve accurate medical image segmentation for a precision diagnostics. Our study indicates the versatility of generalist vision foundation models on medical imaging, and their great potential to achieve desired performance through fine-turning and eventually address the challenges associated with accessing large and diverse medical datasets in support of clinical diagnostics. © 2023 by the authors."
"This article seeks to reorientate ‘digital oral history’ towards a new research paradigm, Multimodal Digital Oral History (MDOH), and in so doing it seeks to build upon Alistair Thomson’s (Thomson, A., 2007, Four paradigm transformations in oral history. Oral History Review, 34(1): 49–70.) characterization of a ‘dizzying digital revolution’ and paradigmatic transformation in oral history (OH). Calling for a recalibration of the current dominance of the textual transcript, and for active engagement with the oral, aural, and sonic affordances of both retro-digitized and born digital OH (DOH) collections, we call for a re-orientation of the digital from passive to generative and self-reflexive in the human–machine study of spoken word recordings. First, we take stock of the field of DOH as it is currently conceived and the ways in which it has or has not answered calls for a return to the orality of the interview by digital means. Secondly, we address the predominant trend of working with transcriptions in digital analysis of spoken word recordings and the tools being used by oral historians. Thirdly, we ask about the emerging possibilities—tools and experimental methodologies—for sonic analysis of spoken word collections within and beyond OH, looking to intersections with digital humanities, sociolinguistics, and sound studies. Lastly, we consider ethical questions and practicalities concomitant with data-driven methods, analyses and technologies like AI for the study of sonic research artefacts, reflections that dovetail with digital hermeneutics and digital tool criticism and point towards a new MDOH departure, a sub-field that has potential to inform the many fields that seek patterns in audio, audio-visual, and post-textual materials, serially and at scale. © The Author(s) 2023."
"This study aims to understand the development trends and research structure of articles on artificial intelligence (AI) and information processing in the past 10 years. In particular, this study analyzed 13,294 papers published from 2012 to 2021 in the Web of Science, used the bibliometric analysis method to visualize the data of the papers, and drew a scientific knowledge map. By exploring the development of mainstream journals, author and country rankings, keyword evolution, and research field rankings in the past 10 years, this study uncovered key trends affecting AI progress and information processing that provide insights and serve as an important reference for future AI research and information processing. The results revealed a gradual increase in publications over the past decade, with explosive growth after 2020. The most prolific researchers in this field were Xu, Z.S.; Pedrycz, W.; Herrera-Viedma, E.; the major contributing countries were China, the USA, and Spain. In the AI and information processing research, keywords including “Deep learning”, “Machine learning”, and “Feature extraction” are components that play a crucial role. Additionally, the most representative research areas were “Engineering”, “Operations Research and Management Science”, and “Automation Control Systems”. Overall, this study used bibliometric analysis to provide an overview of the latest trends in artificial intelligence and information processing. Although AI and information processing have been applied to various research areas, many other sub-topics can be further applied. Based on the findings, this study presented research insights and proposed suggestions for future research directions on AI and information processing. © 2023 by the authors."
"Terahertz (THz) is a promising technology for future wireless communication networks, particularly for 6G and beyond. The ultra-wide THz band, ranging from 0.1 to 10 THz, can potentially address the limited capacity and scarcity of spectrum in current wireless systems such as 4G-LTE and 5G. Furthermore, it is expected to support advanced wireless applications requiring high data transmission and quality services, i.e., terabit-per-second backhaul systems, ultra-high-definition streaming, virtual/augmented reality, and high-bandwidth wireless communications. In recent years, artificial intelligence (AI) has been used mainly for resource management, spectrum allocation, modulation and bandwidth classification, interference mitigation, beamforming, and medium access control layer protocols to improve THz performance. This survey paper examines the use of AI in state-of-the-art THz communications, discussing the challenges, potentials, and shortcomings. Additionally, this survey discusses the available platforms, including commercial, testbeds, and publicly available simulators for THz communications. Finally, this survey provides future strategies for improving the existing THz simulators and using AI methods, including deep learning, federated learning, and reinforcement learning, to improve THz communications. © 2023 by the authors."
"Stroke is an emergency in which delays in treatment can lead to significant loss of neurological function and be fatal. Technologies that increase the speed and accuracy of stroke diagnosis or assist in post-stroke rehabilitation can improve patient outcomes. No resource exists that comprehensively assesses artificial intelligence/machine learning (AI/ML)-enabled technologies indicated for the management of ischemic and hemorrhagic stroke. We queried a United States Food and Drug Administration (FDA) database, along with PubMed and private company websites, to identify the recent literature assessing the clinical performance of FDA-approved AI/ML-enabled technologies. The FDA has approved 22 AI/ML-enabled technologies that triage brain imaging for more immediate diagnosis or promote post-stroke neurological/functional recovery. Technologies that assist with diagnosis predominantly use convolutional neural networks to identify abnormal brain images (e.g., CT perfusion). These technologies perform comparably to neuroradiologists, improve clinical workflows (e.g., time from scan acquisition to reading), and improve patient outcomes (e.g., days spent in the neurological ICU). Two devices are indicated for post-stroke rehabilitation by leveraging neuromodulation techniques. Multiple FDA-approved technologies exist that can help clinicians better diagnose and manage stroke. This review summarizes the most up-to-date literature regarding the functionality, performance, and utility of these technologies so clinicians can make informed decisions when using them in practice. © 2023 by the authors."
"The essay discusses the subject of Artificial Intelligence (AI) algorithms and the legal principles governing this matter. The functioning of the algorithms Turochamp, Stockfish, AlphaZero, used in the game of chess, is described in detail. Based on these examples, the differences between deterministic, non-strictly deterministic algorithms and self-training AI algorithms are analysed, up to the topic of opacity and black box, typical of deep-learning systems. Finally, the issue of the transparency and explicability required by jurisprudence for the use of AI algorithms in the public sphere and the compatibility of these legal principles with the functioning of these new technologies are addressed. © Società editrice il Mulino."
"Background: Individuals with amelogenesis imperfecta (AI) often present with malocclusions, especially a dental or skeletal anterior open bite (AOB). Objectives: To evaluate the craniofacial characteristics in individuals with AI. Material and methods: A systematic literature search was conducted with the PubMed, Web of Science, Embase and Google Scholar databases to identify studies relating to the cephalometric characteristics of individuals with AI, without any language or publication date restrictions. The grey literature was searched using Google Scholar, Opengrey and Worldcat. Only studies with a suitable control group for comparison were included. Data extraction and a risk of bias assessment were carried out. A meta-analysis was performed using the random effects model for cephalometric variables that were evaluated in at least three studies. Results: The initial literature search yielded 1857 articles. Following the removal of duplicates and a screening of the records, seven articles were included in the qualitative synthesis, representing a total of 242 individuals with AI. Four studies were included in the quantitative synthesis. The meta-analysis results showed that individuals with AI present a smaller SNB angle and larger ANB angle than those of control groups in the sagittal plane. In the vertical plane, those with AI present a smaller overbite and larger intermaxillary angle than those without AI. No statistically significant differences were found for the SNA angle when comparing the two groups. Conclusions: Individuals with AI seem to present with more vertical craniofacial growth, leading to an increased intermaxillary angle and decreased overbite. This possibly leads to a more retrognathic mandible with a larger ANB angle due to an anticipated posterior mandibular rotation. © 2023 by the authors."
"Rockfall constitutes a major threat to the safety and sustainability of transport corridors bordered by rocky cliffs. This research introduces a new approach to rockfall susceptibility modeling for the identification of potential rockfall source zones. This is achieved by developing a data-driven model to assess the local slope morphological attributes with respect to the rock slope evolution processes. The ability to address “where” a rockfall is more likely to occur via the analysis of historical event inventories with respect to terrain attributes and to define the probability of a given area producing a rockfall is a critical advance toward effective transport corridor management. The availability of high-quality digital volumetric change detection products permits new developments in rockfall assessment and prediction. We explore the potential of simulating the conceptualization of slope-scale rockfall susceptibility modeling using computer power and artificial intelligence (AI). We employ advanced 3D computer vision algorithms for analyzing point clouds to interpret high-resolution digital observations capturing the rock slope evolution via long-term, LiDAR-based 3D differencing. The approach has been developed and tested on data from three rock slopes: two in Canada and one in the UK. The results indicate clear potential for AI advances to develop local susceptibility indicators from local geometry and learning from recent rockfall activity. The resultant models produce slope-wide rockfall susceptibility maps in high resolution, producing up to 75% agreement with validated occurrences. © 2023 by the authors."
"In recent years, data has become one of the most important resources in the digital economy. Unlike traditional resources, the digital nature of data makes it difficult to value and contract. Therefore, establishing an efficient and standard data-transaction market system would be beneficial for lowering cost and improving productivity among the parties in this industry. Although numerous studies have been dedicated to the issue of complying with data regulations and other data-transaction issues such as privacy and pricing, little work has been done to provide a comprehensive review of these studies in the fields of machine learning and data science. To provide a complete and up-to-date understanding of this topic, this review covers the three key issues of data transaction: data rights, data pricing, and privacy computing. By connecting these topics, this paper provides a big picture of a data ecosystem in which data is generated by data subjects such as individuals, research agencies, and governments, while data processors acquire data for innovational or operational purposes, and benefits are allocated according to the data's respective ownership via an appropriate price. With the long-term goal of making artificial intelligence (AI) beneficial to human society, AI algorithms will then be assessed by data protection regulations (i.e., privacy protection regulations) to help build trustworthy AI systems for daily life. © 2023 THE AUTHORS"
"The utilization of Granite Powder (GP), a by-product of granite processing, has arisen great attention due to environmental challenge posed by its casual disposal. To promote reasonable use of GP in concrete and obtain optimal ratio design of Granite Powder Concrete (GPC), this study explored four advanced machine learning models (Support Vector Machine (SVM), eXtreme Gradient Boosting (XGB), Random Forest (RF), and Multi-Layer Perceptron (MLP)) integrated with two novel metaheuristic algorithms (Salp Swarm Algorithms (SSA) and Ant Lion Optimizer (ALO)) to predict the compressive strength of GPC. The models consider seven effect factors on the compressive strength: cement, water, fine aggregate, granite powder, superplasticizer, coarse aggregates, and curing age. The comprehensive score ranking system was utilized four evaluation metrics to assess models’ performance: Coefficient of Determination (R2), Variance Accounted For (VAF), Mean Absolute Error (MAE), and Mean Square Error (MSE). In addition, Taylor diagrams and Regression Error Characteristic (REC) curves were used to compare the performance of the models. The results showed that the XGB-based hybrid model outperformed the other methods, with an optimal model of ALO-XGB achieving a high level of accuracy in predicting compressive strength. Furthermore, the study highlights the importance of understanding the interplay between various factors and their effect on the mechanical properties of GPC, which can inform the development of more sustainable and environmentally friendly building materials. © 2023 Elsevier Ltd"
"Retinoblastoma is a rare and aggressive form of childhood eye cancer that requires prompt diagnosis and treatment to prevent vision loss and even death. Deep learning models have shown promising results in detecting retinoblastoma from fundus images, but their decision-making process is often considered a “black box” that lacks transparency and interpretability. In this project, we explore the use of LIME and SHAP, two popular explainable AI techniques, to generate local and global explanations for a deep learning model based on InceptionV3 architecture trained on retinoblastoma and non-retinoblastoma fundus images. We collected and labeled a dataset of 400 retinoblastoma and 400 non-retinoblastoma images, split it into training, validation, and test sets, and trained the model using transfer learning from the pre-trained InceptionV3 model. We then applied LIME and SHAP to generate explanations for the model’s predictions on the validation and test sets. Our results demonstrate that LIME and SHAP can effectively identify the regions and features in the input images that contribute the most to the model’s predictions, providing valuable insights into the decision-making process of the deep learning model. In addition, the use of InceptionV3 architecture with spatial attention mechanism achieved high accuracy of 97% on the test set, indicating the potential of combining deep learning and explainable AI for improving retinoblastoma diagnosis and treatment. © 2023 by the authors."
"Due to its distinct production paradigm, additive manufacturing (AM) is positioned to bring about a revolution. It presents the possibility of on-demand, decentralized, and mass-customizable manufacturing. However, several issues related to design principles, standardization, and quality control arise from not only the complexity of production systems but also the need for increasingly complicated and high-quality goods. Artificial Intelligence (AI)-based algorithms, which can effectively monitor quality, optimize processes, model complex systems, and manage energy, is essential in addressing the difficulties. In the present work, we have used three supervised machine learning regression-based algorithms, i.e., XG Boost, Random Forest, and Decision Trees, to determine the Flexural Strength of the Fused Deposition Modeling specimen. The results showed that the XG Boost algorithm resulted in the highest coefficient of determination value of 0.77. Supervised machine learning classification-based algorithms such as the Stochastic Gradient Descent (SGD) algorithm, Decision Tree, and Random Forest algorithm is used to determine good and bad flexural strength specimens. The result showed that the SGD algorithm achieved the highest F1 score of 0.85. © 2023, The Author(s), under exclusive licence to Bharati Vidyapeeth's Institute of Computer Applications and Management."
"Motivation: Bone marrow (BM) examination is one of the most important indicators in diagnosing hematologic disorders and is typically performed under the microscope via oil-immersion objective lens with a total 100× objective magnification. On the other hand, mitotic detection and identification is critical not only for accurate cancer diagnosis and grading but also for predicting therapy success and survival. Fully automated BM examination and mitotic figure examination from whole-slide images is highly demanded but challenging and poorly explored. First, the complexity and poor reproducibility of microscopic image examination are due to the cell type diversity, delicate intralineage discrepancy within the multitype cell maturation process, cells overlapping, lipid interference and stain variation. Second, manual annotation on whole-slide images is tedious, laborious and subject to intraobserver variability, which causes the supervised information restricted to limited, easily identifiable and scattered cells annotated by humans. Third, when the training data are sparsely labeled, many unlabeled objects of interest are wrongly defined as background, which severely confuses AI learners. Results: This article presents an efficient and fully automatic CW-Net approach to address the three issues mentioned above and demonstrates its superior performance on both BM examination and mitotic figure examination. The experimental results demonstrate the robustness and generalizability of the proposed CW-Net on a large BM WSI dataset with 16 456 annotated cells of 19 BM cell types and a large-scale WSI dataset for mitotic figure assessment with 262 481 annotated cells of five cell types.  © 2023 The Author(s). Published by Oxford University Press."
"This “reflections from practice” piece explores some of the implications of emerging, artificially intelligent tools for the futures and foresight prac-ademic community. The authors provide background on these emerging, artificially intelligent tools, and explore, with special emphasis on scenarios, a specific tool named “Chat Generative Pre-trained Transformer” (hereafter, ChatGPT). The authors examine the utility of scenarios generated by artificial intelligence (AI) and explore whether or not the futures and foresight prac-ademic community should selectively embrace advances in AI to assist in the generation of scenarios. In particular, the authors will consider (1) the utility of using scenarios generated completely by AI, (2) whether what is produced, in fact, constitute scenarios, based on conventional definitions, and (3) assess the utility of using AI to assist in the production of scenarios. At this point in time, artificially intelligent tools can now generate numerous scenarios on seemingly any topic at essentially zero cost to the user. Still, the authors insist that the utility of those scenarios is largely predicated on the user's ability to coax the appropriate “raw material” from the artificially intelligent bot, which implicates, the authors contend, that such bots can usefully provide base material for the development of scenarios but are unlikely to fully eclipse scenarists in the production of scenarios. Additionally, the authors recommend that the futures and foresight prac-ademic community pay especially close attention to artificially intelligent tools for novel insights with regard to the differences in human cognition and, in this case, the logic of large language model outputs. © 2023 The Authors. Futures & Foresight Science published by John Wiley & Sons Ltd."
"Background: Endoscopic Ultrasound (EUS) is widely used for the diagnosis of bilio-pancreatic and gastrointestinal (GI) tract diseases, for the evaluation of subepithelial lesions, and for sampling of lymph nodes and solid masses located next to the GI tract. The role of Artificial Intelligence in healthcare in growing. This review aimed to provide an overview of the current state of AI in EUS from imaging to pathological diagnosis and training. Methods: AI algorithms can assist in lesion detection and characterization in EUS by analyzing EUS images and identifying suspicious areas that may require further clinical evaluation or biopsy sampling. Deep learning techniques, such as convolutional neural networks (CNNs), have shown great potential for tumor identification and subepithelial lesion (SEL) evaluation by extracting important features from EUS images and using them to classify or segment the images. Results: AI models with new features can increase the accuracy of diagnoses, provide faster diagnoses, identify subtle differences in disease presentation that may be missed by human eyes, and provide more information and insights into disease pathology. Conclusions: The integration of AI in EUS images and biopsies has the potential to improve the diagnostic accuracy, leading to better patient outcomes and to a reduction in repeated procedures in case of non-diagnostic biopsies. © 2023 by the authors."
"Domestic violence remains a pressing complex social problem of people of any gender, age, socio-economic status, and ethno-cultural background, an issue that worsened worldwide during the COVID-19 pandemic. Digital, online, or artificial intelligence-based smart technological services, applications, and tools provide novel approaches in addressing domestic violence, including intimate partner violence. This systematic literature review analyses the ethical challenges and opportunities these (protective) digital and smart technologies provide to the stakeholders involved. Our results highlight that the public health and societal issue are the leading narratives of domestic violence, which is predominantly interpreted as gender-based violence. The review highlights an emerging trend of the role of machine learning- and artificial intelligence-based approaches in identifying and preventing domestic violence. However, we argue that little recommendation is available to professionals about how to use these approaches in a responsible way, and that the smartness of high-tech technologies is often challenged by basic-level technologies from perpetrators, creating an imbalance that also limits an impactful development of a comprehensive socio-technical regime that serves the safety and resilience of families in their communal setting. © 2023"
"In a world increasingly aware of its carbon footprint, the quest for sustainable energy production and consumption has never been more urgent. A key player in this monumental endeavor is fuel conservation, which helps curb greenhouse gas emissions and preserve our planet’s finite resources. In the realm of the Industrial Internet of Things (IIoT) and artificial intelligence (AI) technologies, Caterpillar (CAT) generator set (genset) operations have been revolutionized, unlocking unprecedented fuel savings and reducing environmental harm. Envision a system that not only enhances fuel efficiency but also anticipates maintenance needs with state-of-the-art technology. This standalone IIoT platform crafted with Visual Basic.Net (VB.Net) and the KEPware Object linking and embedding for Process Control (OPC) server gathers, stores, and analyzes data from CAT gensets, painting a comprehensive picture of their inner workings. By leveraging the Modbus Remote Terminal Unit (RTU) protocol, the platform acquires vital parameters such as engine load, temperature, pressure, revolutions per minute (RPM), and fuel consumption measurements, from a radar transmitter. However, the magic does not stop there. Machine Learning.Net (ML.Net) empowers the platform with machine learning capabilities, scrutinizing the generator’s performance over time, identifying patterns and forecasting future behavior. Equipped with these insights, the platform fine tunes its operations, elevates fuel efficiency, and conducts predictive maintenance, minimizing downtime and amplifying overall efficiency. The evidence is compelling: IIoT and AI technologies have the power to yield substantial fuel savings and enhance performance through predictive maintenance. This research offers a tangible solution for industries eager to optimize operations and elevate efficiency by embracing IIoT and AI technologies in CAT genset operations. The future is greener and smarter, and it starts now. © 2023 by the authors."
"Neuromuscular disease includes a wide range of muscular disorders, but it lacks convenient and effective tools for clinical diagnosis and therapeutic monitoring. As a widely used imaging tool, ultrasound can clearly display muscle structure and create basic conditions for accurate image analysis. At present, many studies have tried to obtain information on muscle function and pathological changes by analyzing the features of muscle ultrasound images, and have shown reliable results. However, the minimal changes in muscle structure and image texture are easy to be neglected, and manual segmentation and data analysis are time-consuming tasks. Artificial intelligence (AI) can accurately identify image changes and improve the efficiency of image analysis, and the muscle ultrasonic image analysis model developed based on AI has shown advantages in a large number of research results. This review summarizes the relevant studies of muscle ultrasound imaging and AI in the field of it, including a variety of research based on traditional AI methods or deep learning methods, as well as discusses the clinical significance of ultrasound analysis assisted by AI and the future exploration directions in this field. © 2023 Authors. All rights reserved."
"Solar irradiation data is essential for the feasibility of solar energy projects. Notably, the intermittent nature of solar irradiation influences solar energy use in all forms, whether energy or agriculture. Accurate solar irradiation prediction is the only solution to effectively use solar energy in different forms. The estimation of solar irradiation is the most critical factor for site selection and sizing of solar energy projects and for selecting a suitable crop selection for the area. But the physical measurement of solar irradiation, due to the cost and technology involved, is not possible for all locations across the globe. Numerous techniques have been implemented to predict solar irradiation for this purpose. The two types of approaches that are most frequently employed are empirical techniques and artificial intelligence (AI). Both approaches have demonstrated good accuracy in various places of the world. To find out the best method, a thorough review of research articles discussing solar irradiation prediction has been done to compare different methods for solar irradiation prediction. In this paper, articles predicting solar irradiation using AI and empirical published from 2017 to 2022 have been reviewed, and both methods have been compared. The review showed that AI methods are more accurate than empirical methods. In empirical models, modified sunshine-based models (MSSM) have the highest accuracy, followed by sunshine-based (SSM) and non-sunshine-based models (NSM). The NSM has a little lower accuracy than MSSM and SSM, but the NSM can give good results in sunshine data unavailability. Also, the literature review confirmed that simple empirical models could predict accurately, and increasing the empirical model's polynomial order cannot improve results. Artificial neural networks (ANN) and Hybrid models have the highest accuracy among AI methods, followed by support vector machine (SVM) and adaptive neuro-fuzzy inference system (ANFIS). The increase in efficiency by hybrid models is minimal, but the complexity of models requires very sophisticated programming knowledge. ANN's most important input factors are maximum and minimum temperatures, temperature differential, relative humidity, clearness index and precipitation. © 2023"
"Cyber deception technology plays an important role in monitoring attackers’ activities and detecting new attack types. However, in a deceptive environment, low-risk attack traffic, such as scanning, is included in large quantities and acts as noise. Therefore, even though high-risk traffic is actually present, it may be overlooked, or the analysis algorithm’s accuracy regarding traffic may be reduced, causing significant difficulties in intrusion detection and analysis processes. In this study, we propose a model that can identify and filter the ordinal scale risk of the source IP in deceptive environment-generated traffic. This model aims to quickly classify low-risk attacks, including information gathering and scanning, which are widely and repeatedly performed, as well as high-risk attacks, rather than classifying specific types of attacks. Most existing deceptive technology-based Cyber Threat Intelligence (CTI) generation studies have been limited in their applicability to real-world environments because data labeling, learning, and detection processes using AI algorithms that consume significant amounts of time and computing resources. Here, the Naive Bayes discriminant analysis-based ordinary scale classification model showed higher accuracy for low-risk attack classification, while consuming significantly fewer resources than the models presented in other studies do. The accuracy of the current active deceptive environment traffic analysis research may be enhanced by filtering low-risk traffic via preprocessing. © 2023 by the authors."
"During the waves of the coronavirus disease (COVID-19) pandemic, emergency departments were overflowing with patients suffering with suspected medical or surgical issues. In these settings, healthcare staff should be able to deal with different medical and surgical scenarios while protecting themselves against the risk of contamination. Various strategies were used to overcome the most critical issues and guarantee quick and efficient diagnostic and therapeutic charts. The use of saliva and nasopharyngeal swab Nucleic Acid Amplification Tests (NAAT) in the diagnosis of COVID-19 was one of the most adopted worldwide. However, NAAT results were slow to report and could sometimes create significant delays in patient management, especially during pandemic peaks. On these bases, radiology has played and continues to play an essential role in detecting COVID-19 patients and solving differential diagnosis between different medical conditions. This systematic review aims to summarize the role of radiology in the management of COVID-19 patients admitted to emergency departments by using chest X-rays (CXR), computed tomography (CT), lung ultrasounds (LUS), and artificial intelligence (AI). © 2023 by the authors."
"Featured Application: Potential applications of the work include the computerization of production processes and the construction of artificial intelligence-based systems to support and optimize tool selection on existing and newly designed assembly lines in line with Industry 4.0 and Internet of Things paradigms. Fast, accurate, and efficient analysis of production data is a key element of the Industry 4.0 paradigm. This applies not only to newly built solutions but also to the digitalization, automation, and robotization of existing factories and production or repair lines. In particular, technologists’ extensive experience and know-how are necessary to design correct technological processes to minimize losses during production and product costs. That is why the proper selection of tools, machine tools, and production parameters during the manufacturing process is so important. Properly developed technology affects the entire production process. This paper presents an attempt to develop a post-hoc model of already existing manufacturing processes with the increased requirements and expectations resulting from the introduction of the Industry 4.0 paradigm. In particular, we relied on fuzzy logic to support the description of uncertainties, incomplete data, and discontinuities in the manufacturing process. This translates into better controls compared to conventional systems. An analysis of the proposed solution’s limitations and proposals for further development constitute the novelty and contribution of the article. © 2023 by the authors."
"As the global population grows, and urbanization becomes more prevalent, cities often struggle to provide convenient, secure, and sustainable lifestyles due to the lack of necessary smart technologies. Fortunately, the Internet of Things (IoT) has emerged as a solution to this challenge by connecting physical objects using electronics, sensors, software, and communication networks. This has transformed smart city infrastructures, introducing various technologies that enhance sustainability, productivity, and comfort for urban dwellers. By leveraging Artificial Intelligence (AI) to analyze the vast amount of IoT data available, new opportunities are emerging to design and manage futuristic smart cities. In this review article, we provide an overview of smart cities, defining their characteristics and exploring the architecture of IoT. A detailed analysis of various wireless communication technologies employed in smart city applications is presented, with extensive research conducted to determine the most appropriate communication technologies for specific use cases. The article also sheds light on different AI algorithms and their suitability for smart city applications. Furthermore, the integration of IoT and AI in smart city scenarios is discussed, emphasizing the potential contributions of 5G networks coupled with AI in advancing modern urban environments. This article contributes to the existing literature by highlighting the tremendous opportunities presented by integrating IoT and AI, paving the way for the development of smart cities that significantly enhance the quality of life for urban dwellers while promoting sustainability and productivity. By exploring the potential of IoT, AI, and their integration, this review article provides valuable insights into the future of smart cities, demonstrating how these technologies can positively impact urban environments and the well-being of their inhabitants. © 2023 by the authors."
"In recent years, the development of deep learning technology has significantly benefited agriculture in domains such as smart and precision farming. Deep learning models require a large amount of high-quality training data. However, collecting and managing large amounts of guaranteed-quality data is a critical issue. To meet these requirements, this study proposes a scalable plant disease information collection and management system (PlantInfoCMS). The proposed PlantInfoCMS consists of data collection, annotation, data inspection, and dashboard modules to generate accurate and high-quality pest and disease image datasets for learning purposes. Additionally, the system provides various statistical functions allowing users to easily check the progress of each task, making management highly efficient. Currently, PlantInfoCMS handles data on 32 types of crops and 185 types of pests and diseases, and stores and manages 301,667 original and 195,124 labeled images. The PlantInfoCMS proposed in this study is expected to significantly contribute to the diagnosis of crop pests and diseases by providing high-quality AI images for learning about and facilitating the management of crop pests and diseases. © 2023 by the authors."
"Background and motivation: Lung computed tomography (CT) techniques are high-resolution and are well adopted in the intensive care unit (ICU) for COVID-19 disease control classification. Most artificial intelligence (AI) systems do not undergo generalization and are typically overfitted. Such trained AI systems are not practical for clinical settings and therefore do not give accurate results when executed on unseen data sets. We hypothesize that ensemble deep learning (EDL) is superior to deep transfer learning (TL) in both non-augmented and augmented frameworks. Methodology: The system consists of a cascade of quality control, ResNet–UNet-based hybrid deep learning for lung segmentation, and seven models using TL-based classification followed by five types of EDL’s. To prove our hypothesis, five different kinds of data combinations (DC) were designed using a combination of two multicenter cohorts—Croatia (80 COVID) and Italy (72 COVID and 30 controls)—leading to 12,000 CT slices. As part of generalization, the system was tested on unseen data and statistically tested for reliability/stability. Results: Using the K5 (80:20) cross-validation protocol on the balanced and augmented dataset, the five DC datasets improved TL mean accuracy by 3.32%, 6.56%, 12.96%, 47.1%, and 2.78%, respectively. The five EDL systems showed improvements in accuracy of 2.12%, 5.78%, 6.72%, 32.05%, and 2.40%, thus validating our hypothesis. All statistical tests proved positive for reliability and stability. Conclusion: EDL showed superior performance to TL systems for both (a) unbalanced and unaugmented and (b) balanced and augmented datasets for both (i) seen and (ii) unseen paradigms, validating both our hypotheses. © 2023 by the authors."
"The global supply networks have been disrupted and weak connections exposed to an extent that few people have ever seen in their lifetime due to the COVID-19 epidemic. As a result of the severity of the crisis, every country and industry is feeling the effects, and the massive shifts in demand and supply that have happened throughout the epidemic are easily distinguishable from the effects of previous crises. We looked into the adaptability of alliance management and AI-driven supply chain analytics in the context of an ever-changing external environment. We examined four hypotheses in this area using survey data from the American auto components manufacturing industry. To do the analysis, we used Smart PLS. Alliance management capabilities, mediated by AI-powered supply chain analytics capacity, have been found to increase an organization's operational and financial performance. We also discovered, with environmental dynamics as a moderating factor, that alliance management capability has a substantial impact on AI-powered supply chain analytics capabilities. Based on our findings, we have a deep appreciation for the interplay between dynamic capacities and the relational view of organization. Finally, we pointed up the limitations of our study and offered a number of directions for future investigation that might help address some of the concerns that our results raise. © 2023 Growing Science Ltd. All rights reserved."
"Agriculture is a big sector in nations like India, and it provides a living for many people. To improve crop productivity, it’s very necessary to identify and classify plant diseases and prevent them from spreading further so that they do not affect the whole plant. Artificial intelligence (AI) and computer vision can help detect plant diseases that humans cannot always catch and overcome the shortcomings of continuous human monitoring. In this article, we aim to detect and classify diseases in tomato and apple leaves using deep learning approaches and compare the results between different models. Because tomatoes and apples are important components of the human diet, crop waste can result in losses for both farmers and ordinary people. These plant diseases have an immediate and negative impact on both the amount and quality of yield. Crop diseases must be identified and prevented as soon as possible to improve crop yield. Therefore, we need to monitor and analyze the growth stages of the plants so that the farmers can produce disease-free and with minimal losses to the crop. Furthermore, we used the sequential convolutional neural network (CNN) model followed by transfer learning models like VGG19, Resnet152V2, Inception V3, and MobileNet and compared the models based on accuracy. The performance of the models was evaluated using various factors such as dropout, batch size, and the number of epochs. For both, the datasets, the tomato, and apple MobileNet architecture performed better than the other existing models. © 2023, Modern Education and Computer Science Press. All rights reserved."
"To reduce the burden on medical professionals and achieve advanced medical care in the rapidly evolving and specializing medical field, the widespread adoption and development of healthcare AI is essential. However, there are common industry issues such as the utilization of various healthcare data, the establishment of common connection procedures based on next-generation standards, ensuring high security against threats such as ransomware, and complying with international standards such as HL7 FHIR. In order to address these challenges and promote the research and development of a Healthcare AI Platform(Healthcare AIPF)as a common industry foundation technology, the""Healthcare AI Platform Collaborative Innovation Partnership""(HAIP)was established with approval from the Minister of Health, Labour and Welfare(MHLW)and the Minister of Economy, Trade and Industry(METI). Healthcare AIPF consists of 3 platforms: the""AI Development Platform,"" which enables the development of healthcare AI using clinical and health diagnosis information, the""Lab Platform,""which supports AI evaluation by multiple experts, and the""Service Platform,""which provides the implementation and distribution functions of healthcare AI services. HAIP aims to provide an integrated platform that enables the entire process from AI development and evaluation to implementation."
Quantum theory presents a unique scenario pertaining to collapse processes. A device that measures variables incompatible with those being detected collapses randomly into one of the states defined by the measuring device. The distinction that a collapsed output is not an accurate description of reality but rather a random selection from a set of values derived from the measuring device allows us to utilize the collapse process to propose a scheme wherein a machine becomes capable of performing interpreting processes. We present herein a basic schematic of a machine that demonstrates the principle of interpretation relying on the polarization phenomenon of photons. The operation of the device is demonstrated using an ambiguous figure. We believe that building an interpreting device can contribute to the field of AI. © 2023 The Author(s)
"Currently, there is an increased global climate change awareness, and the world is tending towards net zero emissions. The transportation sector is leading in Green House Gas emissions (GHG). Furthermore, the diminishing reserves of fossil fuels, the need for cheaper maintenance vehicles, more efficient drive trains, and better performance vehicles presents a challenge in ICE. Electric Vehicles (EVs) offer the perfect mobility solution which can replace the conventional ICE in the near future. This article comprehensively reviews the components and advances in the various technologies employed in electric vehicles to achieve efficiency in motion and optimise energy management in electric vehicles. This article gives an analysis of the current EV scenario globally. It then details the different configurations of electric vehicle architectures available. The battery is discussed, and the various electrochemical technologies are analysed. Battery Management Systems (BMS) to efficiently manage energy are discussed. The charging methods, voltage levels, and relevant standards are outlined in detail. The traction motors and power conversion technologies are reported with advancements in electric vehicle applications. Furthermore, this article gives a comprehensive overview of various emerging technologies, such as the Internet of Things (IoT), Artificial Intelligence (AI) & Machine Learning (ML), to improve EVs' performance through digitalisation. On the other hand, cybersecurity has become an ever-challenging issue in EVs due to the increasing use of digital technologies. This article thoroughly overviews cybersecurity challenges in EV applications and explains possible proactive measures. Interoperability arising from various EV manufacturers' many different charging designs is discussed. © 2023 The Authors. IET Electrical Systems in Transportation published by John Wiley & Sons Ltd."
"Structural health monitoring has become an important research area from last two decades. This research area works on sensing and actuation and reasoning to find the exact location and amount of damage in a structure. The amount of damage must be quantified as to avoid serious catastrophic failure. For the above-mentioned purpose, the current method has been proposed. There are conventional and unconventional methods of damage detection. In the conventional methods, regular NDT methods are used. Among various Artificial Intelligent (AI) based methods, Fuzzy Logic System (FLS) is the one that can code and recode numerical inputs into semantic points, but the procedure of designing the fuzzy rule is troublesome and can turn out to be extremely confounded if the number of input and output parameters are increased. The combination of Fuzzy Logic with Genetic Algorithm (GA) provides an advanced hybrid intelligent algorithm called the automatic generation fuzzy rules, which generates the fuzzy rules automatically from the data. Here, a methodology is proposed for the automatic optimization of the fuzzy rules for the crack assessment in structure. At first, both FLS and GA approaches are implemented to present problem and obtain the results. In this analogy (hybrid-fuzzy), fuzzy rules are optimized with the fuzzy membership functions (MFs) and as rules of fuzzy comprises of fuzzy MFs also. Different GA operators like selection, crossover and mutation are implemented to optimize the fuzzy rules. A special type of objective function has been developed using absoluteness and factor of rules for the novel method and the results are obtained. A numerical model is formed for the entire analyses. The proposed method requires data collection. So, for this, the Finite Element Analysis (FEA) has been used. The data were collected from the FEA and Experimental analysis and then converted to normalized form. The data are normalized so, that the application in the given algorithm will be easier. The acquired results from the current analogy are validated with those of Finite Element Analysis (FEA) method. The same results are also compared with FLS and GA separately. From the results, it was obtained that the method convergence well and will be suitable for condition monitoring structures. © 2023 Jordan Journal of Mechanical and Industrial Engineering. All rights reserved"
"An artificial intelligence (AI) model’s performance is strongly influenced by the input features. Therefore, it is vital to find the optimal feature set. It is more crucial for the survival prediction of the glioblastoma multiforme (GBM) type of brain tumor. In this study, we identify the best feature set for predicting the survival days (SD) of GBM patients that outrank the current state-of-the-art methodologies. The proposed approach is an end-to-end AI model. This model first segments tumors from healthy brain parts in patients’ MRI images, extracts features from the segmented results, performs feature selection, and makes predictions about patients’ survival days (SD) based on selected features. The extracted features are primarily shape-based, location-based, and radiomics-based features. Additionally, patient metadata is also included as a feature. The selection methods include recursive feature elimination, permutation importance (PI), and finding the correlation between the features. Finally, we examined features’ behavior at local (single sample) and global (all the samples) levels. In this study, we find that out of 1265 extracted features, only 29 dominant features play a crucial role in predicting patients’ SD. Among these 29 features, one is metadata (age of patient), three are location-based, and the rest are radiomics features. Furthermore, we find explanations of these features using post-hoc interpretability methods to validate the model’s robust prediction and understand its decision. Finally, we analyzed the behavioral impact of the top six features on survival prediction, and the findings drawn from the explanations were coherent with the medical domain. We find that after the age of 50 years, the likelihood of survival of a patient deteriorates, and survival after 80 years is scarce. Again, for location-based features, the SD is less if the tumor location is in the central or back part of the brain. All these trends derived from the developed AI model are in sync with medically proven facts. The results show an overall 33% improvement in the accuracy of SD prediction compared to the top-performing methods of the BraTS-2020 challenge. © 2023 The Author(s). Published by IOP Publishing Ltd."
"ChatGPT, an artificial intelligence (AI) software developed by OpenAI, is a powerful language model. ChatGPT is expected to perform a variety of tasks in the field of medical writing and publishing, including writing drafts, extracting article abstracts, and embellishing language. At the same time, ChatGPT has technical shortcomings and ethical challenges that have raised concerns. This review summarizes the issues faced by ChatGPT in the field of medical writing and publishing, and provides a reference for the development of standards and systems for the use of AI products such as ChatGPT. © 2023 The Author(s)."
"In the era of industry 5.0, digital twins (DTs) play an increasingly pivotal role in contemporary society. Despite the literature’s lack of a consistent definition, DTs have been applied to numerous areas as virtual replicas of physical objects, machines, or systems, particularly in manufacturing, production, and operations. One of the major advantages of digital twins is their ability to supervise the system’s evolution and run simulations, making them connected and capable of supporting decision-making. Additionally, they are highly compatible with artificial intelligence (AI) as they can be mapped to all data types and intelligence associated with the physical system. Given their potential benefits, it is surprising that the utilization of DTs for warehouse management has been relatively neglected over the years, despite its importance in ensuring supply chain and production uptime. Effective warehouse management is crucial for ensuring supply chain and production continuity in both manufacturing and retail operations. It also involves uncertain material handling operations, making it challenging to control the activity. This paper aims to evaluate the synergies between AI and digital twins as state-of-the-art technologies and examines warehouse digital twins’ (WDT) use cases to assess the maturity of AI applications within WDT, including techniques, objectives, and challenges. We also identify inconsistencies and research gaps, which pave the way for future development and innovation. Ultimately, this research work’s findings can contribute to improving warehouse management, supply chain optimization, and operational efficiency in various industries. © 2023 by the authors."
[No abstract available]
"Nowadays, the social dimension of product sustainability is increasingly in demand, however, industrial designers struggle to pursue it much more than the environmental or economic one due to their unfamiliarity in correlating design choices with social impacts. In addition, this gap is not filled even by the supporting methods that have been conceived to only support specific areas of application. To fill this gap, this study proposed a method to support social failure mode and effect analysis (SFMEA), though the automatic failure determination, based on the use of a chatbot (i.e., an artificial intelligence (AI)-based chat). The method consists of 84 specific questions to ask the chatbot, resulting from the combination of known failures and social failures, elements from design theories, and syntactic structures. The starting hypothesis to be verified is that a GPT Chat (i.e., a common AI-based chat), properly queried, can provide all the main elements for the automatic compilation of a SFMEA (i.e., to determine the social failures). To do this, the proposed questions were tested in three case studies to extract all the failures and elements that express predefined SFMEA scenarios: a coffee cup provoking gender discrimination, a COVID mask denying a human right, and a thermometer undermining the cultural heritage of a community. The obtained results confirmed the starting hypothesis by showing the strengths and weaknesses of the obtained answers in relation to the following factors: the number and type of inputs (i.e., the failures) provided in the questions; the lexicon used in the question, favoring the use of technical terms derived from design theories and social sustainability taxonomies; the type of the problem. Through this test, the proposed method proved its ability to support the social sustainable design of different products and in different ways. However, a dutiful recommendation instead concerns the tool (i.e., the chatbot) due to its filters that limit some answers in which the designer tries to voluntarily hypothesize failures to explore their social consequences. © 2023 by the authors."
"Artificial intelligence (AI) is increasingly being utilized in cybersecurity, particularly for detecting malicious applications. However, the black-box nature of AI models presents a significant challenge. This lack of transparency makes it difficult to understand and trust the results. In order to address this, it is necessary to incorporate explainability into the detection model. There is insufficient research to provide reasons why applications are detected as malicious or explain their behavior. In this paper, we propose a method of a Vision Transformer(ViT)-based malware detection model and malicious behavior extraction using an attention map to achieve high detection accuracy and high interpretability. Malware detection uses a ViT-based model, which takes an image as input. ViT offers a significant advantage for image detection tasks by leveraging attention mechanisms, enabling robust interpretation and understanding of the intricate patterns within the images. The image is converted from an application. An attention map is generated with attention values generated during the detection process. The attention map is used to identify factors that the model deems important. Class and method names are extracted and provided based on the identified factors. The performance of the detection was validated using real-world datasets. The malware detection accuracy was 80.27%, which is a high level of accuracy compared to other models used for image-based malware detection. The interpretability was measured in the same way as the F1-score, resulting in an interpretability score of 0.70. This score is superior to existing interpretable machine learning (ML)-based methods, such as Drebin, LIME, and XMal. By analyzing malicious applications, we also confirmed that the extracted classes and methods are related to malicious behavior. With the proposed method, security experts can understand the reason behind the model’s detection and the behavior of malicious applications. Given the growing importance of explainable artificial intelligence in cybersecurity, this method is expected to make a significant contribution to this field. © 2023 by the authors."
"A rich and effective dataset is an important foundation for the development of AI algorithms, and the quantity and quality of the dataset determine the upper limit level of the algorithms. For aerospace remote sensing datasets, due to the high cost of data collection and susceptibility to meteorological and airway conditions, the existing datasets have two problems: firstly, the number of datasets is obviously insufficient, and, secondly, there is large unevenness between different categories in datasets. One of the effective solutions is to use neural networks to generate fake data by learning from real data, but existing methods still find difficulty in generating remote sensing sample images with good texture detail and geometric distortion. To address the shortcomings of existing image generation algorithms, this paper proposes a gradient structure information-guided attention generative adversarial network (SGA-GAN) for remote sensing image generation, which contains two innovative initiatives: on the one hand, a learnable gradient structure information extraction branch network can be added to the generator network to obtain complex structural information in the sample image, thus alleviating the distortion of the sample geometric structure in remote sensing image generation; on the other hand, a multidimensional self-attention feature selection module is proposed to further improve the quality of the generated remote sensing images by connecting cross-attentive modules as well as spatial and channel attention modules in series to guide the generator to better utilize global information. The algorithm proposed in this paper outperformed other methods, such as StyleGAN-XL and FastGAN, in both the qualitative and quantitative evaluation, whereby the FID on the DOTA dataset decreased by 23.927 and the IS was improved by 2.351. The comparison experiments show that the method proposed in this paper can generate more realistic sample images, and images generated by this method can improve object detection metrics by increasing the number of single-category datasets and the number of targets in fewer categories in multi-category datasets, which means it can be effectively used in the field of intelligent processing of remote sensing images. © 2023 by the authors."
"Invasive Ductal Carcinoma Breast Cancer (IDC-BC) is the most common type of cancer and its asymptomatic nature has led to an increased mortality rate globally. Advancements in artificial intelligence and machine learning have revolutionized the medical field with the development of AI-enabled computer-aided diagnosis (CAD) systems, which help in determining diseases at an early stage. CAD systems assist pathologists in their decision-making process to produce more reliable outcomes in order to treat patients well. In this work, the potential of pre-trained convolutional neural networks (CNNs) (i.e., EfficientNetV2L, ResNet152V2, DenseNet201), singly or as an ensemble, was thoroughly explored. The performances of these models were evaluated for IDC-BC grade classification using the DataBiox dataset. Data augmentation was used to avoid the issues of data scarcity and data imbalances. The performance of the best model was compared to three different balanced datasets of Databiox (i.e., 1200, 1400, and 1600 images) to determine the implications of this data augmentation. Furthermore, the effects of the number of epochs were analysed to ensure the coherency of the most optimal model. The experimental results analysis revealed that the proposed ensemble model outperformed the existing state-of-the-art techniques in relation to classifying the IDC-BC grades of the Databiox dataset. The proposed ensemble model of the CNNs achieved a 94% classification accuracy and attained a significant area under the ROC curves for grades 1, 2, and 3, i.e., 96%, 94%, and 96%, respectively. © 2023 by the authors."
"The counterfactual approach to explainable AI (XAI) seeks to provide understanding of AI systems through the provision of counterfactual explanations. In a recent systematic review, Chou et al. (Inform Fus 81:59–83, 2022) argue that the counterfactual approach does not clearly provide causal understanding. They diagnose the problem in terms of the underlying framework within which the counterfactual approach has been developed. To date, the counterfactual approach has not been developed in concert with the approach for specifying causes developed by Pearl (Causality: Models, reasoning, and inference. Cambridge University Press, 2000) and Woodward (Making things happen: A theory of causal explanation. Oxford University Press, 2003). In this paper, I build on Chou et al.’s work by applying the Pearl-Woodward approach. I argue that the standard counterfactual approach to XAI is capable of delivering causal understanding, but that there are limitations on its capacity to do so. I suggest a way to overcome these limitations. © 2023, The Author(s)."
"Artificial intelligence is profoundly influencing various facets of our lives, indicating its potential to significantly impact sustainability. Nevertheless, capturing the productivity gains stemming from artificial intelligence in macro-level data poses challenges, leading to the question of whether artificial intelligence is reminiscent of the “Solow paradox”. This study employs micro-level manufacturing data to investigate the impact of artificial intelligence on firms’ productivity. The study finds that every 1% increase in artificial intelligence penetration can lead to a 14.2% increase in total factor productivity. This conclusion remains robust even after conducting endogeneity analysis and a series of robustness tests. The study identifies that the positive impact of artificial intelligence on productivity is primarily achieved through the value-added enhancement effect, skill-biased enhancement effect, and technology upgrading effect. Furthermore, the study reveals that the effects of artificial intelligence on productivity vary across different property rights and industry concentration contexts. Additionally, the structure of factor endowments within firms can also influence the productivity gains from artificial intelligence. Our study presents compelling evidence demonstrating the role of artificial intelligence in fostering economic sustainability within the framework of Industry 4.0. © 2023 by the authors."
"Since the 1990s, researchers have been seeking approaches for applying artificial intelligence (AI) to prenatal ultrasound. With the breakthrough of cloud computing technology and the development of deep learning technology, AI in prenatal ultrasound has already entered the clinical application stage in recent years. How does AI combine with clinical prenatal ultrasound? Is the clinical application of AI in prenatal ultrasound effective? What can we expect from AI in prenatal ultrasound? This review introduces the latest developments in this field and explores the challenges and opportunities brought by AI to prenatal ultrasound. © 2023 Authors. All rights reserved."
"Diabetic macular edema (DME) is a significant complication of diabetes that impacts the eye and is a primary contributor to vision loss in individuals with diabetes. Early control of the related risk factors is crucial to reduce the incidence of DME. Artificial intelligence (AI) clinical decision-making tools can construct disease prediction models to aid in the clinical screening of the high-risk population for early disease intervention. However, conventional machine learning and data mining techniques have limitations in predicting diseases when dealing with missing feature values. To solve this problem, a knowledge graph displays the connection relationships of multi-source and multi-domain data in the form of a semantic network to enable cross-domain modeling and queries. This approach can facilitate the personalized prediction of diseases using any number of known feature data. In this study, we proposed an improved correlation enhancement algorithm based on knowledge graph reasoning to comprehensively evaluate the factors that influence DME to achieve disease prediction. We constructed a knowledge graph based on Neo4j by preprocessing the collected clinical data and analyzing the statistical rules. Based on reasoning using the statistical rules of the knowledge graph, we used the correlation enhancement coefficient and generalized closeness degree method to enhance the model. Meanwhile, we analyzed and verified these models’ results using link prediction evaluation indicators. The disease prediction model proposed in this study achieved a precision rate of 86.21%, which is more accurate and efficient in predicting DME. Furthermore, the clinical decision support system developed using this model can facilitate personalized disease risk prediction, making it convenient for the clinical screening of a high-risk population and early disease intervention. © 2023 by the authors."
"Given the considerable importance of the English language as a common method of global communication, supporting and engaging learning environments for studying English as a second language are essential. In this study, we propose an interactive agent that considers differences in learners’ skill levels to help motivate learners and support independent study. The proposed agent is an AI chatbot that can vary the complexity of its natural language interactions as learners’ skills improve and engage in unstructured conversations with users. We first collected a dataset of English conversations among children and then acquired data on learners’ utterances of words and sentences using an interactive interface for skill evaluation. Based on these datasets, we generated a set of personalized conversational agents by using algorithms to evaluate speakers’ pronunciation and sentences. Finally, an expert in English language learning evaluated the interactive system to ensure that it could support learners with different levels of proficiency and that the application may be expected to help motivate students to learn. The results showed that the interactive teachable agent could help motivate students who may not otherwise be interested in learning by using a more personalized approach tailored to their skill level. © 2023 by the authors."
"Background: Healthcare professionals have expressed worries about using AI, while others anticipate more work opportunities in the future and better patient care. Integrating AI into practice will directly impact dentistry practice. The purpose of the study is to evaluate organizational readiness, knowledge, attitude, and willingness to integrate AI into dentistry practice. Methods: a cross-sectional exploratory study of dentists, academic faculty and students who practice and study dentistry in UAE. Participants were invited to participate in a previously validated survey used to collect participants' demographics, knowledge, perceptions, and organizational readiness. Results: One hundred thirty-four responded to the survey with a response rate was 78% from the invited group. Results showed excitement to implement AI in practice accompanied by medium to high knowledge and a lack of education and training programs. As a result, organizations were not well prepared and had to ensure readiness for AI implementation. Conclusion: An effort to ensure professional and student readiness will improve AI integration in practice. In addition, dental professional societies and educational institutions must collaborate to develop proper training programs for dentists to close the knowledge gap. © 2023"
"Ultrasound intelligent diagnosis is an emerging technology that combines artificial intelligence (AI) and medical ultrasonography. It has gained significant attention in recent years due to its potential to improve the accuracy and efficiency of medical diagnosis. The core elements of ultrasound artificial intelligence are the construction of data and algorithm models. Therefore, developing autonomous and controllable models, algorithms, and data platforms is extremely important. In this paper, we provide a comprehensive review of the current state-of-the-art in ultrasound intelligent diagnosis including the aspects of the construction of ultrasonic database, deep learning techniques in ultrasound intelligent diagnosis, and the clinical application of ultrasound-AI products. With continued advancements in AI and ultrasound imaging technology, we believe ultrasound intelligent diagnosis will be a valuable tool in the hands of healthcare professionals, providing them with more accurate and efficient diagnoses and treatment plans in the coming years.  © AUDT 2023."
"Numerous engineering issues have been addressed using Artificial Intelligence (AI) approaches, such as Machine Learning and Artificial Neural Networks (ANN). In this study, Plantain fiber (PF) and multiwall carbon nanotube (MWCNT) were used to create a PF/MWCNT reinforced hybrid nanocomposite (PFRHN) using epoxy resin as the base polymer. To advance adhesion and contact between the fiber/matrix, an oxidizing agent solution of potassium permanganate in acetone (KMnO4-Acetone) was applied to modify the fiber surface. To predict and maximize the impact strength of the hybridized nanocomposite, Response Surface Methodology (RSM) via Box-Behnken Design (BBD) and hyper-parameter optimization in a single-layer-perceptron ANN, with a range of 1-8 neurons in the hidden layer was utilized. The model predicted an impact strength of 44.54 KJ/m2. To verify the viability of the statistical experimental analysis, impact strength testing was carried out for pristine and hybridized composites under optimal conditions. Results showed an average impact strength of 24.32 KJ/m2 and 45.21 KJ/m2 for pristine and hybridized nanocomposite. PFRHN impact strength was significantly increased over pristine epoxy composite. The RSM-ANN technique has been shown in this study to be an effective way of reaching ideal mechanical property values in the shortest time, lowering the costs of production and conserving resources. © Engineered Science Publisher LLC 2023."
"Background: With the recent developments in automated tools, smaller and cheaper machines for lung ultrasound (LUS) are leading us toward the potential to conduct POCUS tele-guidance for the early detection of pulmonary congestion. This study aims to evaluate the feasibility and accuracy of a self-lung ultrasound study conducted by hemodialysis (HD) patients to detect pulmonary congestion, with and without artificial intelligence (AI)-based automatic tools. Methods: This prospective pilot study was conducted between November 2020 and September 2021. Nineteen chronic HD patients were enrolled in the Soroka University Medical Center (SUMC) Dialysis Clinic. First, we examined the patient’s ability to obtain a self-lung US. Then, we used interrater reliability (IRR) to compare the self-detection results reported by the patients to the observation of POCUS experts and an ultrasound (US) machine with an AI-based automatic B-line counting tool. All the videos were reviewed by a specialist blinded to the performer. We examined their agreement degree using the weighted Cohen’s kappa (Kw) index. Results: A total of 19 patients were included in our analysis. We found moderate to substantial agreement between the POCUS expert review and the automatic counting both when the patient performed the LUS (Kw = 0.49 [95% CI: 0.05–0.93]) and when the researcher performed it (Kw = 0.67 [95% CI: 0.67–0.67]). Patients were able to place the probe in the correct position and present a lung image well even weeks from the teaching session, but did not show good abilities in correctly saving or counting B-lines compared to an expert or an automatic counting tool. Conclusions: Our results suggest that LUS self-monitoring for pulmonary congestion can be a reliable option if the patient’s count is combined with an AI application for the B-line count. This study provides insight into the possibility of utilizing home US devices to detect pulmonary congestion, enabling patients to have a more active role in their health care. © 2023 by the authors."
[No abstract available]
"A nonlinear LCR parallel circuit model of a single photon avalanche diode (SPAD) is derived from a Lienard-type nonlinear differential equation. The resistance and the inductance associated with avalanche multiplication (AM) are time-dependent and governed by the avalanche time constant due to the impact ionization ratio. Time dependences of current, voltage, resistance, and inductance in the model are analyzed by numerically solving the equation. During the initial generation of a Geiger-mode pulse, when the voltage reaches the breakdown voltage, the resistance diverges to limit the maximum current and the inductance reduces to give the maximum speed of the voltage variation with the avalanche time constant. In the frequency domain, avalanche impedance (AI) spectra are obtained as a ratio of voltage and current spectra calculated as Fourier transforms of the time domain signals. The AI spectra exhibit a negative-resistance and an inductance. An analytic expression for the impedance is derived and found to comprise only a quenching resistance and a stray capacitance as indicated by a constant radius of a Nyquist plot. Finally, the present model is shown to incorporate the models of impact-ionization-avalanche transit-time (IMPATT) diodes when a very small-signal limit is assumed.  © 1963-2012 IEEE."
"To better predict the timely variation of algal blooms and other vital factors for safer drinking water production, a new AI scanning–focusing process was investigated for improving the simulation and prediction of algae counts. With a feedforward neural network (FNN) as a base, nerve cell numbers in the hidden layer and the permutation and combination of factors, etc., were fully scanned to select the best models and highly correlated factors. All the factors involved in the modeling and selection included the date (year/month/day), sensor data (temperature, pH, conductivity, turbidity, UV254-dissolved organic matter, etc.), lab measurements (algae concentration) and calculated CO2 concentration. The new AI scanning–focusing process resulted in the best models with the most suitable key factors, which are named closed systems. In this case study, models with highest prediction performance are the (1) date–algae–temperature–pH (DATH) and (2) date–algae–temperature–CO2 (DATC) systems. After the model selection process, the best models from both DATH and DATC were used to compare the other two methods in the modeling simulation process: the simple traditional neural network method (SP), where only date and target factor as inputs, and a blind AI training process (BP), which considers all available factors as inputs. Validation results show that all methods except BP had comparable results for algae prediction and other water quality factors, such as temperature, pH and CO2, among which DATC displayed an obviously poorer performance through curve fitting with original CO2 data compared to that of SP. Therefore, DATH and SP were selected for the application test, where DATH outperformed SP due to the uncompromised performance after a long training period. Our AI scanning–focusing process and model selection showed the potential for improving water quality prediction by identifying the most suitable factors. This provides a new method to be considered in the enhancing of numerical prediction for the factors in water quality prediction and broader environment-related areas. © 2023 by the authors."
[No abstract available]
[No abstract available]
"Cardiovascular disease (CVD) is one of the ten leading causes of death worldwide. Atherosclerotic disease, which can lead to myocardial infarction and stroke, is the main cause of CVD. The two main ultrasound image phenotypes used to monitor atherosclerotic load are carotid intima-media thickness (IMT) and plaque area (PA). Early segmentation and measurement methods were based on manual or threshold segmentation, snake models, etc. Usually, these methods are semi-automatic and have poor repeatability and accuracy. Segmentation of the carotid intima-media complex (IMC) and plaque in ultrasound based on artificial intelligence can achieve good accuracy. Compared with two-dimensional ultrasound, three-dimensional/fourdimensional ultrasound can provide spatial dynamic vascular information, which is helpful for doctors to evaluate. This study reviews the progress of artificial intelligence (AI) segmentation methods based on machine learning (ML) and deep learning (DL) used in the segmentation of the IMC and plaque as well as the 3D / 4D reconstruction of carotid ultrasound. © 2023 The Author(s)."
"Cow’s milk allergy (CMA) is one of the most prevalent food allergies in children. Several studies have demonstrated that gut microbiota influences the acquisition of oral tolerance to food antigens at initial stages of life. Changes in the gut microbiota composition and/or functionality (i.e., dysbiosis) have been linked to inadequate immune system regulation and the emergence of pathologies. Moreover, omic sciences have become an essential tool for the analysis of the gut microbiota. On the other hand, the use of fecal biomarkers for the diagnosis of CMA has recently been reviewed, with fecal calprotectin, α-1 antitrypsin, and lactoferrin being the most relevant. This study aimed at evaluating functional changes in the gut microbiota in the feces of cow’s milk allergic infants (AI) compared to control infants (CI) by metagenomic shotgun sequencing and at correlating these findings with the levels of fecal biomarkers (α-1 antitrypsin, lactoferrin, and calprotectin) by an integrative approach. We have observed differences between AI and CI groups in terms of fecal protein levels and metagenomic analysis. Our findings suggest that AI have altered glycerophospholipid metabolism as well as higher levels of lactoferrin and calprotectin that could be explained by their allergic status. © 2023 by the authors."
"The COVID-19 virus is one of the most devastating illnesses humanity has ever faced. COVID-19 is an infection that is hard to diagnose until it has caused lung damage or blood clots. As a result, it is one of the most insidious diseases due to the lack of knowledge of its symptoms. Artificial intelligence (AI) technologies are being investigated for the early detection of COVID-19 using symptoms and chest X-ray images. Therefore, this work proposes stacking ensemble models using two types of COVID-19 datasets, symptoms and chest X-ray scans, to identify COVID-19. The first proposed model is a stacking ensemble model that is merged from the outputs of pre-trained models in the stacking: multi-layer perceptron (MLP), recurrent neural network (RNN), long short-term memory (LSTM), and gated recurrent unit (GRU). Stacking trains and evaluates the meta-learner as a support vector machine (SVM) to predict the final decision. Two datasets of COVID-19 symptoms are used to compare the first proposed model with MLP, RNN, LSTM, and GRU models. The second proposed model is a stacking ensemble model that is merged from the outputs of pre-trained DL models in the stacking: VGG16, InceptionV3, Resnet50, and DenseNet121; it uses stacking to train and evaluate the meta-learner (SVM) to identify the final prediction. Two datasets of COVID-19 chest X-ray images are used to compare the second proposed model with other DL models. The result has shown that the proposed models achieve the highest performance compared to other models for each dataset. © 2023 by the authors."
"With the rapidly increasing reliance on advances in IoT, we persist towards pushing technology to new heights. From ordering food online to gene editing-based personalized healthcare, disruptive technologies like ML and AI continue to grow beyond our wildest dreams. Early detection and treatment through AI-assisted diagnostic models have outperformed human intelligence. In many cases, these tools can act upon the structured data containing probable symptoms, offer medication schedules based on the appropriate code related to diagnosis conventions, and predict adverse drug effects, if any, in accordance with medications. Utilizing AI and IoT in healthcare has facilitated innumerable benefits like minimizing cost, reducing hospital-obtained infections, decreasing mortality and morbidity etc. DL algorithms have opened up several frontiers by contributing towards healthcare opportunities through their ability to understand and learn from different levels of demonstration and generalization, which is significant in data analysis and interpretation. In contrast to ML which relies more on structured, labeled data and domain expertise to facilitate feature extractions, DL employs human-like cognitive abilities to extract hidden relationships and patterns from uncategorized data. Through the efficient application of DL techniques on the medical dataset, precise prediction, and classification of infectious/rare diseases, avoiding surgeries that can be preventable, minimization of over-dosage of harmful contrast agents for scans and biopsies can be reduced to a greater extent in future. Our study is focused on deploying ensemble deep learning algorithms and IoT devices to design and develop a diagnostic model that can effectively analyze medical Big Data and diagnose diseases by identifying abnormalities in early stages through medical images provided as input. This AI-assisted diagnostic model based on Ensemble Deep learning aims to be a valuable tool for healthcare systems and patients through its ability to diagnose diseases in the initial stages and present valuable insights to facilitate personalized treatment by aggregating the prediction of each base model and generating a final prediction. © 2023 by the authors."
"The number of registered cancer cases is 1.1 million in Japan in 2021. Cancer incidence and mortality rates are increasing due to the aging population, and one in two people will be diagnosed with cancer at least once in their lifetime. Cancer drug therapy is not only used as a stand-alone but also used in combination with surgical treatment and radiotherapy in many cases and it is applied in 30.5% of all first-line treatments. This paper describes the research and development of artificial intelligence(AI)-based side effects questionnaire system for patients undergoing cancer drug therapy, which has been conducted in collaboration with The Cancer Institute Hospital of JFCR, under the Innovative Artificial Intelligence(AI)Hospital Program(AI Hospital). AI Hospital is one of twelve in the 2nd term of Cross-ministerial Strategic Innovation Promotion Program(SIP)led by Cabinet Office in Japan since 2018. AI-based side effects questionnaire system is effective in reducing the amount of time that pharmacotherapy pharmacists spend on each patient from 10 minutes to 1 minute, and the implementation rate of interviewing patients who needed to be interviewed was 100%. We have also conducted research and development for the digitalization of patient consent(eConsent), which is required in various situations at medical institutions, such as examination, treatment, and hospitalization and for safe and secure delivery of image diagnosis services using AI by utilizing a healthcare AI platform. By combining these digital technologies, we would like to accelerate the digital transformation of the medical field, which will contribute to reforming the work styles of medical professionals and improving patient quality of life(QoL)."
"Artificial intelligence(AI)and information and communication technology(ICT)are beginning to be used in the digital transformation of endoscopic images. In Japan, several AI systems for digestive organ endoscopy have been approved as programmed medical devices and are being introduced into clinical practice. Although it is expected to improve diagnostic accuracy and efficiency in endoscopic examinations for organs other than the digestive organs, research and development for practical application are still in their infancy. This article introduces AI for gastrointestinal endoscopy and the author's research on cystoscopy."
"With the proliferation of Knowledge Graphs (KGs), knowledge graph completion (KGC) has attracted much attention. Previous KGC methods focus on extracting shallow structural information from KGs or in combination with external knowledge, especially in commonsense concepts (generally, commonsense concepts refer to the basic concepts in related fields that are required for various tasks and academic research, for example, in the general domain, “Country” can be considered as a commonsense concept owned by “China”), to predict missing links. However, the technology of extracting commonsense concepts from the limited database is immature, and the scarce commonsense database is also bound to specific verticals (commonsense concepts vary greatly across verticals, verticals refer to a small field subdivided vertically under a large field). Furthermore, most existing KGC models refine performance on public KGs, leading to inapplicability to actual KGs. To address these limitations, we proposed a novel Scalable Formal Concept-driven Architecture (SFCA) to automatically encode factual triples into formal concepts as a superior structural feature, to support rich information to KGE. Specifically, we generate dense formal concepts first, then yield a handful of entity-related formal concepts by sampling and delimiting the appropriate candidate entity range via the filtered formal concepts to improve the inference of KGC. Compared with commonsense concepts, KGC benefits from more valuable information from the formal concepts, and our self-supervision extraction method can be applied to any KGs. Comprehensive experiments on five public datasets demonstrate the effectiveness and scalability of SFCA. Besides, the proposed architecture also achieves the SOTA performance on the industry dataset. This method provides a new idea in the promotion and application of knowledge graphs in AI downstream tasks in general and industrial fields. © 2023 by the authors."
"Liquid biopsy is a valuable emerging alternative to tissue biopsy with great potential in the noninvasive early diagnostics of cancer. Liquid biopsy based on single cell analysis can be a powerful approach to identify circulating tumor cells (CTCs) in the bloodstream and could provide new opportunities to be implemented in routine screening programs. Since CTCs are very rare, the accurate classification based on high-throughput and highly informative microscopy methods should minimize the false negative rates. Here, we show that holographic flow cytometry is a valuable instrument to obtain quantitative phase-contrast maps as input data for artificial intelligence (AI)-based classifiers. We tackle the problem of discriminating between A2780 ovarian cancer cells and THP1 monocyte cells based on the phase-contrast images obtained in flow cytometry mode. We compare conventional machine learning analysis and deep learning architectures in the non-ideal case of having a dataset with unbalanced populations for the AI training step. The results show the capacity of AI-aided holographic flow cytometry to discriminate between the two cell lines and highlight the important role played by the phase-contrast signature of the cells to guarantee accurate classification. © 2023 Author(s)."
"This paper presents an innovative digital twin dam and watershed management platform, K-Twin SJ, that utilizes real-time data and simulation models to support decision-making for flood response and water resource management. The platform includes a GIS-based geospatial digital twin of the entire Sumjin dam and river water system in Korea, with high-precision geospatial topography and facility information for dams and rivers (watershed area 4913 km2, river length 173 km, and 91 water infrastructures). The platform synchronizes real-time data such as rainfall, dam and river water levels, flow rate, and closed-circuit television (CCTV), and incorporates three hydraulic and hydrological simulation models for efficient dam operation considering the river conditions. AI technology is also used to predict the river water level and suggest optimal dam discharge scenarios. Additionally, the platform includes a geotechnical safety evaluation module for river levees, advanced drone monitoring for dams and rivers, and an AI CCTV video surveillance function. The digital-twin-based platform supports efficient decision-making for smart flood responses and contributes to reducing flooding damage and optimal operation through better smart water management. © 2023 by the authors."
"Currently, applications of the algorithms based on artificial intelligence (AI) principles can be observed in various fields. This can be also noticed in the wide area of electrical drives. Consideration has been limited to neural networks; however, the tasks for the models can be defined as follows: control, state variable estimation, and diagnostics. In the subsequent sections of this paper, electrical machines, as well as power electronic devices, are assumed as the main objects. This paper describes the basics, issues, and possibilities related to the used tools and explains the growing popularity of neural network applications in automatic systems with electrical drives. The paper begins with the overall considerations; following that, the content proceeds with the details, and two specific examples are shown. The first example deals with a neural network-based speed controller tested in a structure with a synchronous reluctance motor. Then, the implementation of recurrent neural networks as state variable estimators is analyzed. The achieved results present a precise estimation of the load speed and the shaft torque signals from a two-mass system. All descriptions in the article are considered in the context of the trends and perspectives in modern algorithm applications for electrical drives. © 2023 by the authors."
"Today's agricultural sector cannot avoid the need for advanced digital technology, such as robots, artificial intelligence, and the Internet of Things to deal with various agricultural problems that can disrupt national stability. Agro-industrial supply chain management ensures that agricultural products reach consumers in quantity, quality, and safety required. The supply chain includes from farm to consumer, to table, from farmer to processing producer, and distribution to the consumer, as well as other long chains that cover from upstream to downstream. Some of the problems that need to find solutions include a lack of information on raw materials needed in processing, transparency of product sources, information on production processes, information on logistics distribution to consumers, and lack of traceability and traceability (products, quality, documents, costs, halal status, and value chain). Digital transformation along the supply chain must be carried out immediately through: (1) implementation and improvement of traceability systems using blockchain technology, and (2) blockchain integration with smart and rapid tests or Internet of Things (IoT) using Artificial Intelligence (AI) to enhance blockchain intelligence. Smart and rapid tests are needed to detect and identify the substance levels of a product, while the IoT architecture applied along the supply chain connects various supply chain actors to share data. Machine learning and cloud computing are used for data processing and communication networks for information transfer. Blockchain and AI-based digital systems allow consumers to track transaction history and product status in just a few seconds. Blockchain integration with AI will bring about a more reliable system called a smart blockchain. © 2023, International Society for Southeast Asian Agricultural Sciences. All rights reserved."
"As the popularity of electric vehicles (EVs) and smart grids continues to rise, so does the demand for batteries. Within the landscape of battery-powered energy storage systems, the battery management system (BMS) is crucial. It provides key functions such as battery state estimation (including state of charge, state of health, battery safety, and thermal management) as well as cell balancing. Its primary role is to ensure safe battery operation. However, due to the limited memory and computational capacity of onboard chips, achieving this goal is challenging, as both theory and practical evidence suggest. Given the immense amount of battery data produced over its operational life, the scientific community is increasingly turning to cloud computing for data storage and analysis. This cloud-based digital solution presents a more flexible and efficient alternative to traditional methods that often require significant hardware investments. The integration of machine learning is becoming an essential tool for extracting patterns and insights from vast amounts of observational data. As a result, the future points towards the development of a cloud-based artificial intelligence (AI)-enhanced BMS. This will notably improve the predictive and modeling capacity for long-range connections across various timescales, by combining the strength of physical process models with the versatility of machine learning techniques. © 2023 by the authors."
"Currently, deep learning aided medical imaging is becoming the hot spot of AI frontier application and the future development trend of precision neuroscience. This review aimed to render comprehensive and informative insights into the recent progress of deep learning and its applications in medical imaging for brain monitoring and regulation. The article starts by providing an overview of the current methods for brain imaging, highlighting their limitations and introducing the potential benefits of using deep learning techniques to overcome these limitations. Then, we further delve into the details of deep learning, explaining the basic concepts and providing examples of how it can be used in medical imaging. One of the key strengths is its thorough discussion of the different types of deep learning models that can be used in medical imaging including convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative adversarial network (GAN) assisted magnetic resonance imaging (MRI), positron emission tomography (PET)/computed tomography (CT), electroencephalography (EEG)/magnetoencephalography (MEG), optical imaging, and other imaging modalities. Overall, our review on deep learning aided medical imaging for brain monitoring and regulation provides a referrable glance for the intersection of deep learning aided neuroimaging and brain regulation. © 2023 by the authors."
"The possibility of using AI systems for the purpose of limiting the excesses of self-referentiality sometimes produced by a stable body of judges is analysed. In addition, the question of whether the problem of correct motivation of a judicial decision in the case of confrontation between indigenous cultures and Western legal systems can be solved with machine learning algorithms is addressed. Regarding the first question, an artificial system can control the excesses of the judiciary's self-referentiality under some conditions. For the second question, the experiment carried out with machine learning on applications for asylum raises some further questions. It is shown that the judgment is based more emotionally than rationally, due to the difficulties of translating meanings between cultures far apart in evolutionary history. The failure of networks to learn supports this hypothesis and confirm the importance of the judge's purely emotional evaluation in order to reach a decision. © Società editrice il Mulino."
"In the field of computer intelligence, it has always been a challenge to construct an agent model that can be adapted to various complex tasks. In recent years, based on the planning algorithm of Monte Carlo tree search (MCTS), a new idea has been proposed to solve the AI problems of two-player zero-sum games such as chess and Go. However, most of the games in the real environment rely on imperfect information, so it is impossible to directly use the normal tree search planning algorithm to construct a decision-making model. Mahjong, which is a popular multiplayer game with a long history in China, attracts great attention from AI researchers because it contains a large game state space and a lot of hidden information. In this paper, we utilize an agent learning approach that leverages deep learning, reinforcement learning, and dropout learning techniques to implement a Mahjong AI game agent. First, we improve the state transition of the tree search based on the learned MDP model, the player position variable and transition information are introduced into the tree search algorithm to construct a multiplayer search tree. Then, the model training based on a deep reinforcement learning method ensures the stable and sustainable training process of the learned MDP model. Finally, we utilize the strategy data generated by the tree search and use the dropout learning method to train the normal decision-making agent. The experimental results demonstrate the efficiency and stability performance of the agent trained by our proposed method compared with existing agents in terms of test data accuracy, tournament ranking performance, and online match performance. The agent plays against human players and acts like real humans. © 2023 by the authors."
"Artificial intelligence technology in the context of smart manufacturing uses manufacturing data to enable the automatic detection, classification, and identification of products in the production process, reducing production costs and human consumption, thereby improving production efficiency and product quality. Federated learning enables the distributed implementation of AI technologies, keeping data local to avoid privacy leaks. However, data heterogeneity factors have an impact on federated learning in a manufacturing context, and this paper proposes a customer degree selection method based on model parameter variation. The method relies on transmitting the local model changes in the participants to reflect the data characteristics, calculates the model similarity of the participants using graph theory and similarity, and uses the Top-K mechanism to filter the original participant set through the similarity scores of graph nodes to reduce the influence of heterogeneity factors in the participant set and maximize the training effect and accuracy of federated learning. The effectiveness of this method was verified by using the Dirichlet distribution to perform non-IID data partitioning on the power system attack dataset and the hard disk fault detection dataset. © 2023 by the authors."
"Ethical reviews of research plans function as a cornerstone of good research practice in order that no harm should come to participants. Ethical concerns have taken on a new salience in a digital world where data can be generated at scale. Big data research has grown rapidly, raising increased ethical concerns. Several intersecting areas of big data research exist within educational research, such as learning analytics, artificial intelligence (AI), and Massive Open Online Courses (MOOCs). In the current study, an investigation was made of peer-reviewed papers on MOOC teaching and learning to determine if they explicitly refer to (a) ethical considerations in their studies, and (b) obtaining formal ethical approval for their research. This investigation was accomplished through a review of MOOC-related, English-language papers available in Scopus database, over the course of a year. The review produced a total of 1,249 articles, of which, 826 articles related to empirical studies involving human participants where full text of the articles could be obtained. The string “ethic” was searched for within these articles, and resulting articles analyzed, which found that a small fraction, 42 articles (5.08%), mention ethics in relation to the study presented in the article, and only 13 articles (1.57%) explicitly mention obtaining formal ethical approval for the research. The findings show a lack of transparency in reporting on and/or engagement with ethical considerations in MOOC teaching and learning research. These findings indicate the need for further stakeholder engagement and sectoral dialogue in relation to ethics education and training for researchers; consideration of ethics in big data studies in education; and norms/policies in academic publishing for authors to report how ethical issues have been considered. © 2023, The Online Learning Consortium. All rights reserved."
"Objective: To evaluate the real-time accuracy of cloud handheld ultrasound system using AI technology in screening carotid plaque. Methods: 2627 ultrasound images of the carotid artery are collected using the cloud handheld system. Bounding boxes of carotid plaques are labeled by qualified sonographers, and the dataset is trained using a lightweight YOLOv3 model. An additional and separate 390 images are collected and tested using the evaluation metrics average recall (AR), average precision (AP), and frames per second (FPS) for quantifying classification performance and time consumption. Results: We use a plaque grading definition with a thickness of 1.2-1.5 mm defined as small plaque, 1.5-3 mm as medium plaque, and more than 3 mm thick as large plaque. Our model achieves APIoU=0.50 with 96.5%, with APlarge is 79.9%, APmedium is 90.7%, APsmall is 93.5%; ARIoU=0.50 is 64.5%, where ARlarge is 60.6%, ARmedium is 58.3%, ARsmall is 70.8%, and FPS is 33.3. Conclusion: We establish a framework for data set construction, model selection, training, and testing of carotid ultrasound images and verify the effectiveness of real-time AI technology in the automatic detection of carotid artery plaque. © 2023 Authors. All rights reserved."
"AI is playing an important role in promoting sustainable development, but the carbon footprint caused by AI is scaling quickly and may partly offset the effort to reduce carbon emissions. However, recommendations for limiting the AI carbon footprint are lacking. In order to address this gap in the literature, this paper first constructs a tripartite evolutionary game model by taking governments, AI industry alliances, and consumers into consideration, and then exploring the impacts of key factors on these three players’ strategy selection based on the case of smart air conditioner consumption in China. The results show that the behavior of governments has an important influence on the behavior of AI industry alliances and consumers. The ideal consequence is that governments adopt an unregulated strategy, AI industry alliances adopt a green development strategy, and consumers adopt a green purchase strategy. Regulation by governments is indispensable for limiting the AI carbon footprint during an early stage but becomes dispensable when the system reaches an optimal state. Although a tendency toward green consumption, image benefit, regulatory cost, carbon price, and the subsidies given to consumers and AI industry alliances can largely influence the strategy selection of governments, governments are most sensitive to carbon prices and the subsidies given to consumers. AI industry alliances are not sensitive to subsidies, reputation improvement, and reputation loss but are most sensitive to carbon prices. Consumers are most sensitive to green consumption tendencies, self-satisfaction, and utility but are not sensitive to subsidies. © 2023 by the authors."
"For the next coming years, metaverse, digital twin and autonomous vehicle applications are the leading technologies for many complex applications hitherto inaccessible such as health and life sciences, smart home, smart agriculture, smart city, smart car and logistics, Industry 4.0, entertainment (video game) and social media applications, due to recent tremendous developments in process modeling, supercomputing, cloud data analytics (deep learning, etc.), communication network and AIoT/IIoT/IoT technologies. AIoT/IIoT/IoT is a crucial research field because it provides the essential data to fuel metaverse, digital twin, real-time Industry 4.0 and autonomous vehicle applications. However, the science of AIoT is inherently multidisciplinary, and therefore, it is difficult for readers to understand its evolution and impacts. Our main contribution in this article is to analyze and highlight the trends and challenges of the AIoT technology ecosystem including core hardware (MCU, MEMS/NEMS sensors and wireless access medium), core software (operating system and protocol communication stack) and middleware (deep learning on a microcontroller: TinyML). Two low-powered AI technologies emerge: TinyML and neuromorphic computing, but only one AIoT/IIoT/IoT device implementation using TinyML dedicated to strawberry disease detection as a case study. So far, despite the very rapid progress of AIoT/IIoT/IoT technologies, several challenges remain to be overcome such as safety, security, latency, interoperability and reliability of sensor data, which are essential characteristics to meet the requirements of metaverse, digital twin, autonomous vehicle and Industry 4.0. applications. © 2023 by the authors."
"Artificial Intelligence has emerged as a transformative force in public service delivery, promising improved efficiency and responsiveness to citizens’ needs, so it is essential to understand the functional factors that influence citizens’ adoption and intention to continue using such services. Drawing on the technology acceptance model, this study investigates the influence of six functional factors, namely, usefulness, ease of use, service reliability, service quality, responsiveness, and security, on the continued use of AI-enabled public services through the mediating effect of user satisfaction. Data were collected from an online survey of AI-enabled public services in Korea during the summer of 2021, and causal mediation analysis was conducted to examine these relationships. The results indicate that usefulness, service reliability, and security significantly influenced users’ intent to continue using AI-based services. Furthermore, causal mediation analysis confirmed that the five components of AI public services—usefulness, service reliability, service quality, responsiveness, and security—had significant effects on the continued use of AI-enabled service platforms, with user satisfaction playing a mediating role in the relationships. The main functional factors can lead to higher levels of satisfaction, and this ultimately drives the sustained adoption and continued use of AI-enabled public services by citizens. © 2023 by the authors."
"Breast cancer is the most common malignancy and the leading cause of death for women. Ultrasound is the main tool for breast cancer screening, but it can be influenced by the subjective factors of sonographers. With the continuous development of medical technology and artificial intelligence (AI), the application of breast ultrasound imaging technology is becoming increasingly widespread. Among them, the application of AI and automated breast volume scanning (ABVS) brings new opportunities and challenges for ultrasound diagnosis of breast diseases, while making breast ultrasound diagnosis more accurate and efficient. This article explores the application and prospects of AI and ABVS in ultrasound diagnosis of breast diseases. © 2023 The Author(s)."
"Cyclophorus saturnus is an edible land snail traditionally harvested for human food, yet little is known about its nutritional value, especially in Thailand. This study aimed to investigate its nutritional potential as an alternative food resource. In the present study, proximate composition, essential mineral content, amino acid, and lipid profiles of the meat were evaluated. Proximate analysis showed that C. saturnus contained 80.04% moisture, 11.88% protein, 6.04% carbohydrate, and 0.93% fat, with 80.01 kcal/100 g fresh matter. For minerals, calcium was the most abundant element in the meat. Its protein contained glutamic and aspartic as the major amino acids, while it was not a good source of tryptophan and methionine but was considered a very rich source of other essential amino acids (amino acid scores greater than 100). Its lipid fraction showed a higher proportion of mono and polyunsaturated fatty acids (MUFA and PUFA, 67.69%) and a lower proportion of saturated fatty acids (SFA) (32.31%). The PUFA/SFA ratio (1.56), hypocholesterolemic/hypercholesterolemic ratio (HH; 5.58), atherogenicity index (AI; 0.48), and thrombogenicity index (TI; 0.20) are considered nutritionally healthy for humans. Overall, this study demonstrates the nutritional potential of C. saturnus to serve as a nutritious part of the human diet and as an alternative ingredient in food systems; therefore, its production and consumption should be more extensively promoted. © 2023 The Authors"
"Artificial intelligence (AI) technology is crucial for developing autonomous ships in the maritime industry. Autonomous ships, based on the collected information, recognize the environment without any human intervention and operate themselves using their own judgment. However, ship-to-land connectivity increased, owing to the real-time monitoring and remote control (for unexpected circumstances) from land; this poses a potential cyberthreat to various data collected inside and outside the ships and to the applied AI technology. For the safety of autonomous ships, cybersecurity around AI technology needs to be considered, in addition to the cybersecurity of the ship systems. By identifying various vulnerabilities and via research cases of the ship systems and AI technologies, this study presents possible cyberattack scenarios on the AI technologies applied to autonomous ships. Based on these attack scenarios, cyberthreats and cybersecurity requirements are formulated for autonomous ships by employing the security quality requirements engineering (SQUARE) methodology. © 2023 by the authors."
"Power quality (PQ) monitoring and detection has emerged as an essential requirement due to the proliferation of sensitive power electronic interfacing devices, electric vehicle charging stations, energy storage devices, and distributed generation energy sources in the recent smart grid and microgrid scenarios. Even though, to date, the traditional approaches play a vital role in providing a solution to the above issue, the limitations, such as the requirement of significant human effort and not being scalable for large-scale power systems, force us to think of alternative approaches. Looking at a better perspective, deep-learning (DL) has gained the main attraction for various researchers due to its inherent capability to classify the data by extracting dominating and prominent features. This manuscript attempts to provide a comprehensive review of PQ detection and classification based on DL approaches to explore its potential, efficiency, and consistency to produce results accurately. In addition, this state-of-the-art review offers an overview of the novel concepts and the step-by-step method for detecting and classifying PQ events. This review has been presented categorically with DL approaches, such as convolutional neural networks (CNNs), autoencoders, and recurrent neural networks (RNNs), to analyze PQ data. This paper also highlights the challenges and limitations of using DL for PQ analysis, and identifies potential areas for future research. This review concludes that DL algorithms have shown promising PQ detection and classification results, and could replace traditional methods. © 2023 by the authors."
"Traditional fundus image-based diabetic retinopathy (DR) grading depends on the examiner’s experience, requiring manual annotations on the fundus image and also being time-consuming. Wireless sensor networks (WSNs) combined with artificial intelligence (AI) technology can provide automatic decision-making for DR grading application. However, the diagnostic accuracy of the AI model is one of challenges that limited the effectiveness of the WSNs-aided DR grading application. Regarding this issue, we propose a WSN architecture and a parallel deep learning framework (HybridLG) for actualizing automatic DR grading and achieving a fundus image-based deep learning model with superior classification performance, respectively. In particular, the framework constructs a convolutional neural network (CNN) backbone and a Transformer backbone in a parallel manner. A novel lightweight deep learning model named MobileViT-Plus is proposed to implement the Transformer backbone of the HybridLG, and a model training strategy inspired by an ensemble learning strategy is designed to improve the model generalization ability. Experimental results demonstrate the state-of-the-art performance of the proposed HybridLG framework, obtaining excellent performance in grading diabetic retinopathy with strong generalization performance. Our work is significant for guiding the studies of WSNs-aided DR grading and providing evidence for supporting the efficacy of the AI technology in DR grading applications. © 2023 by the authors."
An easy method to evaluate a remote place’s snowpack depth has been discussed for helping later-stage elderly persons’ life. The method of using a smartphone camera and an augmented reality marker (AR marker) has been investigated. The general smartphone with a high image resolution camera was used to observe snowpack depth in remote places and remote control the robot via Bluetooth device. And image processing using artificially integrated technology (AI technology) was adapted for detecting the AR markers and for evaluating the snowpack depth. © 2023 by the authors.
"Objective: To evaluate the impact of type and dose of swallowed topical steroids (STS) and concurrent steroid therapy on the development and resolution of adrenal insufficiency (AI) in pediatric eosinophilic esophagitis (EoE). Methods: We performed a retrospective case-control study of pediatric EoE subjects in a single tertiary care center, who were treated with STS for at least 3 months and diagnosed with AI based on a peak stimulated cortisol level of <18 µg/dL (500 nmol/L). Steroid forms and doses, and endoscopy data were collected at the time of AI diagnosis and AI resolution or the last known evaluation. Steroid formulations were converted to a fluticasone-equivalent dose for analysis. Results: Thirty-two EoE subjects with AI were identified, and 20 had AI resolution, including 12 who remained on lower dose STS. Eight of the 32 patients were also treated with extended-release budesonide (ER budesonide), which resulted in a 7-fold higher total daily steroid dose, and thus were analyzed separately. When the 24 cases that were not on ER budesonide were compared to the 81 controls, no difference was found in the STS dose nor total daily steroid dose, although the inhaled steroid dose had marginal significance. Peak eosinophil counts tended to increase when STS doses were decreased, except in subjects on ER budesonide at AI diagnosis. Conclusion: Altering the total daily steroid regimen can lead to resolution of AI in patients with EoE, though this may come at the expense of disease control.  © 2020 the Authors. Published by Wolters Kluwer Health, Inc."
"The use of artificial intelligence (AI) in purchasing and supply management (PSM) has great potential, e.g. for automating processes and supporting PSM employees. Nevertheless, the number of realized AI use cases in PSM is still limited, and mostly in large companies. This paper examines first requirements for the implementation of AI in PSM. Second, design principles are developed on the basis of the identified requirements. Third, exemplary AI use cases in PSM are presented. For this purpose, a literature analysis is first conducted and followed by interviews with 17 PSM and technology experts. Finally, 40 requirements for the implementation of AI in PSM are developed from the conducted expert interviews. Based on these 40 requirements, ten design principles are developed in the sense of the design science research (DSR) approach to describe the relevant activities for the implementation of AI in PSM. The artifacts developed in this paper will contribute to the implementation of AI in PSM. The applicable research results should facilitate the transfer of knowledge into practice and contribute to the establishment of the DSR method in PSM research. © 2023 Elsevier Ltd"
"The intra-sphere and inter-sphere structural attributes of controlled release microsphere drug products can greatly impact their release profile and clinical performance. In developing a robust and efficient method to characterize the structure of microsphere drug products, this paper proposes X-ray microscopy (XRM) combined with artificial intelligence (AI)-based image analytics. Eight minocycline loaded poly(lactic-co-glycolic acid) (PLGA) microsphere batches were produced with controlled variations in manufacturing parameters, leading to differences in their underlying microstructures and their final release performances. A representative number of microspheres samples from each batch were imaged using high resolution, non-invasive XRM. Reconstructed images and AI-assisted segmentation were used to determine the size distribution, XRM signal intensity, and intensity variation of thousands of microspheres per sample. The signal intensity within the eight batches was nearly constant over the range of microsphere diameters, indicating high structural similarity of spheres within the same batch. Observed differences in the variation of signal intensity between different batches suggests inter-batch non-uniformity arising from differences in the underlying microstructures associated with different manufacturing parameters. These intensity variations were correlated with the structures observed from higher resolution focused ion beam scanning electron microscopy (FIB-SEM) and the in vitro release performance for the batches. The potential for this method for rapid at-line and offline product quality assessment, quality control, and quality assurance is discussed. © 2023"
"The COVID-19 pandemic has presented unprecedented challenges to healthcare systems worldwide. One of the key challenges in controlling and managing the pandemic is accurate and rapid diagnosis of COVID-19 cases. Traditional diagnostic methods such as RT-PCR tests are time-consuming and require specialized equipment and trained personnel. Computer-aided diagnosis systems and artificial intelligence (AI) have emerged as promising tools for developing cost-effective and accurate diagnostic approaches. Most studies in this area have focused on diagnosing COVID-19 based on a single modality, such as chest X-rays or cough sounds. However, relying on a single modality may not accurately detect the virus, especially in its early stages. In this research, we propose a non-invasive diagnostic framework consisting of four cascaded layers that work together to accurately detect COVID-19 in patients. The first layer of the framework performs basic diagnostics such as patient temperature, blood oxygen level, and breathing profile, providing initial insights into the patient's condition. The second layer analyzes the coughing profile, while the third layer evaluates chest imaging data such as X-ray and CT scans. Finally, the fourth layer utilizes a fuzzy logic inference system based on the previous three layers to generate a reliable and accurate diagnosis. To evaluate the effectiveness of the proposed framework, we used two datasets: the Cough Dataset and the COVID-19 Radiography Database. The experimental results demonstrate that the proposed framework is effective and trustworthy in terms of accuracy, precision, sensitivity, specificity, F1-score, and balanced accuracy. The audio-based classification achieved an accuracy of 96.55%, while the CXR-based classification achieved an accuracy of 98.55%. The proposed framework has the potential to significantly improve the accuracy and speed of COVID-19 diagnosis, allowing for more effective control and management of the pandemic. Furthermore, the framework's non-invasive nature makes it a more attractive option for patients, reducing the risk of infection and discomfort associated with traditional diagnostic methods. © 2023 The Author(s)"
"The availability of digital infrastructures and the fast-paced development of accompanying revolutionary technologies have triggered an unprecedented reliance on Artificial intelligence (AI) techniques both in theory and practice. Within the AI domain, Machine Learning (ML) techniques stand out as essential facilitator largely enabling machines to possess human-like cognitive and decision making capabilities. This paper provides a focused review of the literature addressing applications of emerging ML tools to solve various Project Scheduling Problems (PSPs). In particular, it employs bibliometric and network analysis tools along with a systematic literature review to analyze a pool of 104 papers published between 1985 and August 2021. The conducted analysis unveiled the top contributing authors, the most influential papers as well as the existing research tendencies and thematic research topics within this field of study. A noticeable growth in the number of relevant studies is seen recently with a steady increase as of the year 2018. Most of the studies adopted Artificial Neural Networks, Bayesian Network and Reinforcement Learning techniques to tackle PSPs under a stochastic environment, where these techniques are frequently hybridized with classical metaheuristics. The majority of works (57%) addressed basic Resource Constrained PSPs and only 15% are devoted to the project portfolio management problem. Furthermore, this study clearly indicates that the application of AI techniques to efficiently handle PSPs is still in its infancy stage bringing out the need for further research in this area. This work also identifies current research gaps and highlights a multitude of promising avenues for future research. © 2022 Author(s)"
"Intuitively, proper referential extensions of psychological and moral terms exclude artifacts. Yet ordinary speakers commonly treat AI robots as moral patients and use psychological terms to explain their behavior. This paper examines whether this referential shift from the human domain to the AI domain entails semantic changes: do ordinary speakers literally consider AI robots to be psychological or moral beings? Three non-literalist accounts for semantic changes concerning psychological and moral terms used in the AI domain will be discussed: the technical view (ordinary speakers express technical senses), the habit view (ordinary speakers subconsciously express ingrained social habits), and the emotion view (ordinary speakers express their own affective empathetic emotional states). I discuss whether these non-literalist accounts accommodate the results of relevant empirical experiments. The non-literalist accounts are shown to be implausible with respect to the ordinary use of agency-terms (e.g., “believe,” “know,” “decide,” etc.), and therefore I conclude that the concepts ordinary speakers express by agency-terms in reference to AI robots are similar to the concepts they express when applying the same terms to humans. When ordinary speakers extend emotion-terms and/or moral-patiency-terms to AI robots, however, I argue that semantic changes have taken place because ordinary speakers are in fact referring to their own affective empathetic emotional states rather than AI robots. This argument suggests that the judgments made by ordinary speakers regarding the proper referential extensions of emotion-terms and moral-patiency-terms are fallacious. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
[No abstract available]
"Background. End-stage kidney disease (ESKD) from lupus nephritis (LN) is a major cause of morbidity and mortality in patients with systemic lupus erythematosus (SLE). Kidney biopsy is the gold standard for diagnosis and prognostication of LN. While interstitial fibrosis and tubular atrophy (IFTA) predict progression to ESKD, the National Institutes of Health (NIH) classification of interstitial inflammation in unscarred cortical parenchyma is not predictive of chronic kidney disease (CKD) progression. The objective of this study was to determine whether total cortical interstitial inflammation that accounts for inflammation in the entire cortical parenchyma could predict CKD progression in patients with LN. Early identification of at-risk patients may improve outcomes. Methods. This retrospective cohort study included 125 SLE patients with LN class III, IV, V or mixed (III/V, IV/V) on the index biopsy (2005-2018). Kidney biopsies were reviewed and assigned based on the 2018 NIH Activity Index (AI) and tubulointerstitial lesion categories. Total interstitial inflammation in the entire cortical parenchyma was graded as 0, 1, 2 or 3, corresponding to <10%, 10-25%, 26-50% and >50%, respectively, of the total cortical parenchyma containing an inflammatory infiltrate (similar to the definition used in the Banff total inflammation score). CKD progression was defined as an estimated glomerular filtration rate decrease of ≥30% within 5 years after the index biopsy. Kaplan-Meier survival curves and Cox proportional hazards models were performed to compare the two scoring systems, the total cortical intestinal inflammation score and the NIH interstitial inflammation score as predictors of CKD progression. Results. Of 125 patients, 46 experienced CKD progression; 21 of 46 subsequently developed ESKD, 28 (22.4%) had moderate-severe total cortical interstitial inflammation and 8 (6.4%) had moderate-severe NIH interstitial inflammation. There were no differences in baseline characteristics between progressors and nonprogressors. Total cortical interstitial inflammation was associated with CKD progression in time-dependent analyses [hazard ratio 2.45 (95% confidence interval 1.2-4.97)] adjusted for age at biopsy, race, sex, LN class and hypertensive vascular change on kidney biopsy. The NIH interstitial inflammation was not associated with CKD progression. Conclusions. In contrast to the current NIH interstitial inflammation classification, accounting for interstitial inflammation in the entire cortical parenchyma allows identification of patients at risk for CKD progression in LN. © The Author(s) 2022. Published by Oxford University Press on behalf of the ERA."
"Objectives: To evaluate the feasibility of combining compressed sense (CS) with a newly developed deep learning-based algorithm (CS-AI) using convolutional neural networks to accelerate 2D MRI of the knee. Methods: In this prospective study, 20 healthy volunteers were scanned with a 3T MRI scanner. All subjects received a fat-saturated sagittal 2D proton density reference sequence without acceleration and four additional acquisitions with different acceleration levels: 2, 3, 4 and 6. All sequences were reconstructed with the conventional CS and a new CS-AI algorithm. Two independent, blinded readers rated all images by seven criteria (overall image impression, visible artifacts, delineation of anterior ligament, posterior ligament, menisci, cartilage, and bone) using a 5-point Likert scale. Signal- and contrast- to-noise ratios were calculated. Subjective ratings and quantitative metrics were compared between CS and CS-AI with similar acceleration levels and between all CS/CS-AI images and the non-accelerated reference sequence. Friedman and Dunn's multiple comparison tests were used for subjective, ANOVA and the Tukey Kramer test for quantitative metrics. Results: Conventional CS images at the lowest acceleration level (CS2) were already rated significantly lower than reference for 6/7 criteria. CS-AI images maintained similar image quality to the reference up to CS-AI three for all criteria, which would allow for a reduction in scan time of 64% with unchanged image quality compared to the unaccelerated sequence. SNR and CNR were significantly higher for all CS-AI reconstructions compared to CS (all p < 0.05). Conclusions AI-based image reconstruction showed higher image quality than CS for 2D knee imaging. Its implementation in the clinical routine yields the potential for faster MRI acquisition but needs further validation in non-healthy study subjects. Advances in knowledge Combining compressed SENSE with a newly developed deep learning-based algorithm using convolutional neural networks allows a 64% reduction in scan time for 2D imaging of the knee. Implementation of the new deep learning-based algorithm in clinical routine in near future should enable better image quality/resolution with constant scan time, or reduced acquisition times while maintaining diagnostic quality. © 2023, British Institute of Radiology. All rights reserved."
"In recent years, ultrasound imaging has become an important means of medical diagnosis because of its safety and radiation-free advantages. With the continuous progress of deep learning, Artificial Intelligence (AI) models can process large amounts of ultrasound data quickly and accurately, providing decision support for clinicians in diagnosis. From the perspective of ultrasound image classification, detection and segmentation, this paper systemically introduces the latest progress of AI technology in ultrasound imaging, and summarizes the recent high-level related work. At the same time, we also discuss the development prospect and bottleneck of AI in ultrasound imaging processing, which provides the future research directions for researchers in related fields. © 2023 Authors. All rights reserved."
"Multi-modal datasets in artificial intelligence (AI) often capture a third-person perspective, but our embodied human intelligence evolved with sensory input from the egocentric, first-person perspective. Towards embodied AI, we introduce the Egocentric Communications (EgoCom) dataset to advance the state-of-the-art in conversational AI, natural language, audio speech analysis, computer vision, and machine learning. EgoCom is a first-of-its-kind natural conversations dataset containing multi-modal human communication data captured simultaneously from the participants' egocentric perspectives. EgoCom includes 38.5 hours of synchronized embodied stereo audio, egocentric video with 240,000 ground-truth, time-stamped word-level transcriptions and speaker labels from 34 diverse speakers. We study baseline performance on two novel applications that benefit from embodied data: (1) predicting turn-taking in conversations and (2) multi-speaker transcription. For (1), we investigate Bayesian baselines to predict turn-taking within 5 percent of human performance. For (2), we use simultaneous egocentric capture to combine Google speech-to-text outputs, improving global transcription by 79 percent relative to a single perspective. Both applications exploit EgoCom's synchronous multi-perspective data to augment performance of embodied AI tasks.  © 1979-2012 IEEE."
"Background: Established randomized trial-based parameters for acute ischemic stroke group patients into generic treatment groups, leading to attempts using various artificial intelligence (AI) methods to directly correlate patient characteristics to outcomes and thereby provide decision support to stroke clinicians. We review AI-based clinical decision support systems in the development stage, specifically regarding methodological robustness and constraints for clinical implementation. Methods: Our systematic review included full-text English language publications proposing a clinical decision support system using AI techniques for direct decision support in acute ischemic stroke cases in adult patients. We (1) describe data and outcomes used in those systems, (2) estimate the systems' benefits compared with traditional stroke diagnosis and treatment, and (3) reported concordance with reporting standards for AI in healthcare. Results: One hundred twenty-one studies met our inclusion criteria. Sixty-five were included for full extraction. In our sample, utilized data sources, methods, and reporting practices were highly heterogeneous. Conclusions: Our results suggest significant validity threats, dissonance in reporting practices, and challenges to clinical translation. We outline practical recommendations for the successful implementation of AI research in acute ischemic stroke treatment and diagnosis.  © 2023 The Authors."
"Policy makers produce digital records on a daily basis. A selection of records is then preserved in archival repositories. However, getting access to these archival materials is extremely complicated for many reasons—including data protection, sensitivity, national security, and copyright. Artificial Intelligence (AI) can be applied to archives to make them more accessible, but it is still at an experimental stage. While skills gaps contribute to keeping archives ‘dark’, it is also essential to examine issues of mistrust and miscommunication. This article argues that although civil servants, archivists, and academics have similar professional principles articulated through professional codes of ethics, these are not often communicated to each other. This lack of communication leads to feelings of mistrust between stakeholders. Mistrust of technology also contributes to the barriers to effective implementation of AI tools. Therefore, we propose that surfacing the shared professional ethics between stakeholders can contribute to deeper collaborations between humans. In turn, these collaborations can lead to the building of trust in AI systems and tools. The research is informed by semi-structured interviews with thirty government professionals, archivists, historians, digital humanists, and computer scientists. Previous research has largely focused on preservation of digital records, rather than access to these records, and on archivists rather than records creators such as government professionals. This article is the first to examine the application of AI to digital archives as an issue that requires trust and collaboration across the entire archival circle (from record creators to archivists, and from archivists to users). © The Author(s) 2022."
"The novel coronavirus that triggered the COVID-19 outburst is still active around the globe. By now, COVID-19 has affected practically every facet of progress, most importantly, it has shaken the healthcare system like never before. At its peak, it forced Governments throughout the world into lockdowns to limit the reach of the epidemic. Based on early advisories of the World Health Organization (WHO), the only method of safeguarding oneself from being infected was to wear a face mask. Even today, with fewer cases being reported, masking oneself remains the single most effective and cheap means of prevention. As urban areas continue to grow, effective city management is essential for mitigating the increase of the deadly COVID-19 disease. The success of smart cities depends on significant upgrades to public transportation, highways, companies, homes, and municipal streets. There is room for improvement in the public bus transportation system now in place, and one of those improvements would be to use artificial intelligence. To determine if the person is wearing a face mask, you need an autonomous mask detection and alert system. Therefore, this study introduced a deep learning-based design that combines the attention-based generative adversarial network (ABGAN) with the multi-objective interactive honeybee mating optimization (MOIHBMO) approach to create an automated face mask recognition system. A set of 1386 images has been used to create a real-time dataset. This database contains 690 pictures without face masks and 686 images with them. The suggested algorithm ABGAN-MOIHBMO is compared to other traditional methods for detection of face masks, such as DL, AI, and DNN. The performance indicators used are error rate, inference speed, precision, recall, accuracy, and over fitting assessments. The results demonstrate that the proposed ABGAN-MOIHBMO outperforms the existing methodologies. It provides 96% of precision, 86% of recall, 93% for the f1 score, which are higher/better than the other, traditional methods. The error rate in ABGAN-MOIHBMO is a low 1.1%, which is lower other approaches. To predict and underline the significance of face mask use, the face mask detection technique may be employed in the future at Saudi airports, shopping centers, and other congested locations. On a larger platform, our research will be an effective instrument in helping many nations throughout the globe combat the rapid spread of this contagious illness. © 2023 NSP Natural Sciences Publishing Cor."
"A 69-year-old man with type 2 diabetes mellitus (hemoglobulin A1c = 7.4) presented with 6 months of insensate anal incontinence (AI). Seven years before presentation, he underwent a lateral internal sphincterotomy for an anal fissure without complication. Six months before presentation of worsening diabetic neuropathy, medication-induced constipation, and a change in the household to one in which restrooms were not immediately available led to the sudden onset of AI after particularly vigorous straining. He reported daily “light staining” of underwear, his wife reported that it could include formed feces, and he was becoming socially isolated. He intermittently used soluble fiber supplements and had 1 soft bowel movement daily that required straining. His rectal examination demonstrated stool staining outside the anus, a palpable internal sphincter defect consistent with prior lateral internal sphincterotomy, weak squeeze, and accessory gluteal muscles use. Anorectal manometry demonstrated dyssynergia on push, normal balloon expulsion, insensate incontinence, weak sphincter muscles, decreased squeeze, and decreased resting pressures. Colonoscopy showed internal hemorrhoids. An endoanal ultrasound demonstrated a 180° defect of the left internal sphincter and diffusely decreased sphincter tone (Fig. 1). He started pelvic floor physical therapy and increased his fiber intake; although straining diminished, AI continued. Therefore, he elected to undergo an implantable sacral neuromodulator. He underwent trial lead placement,  which resolved his fecal incontinence, followed by permanent device implantation (Fig. 2). He has had a complete resolution of AI and a dramatic improvement in his quality of life. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"Liver transplantation (LT) is a life-saving treatment for individuals with end-stage liver disease. The management of LT recipients is complex, predominantly because of the need to consider demographic, clinical, laboratory, pathology, imaging, and omics data in the development of an appropriate treatment plan. Current methods to collate clinical information are susceptible to some degree of subjectivity; thus, clinical decision-making in LT could benefit from the data-driven approach offered by artificial intelligence (AI). Machine learning and deep learning could be applied in both the pre- and post-LT settings. Some examples of AI applications pre-transplant include optimising transplant candidacy decision-making and donor-recipient matching to reduce waitlist mortality and improve post-transplant outcomes. In the post-LT setting, AI could help guide the management of LT recipients, particularly by predicting patient and graft survival, along with identifying risk factors for disease recurrence and other associated complications. Although AI shows promise in medicine, there are limitations to its clinical deployment which include dataset imbalances for model training, data privacy issues, and a lack of available research practices to benchmark model performance in the real world. Overall, AI tools have the potential to enhance personalised clinical decision-making, especially in the context of liver transplant medicine. © 2023 The Authors"
"Computed Tomography Urography (CTU) is a multiphase CT examination optimized for imaging kidneys, ureters, and bladder, complemented by post-contrast excretory phase imaging. Different protocols are available for contrast administration and image acquisition and timing, with different strengths and limits, mainly related to kidney enhancement, ureters distension and opacification, and radiation exposure. The availability of new reconstruction algorithms, such as iterative and deep-learning-based reconstruction has dramatically improved the image quality and reducing radiation exposure at the same time. Dual-Energy Computed Tomography also has an important role in this type of examination, with the possibility of renal stone characterization, the availability of synthetic unenhanced phases to reduce radiation dose, and the availability of iodine maps for a better interpretation of renal masses. We also describe the new artificial intelligence applications for CTU, focusing on radiomics to predict tumor grading and patients’ outcome for a personalized therapeutic approach. In this narrative review, we provide a comprehensive overview of CTU from the traditional to the newest acquisition techniques and reconstruction algorithms, and the possibility of advanced imaging interpretation to provide an up-to-date guide for radiologists who want to better comprehend this technique. © 2023 by the authors."
"Background: The application of artificial intelligence (AI) to minimally invasive surgery has the potential to improve surgical safety, support intraoperative decision making, and reduce operative complications. Computer vision and machine learning are subfields or AI, focused on making statistical inferences and generating predictive calculations about relevant patterns in data. These patterns are extracted from the data based on annotations of clinically relevant target features. While there are numerous real-time applications of AI to medicine, particularly in diagnostic specialties, surgical AI is currently predominantly limited to pre- and postoperative analysis of patient data. Intraoperative deployment of AI, based on retrospective video and image analysis of minimally invasive procedures, requires the fundamental comprehension of surgical workflow. Therefore, the automated detection and prediction of operative phases, tracking of surgical instruments, differentiation of tissue and analysis of tool–tissue interactions has been the central research focus. While these tasks promise tremendous clinical value, surgical AI is still predominantly limited to highly standardized, routine procedures such as laparoscopic cholecystectomy. To reveal its true risk mitigation potential, AI must be generalizable to more complex, and specifically oncological procedures. Objective: This article provides as a review of the existing applications of AI to oncological foregut surgery and illustrates the technology’s current limitations and future potential. © 2023, The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, ein Teil von Springer Nature."
"Purpose:To develop a new virtual surgery simulation platform to predict postoperative corneal stiffness (Kcmean) after laser vision correction (LVC) surgery.Setting:Narayana Nethralaya Eye Hospital and Sankara Nethralaya, India; Humanitas Clinical and Research Center, Italy.Design:Retrospective observational case series.Methods:529 eyes from 529 patients from 3 eye centers and 10 post-small-incision lenticule extraction (SMILE) ectasia eyes were included. The software (called AcuSimX) derived the anisotropic, fibril, and extracellular matrix biomechanical properties (using finite element calculation) of the cornea using the preoperative Corvis-ST, Pentacam measurement, and inverse finite element method assuming published healthy collagen fibril orientations. Then, the software-computed postoperative Kcmean was adjusted with an artificial intelligence (AI) model (Orange AI) for measurement uncertainties. A decision tree was developed to classify ectasia from normal eyes using the software-computed and preoperative parameters.Results:In the training cohort (n = 371 eyes from 371 patients), the mean absolute error and intraclass correlation coefficient were 6.24 N/m and 0.84 (95% CI, 0.80-0.87), respectively. Similarly, in the test cohort (n = 158 eyes from 158 patients), these were 6.47 N/m and 0.84 (0.78-0.89), respectively. In the 10 ectasia eyes, the measured in vivo (74.01 [70.01-78.01]) and software-computed (74.1 [69.03-79.17]) Kcmean were not statistically different (P =.96). Although no statistically significant differences in these values were observed between the stable and ectasia groups (P ≥.14), the decision tree classification had an area under the receiver operating characteristic curve of 1.0.Conclusions:The new software provided an easy-to-use virtual surgery simulation platform for post-LVC corneal stiffness prediction by clinicians and was assessed in post-SMILE ectasia eyes. Further assessments with ectasia after surgeries are required.  Copyright © 2023 Published by Wolters Kluwer on behalf of ASCRS and ESCRS."
"Self-regulation, which is an individual’s ability to control their emotions and behaviors in pursuit of goals, is a complex cognitive function that relies on distributed brain networks. Here, we used activation likelihood estimation (ALE) to conduct two large-scale meta-analyses of brain imaging studies of emotional regulation and behavioral regulation. We used single analysis of ALE to identify brain activation regions associated with behavioral regulation and emotion regulation. The conjunction results of the contrast analysis of the two domains showed that the crucial brain regions of dorsal anterior cingulate cortex (dACC), bilateral anterior insula (AI), and right inferior parietal lobule (IPL) are nested within the brain areas of the two regulation domains at the spatial and functional level. In addition, we assessed the coactivation pattern of the four common regions using meta-analytic connectivity modeling (MACM). The coactivation brain patterns based on the dACC and bilateral AI overlapped with the two regulation brain maps in a high proportion. Furthermore, the functional characters of the identified common regions were reverse-inferenced using the BrainMap database. Collectively, these results indicate that the brain regions of dACC and bilateral AI, playing a crucial role as a hub to other brain regions and networks by effective connectivity in self-regulation, are spatially nested in the brain network of behavioral regulation and emotion regulation. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Text-to-Image artificial intelligence (AI) recently saw a major breakthrough with the release of Dall-E and its open-source counterpart, Stable Diffusion. These programs allow anyone to create original visual art pieces by simply providing descriptions in natural language (prompts). Using a sample of 72,980 Stable Diffusion prompts, we propose a formalization of this new medium of art creation and assess its potential for teaching the history of art, aesthetics, and technique. Our findings indicate that text-to-Image AI has the potential to revolutionize the way art is taught, offering new, cost-effective possibilities for experimentation and expression. However, it also raises important questions about the ownership of artistic works. As more and more art is created using these programs, it will be crucial to establish new legal and economic models to protect the rights of artists. © 2023 The Authors"
"This research adopts the artificial intelligence (AI) device usage acceptance (AIDUA) model and draws on cognitive appraisal theory. It articulates a mediated relationship leading from social influence, hedonic motivation, and anthropomorphism to acceptance of or objection to AI device usage through the mediation of performance expectancy, effort expectancy, and customer emotion in the context of smart hotels. The study further draws on two-factor theory of emotion to identify the moderation of employee presence. Based on a survey design, our research adds to the literature by underscoring why human staff is still critical even in the smart service encounter, especially when customers are looking for superior performance with low effort when using AI devices. However, the presence of employees may be a hindrance in the low performance and high effort expectancies conditions. This research pinpoints a more precise mechanism of the role of employees and technology in delivering intended emotional valence. © 2023 The Authors"
"Background: To investigate the effect of oral contraceptive pills (OCPs) on foveal avascular zone (FAZ), peripapillary capillary plexus, and superficial and deep capillary plexus (SCP and DCP) measurements. Methods: This cross-sectionally designed study included 32 healthy female participants using OCPs (3 mg drospirenone and 0.03 mg ethinylestradiol) for at least one year for contraception and 32 healthy controls that did not use any drugs. All subjects were evaluated using optical coherence tomography angiography (OCTA). Using OCTA, the measurements of SCP, DCP, radial peripapillary capillary (RPC) vessel density; FAZ area and perimeter; acircularity index (AI); and foveal density (FD) were undertaken. Each participant's measurements were taken while they were in the follicular phase of their menstrual cycles (day 3). Results: Age and body mass index did not significantly differ between the groups (p = 0.56 and p = 0.15, respectively). The DCP vessel densities in all the regions were lower in the OCP group (p<0.05 for all). The vessel densities of SCP and RPC, FAZ area and perimeter, AI, and FD were similar between the two groups (p>0.05 for all). Conclusion: We determined that the DCP vessel density was reduced in women using this drug. OCPs can cause changes in retinal microvascular structures. Therefore, OCTA can be used in the follow-up of healthy women using OCP. © 2023 Elsevier B.V."
"Since Shi et al. proposed that the climate in the drylands of Northwest China experienced a significant transition from a “warming and drying” trend to a “warming and wetting” trend in the 1980s, researchers have conducted numerous studies on the variations in precipitation and humidity in the region and even in arid Central Asia. In particular, the process of the “warming and wetting” trend by using obtained measurement data received much attention. However, there remain uncertainties about whether the “warming and wetting” trend has paused and what its future variations may be. In this study, we examined the spatiotemporal variations in temperature, precipitation, the aridity index (AI), vegetation, and runoff during 1950–2019. The results showed that the climate in the drylands of Northwest China and the northern Tibetan Plateau is persistently warming and wetting since the 1980s, with an acceleration since the 1990s. The precipitation/humidity variations in North China, which are mainly influenced by summer monsoon, are generally opposite to those in the drylands of Northwest China. This reverse change is mainly controlled by an anomalous anticyclone over Mongolia, which leads to an anomalous easterly wind, reduced water vapor output, and increased precipitation in the drylands of Northwest China. While it also causes an anomalous descending motion, increased water vapor divergence, and decreased precipitation in North China. Precipitation is the primary controlling factor of humidity, which ultimately forms the spatiotemporal pattern of the “westerlies-dominated climatic regime” of antiphase precipitation/humidity variations between the drylands of Northwest China and monsoonal region of North China. The primary reasons behind the debate of the “warming and wetting” trend in Northwest China were due to the use of different time series lengths, regional ranges, and humidity indices in previous analyses. Since the EC-Earth3 has a good performance for simulating precipitation and humidity in Northwest and North China. By using its simulated results, we found a wetting trend in the drylands of Northwest China under low emission scenarios, but the climate will gradually transition to a “warming and drying” trend as emissions increase. This study suggests that moderate warming can be beneficial for improving the ecological environment in the drylands of Northwest China, while precipitation and humidity in monsoon-dominated North China will persistently increase under scenarios of increased emissions. © 2023, Science China Press."
"This study presents a smart technological framework to efficiently remove azithromycin from natural soil resources using bioremediation techniques. The framework consists of several modules, each with different models such as Penicillium Simplicissimum (PS) bioactivity, soft computing models, statistical optimisation, Machine Learning (ML) algorithms, and Decision Tree (DT) control system based on Removal Percentage (RP). The first module involves designing experiments using a literature review and the Taguchi Orthogonal design method for cultural conditions. The RP is predicted as a function of cultural parameters using Response Surface Methodology (RSM) and three ML algorithms: Instance-Based K (IBK), KStar, and Locally Weighted Learning (LWL). The sensitivity analysis shows that pH is the most important factor among all parameters, including pH, Aeration Intensity (AI), Temperature, Microbial/Food (M/F) ratio, and Retention Time (RT), with a p-value of <0.0001. AI is the next most significant parameter, also with a p-value of <0.0001. The optimal biological conditions for removing azithromycin from soil resources are a temperature of 32 °C, pH of 5.5, M/F ratio of 1.59 mg/g, and AI of 8.59 m3/h. During the 100-day bioremediation process, RP was found to be an insignificant factor for more than 25 days, which simplifies the conditions. Among the ML algorithms, the IBK model provided the most accurate prediction of RT, with a correlation coefficient of over 95%. © 2023 The Authors"
"Introduction: Farm children and youths face unique health risks, including increased risk of agricultural injuries (AI), due to the hazardous machinery, structures and animals on their residential environment. As a result, they experience more severe and complex polytraumatic injuries and longer hospital stays compared to those children injured in homes or residences. A major barrier to the prevention of AI among children and youth residing on farms is a lack of analytic studies about the magnitude and characteristics of these injuries, especially in North Dakota. Methods: We performed a retrospective review of the Sanford Medical Center Fargo trauma registry for pediatric patients (aged 0–19 years) who received care between January 2010 and December 2020 for AI. Patients were grouped for analysis by the age categories of the Agricultural Youth Work Guidelines (AYWG) to compare the mechanisms of injury with the recommended minimum age requirements for specific farm tasks. Results: Of the 41 patients, 26 were male. Mean age was 11 years and one death was reported. The most common mechanism of injury was animals (37%), followed by falls (20%) and machinery (17%). Children under 6 years and youth aged 16 to 19 had the highest number of injuries. Females experienced 53% of animal-related injuries and males accounted for all vehicle-related injuries. Conclusion: The incidence and severity of polytraumatic AI among young children in North Dakota is concerning. Our results underscore the continued need to pursue pediatric injury prevention on farms through educational resources and programs, including the AWYG. Practical applications: Parents require more training on age and ability appropriate farm tasks, especially animal-related interactions. It is imperative that families are given the education and training necessary to integrate children into the farm life while protecting them from injury. © 2023 The Authors"
"Objective In this article, the quality of life (QOL) of Spanish postmenopausal early-stage breast cancer patients who have finished endocrine therapy (ET), QOL changes after endocrine therapy cessation, and the differences between two endocrine therapy modalities (tamoxifen or aromatase inhibitor [AI]) are studied. More QOL information after endocrine therapy cessation is needed. Methods A prospective cohort study was performed. Participating in the study were 158 postmenopausal patients who had received tamoxifen or AI for 5 years. In some cases, endocrine therapy may have changed during those 5 years. Patients completed the European Organisation for Research and Treatment of Cancer QLQ-C30 and QLQ-BR45 questionnaires at baseline, after 6 months, and after 1 year of follow-up. Patients older than 65 years also completed the QLQ-ELD14. Linear mixed-effect models were used to evaluate longitudinal changes in QOL and differences in QOL between endocrine therapy modalities. Results QOL scores for the whole sample throughout follow-up were high (>80/100 points) in most QOL areas. Moderate limitations (>30 points) occurred in the QLQ-BR45 in sexual functioning and sexual enjoyment, future perspective, and joint symptoms. Moderate limitations also occurred in the QLQ-ELD14 in worries about others, maintaining purpose, joint stiffness, future worries, and family support. In those who had finished endocrine therapy, pain was reduced in all three assessments conducted during the 1-year follow-up period in both groups. Tamoxifen patients showed better QOL in functioning (role functioning, global QOL, financial impact), symptoms (pain), and emotional areas (future perspective and worries about others) than AI patients but worse QOL in skin mucosis symptoms. Conclusions The results of this study show that postmenopausal early-stage breast cancer patients adapted well to their disease and endocrine therapy treatment. QOL improvements in the 1-year follow-up period appeared in one key area: pain. Differences between endocrine therapy modalities suggested QOL was better in the tamoxifen group than in the AI group.  © Wolters Kluwer Health, Inc. All rights reserved."
"In the ever-evolving area of digital transformation, following responsible and sustainable practices is essential. This editorial article discusses the importance of responsible digital transformation, emphasizing the need for academia, private and public organizations, civil society, and individuals to work together in developing digital business models that generate shared value while addressing societal challenges. The article highlights the emergence of corporate digital responsibility (CDR) and the shift from industry 4.0 to industry 5.0, which focuses on human-centric approaches and human-AI partnerships. Furthermore, it underscores the need for interdisciplinary research and systematic approaches encompassing various dimensions of sustainability. By integrating sustainable ICT principles into digital transformation initiatives, organizations can contribute to a more sustainable and responsible digital future. The suggestions in this paper, coupled with the nice research contributions included in the special issue, seek to offer a broader foundation to support responsible digital transformations for sustainable societies. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
[No abstract available]
"Purpose: Artificial intelligence (AI) is rapidly reshaping how radiology is practiced. Its susceptibility to biases, however, is a primary concern as more AI algorithms become available for widespread use. So far, there has been limited evaluation of how sociodemographic variables are reported in radiology AI research. This study aims to evaluate the presence and extent of sociodemographic reporting in human subjects radiology AI original research. Methods: All human subjects original radiology AI articles published from January to December 2020 in the top six US radiology journals, as determined by impact factor, were reviewed. Reporting of any sociodemographic variables (age, gender, and race or ethnicity) as well as any sociodemographic-based results were extracted. Results: Of the 160 included articles, 54% reported at least one sociodemographic variable, 53% reported age, 47% gender, and 4% race or ethnicity. Six percent reported any sociodemographic-based results. There was significant variation in reporting of at least one sociodemographic variable by journal, ranging from 33% to 100%. Conclusions: Reporting of sociodemographic variables in human subjects original radiology AI research remains poor, putting the results and subsequent algorithms at increased risk of biases. © 2023 American College of Radiology"
"Contributions in the Special Issue: The special issue assembles papers centring around log data analysis, natural language processing, and machine learning used to advance educational assessment. They demonstrate how semi- and unstructured data such as log and text data can, despite their challenging nature, be handled appropriately to benefit educational assessment. In this editorial, we contextualize the special issue's contributions within the diverse field of modern technology-based assessments. Reflection on Terminology: Moreover, we raise concerns about nowadays' use of the term artificial intelligence (AI) in scientific communication. While the contribution of AI to scientific progress is indisputable, the mere use of methods that have evolved within AI research does not necessarily render tools or studies AI-related. We argue that academics have the social responsibility to adopt accurate terminology, given it is integral to scientific rigour and proper scientific communication. Implications: In view of the inflationary use of the term AI in science, we propose a scheme to locate one's research in the field by focusing on (1) the type of data, (2) the processing involved, and (3) the output of a study and the actions derived from it, which are situated within the (4) scope of a study. © 2023 The Authors. Journal of Computer Assisted Learning published by John Wiley & Sons Ltd."
"The arrival of ChatGPT and other artificial intelligence (AI) writers has captured the popular imagination, but also raised grave concerns. What are the implications of the widespread deployment of such content generation technologies? How should we, as communication scholars, think about and study AI writing tools? We discuss these questions by reflecting on research highlighting the psychological effects of AI as a source of communication. We identify key future research directions, including a redefinition of concepts like creativity, addressing major weaknesses of AI writers, and motivating design of better AI tools with an eye toward reclaiming human agency in the post-ChatGPT era. © 2023 The Author(s)."
"Despite the exponential growth in the popularity of artificial intelligence (AI), our knowledge on the public perception of AI, especially in the context of local government services, is still limited. To bridge this gap, this study aims to provide empirical evidence and insights into public perceptions concerning the use of AI in local government services. Our methodological approach involves collecting data via an online survey from the residents of three major Australian cities—i.e., Sydney, Melbourne, Brisbane—and Hong Kong (n = 850), and performing statistical analyses. We found that: (a) Ease of using AI is significantly and positively influenced by attitude towards AI; (b) Attitude towards AI significantly and positively influences perceived usefulness of AI in local government services; (c) AI is seen useful in resource management and to improve delivery of service, reduction of cost to provide urban-service, improvement of public safety, and monitoring the effectiveness of strategies to manage environmental crisis, and; (d) AI is more positively perceived by Australians in comparison to Hong Kongers, indicating the impact of contextual and cultural differences. The research findings inform local government authorities—e.g., urban policymakers, managers, and planners—on their AI policy, planning and implementation decisions. © 2023 Elsevier Inc."
[No abstract available]
"Objective: Evidence-based medicine (EBM) is a decision-making process based on the conscious and judicious use of the best available scientific evidence. However, the exponential increase in the amount of information currently available likely exceeds the capacity of human-only analysis. In this context, artificial intelligence (AI) and its branches such as machine learning (ML) can be used to facilitate human efforts in analyzing the literature to foster EBM. The present scoping review aimed to examine the use of AI in the automation of biomedical literature survey and analysis with a view to establishing the state-of-the-art and identifying knowledge gaps. Materials and methods: Comprehensive searches of the main databases were performed for articles published up to June 2022 and studies were selected according to inclusion and exclusion criteria. Data were extracted from the included articles and the findings categorized. Results: The total number of records retrieved from the databases was 12,145, of which 273 were included in the review. Classification of the studies according to the use of AI in evaluating the biomedical literature revealed three main application groups, namely assembly of scientific evidence (n = 127; 47%), mining the biomedical literature (n = 112; 41%) and quality analysis (n = 34; 12%). Most studies addressed the preparation of systematic reviews, while articles focusing on the development of guidelines and evidence synthesis were the least frequent. The biggest knowledge gap was identified within the quality analysis group, particularly regarding methods and tools that assess the strength of recommendation and consistency of evidence. Conclusion: Our review shows that, despite significant progress in the automation of biomedical literature surveys and analyses in recent years, intense research is needed to fill knowledge gaps on more difficult aspects of ML, deep learning and natural language processing, and to consolidate the use of automation by end-users (biomedical researchers and healthcare professionals). © 2023 Elsevier Inc."
"The objective of this work was to analyse extra virgin olive oil (EVOO) and soybean oil (SO) and their mixtures, to identify the parameters that distinguish a genuine EVOO from an adulterated one. For this purpose, pure and mixed commercial EVOO and SO were analysed in different ratios: at 50/50%, 40/60%, 30/70%, 20/80% and 10/90%, respectively. For each of the samples, relative density (δ), acidity index (AI) and refractive index (RI) were determined. EVOO and SO presented a δ and RI similar to those reported in the literature with 0.88 and 0.91 g cm−3 and ≤ 1.467 and 1.470 ≥ , respectively. The results obtained for the adulterated EVOOs showed a variation of the δ (0.90 g cm−3) and the RI (1.469 to 1.473), and the AI remained constant (0.56%), suggesting that the RI is the best option to be used in the detection of adulteration. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"In the 6G era, with the hyper-connectivity among humans and everything, we are anticipating Internet of Things (IoT) applications in various fields, including smart city, smart factory, smart home, smart grid, e-health, and smart transportation, accompanied by new services with rich experiences, such as truly immersive VR/AR/MR (XR), high-fidelity mobile hologram, and digital twins. However, in order to facilitate these emerging IoT applications, we have to investigate the collaboration among different network entities to achieve joint optimization under heterogeneous communication, caching, and computing resources. Recently, artificial intelligence (AI) based approaches have been attracting great interest in empowering computer systems. Since the centralized learning approaches face many challenges in terms of scalability, some collaborative learning approaches, such as federated learning and multi-agent systems, have been investigated recently to reduce networking overhead and improve learning efficiency. Based on refined AI technologies, collaborative intelligence can achieve better decisions by aggregating knowledge and enabling efficient coordination among multiple agents with a light communication overhead. It is envisioned that the collaborative intelligence is the enabler for collaborative IoT systems.  © 2017 IEEE."
"In order to improve the lost circulation risk recognition accuracy of artificial intelligence (AI) based models using limited number of data samples, a lost circulation monitoring method involving data augmentation and Bi-directional Long Short-Term Memory (Bi-LSTM) network is proposed. Firstly, the collected lost circulation data samples including pit volume (PIT), flow-out rate (FOR), pump speed (PS) and standpipe pressure (SPP) data sequences as elements are augmented using percentage scaling and random dithering to produce a dataset with increased number of samples. Then, a Bi-LSTM-based lost circulation monitoring model, which can explore both the past and future information of the input data, is established and trained with the augmented dataset. Finally, the obtained lost circulation monitoring model is applied to the PIT, FOR, PS, and SPP field data sequences for risk monitoring. A collected field lost circulation dataset with 2000 sample points was used to train and test the recognition performance of the proposed method, the test results demonstrate that the recognition accuracies of the LSTM model and Bi-LSTM model without data augmentation are 84% and 89%, respectively. After data augmentation is applied, the recognition accuracy of the Bi-LSTM model is improved to 93%. © 2023 Elsevier B.V."
[No abstract available]
"A key unmet need in the management of hemophilia A (HA) is the lack of clinically validated markers that are associated with the development of neutralizing antibodies to Factor VIII (FVIII) (commonly referred to as inhibitors). This study aimed to identify relevant biomarkers for FVIII inhibition using Machine Learning (ML) and Explainable AI (XAI) using the My Life Our Future (MLOF) research repository. The dataset includes biologically relevant variables such as age, race, sex, ethnicity, and the variants in the F8 gene. In addition, we previously carried out Human Leukocyte Antigen Class II (HLA-II) typing on samples obtained from the MLOF repository. Using this information, we derived other patient-specific biologically and genetically important variables. These included identifying the number of foreign FVIII derived peptides, based on the alignment of the endogenous FVIII and infused drug sequences, and the foreign-peptide HLA-II molecule binding affinity calculated using NetMHCIIpan. The data were processed and trained with multiple ML classification models to identify the top performing models. The top performing model was then chosen to apply XAI via SHAP, (SHapley Additive exPlanations) to identify the variables critical for the prediction of FVIII inhibitor development in a hemophilia A patient. Using XAI we provide a robust and ranked identification of variables that could be predictive for developing inhibitors to FVIII drugs in hemophilia A patients. These variables could be validated as biomarkers and used in making clinical decisions and during drug development. The top five variables for predicting inhibitor development based on SHAP values are: (i) the baseline activity of the FVIII protein, (ii) mean affinity of all foreign peptides for HLA DRB 3, 4, & 5 alleles, (iii) mean affinity of all foreign peptides for HLA DRB1 alleles), (iv) the minimum affinity among all foreign peptides for HLA DRB1 alleles, and (v) F8 mutation type. © 2023"
"5G-and-beyond networks are designed to fulfill the communication and computation requirements of various industries, which requires not only transporting the data, but also processing them to meet/address diverse key performance indicators (KPIs). Network Function Virtualization (NFV) has emerged to enable this vision by: (i) collecting the requirements of diverse services, using graphs of Virtual Network Functions (VNFs); and (ii) mapping these requirements into network management decisions. Because of the latter, we need to efficiently allocate computing and network resources to support the desired services, and because of the former such decisions must be jointly optimized considering all KPIs associated with supported services. Thus, this paper proposes an optimized, intelligent network slicing framework to maintain a high performance of network operation by supporting diverse and heterogeneous services, while meeting new KPIs, e.g., reliability, energy consumption, and data quality. Different from the existing works, which are mainly designed considering traditional metrics like throughput and latency, we present a novel methodology and resource allocation schemes that enable high-quality selection of radio points of access, VNF placement and data routing, as well as data compression ratios, from the end users to the cloud. Our results depict the efficiency of the proposed framework in enhancing the network performance when compared to baseline approaches that consider partial network view or fair resource allocation.  © 2004-2012 IEEE."
"With technical advances in robotics and technology, mobile robots (MRs) have become more significant in medical applications that require great subtlety, accuracy and efficiency. MRs are primarily time-varying and nonlinear systems that require intelligent controller rather conventional controllers. When artificial intelligence (AI) is employed in MRs, they become intelligent or autonomous. So, metaheuristic algorithms firefly algorithm (FFA), genetic algorithm (GA), particle swarm optimization (PSO) and grey wolf optimization (GWO) have been investigated in this paper to get the optimal outcome and improve the performance of the MR controller, resulting in an overall system improvement. The study exhibit that GWO control is superior for time-varying and nonlinear MR than conventional control in terms of time response parameters and accuracy assessment. © 2023 The Authors"
[No abstract available]
"Image-based acquisition and digital documentation of geometry and condition of railway bridges. The continuous safety and performance of infrastructure structures is of particular importance for a modern society. The rail network becomes increasingly relevant in the context of the energy transition. Demands for resource efficiency in the construction sector will require significantly longer service lives for structures in the future. New digital maintenance strategies can make this goal achievable while reducing maintenance expenditures at the same time. This report focuses on new image-based methods for condition assessment and digital modelling of the geometry of structures as well as damage characteristics. In particular, the permanent storage of highly accurate damage position and extent information allows the tracking of damage propagation as a basis for novel predictive maintenance strategies. Using the example of railway bridges, it is shown how high-quality digital condition documentation is possible with the concept of digital twins. Photogrammetry is used to generate highly accurate geo-referenced 3D building models from image data acquired by drones, for example, and AI processes are used to perform automated damage detection. The linking in a digital twin enables analyses and high-quality documentation over the life cycle and should start with an initialization measurement on new structures. © 2023, Ernst und Sohn. All rights reserved."
"Background: The daily monitoring of the physiological parameters is essential for monitoring health condition and to prevent health problems. This is possible due to the democratization of numerous types of medical devices and promoted by the interconnection between these and smartphones. Nevertheless, medical devices that connect to smartphones are typically limited to manufacturers applications. Objectives: This paper proposes an intelligent scanning system to simplify the collection of data displayed on different medical devices screens, recognizing the values, and optionally integrating them, through open protocols, with centralized databases. Methods: To develop this system, a dataset comprising 1614 images of medical devices was created, obtained from manufacturer catalogs, photographs and other public datasets. Then, three object detector algorithms (yolov3, Single-Shot Detector [SSD] 320 × 320 and SSD 640 × 640) were trained to detect digits and acronyms/units of measurements presented by medical devices. These models were tested under 3 different conditions to detect digits and acronyms/units as a single object (single label), digits and acronyms/units as independent objects (two labels), and digits and acronyms/units individually (fifteen labels). Models trained for single and two labels were completed with a convolutional neural network (CNN) to identify the detected objects. To group the recognized digits, a condition-tree based strategy on density spatial clustering was used. Results: The most promising approach was the use of the SSD 640 × 640 for fifteen labels. Conclusion: Lastly, as future work, it is intended to convert this system to a mobile environment to accelerate and streamline the process of inserting data into mobile health (mhealth) applications. © 2023"
[No abstract available]
"Background: Critical limb threatening-ischemia (CLTI) can be due to an extensive involvement of both the aorto-iliac (AI) and the infra-inguinal (II) districts and the efficacy of and extensive AI+II vs. only AI revascularization is still matter of debate. The aim of the present study was to evaluate the outcome in CLTI patients with concomitant AI and II peripheral artery disease (PAD) after revascularization limited to the AI or extended also to the II segment. Methods: Patients with CLTI and concomitant AI (TransAtlantic InterSociety Consensus: C-D) and II PAD (Global-Anatomic-Staging-System: II-III) from 2016 to 2021 were retrospectively evaluated. Patients were compared according to type of revascularization: limited to AI vs. AI+II. Common femoral and profunda artery endarterectomy (C/P-TEA) was considered in both groups. Perioperative mortality, limb salvage, foot healing (within 6 months after surgery), necessity of adjunctive revascularization and survival were analyzed and the follow-up performed with clinical and duplex assessment every six months. The primary endpoint was to evaluate the composite event of limb salvage, wound healing and necessity of adjunctive revascularization during follow-up in AI vs. AI+II groups, through Kaplan Meier and Cox regression analysis. Results: Over a total of 1105 peripheral revascularizations for CLTI, 96 (8.7%) patients met the inclusion criteria for the study. AI revascularization was performed in 38 (40%) and AI+II in 58 (60%). AI and AI+II groups were similar for preoperative risk factors and extension of PAD with the exception of American Society of Anesthesiology (ASA) Classification (ASA IV: 50% vs. 25%, P=0.02, respectively). The AI group was treated with angioplasty/stenting in all cases and with C/P-TEA in 20 (52%) cases. In the AI+II group, the AI district was treated by angioplasty/stenting in 55 (95%) and by aorto-bifemoral bypass in 3 (5%) and C/P-TEA in 20 (34%). The II revascularization was performed by femoro-popliteal/tibial bypass in 27 (47%); and endovascular revascularization in 31 (53%) patients. Minor amputation rate was similar between AI and AI+II revascularization (39% vs. 48%, P=1.0); length of stay, blood transfusion units, were significantly higher in AI+II group: 7±4 days vs. 12±5 days, P=0.04 and 2±2 vs. 4±2, P=0.02. The 30-day mortality was 7% with no differences according to the type of treatment. At a mean follow-up of 28±10 months, the overall limb salvage was 87±4% with similar results in AI vs. AI+II revascularization (95±5% vs. 86±6%; P=0.56). AI had a higher necessity of adjunctive revascularization and lower wound healing compared to AI+II (18±9% vs. 0%, P=0.02; 72% vs. 100%, P=0.001, respectively). AI+II was associated with a better primary endpoint compared to AI (87±5% vs. 53±9%, P=0.01), and it was confirmed in Rutherford 5 and 6 patients (100% vs. 54±14%, P=0.01; 78±9 vs. 50±13%, P=0.04), and no differences in Rutherford 4 (100% vs. 100%). Cox regression analysis confirmed AI+II as an independent protector for the primary outcome (hazard ratio: 0.23, 95% confidence interval 0.08-0.71). Conclusions: CLTI with extensive PAD disease can be treated with limited AI revascularization in Rutherford 4 patients however in case of category 5 or 6 an extensive revascularization (AI+II) should be considered. © 2023 EDIZIONI MINERVA MEDICA Online version at https://www.minervamedica.it."
"Drug discovery is researched and developed through many processes, but its overall success rate is extremely low, requiring a very long period of development and considerable costs. Clearly, there is a need to reduce research and development costs by improving the probability of success and increasing process efficiency. One promising approach to this challenge is so-called “in silico drug discovery,” which is drug discovery utilizing information and communications technologies (ICT) such as artificial intelligence (AI) and molecular simulation. In recent years, ICT-based science and technology, such as bioinformatics, systems biology, cheminformatics, and molecular simulation, which have been developed mainly in the life science and chemistry fields, have changed the face of drug development. AI-based methods have been developed in the drug discovery process, mainly in relation to drug target discovery and pharmacokinetic analysis. In drug target discovery, an in silico method has been developed that uses a probabilistic framework that eliminates the problems of conventional experimental approaches and provides a key to understanding the pathways and mechanisms from compounds to phenotypes. In the field of pharmacokinetic analysis, we have seen the development of a method using nonclinical data to predict human pharmacokinetic parameters, which are important for predicting drug efficacy and toxicity in clinical trials. In this article, we provide an overview of these methods. © 2023 The Pharmaceutical Society of Japan."
"Extreme difficulties in species identification of illegally sourced wood with conventional tools have accelerated illicit logging activities, leading to the destruction of natural resources in India. In this regard, the study primarily focused on developing a DNA barcode database for 41 commercial timber tree species which are highly vulnerable to adulteration in south India. The developed DNA barcode database was validated using an integrated approach involving wood anatomical features of traded wood samples collected from south India. Traded wood samples were primarily identified using wood anatomical features using IAWA list of microscopic features for hardwood identification. Consortium of Barcode of Life (CBOL) recommended barcode gene regions (rbcL, matK & psbA-trnH) were employed for developing DNA barcode database. Secondly, we employed artificial intelligence (AI) analytical platform, Waikato Environment for Knowledge Analysis (WEKA) for analyzing DNA barcode sequence database which could append precision, speed, and accuracy for the entire identification process. Among the four classification algorithms implemented in the machine learning algorithm (WEKA), best performance was shown by SMO, which could clearly allocate individual samples to their respective sequence database of biological reference materials (BRM) with 100 % accuracy, indicating its efficiency in authenticating the traded timber species. Major advantage of AI is the ability to analyze huge data sets with more precision and also provides a large platform for rapid authentication of species, which subsequently reduces human labor and time. © 2023, King Abdulaziz City for Science and Technology."
"AI researchers have developed sophisticated language models capable of generating paragraphs of 'synthetic text' on topics specified by the user. While AI text generation has legitimate benefits, it could also be misused, potentially to grave effect. For example, AI text generators could be used to automate the production of convincing fake news, or to inundate social media platforms with machine-generated disinformation. This paper argues that AI text generators should be conceptualised as a dual-use technology, outlines some relevant lessons from earlier debates on dual-use life sciences research, and calls for closer collaboration between ethicists and the machine learning community to address AI language models’ dual-use implications. © 2023, The Author(s)."
"Background: Numerous artificial intelligence (AI)-enabled tools for cardiovascular diseases have been published, with a high impact on public health. However, few have been adopted into, or have meaningfully affected, routine clinical care. Objective: To evaluate current awareness, perceptions, and clinical use of AI-enabled digital health tools for patients with cardiovascular disease, and challenges to adoption. Methods: This mixed-methods study included interviews with 12 cardiologists and 8 health information technology (IT) administrators, and a follow-on survey of 90 cardiologists and 30 IT administrators. Results: We identified 5 major challenges: (1) limited knowledge, (2) insufficient usability, (3) cost constraints, (4) poor electronic health record interoperability, and (5) lack of trust. A minority of cardiologists were using AI tools; more were prepared to implement AI tools, but their sophistication level varied greatly. Conclusion: Most respondents believe in the potential of AI-enabled tools to improve care quality and efficiency, but they identified several fundamental barriers to wide-scale adoption. © 2023 Heart Rhythm Society"
"Computer Aided Detection software based on Artificial Intelligence (AI-CAD), combined with chest X-rays have recently been promoted as an easy fix for a complex problem: ending TB by 2030. WHO has recommended the use of such imaging devices in 2021 and many partnerships have helped propose benchmark analysis and technology comparisons to facilitate their “market access”. Our aim is to examine the socio-political and health issues that stem from using AI-CAD technology in a global health context conceptualized as a set of practice and ideas organizing global intervention “in the life of others”. We also question how this technology, which is not yet fully implemented in routine use, may limit or amplify some inequalities in the care of tuberculosis. We describe AI-CAD through Actor-Network-Theory framework to understand the global assemblage and composite activities associated with detection through AI-CAD, and interrogate how the technology itself may consolidate a specific configuration of “global health”. We explore the various dimensions of AI-CAD “health effects model”: technology design, development, regulation, institutional competition, social interaction and health cultures. On a broader level, AI-CAD represents a new version of global health's accelerationist model centered on “moving and autonomous-presumed technologies”. We finally present key aspects in our research which help discuss the theories mobilized: AI-CAD ambivalent insertion in global health, the social lives of its data: from efficacy to markets and AI-CAD human care and maintenance it requires. We reflect on the conditions that will affect AI-CAD use and its promises. In the end, the risk of new detection technologies such as AI-CAD is indeed that the fight against TB could be reduced to one that is purely technical and technological, with neglect to its social determinants and effects. © 2023 Elsevier Ltd"
"Intelligent integrated sensing and communication is one of key aspects of future wireless networks in which sensing can be leveraged to enhance communications and vice-versa. In this paper, we propose a novel sensing solution that can be used to represent an RF-environment. The proposed solution accounts for practical challenges such as limited time resolution due to limited bandwidth with no angle measurements while providing robustness to wireless propagation phenomena such as diffraction. Our proposed method leverages offline data collection during RF-mapping, and finds the location of virtual anchors (VAs), i.e., mirror images of a physical anchor w.r.t reflectors, through an iterative process called successive tap removal (STR). Afterwards, machine learning (ML) models are trained to predict dominant multipath components of the received wireless channel at a given location. Found VAs and their associated ML models stand for intermediate entities that represent an RF-environment. As an application, we use the developed models in the context of multipath assisted positioning to improve positioning accuracy in challenging indoor environments with heavy non-line-of-sight (NLoS) conditions. Finally, we extend our ideas to systems with multi-antenna transmitters and show that VA detection accuracy can be improved, bringing higher accuracy to the downstream positioning applications.  © 1983-2012 IEEE."
"In this work, a hybrid desalination system uses solar thermal-electric clean energy to maximize production and consistency through optimum temperature management to deliver clean water for good health. It is an effort in the direction of aligning with few of UN’s sustainable developmental goals. BIPV system-powered thermoelectric modules boost evaporation and condensation rates in a unique bio-inspired butterfly roof design-based twin wedge solar still (TWSS). A microcontroller-based temperature control unit (TCU) regulates and maintains the hybrid system to provide practically constant higher yields. To understand system performance, 3 days of testing has been carried out. Average yield, energy efficiency, exergy efficiency, cost per liter of freshwater, and payback period of hybrid TWSS (hTWSS) and passive TWSS over 15 years are 8.64 l/m2/day, 61.93, 9.05, and 0.116 $/l in 44 months, and 1.3 l/m2/day, 23.06, 1.26, 0.068 $/l in 20 months. The hTWSS mitigated 5.1 tons and TWSS 59.6 tons of CO2. This hybrid technology utilizes clean energy to deliver clean water and electricity in green energy buildings with a small footprint. As a futuristic work, AI and machine learning are suggested to be used to enhance and commercialize this solar still desalination method. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Abstract—: A lithofacies and bionomic study of the Artinskian–Kungurian boundary deposits of the Cisuralian Series of the Permian System in the Yuryuzan–Ai Depression was carried out with reference to the Mechetlino section (Southern Urals). This section was a candidate for GSSP of the lower boundary of the Kungurian Stage. Eight lithological facies rock types, or lithofacies, were recognized in terrigenous–carbonate deposits. On the basis of their combinations, the section was divided into packages composed of carbonate–terrigenous flysch rocks, detrital carbonate deposits initiated by storms, and blocky submarine slump deposits. Six ecological assemblages of small foraminifers, reflecting depositional environments in benthic fauna source areas, were identified. The mechanisms of mixing and transport of plant remains along with benthic, planktonic, and nektonic fauna remains are indicated. A model of formation of the Lower Permian sedimentary assemblages in the Yuryuzan–Ai Depression of the Ural foredeep, reflecting the bionomic specificity of the assemblages of biogenic remains in the boundary interval of the Artinskian and Kungurian stages, is proposed. © 2023, Pleiades Publishing, Ltd."
[No abstract available]
"Due to technological advancements and consumer demands, online shopping creates new features and adapts to new standards. A robust customer satisfaction prediction model concerning trust and privacy platforms can encourage an organization to make better decisions about its service and quality. This study presented an approach to predict consumer satisfaction using the blockchain-based framework combining the Multi-Dimensional Naive Bayes-K Nearest Neighbor (MDNB-KNN) and the Multi-Objective Logistic Particle Swarm Optimization Algorithm (MOL-PSOA). A regression model is employed to quantify the impact of various production factors on customer satisfaction. The proposed method yields better levels of measurement for customer satisfaction (98%), accuracy (95%), necessary time (60%), precision (95%), and recall (95%) compared to existing studies. Measuring consumer satisfaction with a trustworthy platform facilitates to development of the conceptual and practical distinctions influencing customers' purchasing decisions. © 2023 The Authors"
"Background: The optimal ablation index (AI) value for cavotricuspid isthmus (CTI) ablation is unknow. Objective: This study investigated the optimal AI value and whether preassessment of local electrogram voltage of CTI could predict first-pass success of ablation. Methods: Voltage maps of CTI were created before ablation. In the preliminary group, the procedure was performed in 50 patients targeting an AI ≥450 on the anterior side (two-thirds segment of CTI) and AI ≥400 on the posterior side (one-third segment of CTI). The modified group also included 50 patients, but the target AI for the anterior side was modified to ≥500. Results: In the modified group, the first-pass rate of success was higher (88% vs 62%; P < .01) than in the preliminary group, and there were no differences in the average bipolar and unipolar voltages at the CTI line. Multivariate logistic regression analysis revealed that ablation with an AI ≥500 on the anterior side was the only independent predictor (odds ratio 4.17; 95% confidence interval 1.44–12.05; P < .01). The bipolar and unipolar voltages were higher at sites without conduction block than at sites with conduction block (both P < .01). The cutoff values for predicting conduction gap were ≥1.94 mV and ≥2.33 mV with areas under the curve of 0.655 and 0.679, respectively. Conclusions: CTI ablation with a target AI >500 on the anterior side was shown to be more effective than an AI >450, and local voltage at a conduction gap was higher than without a conduction gap. © 2023 Heart Rhythm Society"
"Abstract: The drylands of China account for about 10.8% of the global dryland. They are severely affected by human activities and climate change. Dynamic changes in the drylands and within subtype (hyper-arid, arid, semi-arid, and dry sub-humid) areas, and the spatiotemporal characteristics in net primary productivity (NPP) of vegetation, are crucial for evaluating ecosystem functions and management strategies in China. This study defined drylands, and the area was determined based on the aridity index (AI). Our results demonstrated the drylands of China expanded from 5.8 million km2 in 1980 to 6.6 million km2 in 2020, with an average expansion rate of 0.019 × 106 km2 (10 yr)−1. These changes were driven mainly by the mean annual temperature (MAT) and solar radiation (Srad). Among the different subtypes of drylands, the arid and semi-arid areas expanded the fastest at 0.032 × 106 km2 (10 yr)−1, and the main driving factor was MAT. However, the hyper-arid area decreased in size at an average rate of 0.050 × 106 km2 (10 yr)−1, and the main driving factor was mean annual precipitation (MAP). Net primary productivity (NPP) of total vegetation and subtypes increased over 20 years. The NPP of the whole dryland increased at a rate of 10.78 g C m−2 yr−1, which was dominated by the rapid growth of NPP (5.15 g C m−2 yr−1) in dry sub-humid areas. The NPP of four vegetation types (desert, grassland, forest, and cropland) increased (19.14 Tg C yr−1); the NPP in the forest and cropland increased considerably, while that in the grassland and desert increased slightly. The drylands in China might further expand with global warming because the MAT strongly affects the changes in dryland areas. With the development of eco-environmental protection projects (afforestation, construction of desert highway protection forest, etc.) and western region development and construction in China, the contribution of forest and cropland to the total NPP of all vegetation will increase. Research Highlights: 1.The drylands of China expanded from 5.8 million km2 in 1980 to 6.6 million km2 in 2020, with an average expansion rate of 0.019 × 106 km2 (10 yr)−1. These changes were mostly driven by the annual average temperature (MAT) and solar radiation (Srad).2.The NPP of the whole dryland increased at a rate of 10.78 g C m−2 yr−1, which was dominated by the rapid growth of NPP (5.15 g C m−2 yr−1) in dry sub-humid areas. The NPP of four vegetation types (desert, grassland, forest, and cropland) increased (19.14 Tg C yr−1). © 2023, Indian Academy of Sciences."
"Structural design of Reinforced Cement Concrete (RCC) highly depends on the compressive strength of the concrete used. The compressive strength determination techniques are categorized as destructive, non-destructive, and partially destructive. In non-destructive techniques, the equipment is costly and needs expertise. The compressive strength of concrete is influenced by multiple parameters and materials used in making the concrete. Soft computing techniques like Machine learning (ML) and artificial intelligence (AI) have been proven to find hidden relations between multiple parameters and achieve the desired result. The inclusion of AI/ML has enabled the characterization of the strength with advanced techniques based on the individual constituents or images using digital image correlation. Based on the literature reviewed in this study, ML and AI techniques have shown promising outcomes in predicting the compressive strength of concrete. This study systematically examines the contributions made to date in predicting compressive strength utilizing AI-ML-based strategies. It compares and highlights existing literature based on the type of machine learning techniques used, datasets used, evaluation parameters, and performance of different methods. The study does not encompass high strain rate loading or dynamic type of loading. This paper also aims to find the gap in the research conducted and state the potential scope of estimating compressive strength using soft computing techniques. © 2023, Springer Nature Switzerland AG."
"Internet of things (IoT) and Artificial Intelligence (AI) have become the most predominant tools in healthcare applications for pervasive and smart systems for automated diagnosis. Based on the integration of IoT sensors and deep learning algorithms, this study proposes the development of intelligent monitoring systems for maternal and foetal signals in high-risk pregnancies. IoT sensors collect maternal clinical data, such as temperature, blood pressure, oxygen saturation level, heart rate, and fetal heart rate and store them on the cloud for monitoring and prediction purposes. Furthermore, a novel Optimized single-dimensional Convolutional Neural Network (1D-OCNN) is proposed for better classification and prediction of the different emergencies of both mother and fetus. The IoT systems have been designed based on the multiple sensors interfaced with the MICOT boards (Node MCU+MCP3008) and cloud mechanism. Nearly 9000 data were collected and used for the evaluation. The extensive experimentation is carried out using cloud-centric learning algorithms such as K-Nearest neighborhood (KNN), Random forest(RF), Support vector machines(SVM), Convolutional neural networks(CNN) and Extreme learning machines (ELM) and various parameters such as accuracy, precision, recall, sensitivity, and F1-score are calculated. The evaluation shows that the proposed classifier has outperformed the other learning algorithms regarding accuracy, precision, recall and F1-score. Based on the above results, the suggested system is a practical and efficient alternative for maternal and fetal monitoring using IoT and artificial intelligence. © 2023"
"In situations requiring high levels of customization and limited production volumes, additive manufacturing (AM) is a frequently utilized technique with several benefits. To properly configure all the parameters required to produce final goods of the utmost quality, AM calls for qualified designers and experienced operators. This research demonstrates how, in this scenario, artificial intelligence (AI) could significantly enable designers and operators to enhance additive manufacturing. Thus, 48 papers have been selected from the comprehensive collection of research using a systematic literature review to assess the possibilities that AI may bring to AM. This review aims to better understand the current state of AI methodologies that can be applied to optimize AM technologies and the potential future developments and applications of AI algorithms in AM. Through a detailed discussion, it emerges that AI might increase the efficiency of the procedures associated with AM, from simulation optimization to in-process monitoring. © 2023, The Author(s)."
"Critics currently argue that applied ethics approaches to artificial intelligence (AI) are too principles-oriented and entail a theory–practice gap. Several applied ethical approaches try to prevent such a gap by conceptually translating ethical theory into practice. In this article, we explore how the currently most prominent approaches of AI ethics translate ethics into practice. Therefore, we examine three approaches to applied AI ethics: the embedded ethics approach, the ethically aligned approach, and the Value Sensitive Design (VSD) approach. We analyze each of these three approaches by asking how they understand and conceptualize theory and practice. We outline the conceptual strengths as well as their shortcomings: an embedded ethics approach is context-oriented but risks being biased by it; ethically aligned approaches are principles-oriented but lack justification theories to deal with trade-offs between competing principles; and the interdisciplinary Value Sensitive Design approach is based on stakeholder values but needs linkage to political, legal, or social governance aspects. Against this background, we develop a meta-framework for applied AI ethics conceptions with three dimensions. Based on critical theory, we suggest these dimensions as starting points to critically reflect on the conceptualization of theory and practice. We claim, first, that the inclusion of the dimension of affects and emotions in the ethical decision-making process stimulates reflections on vulnerabilities, experiences of disregard, and marginalization already within the AI development process. Second, we derive from our analysis that considering the dimension of justifying normative background theories provides both standards and criteria as well as guidance for prioritizing or evaluating competing principles in cases of conflict. Third, we argue that reflecting the governance dimension in ethical decision-making is an important factor to reveal power structures as well as to realize ethical AI and its application because this dimension seeks to combine social, legal, technical, and political concerns. This meta-framework can thus serve as a reflective tool for understanding, mapping, and assessing the theory–practice conceptualizations within AI ethics approaches to address and overcome their blind spots. © 2023, The Author(s)."
"The Swedish Civil Air Traffic Control (SCAT) dataset consists of 13 weeks of data collected from the area control in Sweden flight information region. The dataset consists of detailed data from almost 170,000 flights as well as airspace data and weather forecasts. The flight data includes system updated flight plans, clearances from air traffic control, surveillance data and trajectory prediction data. Each week of data is continuous but the 13 weeks are spread over one year to provide variations in weather and seasonal traffic patterns. The dataset does only include scheduled flights not involved in any incident reports. Sensitive data such as military and private flight has been removed. The SCAT dataset can be useful for any research related to air traffic control, e.g. analysis of transportation patterns, environmental impact, optimization and automation/AI. © 2023 The Author(s)"
"This article examines the ways in which the US intelligence community is leveraging the power of artificial intelligence (AI) for national security purposes. Drawing on declassified intelligence records, it contends that this community has been fascinated by AI for decades. This is important to acknowledge because this historical context has shaped contemporary projects and thinking within the community. It has given the United States a first-mover advantage, establishing precedents that other global actors need to comply with, negotiate or resist. The article advances three arguments. One, the community has long recognized that it needs to collaborate with the tech sector on AI. However, these relationships bring certain challenges since the sector is a curious compound of ideologies and interests. Two, while the community was initially attracted to the data processing advantages of AI to help human analysts to overcome “data smog,” today it has broadened its focus to consider how AI can improve all stages of the intelligence cycle. Three, while many voices feverishly herald the transformative potential of AI in the global security environment, we argue instead that US agencies will not be able to exploit the full potential of AI, and thus talk of an intelligence revolution is premature. This is because of national and international rules on data collection and retention but also © The Author(s) (2023). Published by Oxford University Press on behalf of the International Studies Association."
[No abstract available]
"GWAS helps to identify QTL and candidate genes of specific traits. Buffalo breeding has primarily focused on milk production, but its negative correlation with reproduction traits resulted in unfavorable decline of reproductive performance among buffaloes. A genome wide scan was performed on a total of 120 Murrah buffaloes genotyped by ddRAD sequencing for 13 traits related to female fertility, production, and growth. The identified 25 significant single nucleotide polymorphisms (SNPs) (P <1×106) are associated with age at first calving (AFC), age at first service (AFS), period from calving to 1st Artifical Insemination (AI), service period (SP) and 6 month body weight (6M). Fifteen genetic variants overlapped with different QTL regions of reported studies. Among the associated loci, outstanding candidate genes for fertility, including AQP1, TRNAE-CUC, NRIP1, CPNE4, and VOPP1, have effect in different fertility traits. AQP1 gene is expressed in ovulatory phase and various stages of pregnancy. TRNAE-CUC gene is associated with AFC and number . of calvings after 4 years of age. Glycogen content–associated gene CPNE4 regulates muscle glycogen and is upregulated during early pregnancy. NRIP1 generegulates ovulation, corpus luteum at pregnancy, and mammary gland development. The objective is to identify potential genomic regions and genetic variants associated with economic traits and to select the most significant SNP which have positive effect on all the traits. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"This article considers film as a form of artificial intelligence (AI). This non-anthropocentric hypothesis was first formulated in 1946 by filmmaker and theorist Jean Epstein and regards film as the thinking performance of a technical apparatus, the cinematograph, which is a manifestation of machine thinking based on the holistic entanglement of thought and world, film and philosophy. The article pursues an enquiry into ‘thinking’: one of the most prominent and oldest topics considered in philosophy, and also essential to art and film. Thinking is not only characterised as a sense (like sight or taste) but as a creative and, ultimately, intra-active act. The possibility of film as AI is approached not only from a Deleuzian angle, long appraised by film-philosophy, but also through questions recently raised by theories of artistic research and the speculative-materialist turn in contemporary philosophy. The latter have as a common denominator a strong critique of anthropocentrism in Western philosophy; the article enquires into this criticism from different angles and applies it to the main hypothesis of this analysis – to regard film as a form of AI. © Christine Reeh Peters."
"Climate change is most evident on the periphery of species distribution ranges. Using four tree-ring chronologies, we identified the most important climatic factors influencing the radial growth of black pine growing along an elevational transect on the eastern slope of Mount Ai-Petri (Crimean Peninsula), at the northernmost part of its range. The relationship between tree-ring width and climate was determined using response function analysis. Results indicate an increase in correlation between radial growth and hydrothermal data along the transect: from a near absence of any correlation at the lowest elevation to r = 0.6 at the top. This change in response was not only caused by differences in climatic variables, but also related to topography, soil, and bedrock features. The currently ongoing aridization may lead to a decrease in the stability and persistence of black pine stands only in the upper parts of Mount Ai-Petri. © 2023 Elsevier GmbH"
"Intelligence and low latency are particularly desired in the vision of industrial intelligence. To support intelligence, massive amount of data is generated from distributed Internet of Things (IoT) devices, and expected to quickly process with artificial intelligence (AI) for data value maximization. To reduce the network transmission delay from data source to computing nodes, edge computing is promising. However, the scarce and dynamic wireless resource as well as limited computing capability of edge servers challenge the edge intelligence. This paper studies a reconfigurable intelligent surface (RIS)-assisted edge-device-to-device (D2D) cooperative edge computing for end-to-end delay reduction in industrial environments. In special, we consider such an edge-D2D cooperative edge computing system, where the IoT devices could offload their tasks to edge server via cellular links for edge computing, or transmit the tasks to helper nodes via D2D links for local computing. We also consider the base station (BS)-based and D2D-based RISs deployed in cellular and D2D networks respectively. We formulate the joint computation offloading, beamforming optimization of both BS-based and D2D based RISs, and CPU resource allocation problem for average end-to-end delay minimization. Then, a distributed and cooperative scheme, called RIS-assisted edge-D2D cooperative computation offloading (RIS-assisted EDCO), is proposed to address the problem. The simulation results have illustrated the efficiency of the proposal for low end-to-end delay performance provisioning. © 2023 Elsevier B.V."
"Identifying and measuring soccer playing styles is a very important step toward a more effective performance analysis. Exploring the different game styles that a team can adopt to enable a great performance remains under-researched. To address this challenge and identify new directions in future research in the area, this paper conducted a critical review of 40 research articles that met specific criteria. Following the 22-item Preferred Reporting Items for Systematic Reviews and Meta-Analyses extension for Scoping Reviews (PRISMA-ScR) guidelines, this scoping review searched for literature on Google Scholar, Web of Science, and Pub Med databases. The descriptive and thematic analysis found that the objectives of the identified papers can be classified into three main categories (recognition and effectiveness of playing styles and contextual variables that affect them). Critically reviewing the studies, the paper concluded that: (i) factor analysis seems to be the best technique among inductive statistics; (ii) artificial intelligence (AI) opens new horizons in performance analysis, and (iii) there is a need for further research on the effectiveness of different playing styles, as well as on the impact of contextual variables on them. © 2023 by the authors."
"The Society for Birth Defects Research and Prevention (BDRP) strives to understand and protect against potential hazards to developing embryos, fetuses, children, and adults by bringing together scientific knowledge from diverse fields. The theme of 62nd Annual Meeting of BDRP, “From Bench to Bedside and Back Again”, represented the cutting-edge research areas of high relevance to public health and significance in the fields of birth defects research and surveillance. The multidisciplinary Research Needs Workshop (RNW) convened at the Annual Meeting continues to identify pressing knowledge gaps and encourage interdisciplinary research initiatives. The multidisciplinary RNW was first introduced at the 2018 annual meeting to provide an opportunity for annual meeting attendees to participate in breakout discussions on emerging topics in birth defects research and to foster collaboration between basic researchers, clinicians, epidemiologists, drug developers, industry partners, funding agencies, and regulators to discuss state-of-the-art methods and innovative projects. Initially, a list of workshop topics was compiled by the RNW planning committee and circulated among the members of BDRP to obtain the most popular topics for the Workshop discussions. Based on the pre-meeting survey results, the top three discussion topics selected were, A) Inclusion of pregnant and lactating women in clinical trials. When, why, and how? B) Building multidisciplinary teams across disciplines: What cross-training is needed? And C) Challenges in applications of Artificial Intelligence (AI) and machine learning for risk factor analysis in birth defects research. This report summarizes the key highlights of the RNW workshop and specific topic discussions. © 2023 Wiley Periodicals LLC."
[No abstract available]
"Background: Early diagnosis of breast cancer has always been a difficult clinical challenge. We developed a deep-learning model EDL-BC to discriminate early breast cancer with ultrasound (US) benign findings. This study aimed to investigate how the EDL-BC model could help radiologists improve the detection rate of early breast cancer while reducing misdiagnosis. Methods: In this retrospective, multicentre cohort study, we developed an ensemble deep learning model called EDL-BC based on deep convolutional neural networks. The EDL-BC model was trained and internally validated on B-mode and color Doppler US image of 7955 lesions from 6795 patients between January 1, 2015 and December 31, 2021 in the First Affiliated Hospital of Army Medical University (SW), Chongqing, China. The model was assessed by internal and external validations, and outperformed radiologists. The model performance was validated in two independent external validation cohorts included 448 lesions from 391 patients between January 1 to December 31, 2021 in the Tangshan People's Hospital (TS), Chongqing, China, and 245 lesions from 235 patients between January 1 to December 31, 2021 in the Dazu People's Hospital (DZ), Chongqing, China. All lesions in the training and total validation cohort were US benign findings during screening and biopsy-confirmed malignant, benign, and benign with 3-year follow-up records. Six radiologists performed the clinical diagnostic performance of EDL-BC, and six radiologists independently reviewed the retrospective datasets on a web-based rating platform. Findings: The area under the receiver operating characteristic curve (AUC) of the internal validation cohort and two independent external validation cohorts for EDL-BC was 0.950 (95% confidence interval [CI]: 0.909–0.969), 0.956 (95% [CI]: 0.939–0.971), and 0.907 (95% [CI]: 0.877–0.938), respectively. The sensitivity values were 94.4% (95% [CI]: 72.7%–99.9%), 100% (95% [CI]: 69.2%–100%), and 80% (95% [CI]: 28.4%–99.5%), respectively, at 0.76. The AUC for accurate diagnosis of EDL-BC (0.945 [95% [CI]: 0.933–0.965]) and radiologists with artificial intelligence (AI) assistance (0.899 [95% [CI]: 0.883–0.913]) was significantly higher than that of the radiologists without AI assistance (0.716 [95% [CI]: 0.693–0.738]; p < 0.0001). Furthermore, there were no significant differences between the EDL-BC model and radiologists with AI assistance (p = 0.099). Interpretation: EDL-BC can identify subtle but informative elements on US images of breast lesions and can significantly improve radiologists' diagnostic performance for identifying patients with early breast cancer and benefiting the clinical practice. Funding: The National Key R&D Program of China. © 2023 The Authors"
"This study reports the relationships between structure and thermal degradation behavior of industrial softwood and hardwood kraft lignin (KL) after acetone fractionation to obtain acetone soluble (AS) and acetone insoluble (AI) fractions with reduced structural polydispersity. The structure and thermal degradation behavior of AS-KL and AI-KL was examined by gel permeation chromatography, Fourier transform infrared, quantitative carbon-13 nuclear magnetic resonance spectroscopy, and thermogravimetric analysis. The results showed that the AS-KL fractions had reduced apparent molecular weight, lower polydispersity, less native wood lignin side chains, greater aromatic hydroxyl groups, and more condensed structures. In contrast, the AI-KL fractions showed substantially larger apparent molecular weight and polydispersity, as well as more aliphatic hydroxyl groups and native lignin side chains. Consequently, AI-KL samples exhibited greater thermal degradation activation energy than those of AS-KL samples because the former fractions had a larger apparent molecular weight as well as more aliphatic OH groups, which facilitated hydrogen bonding between lignin polymers, improving their thermal stability. This finding suggests that acetone fractionation of KL can be used to examine the relationship between structure and thermal degradation of industrial KL. These results also provide important information on the thermal degradation behavior of acetone fractionated products with relevant chemical and physical properties for a specific application, such as raw materials for lignin valorization. © 2023 Elsevier B.V."
"With the development of multi-beam Light Detection and Ranging (LiDAR) sensors, fast and accurate LiDAR-based localization has become a crucial issue in robotics and autonomous driving. However, balancing accuracy and efficiency remains challenging in existing methods. In this paper, we propose a super-fast LiDAR global localization approach that can achieve state-of-the-art (SOTA) accuracy with superior efficiency. Our method leverages template descriptors to capture structural environments and approximates the vehicle's position via map candidate points. Additionally, we create an offline map database to evenly simulate vehicle orientations. We design a loss function to improve localization accuracy. We extensively evaluated the proposed method in public KITTI outdoor sequences and self-collected indoor datasets. The experimental results show that our approach can run at close to 100 frames per second (FPS) on a single-thread CPU, which is much faster than current SOTA methods. Our average absolute translation errors (ATEs) are 0.20m (indoor) and 0.44m (outdoor), and the average localization success rates are 93% (indoor) and 90% (outdoor). The average localization success rates can exceed 97% in large outdoor scenarios with fine-tuned parameters. The source code will be available in https://github.com/ShiPC-AI. © 2023 The Author(s)"
"In the modern age of digitalization, electronics are fundamental to any engineering system. With the current strong focus on the Internet of Things (IoT), autonomous vehicles and Industry 4.0, reliable electronics are gaining crucial importance. Predicting the health of complex systems is able to avoid catastrophic failures. Prognostic and Health Monitoring (PHM) approaches are an important step toward trustable and reliable electronics. Nowadays, Artificial Intelligence (AI) and machine learning (ML) algorithms are integrated into PHM approaches, enabling complex fault diagnosis. In this contribution, we provide an overview of the application of intelligent algorithms in PHM of electronics in a systematic manner. The challenges of prognostics in electronics are provided and a detailed overview of the available PHM precursors for various electronic components and the associated selection process is given. Based on the literature review conducted, the main research challenges with ML algorithms in PHM are discussed along with performances of each model. © 2023"
"Artificial Intelligence (AI) has opened up tremendous opportunities in the workplace through robotics innovation, which envelops both AI and the Internet of Things (IoT). Precision, Efficiency, and Flexibility are considered the potential benefits of Industry 4.0. The implementation of Industry 4.0 requires a lot of changes, including the Human Resource (HR) function. In Industry 4.0, the HR capability is more critical and gives an upper hand to the organization. The HR capability should be more cautious and adaptable to adjust to the difficulties and requirements. We study the contributions of AI in HR digitalization and practices in Industry 4.0. 271 HR experts working in Information Technology (IT), Manufacturing, and administration are selected to participate in this review focusing on five AI applications in HR capability and three elements of HR readiness. The information collected was examined utilizing the Statistical Package for Social Sciences (SPSS) tool and Analysis of Moment Structures (AMOS). The results uncovered that hierarchical organization examination is a fundamental part of acquiring sustainable development. Adaptability and human asset capability are upheld by each of the five components of AI application areas of HR. Well-being and Safety improvement were viewed as vital components under the AI application in HR. © 2023 The Author(s)"
"BACKGROUND: Artificial intelligence (AI) techniques play a major role in anesthesiology, even though their importance is often overlooked. In the extant literature, AI approaches, such as artificial neural networks (ANNs), have been underutilized, being used mainly to model patient’s consciousness state, to predict the precise number of anesthetic gases, the level of analgesia, or the need of anesthesiological blocks, among others. In the field of neurosurgery, ANNs have been effectively applied to the diagnosis and prognosis of cerebral tumors, seizures, low back pain, and also to the monitoring of intracranial pressure (ICP). METHODS: A multilayer perceptron (MLP), which is a feedforward ANN, with hyperbolic tangent as activation function in the input/hidden layers, softmax as activation function in the output layer, and cross-entropy as error function, was used to model the impact of prone versus supine position and the use of positive end expiratory pressure (PEEP) on ICP in a sample of 30 patients undergoing spinal surgery. Different noninvasive surrogate estimations of ICP have been used and compared: namely, mean optic nerve sheath diameter (ONSD), noninvasive estimated cerebral perfusion pressure (NCPP), Pulsatility Index (PI), ICP derived from PI (ICP-PI), and flow velocity diastolic formula (FVDICP). RESULTS: ONSD proved to be a more robust surrogate estimation of ICP, with a predictive power of 75%, whilst the power of NCPP, ICP-PI, PI, and FVDICP were 60.5%, 54.8%, 53.1%, and 47.7%, respectively. CONCLUSIONS: Our MLP analysis confirmed our findings previously obtained with regression, correlation, multivariate receiving operator curve (multi-ROC) analyses. ANNs can be successfully used to predict the effects of prone versus supine position and PEEP on ICP in patients undergoing spinal surgery using different noninvasive surrogate estimators of ICP. C 2018 EDIZIONI MINERVA MEDICA."
"Several studies have documented that when presented with data from social media platforms machine learning (ML) models can make accurate predictions about users, e.g., about whether they are likely to suffer health-related conditions such as depression, mental disorders, and risk of suicide. In a recent article, Ploug (Philos Technol 36:14, 2023) defends a right not to be subjected to AI profiling based on publicly available data. In this comment, I raise some questions in relation to Ploug’s argument that I think deserves further discussion. © 2023, The Author(s)."
"Automatic identification of salient features in large medical datasets, particularly in chest x-ray (CXR) images, is a crucial research area. Accurately detecting critical findings such as emphysema, pneumothorax, and chronic bronchitis can aid radiologists in prioritizing time-sensitive cases and screening for abnormalities. However, traditional deep neural network approaches often require bounding box annotations, which can be time-consuming and challenging to obtain. This study proposes an explainable ensemble learning approach, CX-Net, for lung segmentation and diagnosing lung disorders using CXR images. We compare four state-of-the-art convolutional neural network models, including feature pyramid network, U-Net, LinkNet, and a customized U-Net model with ImageNet feature extraction, data augmentation, and dropout regularizations. All models are trained on the Montgomery and VinDR-CXR datasets with and without segmented ground-truth masks. To achieve model explainability, we integrate SHapley Additive exPlanations (SHAP) and gradient-weighted class activation mapping (Grad-CAM) techniques, which enable a better understanding of the decision-making process and provide visual explanations of critical regions within the CXR images. By employing ensembling, our outlier-resistant CX-Net achieves superior performance in lung segmentation, with Jaccard overlap similarity of 0.992, Dice coefficients of 0.994, precision of 0.993, recall of 0.980, and accuracy of 0.976. The proposed approach demonstrates strong generalization capabilities on the VinDr-CXR dataset and is the first study to use these datasets for semantic lung segmentation with semi-supervised localization. In conclusion, this paper presents an explainable ensemble learning approach for lung segmentation and diagnosing lung disorders using CXR images. Extensive experimental results show that our method efficiently and accurately extracts regions of interest in CXR images from publicly available datasets, indicating its potential for integration into clinical decision support systems. Furthermore, incorporating SHAP and Grad-CAM techniques further enhances the interpretability and trustworthiness of the AI-driven diagnostic system. © 2023 The Author(s). Published by IOP Publishing Ltd."
"Over the past decade, machine learning (ML) and artificial intelligence (AI) have attracted great interest in research and various practical applications. Currently, smart, fast, and high sensitivity with excellent selectivity are becoming increasingly interesting due to the high need for environmental safety and medical applications. The main challenge is to improve sensor selectivity, which requires the combination of interdisciplinary research areas to successfully develop smart gas/chemical sensing devices with better performance. In this review, we present a few principles of gas sensing based on low-cost interdigital electrodes (IDEs), such as electrochemical, resistive, capacitive, and acoustic sensors. In addition, the most important current methods for improving gas sensing performance, the different materials, the different techniques used to fabricate IDE gas sensors, and their advantages and limitations are presented. In addition, a comparison between different ML and AI algorithms for pattern recognition and classification algorithms is also discussed. The discussion then establishes application cases of smart ML algorithms, which provide efficient data processing methods, for the design of smart gas sensors that are highly selective. In addition, the challenges and limitations of ML in gas sensor applications are critically discussed. The study shows the importance of ML with the need for structural optimization to develop and improve smart, sensitive, and selective sensors.  © 2001-2012 IEEE."
"The objective was to assess the effect of sire breed on birth weight, average daily gain from birth to weaning, and actual weaning weight of calves. The calves were produced by AI using semen of five Akaushi (Wagyu), six Angus, and six Brahman bulls. Dams of calves were Beefmaster (n = 60) and Brown Swiss x Zebu (n = 21). The three sire breeds were used on both dam genetic types to produce the calves (45 males and 36 females). Each dam genetic type was raised in two ranches; therefore, all calves were born in four ranches, in the same calendar year. Average age at weaning weight m easurement was 186 days. The traits were analyzed with the MIXED procedure of SAS. The statistical model included the fixed effects of sire breed, dam genetic type, calf sex, ranch, and birth season within sire breed-ranch; sire within breed of sire was included as a random effect (except for weaning weight; P > 0.05). In addition, the model for weaning weight included calf age at weaning as a covariate. Birth weights and average daily gains of Akaushi-, Angus- and Brahman-sired calves were similar (P > 0.05). In contrast, Angus-sired calves were heavier (P < 0.05) at weaning than Akaushi- and Brahman-sired calves. Calves out of Brown Swiss x Zebu dams had higher (P < 0.05) pre-weaning average daily gains than calves out of Beefmaster dams. Angus-sired calves performed better at weaning. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"In recent years, with a rise in climate change, the notion of sustainable development and resource productivity has received a lot of attention around the world. Meanwhile, advances in artificial intelligence based financial management and the creation of new blockchains or cryptocurrencies have transformed the day-to-day operations of economic actors. The aim of the present study is to investigate the impact of financial management and blockchains on environmental sustainability and resource productivity. The monthly time series data of the United States of America economy data is used covering the period from 2000M01–2022M09. Firstly, the stationary variable is tested using the Augmented Dickey-Fuller and Phillips-Perron tests. The findings reveal that all the variables follow I(1). The long-term estimations are investigated through the Fully Modified Ordinary Least Squares. The findings reveal that artificial intelligence-based financial management helps to improve environmental sustainability and resource productivity by reducing carbon emissions and increasing combustible and renewable waste, metals, and non-metallic minerals resources in the long run. In addition, blockchain technology appears to be a booster for environmental unsustainability while also reducing metallic resources. These findings are particularly important and critical since they indicate the path to achieving Sustainable Development Goals. Overall, the findings imply that the United States of America authorities should continue to push AI-based sustainability models and move to green blockchains in order to mitigate the negative environmental effects of blockchains. © 2023 Elsevier Ltd"
"Objective To explore the serological characteristics of ABO blood group and molecular genetic mechanism for a Chinese pedigree with cisAB09 subtype. Methods A pedigree undergoing ABO blood group examination at the Department of Transfusion, Zhongshan Hospital Affiliated to Xiamen University on February 2, 2022 was selected as the study subjects. Serological assay was carried out to determine the ABO blood group of the proband and his family members. Activities of A and B glycosyltransferases in the plasma of the proband and his mother were measured with an enzymatic assay. Expression of A and B antigens on the red blood cells of the proband was analyzed by flow cytometry. Peripheral blood samples of the proband and his family members were collected. Following extraction of genomic DNA, exons 1 to 7 of the ABO gene and their flanking introns were sequenced, and Sanger sequencing of exon 7 was carried out for the proband, his elder daughter and mother. Results 丁he results of serological assay suggested that the proband and his elder daughter and mother had an A2 B phenotype, whilst his wife and younger daughter had an () phenotype. Measurement of plasma A and B glycosyltransferase activity suggested that the titers of B-glycosyltransferase activity were 32 and 256 for the proband and his mother, which were respectively below and above that of Ai B phenotype-positive controls (128). Flow cytometry analysis showed that the expression of A antigen on the red blood cell surface of the proband has decreased, whilst the expression of B antigen was normal. Genetic sequencing confirmed that, in addition to an ABO * B.Ol allele, the proband, his elder daughter and mother have harbored a c.796A〉G variant in exon 7, which has resulted in substitution of the methionine at 266th position of the B-glycosyltransferase by valine and conformed to the characteristics of ABO * cisAB.OQ allele. The genotypes of the proband and his elder daughter were determined as ABO -》cisAB.09/ABO * ().01.01, his mother was ABO -X- cisAB.09/ABO - x- B.Ol, and his wife and younger daughter were ABO -》0.01.01/ABO -》O.Ol.Ol. Conclusion The c. 796A> G variant of the ABO * B. 01 allele has resulted in an amino acid substitution p.Met266Val, which probably underlay the cisAB09 subtype. The ABO - x- cisA B.09 allele encodes a special glycosyltransferase which can synthesize normal level of B antigen and low level of A antigen on the red blood cells. © 2023 West China University of Medical Sciences. All rights reserved."
[No abstract available]
"Background: With the free provision of the chat robot “ChatGPT” by the company OpenAI in November 2022, an application of artificial intelligence (AI) became tangible for everyone. Objectives: An explanation of the basic functionality of large language models (LLM) is given, followed by a presentation of application options of ChatGPT in medicine, and an outlook and discussion of possible dangers of AI applications. Methods: Problem solving with ChatGPT using concrete examples. Analysis and discussion of the available scientific literature. Results: There has been a significant increase in the use of AI applications in scientific work, especially in scientific writing. Wide application of LLM in writing medical documentation is conceivable. Technical functionality allows the use of AI applications as a diagnostic support system. There is a risk of spreading and entrenching inaccuracies and bias through application of LLM. Regulation of this new technology is pending. Conclusion: AI applications such as ChatGPT have the potential to permanently change everyday medical practice. An examination of this technology and evaluation of opportunities and risks is warranted. © 2023, The Author(s), under exclusive licence to Springer Medizin Verlag GmbH, ein Teil von Springer Nature."
"The OpenAI chatbot ChatGPT is an artificial intelligence (AI) application that uses state-of-the-art language processing AI. It can perform a vast number of tasks, from writing poetry and explaining complex quantum mechanics, to translating language and writing research articles with a human-like understanding and legitimacy. Since its initial release to the public in November 2022, ChatGPT has garnered considerable attention due to its ability to mimic the patterns of human language, and it has attracted billion-dollar investments from Microsoft and PricewaterhouseCoopers. The scope of ChatGPT and other large language models appears infinite, but there are several important limitations. This editorial provides an introduction to the basic functionality of ChatGPT and other large language models, their current applications and limitations, and the associated implications for clinical practice and research. © 2023 The British Editorial Society of Bone & Joint Surgery."
"Artificial intelligence (AI) or machine learning (ML) based beam prediction is currently studied in the 3rd Generation Partnership Project (3GPP) fifth generation (5G)-Advanced new ratio (NR) standardization for future commercialization and standard evolution towards sixth generation (6G) communications, wherein time domain (TD) beam prediction is an important use case. The targets for such 3GPP studies and standardization are to lower power consumed at user equipment (UE) and reference signal (RS) overhead that are currently needed by frequent beam measurements due to UE rotation and mobility. To meet such targets, in this paper, we investigate AI/ML based algorithms facilitating TD beam prediction suitable for 5G-Advanced beam management (BM), including RS receive power (RSRP) prediction and beam change prediction. The proposed AI/ML algorithms are first evaluated through computer simulations with new UE mobility models based on recent standard evolutions in 3GPP. Then we further present over-the-air test results achieved by such AI/ML algorithms, using based station (BS) and UE compliant with 3GPP standards. Evaluation results show that the proposed schemes can accurately predict future beams and reduce large amount of the power consumed at the UE for BM, which also demonstrate feasibility of AI/ML based BM for 5G-Advanced and future 6G communications.  © 1983-2012 IEEE."
A human-centered artificial intelligence (AI) approach is proposed as a theoretical foundation and a practical guideline to achieve green and sustainable AI. The goal of this study is to contribute to the discussion of AI as both more sustainable and greener. © 1970-2012 IEEE.
"Most experts agree that large language models (LLMs), such as those used by Copilot and ChatGPT, are expected to revolutionize the way in which software is developed. Many papers are currently devoted to analyzing the potential advantages and limitations of these generative AI models for writing code. However, the analysis of the current state of LLMs with respect to software modeling has received little attention. In this paper, we investigate the current capabilities of ChatGPT to perform modeling tasks and to assist modelers, while also trying to identify its main shortcomings. Our findings show that, in contrast to code generation, the performance of the current version of ChatGPT for software modeling is limited, with various syntactic and semantic deficiencies, lack of consistency in responses and scalability issues. We also outline our views on how we perceive the role that LLMs can play in the software modeling discipline in the short term, and how the modeling community can help to improve the current capabilities of ChatGPT and the coming LLMs for software modeling. © 2023, The Author(s)."
"Artificial intelligent (AI) based on deep learning has been used in medical imaging analysis for years. Improvements have been made in the diagnosis of various diseases with the help of deep learning. Multimodal medical imaging combines two or more imaging modalities, providing comprehensive diagnostic information of the diseases. However, some modality problems always exist in clinical practice. Recently, AI-based deep learning technologies have realized the modality conversion. Investigations on modality conversion have gradually been reported in order to acquire multimodal information. MRI images could be generated from CT images while ultrasound elastography could be generated from B mode ultrasonography. Continuous researches and development of new technologies around deep learning are still under investigation and provide huge clinical potentials in the future. The purpose of this review is to summarize an overview of the current applications and prospects of deep learning-based modality conversion of medical imaging. © 2023 The Author(s)."
"ChatGPT, an artificial intelligence (AI) tool developed by OpenAI, has been in the news recently because of its ability to write convincingly about virtually any topic. It's also capable of engaging in conversational dialog. You can find ChatGPT at https://openai.com/.  © 2000-2012 IEEE."
"Head and neck squamous cell carcinoma (HNSCC) is the world’s 6th most common malignancy. Oral cavity SCC (OCSCC) represents approximately one third of the HNSCC cases diagnosed annually in the United States. Despite therapeutic advances, OCSCC is frequently lethal, with a modest 5-year survival. Because OCSCC is often preceded by premalignant lesions, it is an ideal disease for screening initiatives. The conventional visual and tactile exam (CVTE), coupled with a tissue biopsy, remains the gold standard. However, CVTE alone cannot reliably differentiate between reactive/inflammatory and dysplastic lesions. Further, the histologic diagnosis of dysplasia is subjective in nature and a highly imperfect predictor of malignant transformation. This prognostic uncertainty creates a significant clinical management dilemma—watchful waiting with increased patient psychological and economic burdens versus unnecessary aggressive treatment. As such, the development and validation of novel diagnostic platforms such as Artificial Intelligence (AI) and prognostic molecular biomarkers may help address these critical unmet clinical needs. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Implementing neural networks (NN) on edge devices enables AI to be applied in many daily scenarios. The stringent area and power budget on edge devices impose challenges on conventional NNs with massive energy-consuming Multiply Accumulation (MAC) operations and offer an opportunity for Spiking Neural Networks (SNN), which can be implemented within sub-mW power budget. However, mainstream SNN topologies varies from Spiking Feedforward Neural Network (SFNN), Spiking Recurrent Neural Network (SRNN), to Spiking Convolutional Neural Network (SCNN), and it is challenging for the edge SNN processor to adapt to different topologies. Besides, online learning ability is critical for edge devices to adapt to local environments but comes with dedicated learning modules, further increasing area and power consumption burdens. To alleviate these problems, this work proposed RAINE, a reconfigurable neuromorphic engine supporting multiple SNN topologies and a dedicated trace-based rewarded spike-timing-dependent plasticity (TR-STDP) learning algorithm. Sixteen Unified-Dynamics Learning-Engines (UDLEs) are implemented in RAINE to realize a compact and reconfigurable implementation of different SNN operations. Three topology-aware data reuse strategies are proposed and analyzed to optimize the mapping of different SNNs on RAINE. A 40-nm prototype chip is fabricated, achieving energy-per-synaptic-operation (SOP) of 6.2 pJ/SOP at 0.51 V, and power consumption of 510 $\mu$W at 0.45 V. Finally, three examples with different SNN topologies, including SRNN-based ECG arrhythmia detection, SCNN-based 2D image classification, and end-to-end on-chip learning for MNIST digit recognition, are demonstrated on RAINE with ultra-low energy consumption of 97.7nJ/step, 6.28 $\mu$J/sample, and 42.98 $\mu$J/sample respectively. These results show the feasibility of obtaining high reconfigurability and low power consumption simultaneously on a SNN processor.  © 2007-2012 IEEE."
"Nowadays, to enhance the agricultural productivity of fruits and vegetables, they are sprayed with excess pesticides. The excess pesticides do not wear off even after washing and can be hazardous to human health when consumed; the ill effects include nausea, reproductive harm, cancer, etc., which can lead to death. Traditional laboratory methods such as liquid chromatography and screening cards consume a lot of time, and the price involved is high. Also, advancements in the techniques such as automation using sensors and other methods integrated with embedded technology and machine learning decrease the time required and lower the cost of pesticide detection. The methods discussed here detect the type of pesticide residue along with its concentration that helps in determining the toxicity and quality of fruits and vegetables. This paper discusses 96 papers on AI and sensor technology advancement in detecting pesticide residue on crops in a detailed manner with tabulated and graphical findings of the work done from 1994 to 2022. © 2023 The Author(s)"
"The application of artificial intelligence (AI) and big data in geohazard investigations has gained popularity due to the development of machine learning algorithms and data collection methods. Previous studies have compared and applied various machine learning-based methods, such as conventional machine learning, deep learning, and transfer learning in different areas. This special issue provides state-of-the-art information on the use of AI in geotechnical research, particularly in the Three Gorges Reservoir (TGR) area and adjoining regions. The aim of this volume is to serve as a reference for future researchers interested in exploring the potential of AI in geohazard investigations. It is hoped that this special issue will contribute to the development of guidelines for enhancing the application of AI and big data in geotechnical research, thereby improving our understanding of geological terrains and their associated hazards. © 2023 John Wiley & Sons Ltd."
"The medical environment is on the verge of a dramatic transformation as artificial intelligence (AI) evolves. With the inevitable shift toward AI in health care delivery, there are concerns around its implementation, including ethics, privacy, data representation, and the potential for eliminating physicians. However, AI cannot replicate a physician's knowledge and understanding of the patient as a person and the conditions in which he or she lives. Therefore, provider-patient communication will be paramount in providing safe and effective health care. This piece describes the importance of patient-centered communication and the unintentional move away from this in recent times. We argue that patient-provider communication is vital in the age of AI as it will integrate into the way medicine is practiced, thus leading to more time with the patient to build rapport, trust, and empathy. This will ultimately lead to optimal health-related outcomes.  © Copyright 2023, Mary Ann Liebert, Inc., publishers 2023."
"Crude petroleum oil spillage is becoming a global concern for environmental pollution and poses a severe threat to flora and fauna. Bioremediation is considered a clean, eco-friendly, and cost-effective process to achieve success among the several technologies adopted to mitigate fossil fuel pollution. However, due to the hydrophobic and recalcitrant nature of the oily components, they are not readily bioavailable to the biological components for the remediation process. In the last decade, nanoparticle-based restoration of oil-contaminated, owing to several attractive properties, has gained significant momentum. Thus, intertwining nano- and bioremediation can lead to a suitable technology termed ‘nanobioremediation’ expected to nullify bioremediation’s drawbacks. Furthermore, artificial intelligence (AI), an advanced and sophisticated technique that utilizes digital brains or software to perform different tasks, may radically transfer the bioremediation process to develop an efficient, faster, robust, and more accurate method for rehabilitating oil-contaminated systems. The present review outlines the critical issues associated with the conventional bioremediation process. It analyses the significance of the nanobioremediation process in combination with AI to overcome such drawbacks of a traditional approach for efficiently remedying crude petroleum oil-contaminated sites. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Further efforts are still needed to refine and optimise complex thermochemical pyrolysis processes crucial in waste management and clean energy production. In this work, a comparative artificial intelligence (AI) based modelling study is conducted using four supervised machine learning models, including artificial neural network (ANN), random forest (RF), support vector regression (SVR), and extreme gradient boosting (XGB) to predict the three-phase product yields of pyrolysis. The models were trained using a database of previous experiments focused on continuous pyrolysis in fluidised bed reactors, with biomass feedstock characteristics and pyrolysis conditions as input features. A reactor dimension parameter through H/D (the ratio of the reactor height, H and the reactor diameter, D), for the first time, is also included as an input feature. The models are optimised through feature reduction and 5-fold cross-validation hyperparameter tuning. They show that reducing the organic composition of biomass to include only chemical composition results in the best feature-reduced model. After the comparison of performance scores and total feature importance, the general ranking for AI model accuracy for this study is XGB>RF>ANN>SVR. The H/D ratio also has the highest feature importance scores of 21.71% and 29.52% in predicting the oil and gas yield of the feature-reduced XGB model, confirming the importance of this added parameter. Preliminary contour plot analysis of the database shows that for the considered reactors, optimum oil yields are obtained at H/D ratio< 5, while the optimum gas yields are expected at H/D ratioc closer to 10 for fluidised bed reactors as another indicator of factor importance. © 2023 Elsevier B.V."
"As the Internet of Things (IoT) keeps evolving, next-generation IoT (NG-IoT) scenarios empower various applications which require low-latency connections or high bandwidth support. Mobile edge computing (MEC) is then proposed to provide users with low-latency computing services. However, the massive and heterogeneous nature of user devices and MEC servers also brings some new challenges for resource management and task offloading. Existing works have some shortcomings because of adopting a coarse-grained task model or neglecting task graph information. The booming of artificial intelligence (AI) provides us with a more robust approach to addressing these issues. In this paper, we propose an NG-IoT user task offloading and resource scheduling architecture in the MEC scenario. We formulate our problem objective as minimizing average user task completion time (TCT). To solve the problem, we propose a Reinforcement Learning based algorithm for Container Scheduling (RLCS) and cooperating with the graph convolutional network (GCN) technique. We perform RLCS training and evaluate RLCS performance in the simulated environment. Evaluation results indicate that RLCS outperforms other baselines (e.g., reinforcement learning based algorithm, heuristic algorithm) in multiple experimental settings. © 2023 Elsevier B.V."
"Oral leukoplakia (OLK) is one of the most common potentially malignant disorders. High-risk lesions require early intervention before developing into oral cancer. Photodynamic therapy (PDT) is a noninvasive technique for premalignant lesions. Scalpel biopsy remains a reliable method for monitoring the prognosis of OLK, but it is an invasive procedure with poor reproducibility to suspicious lesions. DNA aneuploidy cytology by oral cytobrush has been proposed as a promising objective and noninvasive tool in screening and diagnosing premalignant and malignant lesions. Here, we discussed the significance of artificial intelligence (AI)-assisted DNA aneuploidy cytology by image cytometry (DNA-ICM) for surveilling non-homogeneous OLK with moderate-to-severe dysplasia that was treated by 5-aminolevulinic acid-mediated PDT (ALA-PDT). The present study provides a scheme of the sequential management and surveillance strategy for OLK. © 2023 Elsevier B.V."
"The author regrets that in the original version of the article one of the hyperlinks was incorrect. Original incorrect reference: Vinnova - The Swedish Government's Innovation Agency (Vinnova), 2018 Vinnova - The Swedish Government's Innovation Agency (Vinnova). (2018). Artificial intelligence in Swedish Business and society. Retrieved from 〈https://www.vinnova.se/contentassets/29cd313d690e4be3a8d861ad05a4ee48/vr_18_09.pdf?cb=20180519112803〉, (Accessed 13.11.21). Updated correct reference: Vinnova - The Swedish Government's Innovation Agency (Vinnova), 2018, Vinnova - The Swedish Government's Innovation Agency (Vinnova). (2018). Artificial intelligence in Swedish business and society. Retrieved from 〈https://www.vinnova.se/contentassets/72ddc02d541141258d10d60a752677df/vr-18_12.pdf〉, (Accessed 13.11.21). © 2023 The Authors"
"Knowledge graph embedding aims to represent entities and relations as low-dimensional vectors, which is an effective way for robotics to learn and reason about semantic knowledge. It is crucial for knowledge graph embedding models to infer various relation patterns, such as symmetry/antisymmetry. However, most existing approaches ignore the latent semantic categories information of entities. For example, when robots use a faucet with relation OperatesOn, its tail entity category should be washable objects, such as apple, bowl, rather than non-washable objects, such as mobile phone, computer. To address such issues, we propose a novel and simple framework for knowledge graph embedding, which utilizes contrastive learning to cluster entities based on the constraints arising from relations, and enhances the discriminative ability for entities with the latent same category. Experimental results demonstrate the effectiveness of our proposed method on four standard knowledge graph completion benchmarks. It is noteworthy that our method can yield some new state-of-the-art results, achieving 88.2% MRR, 84.9% Hits@1 on the AI2Thor dataset, and 39.8% MRR, 30.0% Hits@1 on the FB15k-237 dataset.  © 2016 IEEE."
"Recently, the development of material technology for application to wearable devices has been promoted. In particular, as core technologies for e-skin, Internet of Things, and AI robots, studies on flexible circuits required for connection and operation between devices are attracting attention. In this study, an epoxy-based flexible electrical conductor (EFEC) that can be applied to flexible circuits was developed. It was made by mixing a 2-µm Ag flake, epoxy resin, epoxy hardener, and IPA at a ratio of 50:10:5:5. Flexibility can be achieved by adjusting the ratio of the epoxy base and hardener. IPA, a volatile solvent, was added to lower the viscosity of the epoxy, and Ag flakes were added to increase the electrical conductivity. It was dispersed with a sonicator and magnetic stirrer for overall resistance uniformity. A specific resistance of 2.85×10-3Ωcm was measured. Through scanning electron microscopy and energy-dispersive X-ray spectra analyses, the cause of the low resistance was identified. In addition, 100% strain was achieved in all cases through the tensile test and resistance deviation results. Among them, the lowest resistance deviation was confirmed in CASE 4. Although the specific resistance of the EFEC was approximately 10 times higher than those of electrically conductive adhesives (ECAs) (3.6×10-4Ωcm), it could efficiently be used as a conducting material. ECAs become brittle after hardening. However, we observed that the EFEC exhibited flexibility and that hardening did not proceed even after an extended period. Therefore, the applicability of the EFEC’s flexible circuit was confirmed by securing high electrical conductivity and flexibility. © 2023, The Author(s), under exclusive licence to Korean Society for Precision Engineering."
"In wireless sensor networks (WSNs) with constrained sensor node energy, data compression is crucial. Generally speaking, data communication uses up energy. By providing and receiving less data, a sensor node's lifespan is often increased. In this paper, provide an unique unsupervised neural network architecture called Partly-Informed Sparse Autoencoder (PISAE) that tries to reconstruct all sensor readings from chosen prime numbers. This architecture is used to implement the sensor selection approach that we propose. A key challenge for WSN is the selection of the cluster head (CH). Utilizing the K-Medoid, the clustering of sensor nodes is improved. The effect on quality of service (QoS), the location of the sensor nodes, the distance involved, and the energy status requirements are important factors. For the best choice of cluster heads in WSN with regard to distance and energy, the hybridization of two well-known optimization methods, namely Bacteria Foraging Optimization and Harmony Search Algorithm (HSA), is carried out in this study. Opportunistic routing protocol for WSN is proposed in the current work as a cross-layer based opportunistic Routing protocol (CORP). The suggested CORP approach is utilised to identify the best course of action to cut down on computation time and energy usage while enhancing data transmission dependability. Future generations of ubiquitous sensor networks won't have a single AI solution to the problems with energy and load. In addition, this study puts forth the idea of an energy-efficient routing using Fuzzy neural network (ERFN), which can increase the lifespan of WSNs by lowering energy consumption while maintaining energy usage balance. Based on the simulation findings, it can be stated that the proposed CORP approach increases QoS performance metrics such energy consumption, packet delivery rate, packet delay, network lifetime, throughput, and packet loss rate. The performance of the suggested method outperforms that of already-in-use algorithms like FRLDG, MOBFO-EER, and FEEC-IIR. © 2023"
"In traditional industries, such as the automotive industry, incumbents must draw on big data and artificial intelligence (AI) technologies by designing AI-embedded systems integrated into their end products. While such systems are predominantly presented as paving the way for new knowledge explorative approaches, traditional industry incumbents may face challenges integrating such disruptive technology in their optimized new product development processes. Hence, this study investigates the extent to which incumbents innovate through the design of AI-embedded systems—either via explorative or exploitative strategies—by focusing on the case of the automotive industry. It employed a sequential explanatory mixed-method design and a knowledge search theoretical framework. A quantitative analysis of 46,145 patents from the top 19 traditional companies to identify AI and non-AI patents revealed that firms primarily rely on knowledge exploitation when designing and integrating AI-embedded systems, surprisingly fostering innovativeness. Complementary qualitative insights reveal that big data and AI technologies are integrated into the industrialization phase of new vehicle development, per a creative problem-solving patch. Notably, this study's findings reveal the technical and organizational challenges limiting data-driven innovation, thereby paving a way for more technologically original innovation with big data and AI. © 2023 Elsevier Ltd"
"The development of Generative Adversarial Networks (GANs) has accelerated the research of Artificial Intelligence (AI) in architecture as a generative tool. However, since their initial invention, many versions have been developed that only focus on 2D image datasets for training and images as output. The current state of 3DGAN research has yielded promising results. However, these contributions focus primarily on building mass, extrusion of 2D plans, or the overall shape of objects. In comparison, our newly developed 3DGAN approach, using fully spatial building datasets, demonstrates that unprecedented interconnections across different scales are possible resulting in unconventional spatial configurations. Unlike a traditional design process, based on analyzing only a few precedents (typology) according to the task, by collaborating with the machine we can draw on a significantly wider variety of buildings across multiple typologies. In addition, the dataset was extended beyond the scale of complete buildings and involved building components that define space. Thus, our results achieve a high spatial diversity. A detailed analysis of the results also revealed new hybrid architectural elements illustrating that the machine continued the interconnections of scale since elements were not explicitly part of the dataset, becoming a true design collaborator. © The Author(s) 2023."
"Climate models are developed based on well-established physical principles applied to past and recent climate changes. There is considerable confidence that the models can also provide estimates of some climate variables (i.e., surface temperature, CO2 levels, ocean heat content). Despite advanced mathematical developments in the field of climate modeling, the existing climate models suffer from the following major limitations: first, the models do not consider that their estimations will be highly unreliable when a tipping point is triggered; secondly, many of the environmental tipping points are already triggered, however their existence is overlooked; and third, the existing climate models do not consider the interrelations among the tipping points (i.e., one tipping point can trigger other tipping points to be tipped more rapidly). Our objective is to describe the importance of environmental “tipping points,” the importance of which is often ignored or downplayed in relevant literature. Our analysis, based on extensive multidisciplinary literature searches, reveals that there are many environmental tipping points which are overlooked in climate-modeling studies. We argue that climate modeling could be improved when the tipping points and their interrelations are all considered within the modeling process. We further discuss two other important issues regarding environmental tipping points: first, all tipping points might not be as impactful on the climate system, therefore their relative impacts should be ranked; second, it is in principle impossible to know the exact number of environmental tipping points, therefore even though it could be possible to devise improvements to the existing climate models with our suggestions, it may be impossible to achieve a perfect model to estimate the climate variables of the upcoming years. The remainder of this paper is structured as follows: In the background section, we introduce research on tipping points within commonly used climate models. We explain the aerosol masking effect and ocean dynamics with respect to their commonly overlooked roles as important contributors to environmental change. We introduce remote sensing and AI methods that serve as promising approaches for identification of currently unknown tipping points. We mention perturbation theory, a standard set of mathematical methods in physics that serves as a potentially systematic method to rank environmental tipping points according to their impact on extant climate models. In the discussion section, we make suggestions regarding further research on identifying the typically overlooked tipping points, and we make suggestions to improve climate models by considering additional information presented in the current paper. Finally, we conclude this article summarizing our chief methodological recommendations. © 2023 The Author(s)"
"Developing innovative and effective herbicides is of utmost importance since weed management has become a worldwide agricultural production concern, resulting in severe economic losses every year. In this study, a series of new pyrimidinedione compounds were developed via combination of pyrimidinediones with N-phenylacetamide moiety. The herbicidal activity test (37.5–150 g of ai/ha) indicated that most of the new derivatives exhibited excellent herbicidal activity against dicotyledonous weeds, but less against grasses. Among them, compound 34 was identified as the best postemergence herbicidal activities against six species of weeds (Amaranthus retrof lexus, AR; Abutilon theophrasti, AT; Veronica polita, VP; Echinochloa crusgalli, EC; Digitaria sanguinalis, DS; Setaria viridis, SV), which were comparable to the commercial control agent saflufenacil (≥90%). The protoporphyrinogen oxidase (PPO; EC. 1.3.3.4) activity experiment suggested that compound 34 could significantly reduce the PPO content in weeds, the relative expression levels of the PPO gene were verified by real-time quantitative polymerase chain reaction (RT-qPCR), and the results were consistent with the trend of the enzyme activity data. Molecular docking showed that compound 34 could occupy the PPO enzyme catalytic substrate pocket, which played an excellent inhibitory effect on the activity of receptor protein. Meanwhile, the tolerance of compound 34 to cotton was better than that of the commercial agent saflufenacil at 150 g of ai/ha. Thus, compound 34 exhibits the potential to be a new PPO herbicide for weed control in cotton fields. This study provided a basis for the subsequent structural modification and mechanism research of pyrimidinedione derivatives. © 2023"
"In vitro tumor models have played vital roles in enhancing the understanding of the cellular and molecular composition of tumors, as well as their biochemical and biophysical characteristics. Advances in technology have enabled the evolution of tumor models from two-dimensional cell cultures to three-dimensional printed tumor models with increased levels of complexity and diverse output parameters. With the increase in complexity, the new generation of models is able to replicate the architecture and heterogeneity of the tumor microenvironment more realistically than their predecessors. In recent years, artificial intelligence (AI) has been used extensively in healthcare and research, and AI-based tools have also been applied to the precise development of tumor models. The incorporation of AI facilitates the use of high-throughput systems for real-time monitoring of tumorigenesis and biophysical tumor properties, raising the possibility of using AI alongside tumor modeling for personalized medicine. Here, the integration of AI tools within tumor modeling is reviewed, including microfluidic devices and cancer-on-chip models. © 2023 Wiley-VCH GmbH."
"Societal health is facing a number of new challenges, largely driven by ongoing climate change, demographic ageing, and globalization. The One Health approach links human, animal, and environmental sectors with the goal of achieving a holistic understanding of health in general. To implement this approach, diverse and heterogeneous data streams and types must be combined and analyzed. To this end, artificial intelligence (AI) techniques offer new opportunities for cross-sectoral assessment of current and future health threats. Using the example of antimicrobial resistance as a global threat in the One Health context, we demonstrate potential applications and challenges of AI techniques. This article provides an overview of different applications of AI techniques in the context of One Health and highlights their challenges. Using the spread of antimicrobial resistance (AMR), an increasing global threat, as an example, existing and future AI-based approaches to AMR containment and prevention are described. These range from novel drug development and personalized therapy, to targeted monitoring of antibiotic use in livestock and agriculture, to comprehensive environmental surveillance. © 2023, The Author(s)."
"Artificial Intelligence (AI) integrated with Blockchain distributed ledger technology (BDLT) has become the most attractive research area in the domain of renewable energy and related power automations. However, the increased use of renewable energy-enabled devices raised various challenging problems, such as smart grid-based control management, power distribution, and automations. In this manner, the collaborative approach of blockchain, AI, and wireless sensor networks (WSN) provides a secure platform for control centers to estimate state, which helps to detect and analysis of bad data movement. There are various emerging issues in the field of Blockchain-AI in a renewable environment that poses a serious impact on the technological evaluation, for example, monitoring of contingency, optimal power flow, network reconfiguration, and the commitment of security-constrained units, and control auto-generation. So, in this paper, we perform a systematic review of the state-of-the-art integrated artificial intelligence and blockchain-enabled scheduling, management, optimization, privacy, and security of the smart grid and power distribution automation. One of the focusing aspects of this research is the real-time analysis of the physical layer of the smart grid. However, in this paper, we design a framework of a unified and abstracted state-space, in which the system analysis involves malicious attacks and maintain an effective generalized defense hierarchy in real-time. The current mechanism of analysis of smart grid-enabled physical layer-based malicious attacks is categorized into their associative and targeted components. Thus, we present three different pseudo-smart contracts and digital signatures with consensus policies; that provide an understanding of the new registry of renewable smart grids details, participating stakeholders and their roles, and an updated power distribution automation ledger. We then highlighted the list of emerging power distribution automation-related limitations along with the informational management approaches that present the existing state-of-the-art in this domain, including data-driven, target defense, computation, preservation, etc. Finally, we discuss open research issues and future directions of the smart grid and automation of power distribution security and privacy. © 2023 Elsevier Ltd"
"Purpose: To develop a deep learning-based metal artifact reduction technique (DL-MAR) and quantitatively compare metal artifacts on DL-MAR-corrected CT-images, orthopedic metal artifact reduction (O-MAR)-corrected CT-images and uncorrected CT-images after sacroiliac (SI) joint fusion. Methods: DL-MAR was trained on CT-images with simulated metal artifacts. Pre-surgery CT-images and uncorrected, O-MAR-corrected and DL-MAR-corrected post-surgery CT-images of twenty-five patients undergoing SI joint fusion were retrospectively obtained. Image registration was applied to align pre-surgery with post-surgery CT-images within each patient, allowing placement of regions of interest (ROIs) on the same anatomical locations. Six ROIs were placed on the metal implant and the contralateral side in bone lateral of the SI joint, the gluteus medius muscle and the iliacus muscle. Metal artifacts were quantified as the difference in Hounsfield units (HU) between pre- and post-surgery CT-values within the ROIs on the uncorrected, O-MAR-corrected and DL-MAR-corrected images. Noise was quantified as standard deviation in HU within the ROIs. Metal artifacts and noise in the post-surgery CT-images were compared using linear multilevel regression models. Results: Metal artifacts were significantly reduced by O-MAR and DL-MAR in bone (p < 0.001), contralateral bone (O-MAR: p = 0.009; DL-MAR: p < 0.001), gluteus medius (p < 0.001), contralateral gluteus medius (p < 0.001), iliacus (p < 0.001) and contralateral iliacus (O-MAR: p = 0.024; DL-MAR: p < 0.001) compared to uncorrected images. Images corrected with DL-MAR resulted in stronger artifact reduction than images corrected with O-MAR in contralateral bone (p < 0.001), gluteus medius (p = 0.006), contralateral gluteus medius (p < 0.001), iliacus (p = 0.017), and contralateral iliacus (p < 0.001). Noise was reduced by O-MAR in bone (p = 0.009) and gluteus medius (p < 0.001) while noise was reduced by DL-MAR in all ROIs (p < 0.001) in comparison to uncorrected images. Conclusion: DL-MAR showed superior metal artifact reduction compared to O-MAR in CT-images with SI joint fusion implants. © 2023 Elsevier B.V."
"Artificial Intelligence (AI) techniques have evolved into practical, fast and effective tools in combination with detecting devices for quality assessment, particularly in adulteration and deficiency detection in the food and agriculture industry. This review discusses recent advances in AI techniques and their integration with a variety of sensing devices to detect food adulteration and agricultural product defects. The results obtained from the sensors require the application of case-specific AI techniques aimed at improving the acquired high-dimensional dataset's understanding, as well as classification and prediction. It is evident that the coupling of AI technique and sensors have shown promising outcome between 81.2 and 100% range of accuracy in adulteration and defect detection for food and agricultural product. The research trends and guidelines are also proposed with the aim to provide references and guidance to both scientific researchers and industrial players in the field of food and agriculture quality assessment. The challenges and prospects regarding AI techniques were also revealed. Furthermore, future potential developments, new sensors and novel algorithms must be pursued and validated. Future AI detection of food adulteration and agricultural product defects can be anticipated, especially with the integration of various sensors and the application of deep learning algorithms. Real-time monitoring and predictive modeling may also receive great attention, which could assist to avoid quality problems before they arise, lower the risk of fraud, and ensure high-valuable products. © 2023 The Authors"
"The study aims to develop a computerized hybrid model using artificial intelligence (AI) for the detection of rheumatoid arthritis (RA) from hand radiographs. The objectives of the study include (i) segmentation of proximal interphalangeal (PIP), and metacarpophalangeal (MCP) joints using the deep learning (DL) method, and features are extracted using handcrafted feature extraction technique (ii) classification of RA and non-RA participants is performed using machine learning (ML) techniques. In the proposed study, the hand radiographs are resized to 256 × 256 pixels and pre-processed using the various image processing techniques such as sharpening, median filtering, and adaptive histogram equalization. The segmentation of the finger joints is carried out using the U-Net model, and the segmented binary image is converted to gray scale image using the subtraction method. The features are extracted using the Harris feature extractor, and classification of the proposed work is performed using Random Forest and Adaboost ML classifiers. The study included 50 RA patients and 50 normal subjects for the evaluation of RA. Data augmentation is performed to increase the number of images for U-Net segmentation technique. For the classification of RA and healthy subjects, the Random Forest classifier obtained an accuracy of 91.25% whereas the Adaboost classifier had an accuracy of 90%. Thus, the hybrid model using a Random Forest classifier can be used as an effective system for the diagnosis of RA. © 2023 World Scientific Publishing Co. Pte Ltd. All rights reserved."
"With the informatization of the building and infrastructure industry, conventional analysis methods are gradually proving inadequate in meeting the demands of the new era, such as intelligent synchronization and real-time simulation. Artificial intelligence (AI) technology has emerged as a promising alternative due to its high expressiveness, efficiency, and scalability. This has given rise to a new research field of AI-based computation in civil engineering. In this study, a state-of-the-art review of the research on material and structural analyses using AI technology in civil engineering was performed to provide a general introduction to the current progress. The research was classified into static feature studies, dynamic feature studies, and composite feature studies according to the problem inputs. The general methodology, commonly used AI models, and representative applications of each research category were elaborated. On these bases, the strengths and weaknesses of current studies were discussed. To demonstrate the accuracy and efficiency of AI models in comparison with conventional numerical methods, a concrete example of an end-to-end deep learning framework for structural analysis was highlighted. Finally, we suggested four open problems from the perspective of engineering applications, indicating the major challenges and future research directions regarding AI-based computational analysis in civil engineering. © 2023"
"Definition of the problem: This article critically addresses the conceptualization of trust in the ethical discussion on artificial intelligence (AI) in the specific context of social robots in care. First, we attempt to define in which respect we can speak of ‘social’ robots and how their ‘social affordances’ affect the human propensity to trust in human–robot interaction. Against this background, we examine the use of the concept of ‘trust’ and ‘trustworthiness’ with respect to the guidelines and recommendations of the High-Level Expert Group on AI of the European Union. Arguments: Trust is analyzed as a multidimensional concept and phenomenon that must be primarily understood as departing from trusting as a human functioning and capability. To trust is an essential part of the human basic capability to form relations with others. We further want to discuss the concept of responsivity which has been established in phenomenological research as a foundational structure of the relation between the self and the other. We argue that trust and trusting as a capability is fundamentally responsive and needs responsive others to be realized. An understanding of responsivity is thus crucial to conceptualize trusting in the ethical framework of human flourishing. We apply a phenomenological–anthropological analysis to explore the link between certain qualities of social robots that construct responsiveness and thereby simulate responsivity and the human propensity to trust. Conclusion: Against this background, we want to critically ask whether the concept of trustworthiness in social human–robot interaction could be misguided because of the limited ethical demands that the constructed responsiveness of social robots is able to answer to. © 2023, The Author(s)."
"The issue of room ventilation has recently gained momentum due to the COVID-19 pandemic. Ventilation is in fact of particular relevance in educational environments. Smart University platforms, today widespread, are a good starting point to offer control services of different relevant indicators in universities. This study advances a Ventilation Quality Certificate (VQC) for Smart Universities. The certificate informs the university community of the ventilation status of its buildings and premises. It also supports senior management's decision-making, because it allows assessing preventive measures and actions taken. The VQC algorithm models the adequacy of classroom ventilation according to the number of persons present. The input used is the organisation's existing data relating to CO2 concentration and number of room occupants. AI techniques, specifically Artificial Neural Networks (ANN), were employed to determine the relationship between the different data sources included. A prototype of value-added services was developed for the Smart University platform of the University of Alicante, which allowed to implement the resulting models, together with the VQC. The prototype is currently being replicated in other universities. The case study allowed us to validate the VQC, demonstrating both its usefulness and the advantage of using pre-existing university services and resources. © 2023 The Author(s)"
"Rapid advances in artificial intelligence (AI) have the potential to significantly increase productivity, quality, and profitability in future manufacturing systems. (Caveat: The panel did not attempt to disentangle artificial intelligence from machine learning and used the two terms loosely interchangeably during the discussion.) Traditional mass production will give way to personalized production, with each item made to order, at the low cost and high quality consumers have come to expect. Manufacturing systems will have the intelligence to be resilient to multiple disruptions, from small-scale machine breakdowns to large-scale natural disasters. Products will be made with higher precision and lower variability. While gains have been made toward the development of these factories of the future, many challenges remain to fully realize the vision shown in Figure 1.  © 1994-2011 IEEE."
"Training artificial intelligence (AI) systems to perform autonomous experiments would vastly increase the throughput of microbiology; however, few microbes have large enough datasets for training such a system. In the present study, we introduce BacterAI, an automated science platform that maps microbial metabolism but requires no prior knowledge. BacterAI learns by converting scientific questions into simple games that it plays with laboratory robots. The agent then distils its findings into logical rules that can be interpreted by human scientists. We use BacterAI to learn the amino acid requirements for two oral streptococci: Streptococcus gordonii and Streptococcus sanguinis. We then show how transfer learning can accelerate BacterAI when investigating new environments or larger media with up to 39 ingredients. Scientific gameplay and BacterAI enable the unbiased, autonomous study of organisms for which no training data exist. © 2023, The Author(s), under exclusive licence to Springer Nature Limited."
"With the rapid development of artificial intelligence (AI), there is an urgent need for developing a biological sensory perception system that can simulate the human brain for information processing. Inspired by the biological vision system, photo-responsive photonic synapses are ideal devices for constructing photosensitive artificial neural networks for neuromorphic computing tasks. This paper reports a stable photonic synaptic device in an array layout with adjustable synaptic plasticity under ultraviolet light pulses. Since the heterojunction has a photoconductivity effect and the trap layer provides superior charge carrier trapping capability, optical sensing, memory, and neuromorphic computing are integrated into a single device. Meanwhile, supervised learning of handwritten digitals is achieved by exploiting the multistate conductance by photoelectric co-modulation and the specific decay law. The recognition rate reaches 90.6% and hardly changes with time. Additionally, the device can simplify the artificial neural network (ANN) and reduce its size to 3.78% of the original network while retaining strong fault tolerance and learning ability. The photonic artificial synapses based on ultraviolet light modulation provide a novel and effective approach for photosensory ANNs to perform in situ computation. © 2023 The Authors. Advanced Electronic Materials published by Wiley-VCH GmbH."
"In this paper, we study the extremal problems for Sylvester–Busemann type functionals with respect to Orlicz centroid bodies, Ai(K; Γ ϕ; ψ) , for given convex bodies K, even convex functions ϕ, and i= 0 , 1 , ⋯ , n. We split the results into three cases. In the first case of i= n, it is clear that Ai(K; Γ ϕ; ψ) is constant for every convex body K. In the second case that i= 0 , 1 , ⋯ , n- 1 and ψ is an increasing convex function, ellipsoids are the only minimizers of Ai(K; Γ ϕ; ψ) , and in two-dimensional case triangles (or parallelograms in the symmetric case) are its maximizers. In the third case that i= 0 , 1 , ⋯ , n- 1 and ψ is a decreasing concave function, ellipsoids are the only maximizers of Ai(K; Γ ϕ; ψ) , and in two-dimensional case triangles (or parallelograms in the symmetric case) are its minimizers. As a consequence of our result for i= 0 , ϕ(t) = tp and ψ(t) = tq with p, q≥ 1 , we can get the classical result originally proved by Campi and Gronchi. Using the main statements, we also get the answer to the relevant extremal problems associated with intrinsic volumes. © 2023, Mathematica Josephina, Inc."
"Artificial intelligence (AI) techniques have been widely implemented in the domain of autonomous vehicles (AVs). However, existing AI techniques, such as deep learning and ensemble learning, have been criticized for their black-box nature. Explainable AI is an effective methodology to understand the black box and build public trust in AVs. In this article, a maximum entropy-based Shapley Additive exPlanation (SHAP) is proposed for explaining lane change (LC) decision. Specifically, we first build an LC decision model with high accuracy using eXtreme Gradient Boosting. Then, to explain the model, a modified SHAP method is proposed by introducing a maximum entropy base value. The core of this method is to determine the base value of the LC decision model using the maximum entropy principle, which provides an explanation more consistent with the human intuition. This is because it brings two properties: 1) maximum entropy has a clear physical meaning that quantifies a decision from chaos to certainty, and 2) the sum of the explanations is always isotropic and positive. Furthermore, we develop exhaustive statistical analysis and visualization to present intuitive explanations of the LC decision model. Based on the explanation results, we attribute the causes of predictions with wrong results to model defects or sample sparsity, which provides guidance to users for model optimization.  © 2016 IEEE."
"Artificial intelligence (AI) and NASA may not have been on Dr. Evana Gizzi's mind when she was a math major at the University of Massachusetts (UMass) Lowell, but she's an example of how you don't have to have your career path figured out when you're an undergraduate. © 2007-2011 IEEE."
"Artificial intelligence methods are emerging techniques used in the field of environmental protection, especially in the analysis of air, water, and soil quality. AI analyzes vast amounts of environmental data to predict pollution and provide decision-makers with the information they need to develop efficient policies. One of the most important problems in environmental analysis is data security, and many organizations are actively working to ensure the secure collection, storage, and utilization of sensitive environmental data. In addition, organizations are focusing on developing strategies to protect their data from malicious attacks, such as cyber-attacks, as well as from accidental misuses, like unauthorized access. For this purpose, we have introduced a novel water quality prediction using the Federated Learning Technique. Federated learning enables multiple parties to collaborate and train a model on their local data without sharing it with others, thereby preserving data privacy. The proposed method is applied to a Cauvery River dataset of water quality parameters, and the results demonstrate that the PSO-optimized federated learning process achieves better prediction accuracy of 87%, a precision of 85%, a recall of 93%, and an 89% F1 score. © 2023"
"Scholars, policymakers and organizations in the EU, especially at the level of the European Commission, have turned their attention to the ethics of (trustworthy and human-centric) Artificial Intelligence (AI). However, there has been little reflexivity on (1) the history of the ethics of AI as an institutionalized phenomenon and (2) the comparison to similar episodes of “ethification” in other fields, to highlight common (unresolved) challenges. Contrary to some mainstream narratives, which stress how the increasing attention to ethical aspects of AI is due to the fast pace and increasing risks of technological developments, Science and Technology Studies(STS)-informed perspectives highlight that the rise of institutionalized assessment methods indicates a need for governments to gain more control of scientific research and to bring EU institutions closer to the public on controversies related to emerging technologies. This article analyzes how different approaches of the recent past (i.e. bioethics, technology assessment (TA) and ethical, legal and social (ELS) research, Responsible Research and Innovation (RRI)) followed one another, often “in the name of ethics”, to address previous criticisms and/or to legitimate certain scientific and technological research programs. The focus is on how a brief history of the institutionalization of these approaches can provide insights into present challenges to the ethics of AI related to methodological issues, mobilization of expertise and public participation. © 2023, The Author(s)."
"Nucleotides (NTs) are extremely important low-molecular-weight compounds in organisms, and play an important role in cell metabolism, energy and functional regulation. Our previous study found that NTs regulated the intestinal flora of rapid-aging mice (senescence-accelerated mouse prone-8, SAMP-8) and played a significant role in promoting the growth of Lactobacillus casei. However, the mechanism by which NTs promote the antagonization of intestinal pathogens by probiotics remains unclear. Therefore, we explored the mechanism by which NTs promote the antagonization of intestinal pathogens by L. casei via in vitro coculture. The results showed that NTs promoted the growth of L. casei, while negatively affecting Salmonella enterica. NTs increased the antagonism of S. enterica biofilm formation, swimming, swarming, and secretion of siderophore by L. casei. This antagonism was probably due to NTs activating the LuxS protein in L. casei, which closely combines with NTs, promoting the secretion of AI-2 signaling molecule and triggering the expression of genes related to the quorum sensing system, enabling L. casei to produce specific physiological activities. This study provides a theoretical basis for exploring using NTs as potential prebiotics. © 2023 The Authors"
[No abstract available]
"This article deals with the subject of monitoring the settlements of embankment dams, taking as an example Kerrada’s dam, located in Algeria, and made of rockfill with a central clay core. To analyse the evolution of the deformations, it proposes a multidisciplinary approach which combines geotechnical measurements of settlements and geodetic measurements of levelling. These two types of measurements are diagnosed by the so-called multi-sensor data fusion (MDF) method. In this method, we use “a first level of fusion” which allows us to estimate the settlements on the surface of the dam. Then this estimation is improved during fusion at “a second level” by introducing an interpolation by artificial intelligence of the radial basis functions of neural networks (AI-RBFN), with three AI-RBFNs algorithms, which are the conventional (Newrb), the exact (Newrbe) and the generalized regression of neural networks (Grnn). To generate the settlements of the entire surface of the dam. The goals of MDF use aims to provide better accuracy, robustness against uncertainty and reliable spatiotemporal integration. However, the appropriate surface interpolation by AI-RBFNs aims to handle cases of insufficiency in observations (spatiotemporal) and low-number sensors. The MDF results obtained, in terms of average displacement value and value of the empirical settlement index (SI), were found to be satisfactory. In addition, the MDF-AI-RBFNs model has achieved an improvement rate in the accuracy of 81% compared to that obtained with the geotechnical data only. These results reinforce the need to continue in the study of the optimization of the MDF model by the heuristic algorithm of AI. © 2023, Springer-Verlag GmbH Germany, part of Springer Nature."
"Heating, Ventilation, and Air Conditioning (HVAC) systems play a vital role in building energy management by controlling the indoor temperature and ensuring the occupant's comfort. However, the energy consumption of HVACs contributes significantly towards overall energy usage of a building and carbon footprint. To address this challenge, this research proposes the development of a predictive model for HVAC temperature forecasting using Machine Learning (ML) algorithms to optimize energy efficiency while maintaining thermal comfort in buildings. The study focuses on comparing the performance of Transformer Neural Networks and CNN-LSTM, a seq2seq model combining Convolutional Neural Networks (CNN) and Long-Short Term Memory (LSTM) on multiple forecasting horizons using data obtained from multiple devices deployed in a room verified by feedback survey forms filled by occupants. The transformer model outperformed, achieving an R2 score of 0.936 at a 1-minute forecasting horizon, surpassing the performance of CNN-LSTM model at all tested forecasting horizons. The transformer model yielded significant energy savings thereby reducing energy consumption by almost 50 % compared to the non-AI conventional methods, particularly at forecasting horizons of 1 min and 60 min, while the occupant survey also favoured a 60-minute forecasting horizon. The performance of transformer model particularly with a 60-minute forecasting horizon underscores its potential to optimize energy efficiency while ensuring thermal comfort in building energy management systems. © 2023 International Energy Initiative"
"Focused on city-scale automation, and using self-driving cars (SDCs) as a case study, this article reflects on the role of AI—and in particular, computer vision systems used for mapping and navigation—as a catalyst for urban transformation. Urban research commonly presents AI and cities as having a one-way cause-and-effect relationship, giving undue weight to AI’s impact on cities and overlooking the role of cities in shaping AI. Working at the intersection of data science and social research, this paper aims to counter this trend by exploring the reverse perspective: how do cities affect the development, and expose the present limits, of SDCs? The contribution of this paper is threefold. First, by comparing urban and nonurban environments and thoroughly examining the relationship between computer vision and city-specific sociality and form, it defines machine autonomy/automation as a function of the sociotechnical milieu in which an AI system operates. Second, and related, the paper problematizes the notion of SDCs as autonomous technologies and the role it plays in envisioning contending policy arrangements and technical solutions for achieving full driving automation. Finally, the article offers insight into a materialist and spatialized understanding of AI—namely, not as an abstract quality susceptible to replication within discrete machines, but rather as a distributed property emerging through embodied interactions among a multiplicity of agents (human, non-human, and technological) within/with their environments. © 2023, The Author(s)."
"With the significant success of machine learning, there are plenty of innovative neural network designs nowadays. The related applications become more and more pervasive in our daily life, even in life-critical domains such as autopilot and medical diagnosis, etc. In these domains, whether the AI-based system is “secure” or not is a critical issue. In this work, we first present six Hardware Trojan attacks with demonstrations of their impacts on the hardware design of neural networks. When data leakage occurs, we encode the leakage data to the output and make it more difficult to be detected. Most of our attacks can either achieve more than 98% attack success rate or leak out confidential data without causing any functional violation, with less than 1.5% overhead. We also discuss how to effectively and efficiently detect these Hardware Trojans with formal verification methods and further propose a risk assessment process to constitute a priority guidance to suggest security verification tasks of neuron network hardware. Based on our results, we strongly suggest that security specification and total verification are essential to neuron network designs. © 2023 The Authors"
"Introduction: Artificial Intelligence-based Medical Devices (AI-based MDs) are experiencing exponential growth in healthcare. This study aimed to investigate whether current studies assessing AI contain the information required for health technology assessment (HTA) by HTA bodies. Methods: We conducted a systematic literature review based on the Preferred Reporting Items for Systematic Reviews and Meta-Analyses methodology to extract articles published between 2016 and 2021 related to the assessment of AI-based MDs. Data extraction focused on study characteristics, technology, algorithms, comparators, and results. AI quality assessment and HTA scores were calculated to evaluate whether the items present in the included studies were concordant with the HTA requirements. We performed a linear regression for the HTA and AI scores with the explanatory variables of the impact factor, publication date, and medical specialty. We conducted a univariate analysis of the HTA score and a multivariate analysis of the AI score with an alpha risk of 5 %. Results: Of 5578 retrieved records, 56 were included. The mean AI quality assessment score was 67 %; 32 % of articles had an AI quality score ≥ 70 %, 50 % had a score between 50 % and 70 %, and 18 % had a score under 50 %. The highest quality scores were observed for the study design (82 %) and optimisation (69 %) categories, whereas the scores were lowest in the clinical practice category (23 %). The mean HTA score was 52 % for all seven domains. 100 % of the studies assessed clinical effectiveness, whereas only 9 % evaluated safety, and 20 % evaluated economic issues. There was a statistically significant relationship between the impact factor and the HTA and AI scores (both p = 0.046). Discussion: Clinical studies on AI-based MDs have limitations and often lack adapted, robust, and complete evidence. High-quality datasets are also required because the output data can only be trusted if the inputs are reliable. The existing assessment frameworks are not specifically designed to assess AI-based MDs. From the perspective of regulatory authorities, we suggest that these frameworks should be adapted to assess the interpretability, explainability, cybersecurity, and safety of ongoing updates. From the perspective of HTA agencies, we highlight that transparency, professional and patient acceptance, ethical issues, and organizational changes are required for the implementation of these devices. Economic assessments of AI should rely on a robust methodology (business impact or health economic models) to provide decision-makers with more reliable evidence. Conclusion: Currently, AI studies are insufficient to cover HTA prerequisites. HTA processes also need to be adapted because they do not consider the important specificities of AI-based MDs. Specific HTA workflows and accurate assessment tools should be designed to standardise evaluations, generate reliable evidence, and create confidence. © 2023 The Author(s)"
"The aim of this article is to explore the conception of artificial life forms and the interactions we have with them by paying a particular attention to the analogies that characterize them and the mental processes they give rise to. The article adopts a crossed perspective, focusing on the representations conveyed by artificial life but also on the way we deal with the presence of so-called intelligent or social machines. Based on a multi-sited ethnography of design practices and human-machine interaction experiments, this article hypothesizes that robots and AI constitute a symbolic means of addressing problems regarding our understanding of what life could be whether it is biological or social. Starting from the history of automata, this article will first address the modalities by which an “artificial life” is conceived by analogy with vital processes. It will then focus on the way these processes come into play in an experimental interaction situation. © 2023 Elsevier Ltd"
"Reference Evapotranspiration (ETo) is a complex, dynamic and non-linear hydrological process. Accurate estimation of ETo has long been an eminent topic of interest in the research community for its importance in effective planning and sustainable water resource management. Although the FAO-56 Penman-Monteith (PM) equation has been accepted as a standard equation for ETo measurement, the primary concern that inhibits the applicability of this equation is the requirement for all the climatological variables, which might not be available at a given location. Owning to the remarkable success and accuracy achieved by Artificial Intelligence (AI) in almost every sphere, scientists have proposed the usage AI models for ETo prediction as an alternate to the conventional methods. This comprehensive review will serve to raise awareness regarding the various state-of-the-art standalone AI frameworks, along with capturing the intriguing developments in the advanced AI space such as the hybrid and ensemble models, evolutionary models and a range of optimization techniques. The results from the publications published over the last 15 years (2007–2022) for ETo prediction using AI under varied agro-climatic scenarios have been analysed and synthesized. The advantages and disadvantages of the established AI techniques have been discussed in each subsection. Some of the derived insights and major findings are discussed along with the future research recommendations. This review will not only provide a research vision for the novice researchers in the applicability of the aforementioned techniques, in context of ETo prediction, but also be helpful as a compilation of the AI modelling studies for ETo prediction for the established water resource engineers and hydrologists. © 2023 Elsevier B.V."
"AI livestream with virtual anchors enables the global brands to promote products by transcending cultural disparities. So the novel business model incorporating AI livestream in cross-border operations is worth investigating to show the tradeoffs in the global brands’ decision between AI and the widely adopted key-opinion-leader (KOL) livestream. We focus on the direct channel because such concern does not exist in the reselling channel by the use of local retailers. We develop a cross-border co-opetition model to show the bright and dark sides of AI livestream, where the tariff cost difference between the livestream channel and the existing retail channel are taken into account. We show that, although the novel features of AI livestream are extensively formulated, this new promotion tool can be beneficial only when (1) the network externality among livestream fans is very strong; or (2) the network externality is relatively weak but the tariff cost in the retail channel is high. The underlying rationality can be interpreted by two interesting effects for the offline retail channel, namely the “suppress effect” in AI livestream and the “bailout effect” in KOL livestream. We further examine the impact of cross-border logistics cost and the KOL's stronger network externality to show the robustness of the main findings. © 2023 Elsevier Ltd"
"Impacts of vehicle automation and connectivity have been studied widely from the perspectives of fuel economy, ecology, and safety. Synthesis of various segments of this literature through review papers has also been presented. However, a systematic review is needed for the growing body of recent literature examining how energy consumption of automated vehicles is influenced by the advancements in powertrain operation and planning/control of driving patterns in different traffic conditions. To address this need, we have first developed an AI-based methodology to effectively find the most relevant papers from a very high volume of related literature. The methodology, comprising natural language processing and machine learning models with humans in the loop, has two phases: search query refinement and relevancy determination. The former ensures that almost all the potentially relevant papers are identified. The latter seeks to automatically eliminate (most) irrelevant papers. Application of our method reduced several thousands of papers from an initial step to 430 potentially relevant papers. Manual review of these papers further characterized many as irrelevant resulting in the final pool of 172 papers. We organized these papers based on various means of influencing power consumption, which are powertrain control, platooning, car-following, intersection management, speed planning, traffic control, lane changing, and on-ramp merging. Synthesis of the papers reveals that the existing studies vary greatly in their design, implementation, and comparison baselines, and thus offer widely differing predictions. Hence, more application specific and comprehensive studies are required to appropriately benchmark energy consumption impact of vehicle automation.  © 2023 IEEE."
"This manuscript introduces a new socio-inspired metaheuristic technique referred to as leader–advocate–believer-based optimization algorithm (LAB) for engineering and global optimization problems. The proposed algorithm is inspired by the AI-based competitive behavior exhibited by the individuals in a group while simultaneously improving themselves and establishing a role (leader, advocate, believer). LAB performance in computational time and function evaluations are benchmarked using other metaheuristic algorithms. The algorithm is validated using the CEC 2005 and CEC 2017 benchmark functions. The algorithm was applied to solve engineering problems, including abrasive water jet machining, electric discharge machining, micro-machining processes and turning of titanium alloy in a minimum quantity lubrication environment. LAB algorithm was validated using the Friedman rank test. The results were compared with other algorithms such as FA, CI, GA, SA, PSO, Multi-CI, CMAES, ABC, SADE, CLPSO, BSA, IA, WOA, SHO, AVOA, LSHADE-Cn-EpsiN, FDB-SFS and LSHADE. For real-world problems, LAB outperformed SA, fbest and fbetter by achieving 76%, 85% and 75% minimization of Ra, respectively, for micro-milling with 0.7 mm tool diameter. For real-world problems, LAB achieved 81%, 72%, 85% minimization of Ra when compared to SA, fbest and fbetter for 1 mm tool diameter. LAB also achieved 24% and 34% minimization of Bh and Bt as compared to SA for micro-drilling with a tool diameter 0.5 mm. For tool diameters 0.8 mm and 0.9 mm, 16% and 3% minimization of Bt, respectively, were achieved as compared to SA. The results from this study highlighted that the LAB outperforms the other algorithms in terms of function evaluations and computational time. The prominent features and limitations of the algorithm are also discussed. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Because contractions signal the approach of labor, pregnant women - especially primigravidas (i.e., women pregnant for the first time) - usually go to the hospital to seek medical intervention when they begin experiencing contractions, which is not conductive to good perinatal outcomes. Conventionally, uterine contraction monitoring requires specialized medical devices and relies on the doctor's clinical experience. Therefore, exploring an objective method to detect labor onset at home and avoid early hospital admission has essential importance. In this article, a labor progress monitoring system based on a sensing device, edge service, and Internet of things (IoT) platform is proposed, aiming to suggest suitable hospital admission times for low-risk primigravidas. The pregnant woman places the sensing device on her abdomen with the help of a belt to detect contraction activities. An intelligent edge service for contraction classification is deployed on a mobile phone. The system's artificial intelligence (AI)-assisted algorithm is lightweight, with 670 kB and 194 kB of memory dedicated to a convolutional neural network and long short-term memory, respectively. It classifies the pregnant woman as deferred admission, optional admission, or recommended admission according to different contraction states. An IoT platform connected to the hospital is implemented, providing professional suggestions from doctors. The test set collected in an emergency clinic shows that the proposed system can reach a classification accuracy of more than 96%. In conclusion, the proposed system enables remote labor progress monitoring at home and avoids early hospital admission.  © 2013 IEEE."
"Artificial Intelligence (AI) contributes to public administration by augmenting public services, including managing big data and information technology. AI is currently utilized in e-government services that contribute value to American citizens, including providing a personalized customer experience, such as linking individualized data and processing individual assistance requests. Furthermore, AI can take over redundant routine tasks such as digital information provisioning and delivering services. However, in the United States, there is no overarching policy that governs AI. Inconsistent public policy contributes to public distrust of AI, concerns about privacy intrusion, and fears of inequitable service delivery. This review article discusses the rapid emergence of government applications of AI and provides a systematic review of proposed AI governance models. It then outlines their common components for the purpose of building a coherent, yet adaptable, AI Governance Framework and proposes a research agenda for policy and administration researchers. Related Articles: Glen, Carol M. 2014. “Internet Governance: Territorializing Cyberspace?” Politics & Policy 42(5): 635–57. https://doi.org/10.1111/polp.12093. Glen, Carol M. 2021. “Norm Entrepreneurship in Global Cybersecurity.” Politics & Policy 49(5): 1121–45. https://doi.org/10.1111/polp.12430. Zeng, Jinghan, Tim Stevens, and Yaru Chen. 2017. “China's Solution to Global Cyber Governance: Unpacking the Domestic Discourse of ‘Internet Sovereignty.’” Politics & Policy 45(3): 432–64. https://doi.org/10.1111/polp.12202. © 2023 The Authors. Politics & Policy published by Wiley Periodicals LLC on behalf of Policy Studies Organization."
"Purpose of reviewAdrenal insufficiency (AI) is the clinical manifestation of deficient production of glucocorticoids with occasionally deficiency also in mineralocorticoids and adrenal androgens and constitutes a fatal disorder if left untreated. The aim of this review is to summarize the new trends in diagnostic methods used for determining the presence of AI.Recent findingsNovel aetiologies of AI have emerged; severe acute respiratory syndrome coronavirus 2 infection was linked to increased frequency of primary AI (PAI). A new class of drugs, the immune checkpoint inhibitors (ICIs) widely used for the treatment of several malignancies, has been implicated mostly with secondary AI, but also with PAI. Salivary cortisol is considered a noninvasive and patient-friendly tool and has shown promising results in diagnosing AI, although the normal cut-off values remain an issue of debate depending on the technique used. Liquid chromatography-mass spectrometry (LC-MS/MS) is the most reliable technique although not widely available.SummaryOur research has shown that little progress has been made regarding our knowledge on AI. Coronavirus disease 2019 and ICIs use constitute new evidence on the pathogenesis of AI. The short synacthen test (SST) remains the 'gold-standard' method for confirmation of AI diagnosis, although salivary cortisol is a promising tool. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"In the paper, we propose two models of Artificial Intelligence (AI) patents in European Union (EU) countries addressing spatial and temporal behaviour. In particular, the models can quantitatively describe the interaction between countries or explain the rapidly growing trends in AI patents. For spatial analysis Poisson regression is used to explain collaboration between a pair of countries measured by the number of common patents. Through Bayesian inference, we estimated the strengths of interactions between countries in the EU and the rest of the world. In particular, a significant lack of cooperation has been identified for some pairs of countries. Alternatively, an inhomogeneous Poisson process combined with the logistic curve growth accurately models the temporal behaviour by an accurate trend line. Bayesian analysis in the time domain revealed an upcoming slowdown in patenting intensity. © 2023, The Author(s)."
"Purpose of review Artificial intelligence (AI) is a transformative technology that has the potential to improve and augment the clinical workflow in supportive and palliative care (SPC). The objective of this study was to provide an overview of the recent studies applying AI to SPC in cancer patients. Recent findings Between 2020 and 2022, 29 relevant studies were identified and categorized into two applications: predictive modeling and text screening. Predictive modeling uses machine learning and/or deep learning algorithms to make predictions regarding clinical outcomes. Most studies focused on predicting short-term mortality risk or survival within 6 months, while others used models to predict complications in patients receiving treatment and forecast the need for SPC services. Text screening typically uses natural language processing (NLP) to identify specific keywords, phrases, or documents from patient notes. Various applications of NLP were found, including the classification of symptom severity, identifying patients without documentation related to advance care planning, and monitoring online support group chat data. Summary This literature review indicates that AI tools can be used to support SPC clinicians in decision-making and reduce manual workload, leading to potentially improved care and outcomes for cancer patients. Emerging data from prospective studies supports the clinical benefit of these tools; however, more rigorous clinical validation is required before AI is routinely adopted in the SPC clinical workflow. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"Statistical crop modeling is pivotal for understanding climate impacts on crop yields. Choices of models matter: Linear regression is interpretable but limited in predictive power; machine learning predicts well but often remains a black box. To develop explainable artificial intelligence (AI) for exploring historical crop yield data and predicting crop yield, here we reported a Bayesian ensemble model (BM) that is interpretable with great explanatory and predictive power. BM embraces many competitive models via Bayesian model averaging, fits complex functions, and quantifies model uncertainty. Long-term crop yields are driven by both climate and technology; the common practice of first detrending and then analyzing the detrended data has an incorrigible bias. Therefore, BM was also aimed at decomposing historical yield data to jointly estimate technological trends and climate effects on crop yield. We compared BM with ElasticNet, Neural Network, MARS, SVM, Random Forests, and XGBoost. BM excelled at both predicting and explaining. When tested on synthetic data, BM was the only method unveiling the true relationships: BM has stronger interpretability; other methods predicted well but for wrong reasons. When tested on maize yield data in Ohio, BM detected two technological shifts, attributable to hybrid corn adoption in the 1940′s and the technological slowing-down in the 1970′s: No other methods detected such changepoints. BM derived nonlinear asymmetric crop responses to climate and non-negligible temperature-precipitation interacting effects, with patterns consistent with theoretical or experimental evidence. Extrapolation of all the models for future yield prediction was highly uncertain, but BM provided more reliable predictions under novel climate whereas Random Forests and XGBoost proved unsuitable for extrapolation. Overall, BM provided new insights unattainable by the existing black-box methods. We caution against blind use of black-box machine learning for statistical crop modeling and call for more efforts to apply interpretable machine learning for mechanistic understandings of crop-climate interactions. © 2023 Elsevier B.V."
"The existence of leakage current pathways leading to the appearance of impact ionization and the potential device breakdown in planar Gunn GaN diodes is analyzed by means of a combined Monte Carlo (MC)-deep learning approach. Front-view (lateral) MC simulations of the devices show the appearance of a high-field hotspot at the anode corner of the etched region, just at the boundaries between the dielectric, the GaN-doped layer, and the buffer. Thus, if the isolation created by the etched trenches is not complete, a relevant hot carrier population within the buffer is observed at sufficiently high applied voltages, provoking the appearance of a very significant number of impact ionizations and the consequent avalanche process before the onset of Gunn oscillations. A neural network trained from MC simulations allows predicting with extremely good precision the breakdown voltage of the diodes depending on the doping of the GaN active layer, the permittivity of the isolating dielectric, and the lattice temperature. Low doping, high temperature, and high permittivity provide larger operational voltages, which implies a tradeoff with the conditions required to achieve terahertz (THz) Gunn oscillations at low voltages.  © 1963-2012 IEEE."
[No abstract available]
"The term Explainable Artificial Intelligence (xAI) groups together the scientific body of knowledge developed while searching for methods to explain the inner logic behind the AI algorithm and the model inference based on knowledge-based interpretability. The xAI is now generally recognized as a core area of AI. A variety of xAI methods currently are available to researchers; nonetheless, the comprehensive classification of the xAI methods is still lacking. In addition, there is no consensus among the researchers with regards to what an explanation exactly is and which are salient properties that must be considered to make it understandable for every end-user. The SIRM introduces an xAI-white paper, which is intended to aid Radiologists, medical practitioners, and scientists in the understanding an emerging field of xAI, the black-box problem behind the success of the AI, the xAI methods to unveil the black-box into a glass-box, the role, and responsibilities of the Radiologists for appropriate use of the AI-technology. Due to the rapidly changing and evolution of AI, a definitive conclusion or solution is far away from being defined. However, one of our greatest responsibilities is to keep up with the change in a critical manner. In fact, ignoring and discrediting the advent of AI a priori will not curb its use but could result in its application without awareness. Therefore, learning and increasing our knowledge about this very important technological change will allow us to put AI at our service and at the service of the patients in a conscious way, pushing this paradigm shift as far as it will benefit us. © 2023, The Author(s)."
"Although medical imaging technology has persisted in evolving over the last decades, the techniques and technologies used for analytical and visualization purposes have remained constant. Manual or semiautomatic segmentation is, in many cases, complicated. It requires the intervention of a specialist and is time-consuming, especially during the coronavirus disease (COVID-19) pandemic, which has had devastating medical and economic consequences. Processing and visualizing medical images with advanced techniques represent medical professionals' breakthroughs. This article studies how augmented reality (AR) and artificial intelligence (AI) can transform medical practice during COVID-19 and post-COVID-19 pandemic. Here, we report an AR visualization and interaction platform; it covers the whole process from uploading chest computed tomography (CT)-scan images to automatic segmentation-based deep learning, 3-D reconstruction, 3-D visualization, and manipulation. AR provides a more realistic 3-D visualization system, allowing doctors to effectively interact with the generated 3-D model of segmented lungs and COVID-19 lesions. We use the U-Net neural network (NN) for automated segmentation. The statistical measures obtained using the Dice score, pixel accuracy, sensitivity, G -mean, and specificity are 0.749, 0.949, 0.956, 0.955, and 0.954, respectively. The user-friendliness and usability are objectified by a formal user study that compared our AR-assisted design to the standard diagnosis setup. One hundred and six doctors and medical students, including eight senior medical lecturers, volunteered to assess our platform. The platform could be used as an aid-diagnosis tool to identify and analyze the COVID-19 infectious or as a training tool for residents and medical students. The prototype can be extended to other pulmonary pathologies.  © 2001-2012 IEEE."
"For critical operational decisions (e.g. consulting services), explanations and interpretable results of powerful Artificial Intelligence (AI) systems are becoming increasingly important. Knowledge graphs possess a semantic model that integrates heterogeneous information sources and represents knowledge elements in a machine-readable form. The integration of knowledge graphs and machine learning methods represents a new form of hybrid intelligent systems that benefit from each other's strengths. Our research aims at an explainable system with a specific knowledge graph architecture that generates human-understandable results even when no suitable domain experts are available. Against this background, the interpretability of a knowledge graph-based explainable AI approach for business process analysis is focused. We design a framework of interpretation, show how interpretable models are generated by a single case study and evaluate the applicability of our approach by different expert interviews. Result paths on weaknesses and improvement measures related to a business process are used to produce stochastic decision trees, which improve the interpretability of results. This can lead to interesting consulting self-services for clients or be applied as a device for accelerating classical consulting projects.  © 2023 World Scientific Publishing Company."
"Purpose of Review: Artificial intelligence (AI) has increasingly been used in healthcare. Given the capacity of AI to handle large data and complex relationships between variables, AI is well suited for applications in healthcare. Recently, AI has been applied to allergy research. Recent Findings: In this article, we review how AI technologies have been utilized in basic science and clinical allergy research for asthma, atopic dermatitis, rhinology, adverse reactions to drugs and vaccines, food allergy, anaphylaxis, urticaria, and eosinophilic gastrointestinal disorders. We discuss barriers for AI adoption to improve the care of patients with atopic diseases. Summary: These studies demonstrate the utility of applying AI to the field of allergy to help investigators expand their understanding of disease pathogenesis, improve diagnostic accuracy, enable prediction for treatments and outcomes, and for drug discovery. © 2023, This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply."
"Purpose of Review: Artificial intelligence (AI) is a rapidly growing field in gastrointestinal endoscopy, and its potential applications are virtually endless, with studies demonstrating use of AI for early gastric cancer, inflammatory bowel disease, Barrett’s esophagus, capsule endoscopy, as well as other areas in gastroenterology. Much of the early studies and applications of AI in gastroenterology have revolved around colonoscopy, particularly with regards to real-time polyp detection and characterization. This review will cover much of the existing data on computer-aided detection (CADe), computer-aided diagnosis (CADx), and briefly discuss some other interesting applications of AI for colonoscopy, while also considering some of the challenges and limitations that exist around the use of AI for colonoscopy. Recent Findings: Multiple randomized controlled trials have now been published which show a statistically significant improvement when using AI to improve adenoma detection and reduce adenoma miss rates during colonoscopy. There is also a growing pool of literature showing that AI can be helpful for characterizing/diagnosing colorectal polyps in real time. AI has also shown promise in other areas of colonoscopy, including polyp sizing and automated measurement and monitoring of quality metrics during colonoscopy. Summary: AI is a promising tool that has the ability to shape the future of gastrointestinal endoscopy, with much of the early data showing significant benefits to use of AI during colonoscopy. However, there remain several challenges that may delay or hamper the widespread use of AI in the field. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Gaining more appreciation on the protective/damaging aspects of anti-SARS-CoV-2 immunity associated with disease severity is of great importance. This study aimed to evaluate the avidity of serum IgG antibodies against SARS-CoV-2 spike (S) and nucleocapsid (N) in hospitalized symptomatic COVID-19 patients and asymptomatic RT-PCR-confirmed SARS-CoV-2 carriers as well as to compare antibody avidities with respect to vaccination status, vaccination dose and reinfection status. Serum levels of anti-S and anti-N IgG were determined using specific ELISA kits. Antibody avidity was determined by urea dissociation assay and expressed as avidity index (AI) value. Despite higher IgG levels in the symptomatic group, AI values of both anti-S and anti-N IgG were significantly lower in this group compared to asymptomatic individuals. In both groups, anti-S AI values were elevated in one-dose and two-dose vaccinees versus unvaccinated subjects, although significant differences were only detected in the symptomatic group. However, anti-N avidity showed no significant difference between the vaccinated and unvaccinated subgroups. Almost all vaccinated patients of different subgroups (based on vaccine type) had higher anti-S IgG avidity, while the statistical significance was detected only between those receiving Sinopharm compared to the unvaccinated subgroup. Also, statistically significant differences in antibody AIs were only found between primarily infected individuals of the two groups. Our findings indicate a key role for anti-SARS-CoV-2 IgG avidity in protection from symptomatic COVID-19 and calls for the incorporation of antibody avidity measurement into the current diagnostic tests to predict effective immunity toward SARS-CoV-2 infection or even for prognostic purposes. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"With the vigorous development of artificial intelligence (AI), intelligence applications based on deep neural networks (DNNs) have changed people’s lifestyles and production efficiency. However, the large amount of computation and data generated from the network edge becomes the major bottleneck, and the traditional cloud-based computing mode has been unable to meet the requirements of realtime processing tasks. To solve the above problems, by embedding AI model training and inference capabilities into the network edge, edge intelligence (EI) becomes a cutting-edge direction in the field of AI. Furthermore, collaborative DNN inference among the cloud, edge, and end devices provides a promising way to boost EI. Nevertheless, at present, EI oriented collaborative DNN inference is still in its early stage, lacking systematic classification and discussion of existing research efforts. Motivated by it, we have comprehensively investigated recent studies on EI-oriented collaborative DNN inference. In this paper, we first review the background and motivation of EI. Then, we classify four typical collaborative DNN inference paradigms for EI, and analyse their characteristics and key technologies. Finally, we summarize the current challenges of collaborative DNN inference, discuss future development trends and provide future research directions. © 2023, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag GmbH Germany, part of Springer Nature."
"The generation of food production that meets the rising demand for food and ecosystem security is a big challenge. With the development of Artificial Intelligence (AI) models, there is a growing need to use them to achieve sustainable agriculture. The continuous enhancement of AI in agriculture, researchers have proposed many models in agriculture functions such as prediction,weed control, resource management, advance care of crops, and so on. This article evaluates on a systematic review of AI models in agriculture functions. It also reviews how AI models are used in identified sustainable objectives. Through this extensive review, this paper discusses considerations and limitations for building the next generation of sustainable agriculture using AI. © 2023"
"Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. The robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this letter, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning and allow the agent to further optimize these potentially suboptimal policies. Our approach warm-starts an RL agent by utilizing non-expert natural language specifications without incurring the additional domain exploration costs. We validate our approach by showing that our model is able to produce >80% translation accuracy, and that policies initialized by a human can match the performance of relevant RL baselines in two domains.  © 2016 IEEE."
"Background: The clinical documentation of cystoscopy includes visual and textual materials. However, the secondary use of visual cystoscopic data for educational and research purposes remains limited due to inefficient data management in routine clinical practice. Methods: A conceptual framework was designed to document cystoscopy in a standardized manner with three major sections: data management, annotation management, and utilization management. A Swiss-cheese model was proposed for quality control and root cause analyses. We defined the infrastructure required to implement the framework with respect to FAIR (findable, accessible, interoperable, reusable) principles. We applied two scenarios exemplifying data sharing for research and educational projects to ensure compliance with FAIR principles. Results: The framework was successfully implemented while following FAIR principles. The cystoscopy atlas produced from the framework could be presented in an educational web portal; a total of 68 full-length qualitative videos and corresponding annotation data were sharable for artificial intelligence projects covering frame classification and segmentation problems at case, lesion, and frame levels. Conclusion: Our study shows that the proposed framework facilitates the storage of visual documentation in a standardized manner and enables FAIR data for education and artificial intelligence research. © 2023 Elsevier Inc."
"Personalised and precision nutrition uses information on individual characteristics and responses to nutrients, foods and dietary patterns to develop targeted nutritional advice that is more effective in improving the diet and health of each individual. Moving away from the conventional ‘one size fits all’, such targeted intervention approaches may pave the way to better population health, including lower burden of non-communicable diseases. To date, most personalised and precision nutrition approaches have been focussed on tackling obesity and cardiometabolic diseases with limited efforts directed to cancer prevention and for cancer survivors. Advances in understanding the biological basis of cancer and of the role played by diet in cancer prevention and in survival after cancer diagnosis, mean that it is timely to test and to apply such personalised and precision nutrition approaches in the cancer area. This endeavour can take advantage of the enhanced understanding of interactions between dietary factors, individual genotype and the gut microbiome that impact on risk of, and survival after, cancer diagnosis. Translation of these basic research into public health action should include real-time acquisition of nutrigenomic and related data and use of AI-based data integration methods in systems approaches that can be scaled up using mobile devices. © 2023 The Authors"
"The last decade has seen significant improvements in artificial intelligence (AI) technologies, including robotics, machine vision, speech recognition, and text generation. Increasing automation will undoubtedly affect the future of work, and discussions on how the development of AI in the workplace will impact labor markets often include two scenarios: (1) labor replacement and (2) labor enabling. The former involves replacing workers with machines, while the latter assumes that human–machine cooperation can significantly improve worker productivity. In this context, it is often argued that (1) could lead to mass unemployment and that (2) therefore would be more desirable. We argue, however, that the labor-enabling scenario conflates two distinct possibilities. On the one hand, technology can increase productivity while also promoting “the goods of work,” such as the opportunity to pursue excellence, experience a sense of community, and contribute to society (human augmentation). On the other hand, higher productivity can also be achieved in a way that reduces opportunities for the “goods of work” and/or increases “the bads of work,” such as injury, reduced physical and mental health, reduction of autonomy, privacy, and human dignity (human stunting). We outline the differences of these outcomes and discuss the implications for the labor market in the context of contemporaneous discussions on the value of work and human wellbeing. © 2023, The Author(s)."
"Aim: The primary aim of this study was to compare tamoxifen versus aromatase inhibitors (AI) in terms of urinary incontinence (UI) in premenopausal female patients receiving adjuvant hormone therapy for breast cancer. A secondary aim was to investigate the prevalence and the affecting factors of UI. Methods: This study was designed as a multicenter, cross-sectional that included consecutive premenopausal breast cancer patients ≤50 years of age receiving tamoxifen (with/without LHRHa) or AI (with LHRHa) for at least 6 months, between June 2021 and September 2022. Patients with urinary incontinence before hormone treatments and metastatic patients were excluded from the study. Turkish validation of The International Consultation on Incontinence Modular Questionnaire Urinary Incontinence Short Form (ICIQ UI-SF) was used to determine the UI. Using logistic regression methods, we analyzed potential predictive factors for UI. Results: A total of 206 breast cancer patients were included in this study. A total of 120 (58.2%) patients were receiving tamoxifen plus LHRHa, 40 (19.4%) patients were receiving aromatase inhibitor plus LHRHa, and 46 (22.3%) patients were receiving tamoxifen only. In this study, the prevalence of urinary incontinence was found to be 35.9% (n:74). 41% of the patients receiving tamoxifen and 15.0% of those receiving aromatase inhibitors had complaints of urinary incontinence. There was a statistically significant difference between patients receiving tamoxifen or aromatase inhibitor in terms of urinary incontinence (p=0.001). In the univariate analysis established to predict UI, parity (≥2 vs <2) (OR = 3.23, 95% CI: 1.62–6.46, p= 0.001), tamoxifen (vs AI) (OR = 3.97, 95% CI: 1.58–9.98, p= 0.003), age (≥40 vs. <40) (OR = 2.80, 95% CI: 1.37–5.71, p= 0.005), vaginal deliveries (≥2 vs. <2) (OR = 3.28, 95% CI: 1.44–7.46, p= 0.005), hypertension (OR = 3.59, 95% CI: 1.43–9.02, p= 0.007), diuretic use (OR = 2.55, 95% CI: 1.09–5.95, p= 0.031)), and body mass index (≥25 vs <25) (OR = 1.94, 95% CI: 1.05–3.63), p= 0.034) was found to be predictive. Tamoxifen (OR = 4.71, 95% CI: 1.77–12.56, p= 0.002), hypertension (OR = 3.48, 95% CI: 1.27–9.52, p= 0.015), and age (OR = 2.35, 95% CI: 1.10–5.02, p= 0.027) remained independent predictors for incontinence in multivariate analyses. Conclusion: We found that tamoxifen had increased the risk of urinary incontinence compared to aromatase inhibitors in patients receiving hormone therapy for breast cancer. In addition, we showed that age and hypertension were also independent predictors for UI. In the context of quality of life, we recommend close follow-up of these patients, as drug adherence may be affected in the event of urinary incontinence. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Objectives: The rapid advances in artificial intelligence (AI), big data, and machine learning (ML) technologies hold promise for personalized, equitable cancer care and improved health outcomes within the context of cancer and beyond. Furthermore, integrating these technologies into cancer research has been effective in addressing many of the challenges for cancer control and cure. This can be achieved through the insights generated from massive amounts of data, in ways that can help inform decisions, interventions, and precision cancer care. AI, big data, and ML technologies offer, either in isolation or in combination, unconventional pathways that facilitate the better understanding and management of cancer and its impact on the person. The value of AI, big data, and ML technologies has been acknowledged and integrated within the Cancer Moonshot program in the U.S. and the EU Beating Cancer Plan in Europe. Data Sources: Relevant studies on the topic have formed the basis for this article. Conclusion: In a shifting health care environment where cancer care is becoming more complex and demanding, big data and AI technologies can act as a vehicle to facilitating the care continuum. An increasing body of literature demonstrates their impactful contributions in areas such as treatment and diagnosis. These technologies, however, create additional requirements from health care professionals in terms of capacity and preparedness to integrate them effectively and efficiently in clinical practice. Therefore, there is an increasing need for investment and training in oncology to combat and overcome some of the challenges posed by cancer control. Implications for Nursing Practice: AI, big data, and ML are increasingly integrated in various aspects of health care. As a result, health care professionals, including nurses, will need to adjust in an ever-changing practice environment where these technologies have potential applications in clinical settings to improve risk stratification, early detection, and surveillance management of cancer patients. © 2023 Elsevier Inc."
"Background: Interposition nerve grafting is an indispensable technique for facial nerve reconstruction in head and neck, and skull base surgery. The prognostic factors are inconclusive, partly due to limited objective assessment systems for facial nerve function. This study aimed to apply an artificial intelligence (AI)-based facial asymmetry measurement system to assess facial nerve grafting outcomes. Methods: We retrospectively reviewed data of 23 patients who underwent facial nerve grafting between 2011 and 2020. Oral asymmetry and synkinesis severity were measured using AI. Results: Oral movement recovered at 12–18 months postoperatively. Postoperative radiotherapy and a larger number of anastomosed distal stumps were significantly associated with poor and good final oral symmetry, respectively. Synkinesis severity was weakly correlated with the degree of oral movement recovery. Conclusions: Oral function recovered without a strong correlation with synkinesis. Caution should be exercised in facial nerve grafting for cases with postoperative radiotherapy. © 2023 Wiley Periodicals LLC."
"Current development methods for modeling and optimization for vehicle clutches reach their limits in the field of rising expectations of ride comfort and energy efficiency. This article examines the use of AI methods for clutch development and provides an overview based on various application examples in current Mercedes-Benz AG research projects. By means of supervised learning and deep neural networks, a friction coefficient model and a temperature model of a clutch with high accuracy are developed. Reinforcement learning with deep neural networks is used to synthesize controllers for various gear changes. Vehicle measurement data is analyzed using cluster algorithms to derive action recommendations for the application of the engine restart of a hybrid drivetrain. The methods shown increase the automation potential in development and may reduce the effort required to adopt complex development processes for new transmission variants and generations. © 2023, The Author(s)."
"ChatGPT is an AI-powered chatbot platform that enables human users to converse with machines. It utilizes natural language processing and machine learning algorithms, transforming how people interact with AI technology. ChatGPT offers significant advantages over previous similar tools, and its potential for application in various fields has generated attention and anticipation. However, some experts are wary of ChatGPT, citing ethical implications. Therefore, this paper shows that ChatGPT has significant potential to transform marketing and shape its future if certain ethical considerations are taken into account. First, we argue that ChatGPT-based tools can help marketers create content faster and potentially with quality similar to human content creators. It can also assist marketers in conducting more efficient research and understanding customers better, automating customer service, and improving efficiency. Then we discuss ethical implications and potential risks for marketers, consumers, and other stakeholders, that are essential for ChatGPT-based marketing; doing so can help revolutionize marketing while avoiding potential harm to stakeholders. © 2023 by the authors."
"Carbon capture and storage (CCS) projects rely on numerical reservoir simulations for determining where and how much CO2 can safely be stored underground. Conventional simulations are computationally expensive and time consuming, thereby increasing the turn-around time of decision-making processes and projects. Recent research on artificial intelligence (AI) based approaches for solving the underlying two-phase flow equations has shown enormous promise, as the computational burden can be shifted to the training time, while simulations can be performed in fractions of a second at test time. Applications of AI methods for simulating CO2 flow and other differential equations have so far been limited to 2D or small 3D problems with relatively simple geometries. We introduce a cloud-native framework for parallel simulations of large-scale training data sets using the Open Porous Media (OPM) simulator and we propose a new AI architecture based on Fourier Neural Operators (FNOs) with 3D wavelet transforms. This architecture performs better than FNOs on data with shock fronts, such as simulating CO2 saturation. We also provide a new training dataset for CO2 flow based on the Sleipner CO2 storage geomodel from the Norwegian continental shelf and show that both FNOs and Wavelet Neural Operators (WNOs) can be trained on models with over 200,000 cells and lead to significant speed-ups of up to 50,000× (FNOs) and 100× (Wavelet NOs) over numerical simulations of CO2 saturation with OPM. © 2023 Elsevier Ltd"
"This study proposes and evaluates the OCEL.AI (Open Collaborative Experiential Learning. AI) paradigm that aims at broadening participation in data science education and enhancing undergraduate students’ data literacy. The core of the paradigm is the “Tell Stories” approach. This approach applies the 5W+1H (Who, What, When, Where, Why, and How) conceptual schema of stories as a transdisciplinary language for data science education for STEM and non-STEM majors. Accordingly, this study reported findings from the OCEL.AI project that implemented and evaluated the paradigm. A field experiment, in addition to classroom observations, was conducted to compare the learning outcomes of students in data science competence, appreciation, career motivation, life-long willingness to learn, and self-efficacy in data science between the treatment group and the control group. The results showed that the OCEL.AI paradigm improved undergraduates’ data science competence and career motivation despite majors or gender. © 2023"
"This article tackles the topic of the special issue “Biology in AI: New Frontiers in Hardware, Software and Wetware Modeling of Cognition” in two ways. It addresses the problem of the relevance of hardware, software, and wetware models for the scientific understanding of biological cognition, and it clarifies the contributions that synthetic biology, construed as the synthetic exploration of cognition, can offer to artificial intelligence (AI). The research work proposed in this article is based on the idea that the relevance of hardware, software, and wetware models of biological and cognitive processes—that is, the concrete contribution that these models can make to the scientific understanding of life and cognition—is still unclear, mainly because of the lack of explicit criteria to assess in what ways synthetic models can support the experimental exploration of biological and cognitive phenomena. Our article draws on elements from cybernetic and autopoietic epistemology to define a framework of reference, for the synthetic study of life and cognition, capable of generating a set of assessment criteria and a classification of forms of relevance, for synthetic models, able to overcome the sterile, traditional polarization of their evaluation between mere imitation and full reproduction of the target processes. On the basis of these tools, we tentatively map the forms of relevance characterizing wetware models of living and cognitive processes that synthetic biology can produce and outline a programmatic direction for the development of “organizationally relevant approaches” applying synthetic biology techniques to the investigative field of (embodied) AI. © 2023 Massachusetts Institute of Technology."
"Deep learning has been widely applied in frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems to achieve the accurate and effective channel state information (CSI) feedback. The challenges of the network models applied in real-world systems has also obtained more attention, especially the problem of generalization. In this letter, a model transmission-based online updating approach is proposed to achieve the real-time adaptation of the model and solve the problem of model mismatch when unseen data occurs. The feedback model will be updated using the real-time data with limited overhead, and the updating procedure is designed considering the feedback accuracy, requirement on training data, and storage usage. Experimental results indicate that the proposed approach can achieve quick model adjustment in changing scenarios and achieve comprehensive accuracy with limited training cost and storage usage, contributing to the feasibility of AI-based schemes.  © 1997-2012 IEEE."
"Background: Quality of surgery has substantial impact on both short- and long-term clinical outcomes. This stresses the need for objective surgical quality assessment (SQA) for education, clinical practice and research purposes. The aim of this systematic review was to provide a comprehensive overview of all video-based objective SQA tools in laparoscopic procedures and their validity to objectively assess surgical performance. Methods: PubMed, Embase.com and Web of Science were systematically searched by two reviewers to identify all studies focusing on video-based SQA tools of technical skills in laparoscopic surgery performed in a clinical setting. Evidence on validity was evaluated using a modified validation scoring system. Results: Fifty-five studies with a total of 41 video-based SQA tools were identified. These tools were used in 9 different fields of laparoscopic surgery and were divided into 4 categories: the global assessment scale (GAS), the error-based assessment scale (EBAS), the procedure-specific assessment tool (PSAT) and artificial intelligence (AI). The number of studies focusing on these four categories were 21, 6, 31 and 3, respectively. Twelve studies validated the SQA tool with clinical outcomes. In 11 of those studies, a positive association between surgical quality and clinical outcomes was found. Conclusion: This systematic review included a total of 41 unique video-based SQA tools to assess surgical technical skills in various domains of laparoscopic surgery. This study suggests that validated SQA tools enable objective assessment of surgical performance with relevance for clinical outcomes, which can be used for training, research and quality improvement programs. © 2023, The Author(s)."
"Purpose: One of the recent advances in surgical AI is the recognition of surgical activities as triplets of ⟨ instrument, verb, target⟩. Albeit providing detailed information for computer-assisted intervention, current triplet recognition approaches rely only on single-frame features. Exploiting the temporal cues from earlier frames would improve the recognition of surgical action triplets from videos. Methods: In this paper, we propose Rendezvous in Time (RiT)—a deep learning model that extends the state-of-the-art model, Rendezvous, with temporal modeling. Focusing more on the verbs, our RiT explores the connectedness of current and past frames to learn temporal attention-based features for enhanced triplet recognition. Results: We validate our proposal on the challenging surgical triplet dataset, CholecT45, demonstrating an improved recognition of the verb and triplet along with other interactions involving the verb such as ⟨ instrument, verb⟩. Qualitative results show that the RiT produces smoother predictions for most triplet instances than the state-of-the-arts. Conclusion: We present a novel attention-based approach that leverages the temporal fusion of video frames to model the evolution of surgical actions and exploit their benefits for surgical triplet recognition. © 2023, CARS."
"The novel coronavirus disease, or COVID-19, is a hazardous disease. It is endangering the lives of many people living in more than two hundred countries. It directly affects the lungs. In general, two main imaging modalities, i.e., computed tomography (CT) and chest x-ray (CXR) are used to achieve a speedy and reliable medical diagnosis. Identifying the coronavirus in medical images is exceedingly difficult for diagnosis, assessment, and treatment. It is demanding, time-consuming, and subject to human mistakes. In biological disciplines, excellent performance can be achieved by employing artificial intelligence (AI) models. As a subfield of AI, deep learning (DL) networks have drawn considerable attention than standard machine learning (ML) methods. DL models automatically carry out all the steps of feature extraction, feature selection, and classification. This study has performed comprehensive analysis of coronavirus classification using CXR and CT imaging modalities using DL architectures. Additionally, we have discussed how transfer learning is helpful in this regard. Finally, the problem of designing and implementing a system using computer-aided diagnostic (CAD) to find COVID-19 using DL approaches highlighted a future research possibility. © 2023 Xi'an Jiaotong University"
"Cytokine release syndrome (CRS), also known as cytokine storm, is one of the most consequential adverse effects of chimeric antigen receptor therapies that have shown otherwise promising results in cancer treatment. When emerging, CRS could be identified by the analysis of specific cytokine and chemokine profiles that tend to exhibit similarities across patients. In this paper, we exploit these similarities using machine learning algorithms and set out to pioneer a meta-review informed method for the identification of CRS based on specific cytokine peak concentrations and evidence from previous clinical studies. To this end we also address a widespread challenge of the applicability of machine learning in general: reduced training data availability. We do so by augmenting available (but often insufficient) patient cytokine concentrations with statistical knowledge extracted from domain literature. We argue that such methods could support clinicians in analyzing suspect cytokine profiles by matching them against the said CRS knowledge from past clinical studies, with the ultimate aim of swift CRS diagnosis. We evaluate our proposed methods under several design choices, achieving performance of more than 90% in terms of CRS identification accuracy, and showing that many of our choices outperform a purely data-driven alternative. During evaluation with real-world CRS clinical data, we emphasize the potential of our proposed method of producing interpretable results, in addition to being effective in identifying the onset of cytokine storm. © 2023 The Author(s)"
[No abstract available]
"The past century has witnessed an exponential increase in our atomic-level understanding of molecular and cellular mechanisms from a structural perspective, with multiple landmark achievements contributing to the field. This, coupled with recent and continuing breakthroughs in artificial intelligence methods such as AlphaFold2, and enhanced computational power, is enabling our understanding of protein structure and function at unprecedented levels of accuracy and predictivity. Here, we describe some of the major recent advances across these fields, and describe, as these technologies coalesce, the potential to utilise our enhanced knowledge of intricate cellular and molecular systems to discover novel therapeutics to alleviate human suffering. © 2023"
"Despite great academic interest in global green supply chain management (GSCM) practices, its effectiveness for environmental management systems (EMS) and market competitiveness during COVID-19 remains untapped. Existing literature suggests that a fundamental link between GSCM, EMS, and market competitiveness is missing, as supply management is critical to maintain market competitiveness. To fill this gap in the literature, this study examines whether environmental management systems influence the link between GSCM practice and market competitiveness in China. We also propose the articulating role of big data analytics and artificial intelligence (BDA-AI) and environmental visibility toward these associations in the context of the COVID-19 pandemic. We evaluated the proposed model using regression-based structural equation modeling (SEM) with primary data (n = 330). This result provides empirical evidence of the impact of GSCM on EMS and market competitiveness. Moreover, the results show that the BDA-AI and the environmental visibility enhanced the positive relationship between GSCM-EMS and EMS and market competitiveness in China. Recent research shows that supply chain professionals, policymakers, managers, and researchers are turning to formal EMS, BDA-AI, and environmental visibility to help their organizations achieve the competitiveness that the market indicates they need. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"This article presents an X -band single-chip fully integrated multifrequency difference frequency-shift keying (FSK) in-phase and quadrature (I/Q) direct-conversion radar transceiver implemented in a 130-nm CMOS process. Combined with the artificial intelligence (AI)-based algorithm, accurate gesture recognition is realized. The phase-locked loop (PLL)-based dual-frequency difference FSK waveform and ranging algorithm are proposed to solve the tradeoff between the maximum unambiguous range and the ranging accuracy. The loop bandwidth optimized integer- N PLL is integrated to generate standard-compliant FSK signals with predictable frequency differences. The relationship between instantaneous ranging bias and PLL loop parameters is analyzed in detail. The proposed bias noise filter fully suppresses the flicker noise and improves the receiver noise figure (NF). The quadrature signal generator (QSG) adopts a fully differential amplifier buffered resistance compensation quadrature all-pass filter (QAF) to improve the I/Q balance and insertion loss. The chip consumes a current of 56 mA and provides a conversion gain (CG) of 22-102 dB. The single-ended transmit power is 1.5 dBm. To the best of our knowledge, the implemented transceiver is the first recorded X -band FSK radar transceiver and provides a low-cost solution for indoor human detection.  © 2001-2012 IEEE."
"Artificial Intelligence (AI) uses systems and machines to simulate human intelligence and solve common real-world problems. Machine learning and deep learning are Artificial intelligence technologies that use algorithms to predict outcomes more accurately without relying on human intervention. However, the opaque black box model and cumulative model complexity can be used to achieve. Explainable Artificial Intelligence (XAI) is a term that refers to Artificial Intelligence (AI) that can provide explanations for their decision or predictions to human users. XAI aims to increase the transparency, trustworthiness and accountability of AI system, especially when they are used for high-stakes application such as healthcare, finance or security. This paper offers systematic literature review of XAI approaches with different application and observes 91 recently published articles describing XAI development and applications in healthcare, manufacturing, transportation, and finance. We investigated the Scopus, Web of Science, IEEE Xplore and PubMed databases, to find the pertinent publications published between January 2018 to October 2022. It contains the published research on XAI modelling that were retrieved from scholarly databases using pertinent keyword searches. We think that our systematic review extends to the literature on XAI by working as a roadmap for further research in the field. © 2023"
"In this communication, an artificial intelligent method based on the prevailing multihead attention mechanism for prediction of scattering properties of 2-D targets is presented. To make the predicting approach independent of the incident direction of an excitation plane wave, a kind of inherent feature parameters (IFPs) for a specific target is defined and applied as the output of the artificial neural network (ANN). Two types of targets are experimented, one of which is composed of polygons and the other of smooth shapes. Numerical results show that the proposed method has satisfactory prediction accuracy and computing speed, as well as good generalization ability.  © 1963-2012 IEEE."
"Objectives: To navigate the field of digital cancer care and define and discuss key aspects and applications of big data analytics, artificial intelligence (AI), and data-driven interventions. Data Sources: Peer-reviewed scientific publications and expert opinion. Conclusion: The digital transformation of cancer care, enabled by big data analytics, AI, and data-driven interventions, presents a significant opportunity to revolutionize the field. An increased understanding of the lifecycle and ethics of data-driven interventions will enhance development of innovative and applicable products to advance digital cancer care services. Implications for Nursing Practice: As digital technologies become integrated into cancer care, nurse practitioners and scientists will be required to increase their knowledge and skills to effectively use these tools to the patient's benefit. An enhanced understanding of the core concepts of AI and big data, confident use of digital health platforms, and ability to interpret the outputs of data-driven interventions are key competencies. Nurses in oncology will play a crucial role in patient education around big data and AI, with a focus on addressing any arising questions, concerns, or misconceptions to foster trust in these technologies. Successful integration of data-driven innovations into oncology nursing practice will empower practitioners to deliver more personalized, effective, and evidence-based care. © 2023 Elsevier Inc."
"Introduction: The research described in this paper explored the factors contributing to the injury severity resulting from the male and female older driver (65 years and older) at-fault crashes at unsignalized intersections in Alabama. Method: Random parameter logit models of injury severity were estimated. The estimated models identified a variety of statistically significant factors influencing the injury severities resulting from older driver at-fault crashes. Results: According to these models, some variables were found to be significant only in one model (male or female) but not in the other one. For example, variables such as driver under the influence of alcohol/drugs, horizontal curve, and stop sign were found significant only in the male model. On the other hand, variables such as intersection approaches on tangents with flat grade, and driver older than 75 years were found significant only in the female model. In addition, variables such as making turning maneuver, freeway-ramp junction, high speed approach, and so forth were found significant in both models. Estimation findings showed that two parameters in the male model and another two parameters in the female model could be modeled as random parameters, indicating their varying influences on the injury severity due to unobserved effects. In addition to the random parameter logit approach, a deep learning approach based on Artificial Neural Networks was introduced to predict the outcome of the crashes based on 164 variables that are listed in the crash database. The artificial intelligence (AI)-based method achieved an accuracy of 76% indicating the role of the variables in deciding the final outcome. Practical Applications: Future plans are set to study the use of AI on large sized datasets to achieve a relatively high-performance, and hence to be able to identify which variables contribute the most to the final outcome. © 2023 National Safety Council and Elsevier Ltd"
"To provide direction and advice for future research on Industry 4.0 maintenance, we conducted a comprehensive analysis of 344 eligible journal papers published between 2013 and 2022. Our systematic literature review identifies key trends in advanced maintenance techniques and the consolidation of traditional maintenance concepts, which are driven by the increasing adoption of Industry 4.0 technologies and the need to optimize manufacturing systems' performance and reliability. In light of our findings, we highlight the importance of addressing sustainability factors, human aspects, and the implementation of environmental KPIs in future research. Building upon these insights, we introduce the Maintenance 5.0 framework, which emphasizes the integration of human-centered and AI-driven strategies for achieving efficient and sustainable maintenance in Zero-Defect Manufacturing (ZDM) systems. We propose a novel framework that links traditional and advanced maintenance policies for small and medium-sized enterprises (SMEs) to facilitate the adoption of Industry 4.0 technologies in the maintenance field. This work underscores the need for future research to bridge the gap between these policies, enabling a seamless transition for SMEs towards Industry 4.0 maintenance practices, while fostering sustainable and socially responsible operations. © 2023 The Authors"
"The uncertainty coming from various sources will have a considerable influence on the credibility of the structural performance evaluation of tall buildings. This paper proposes a hybrid AI-Bayesian-based methodology for fragility estimates of tall buildings subjected to simultaneous earthquake and wind events that is feasible to incorporate into both the epistemic and aleatory uncertainties. The main contributions and concept of this proposed methodology include (1) The Back Propagation (BP) Artificial Neural Network (ANN) technique is applied to train a surrogate model to replace finite element (FE) analysis of tall buildings under concurrent seismic and wind excitations, which can highly reduce computing time of nonlinear dynamic analyses and is beneficial for quantifying the aleatory uncertainty with material properties and structural characteristics. (2) A physics-based demand model is developed for fragility estimates, and the Bayesian statistics method is utilized to obtain the posterior probability distributions of the unknown parameters in the demand model, which is beneficial for quantifying the epistemic uncertainty in the fragility estimates. Finally, this proposed method is implemented in a representative composite tall building. The application highlights the importance of incorporating both the epistemic and aleatory uncertainties into the fragility estimates of tall buildings under multiple hazards. This AI-Bayesian-based method provides great help for quantifying various uncertainties and improving resilience assessment reliability of tall buildings under multiple hazards. © 2023 Elsevier Ltd"
[No abstract available]
"Unsupervised learning algorithms are widely used for many important statistical tasks with numerous applications in science and industry. Yet despite their prevalence, they have attracted remarkably little philosophical scrutiny to date. This stands in stark contrast to supervised and reinforcement learning algorithms, which have been widely studied and critically evaluated, often with an emphasis on ethical concerns. In this article, I analyze three canonical unsupervised learning problems: clustering, abstraction, and generative modeling. I argue that these methods raise unique epistemological and ontological questions, providing data-driven tools for discovering natural kinds and distinguishing essence from contingency. This analysis goes some way toward filling the lacuna in contemporary philosophical discourse on unsupervised learning, as well as bringing conceptual unity to a heterogeneous field more often described by what it is not (i.e., supervised or reinforcement learning) than by what it is. I submit that unsupervised learning is not just a legitimate subject of philosophical inquiry but perhaps the most fundamental branch of all AI. However, an uncritical overreliance on unsupervised methods poses major epistemic and ethical risks. I conclude by advocating for a pragmatic, error-statistical approach that embraces the opportunities and mitigates the challenges posed by this powerful class of algorithms. © 2023, The Author(s)."
"Cannabis use disorder (CUD) is common and has in part a genetic basis. The risk factors underlying its development likely involve multiple genes that are polygenetic and interact with each other and the environment to ultimately lead to the disorder. Co-morbidity and genetic correlations have been identified between CUD and other disorders and traits in select populations primarily of European descent. If two or more traits, such as CUD and another disorder, are affected by the same genetic locus, they are said to be pleiotropic. The present study aimed to identify specific pleiotropic loci for the severity level of CUD in three high-risk population cohorts: American Indians (AI), Mexican Americans (MA), and European Americans (EA). Using a previously developed computational method based on a machine learning technique, we leveraged the entire GWAS catalog and identified 114, 119, and 165 potentially pleiotropic variants for CUD severity in AI, MA, and EA respectively. Ten pleiotropic loci were shared between the cohorts although the exact variants from each cohort differed. While majority of the pleiotropic genes were distinct in each cohort, they converged on numerous enriched biological pathways. The gene ontology terms associated with the pleiotropic genes were predominately related to synaptic functions and neurodevelopment. Notable pathways included Wnt/β-catenin signaling, lipoprotein assembly, response to UV radiation, and components of the complement system. The pleiotropic genes were the most significantly differentially expressed in frontal cortex and coronary artery, up-regulated in adipose tissue, and down-regulated in testis, prostate, and ovary. They were significantly up-regulated in most brain tissues but were down-regulated in the cerebellum and hypothalamus. Our study is the first to attempt a large-scale pleiotropy detection scan for CUD severity. Our findings suggest that the different population cohorts may have distinct genetic factors for CUD, however they share pleiotropic genes from underlying pathways related to Alzheimer's disease, neuroplasticity, immune response, and reproductive endocrine systems. © 2023 Elsevier Inc."
"Objective. A major challenge in designing closed-loop brain-computer interfaces is finding optimal stimulation patterns as a function of ongoing neural activity for different subjects and different objectives. Traditional approaches, such as those currently used for deep brain stimulation, have largely followed a manual trial-and-error strategy to search for effective open-loop stimulation parameters, a strategy that is inefficient and does not generalize to closed-loop activity-dependent stimulation. Approach. To achieve goal-directed closed-loop neurostimulation, we propose the use of brain co-processors, devices which exploit artificial intelligence to shape neural activity and bridge injured neural circuits for targeted repair and restoration of function. Here we investigate a specific type of co-processor called a ‘neural co-processor’ which uses artificial neural networks and deep learning to learn optimal closed-loop stimulation policies. The co-processor adapts the stimulation policy as the biological circuit itself adapts to the stimulation, achieving a form of brain-device co-adaptation. Here we use simulations to lay the groundwork for future in vivo tests of neural co-processors. We leverage a previously published cortical model of grasping, to which we applied various forms of simulated lesions. We used our simulations to develop the critical learning algorithms and study adaptations to non-stationarity in preparation for future in vivo tests. Main results. Our simulations show the ability of a neural co-processor to learn a stimulation policy using a supervised learning approach, and to adapt that policy as the underlying brain and sensors change. Our co-processor successfully co-adapted with the simulated brain to accomplish the reach-and-grasp task after a variety of lesions were applied, achieving recovery towards healthy function in the range 75%-90%. Significance. Our results provide the first proof-of-concept demonstration, using computer simulations, of a neural co-processor for adaptive activity-dependent closed-loop neurostimulation for optimizing a rehabilitation goal after injury. While a significant gap remains between simulations and in vivo applications, our results provide insights on how such co-processors may eventually be developed for learning complex adaptive stimulation policies for a variety of neural rehabilitation and neuroprosthetic applications. © 2023 The Author(s). Published by IOP Publishing Ltd."
"Marine plastic pollution is a pressing global issue nowadays. To address this problem, automated image analysis techniques that can identify plastic litter are necessary for scientific research and coastal management purposes. The Beach Plastic Litter Dataset version 1 (BePLi Dataset v1) comprises 3709 original images taken in various coastal environments, along with instance-based and pixel-level annotations for all plastic litter objects visible in the images. The annotations were compiled in the Microsoft Common Objects in Context (MS COCO) format, which was partially modified from the original format. The dataset enables the development of machine-learning models for instance-level and/or pixel-wise identification of beach plastic litter. All original images in the dataset were extracted from beach litter monitoring records operated by the local government of Yamagata Prefecture in Japan. Litter images were taken in different backgrounds, such as sand beaches, rocky beaches, and tetrapods. The annotations for instance segmentation of beach plastic litter were made manually, and were given for all plastics objects, including PET bottles, containers, fishing gear, and styrene foams,all of which were categorized in a single class “plastic litter”. Technologies developed using this dataset have the potential to enable further scalability for the estimation of plastic litter volume. This would help researchers, including individuals, and the the government to monitor or analyze beach litter and the corresponding pollution levels. © 2023 The Author(s)"
"In recent decades, we have witnessed great advances on the Internet of Things, mobile devices, sensor-based systems, and resulting big data infrastructures, which have gradually, yet fundamentally influenced the way people interact with and in the digital and physical world. Many human activities now not only operate in geographical (physical) space but also in cyberspace. Such changes have triggered a paradigm shift in geographic information science (GIScience), as cyberspace brings new perspectives for the roles played by spatial and temporal dimensions, e.g., the dilemma of placelessness and possible timelessness. As a discipline at the brink of even bigger changes made possible by machine learning and artificial intelligence, this paper highlights the challenges and opportunities associated with geographical space in relation to cyberspace, with a particular focus on data analytics and visualization, including extended AI capabilities and virtual reality representations. Consequently, we encourage the creation of synergies between the processing and analysis of geographical and cyber data to improve sustainability and solve complex problems with geospatial applications and other digital advancements in urban and environmental sciences. © 2023 The Authors"
"This research focuses on studying low-cost techniques for rapid mapping, utilizing sensors equipped on smartphones. These devices were installed on a radio-controlled vehicle to conduct an experimental campaign aimed at evaluating the performance of LiDAR sensor. By collecting data, machine learning algorithms were employed for the detection of architectural defects. © 2023, The Author(s)."
[No abstract available]
"Combinatorial synthesis of solid-state materials comprises the use of automation or parallelization to systematically vary synthesis parameters. This approach to materials synthesis is a natural fit for accelerated mapping of composition–structure–property relationships, a central tenet of materials research. By considering combinatorial synthesis in the context of experimental workflows, we envision a future for accelerated materials science promoted by the co-development of combinatorial synthesis and artificial intelligence (AI) techniques. To evaluate the suitability of a synthesis technique for a given experimental workflow, we establish a collection of ten metrics spanning speed, scalability, scope and quality of synthesis. We summarize select combinatorial synthesis techniques in the context of these metrics, elucidating opportunities for further development. These opportunities span initial deployment in high-throughput experimentation through to seminal demonstrations of automated decision-making using AI. Historical analysis of combinatorial synthesis in the context of the Gartner hype cycle establishes a recent rise in productivity, indicating that the field is poised to realize accelerated materials science workflows that transform materials discovery and development. [Figure not available: see fulltext.] © 2023, Springer Nature Limited."
"In the last decade, new design-driven approaches, such as Design Thinking, Customer Discovery, and Lean Start-up, have gained popularity in entrepreneurship education (EE). However, their adoption has been characterised by confusion in understanding their theoretical underpinnings and the challenge of introducing these new methods into a pedagogic culture emphasising ideation over experience, emotional intelligence, and making. This article argues that the implementation of these new pedagogic approaches can be improved by better translating the principles of design-driven and artifact-centered entrepreneurship into pedagogical practices. To achieve this goal, a model for a pedagogy of making in EE is proposed along with theoretical and economic arguments based on recent advances in the debate on entrepreneurship as a design science, the growing importance of intangibles in the economy, and the challenges of artificial intelligence (AI) to the job market and student employability. The critical elements for successfully adopting such pedagogy and common misconceptions that can hinder its full deployment are outlined. © The Author(s) 2023."
"To improve the catalytic performance of Bi2WO6, a dual S-scheme BiOI/AgI/Bi2WO6 ternary heterojunction photocatalyst was successfully constructed. Under visible light irradiation, 93.6% of 2,4-dinitrophenylhydrazine (2,4-DNPH) was removed by BOI/AI/BWO within 30 min, and the apparent rate constant was 30 times that of pure Bi2WO6. In addition, the methyl orange (MO) and tetracycline (TC) degradation rates were 87% and 94%, respectively. Therefore, BOI/AI/BWO exhibited more remarkable photocatalytic performance for the elimination of organic pollutants and aqueous solution containing antibiotics. Cyclic voltammetry test indicated that ternary heterojunction photocatalyst can able to provide more active surface area. Quenching experiments and electron paramagnetic resonance (EPR) analysis revealed that ∙O2− and h+ are main active species. Characterization test and mechanism analysis indicated that excellent photocatalytic performance of ternary heterostructure was ascribed to dual S-scheme charge transfer mechanism leaded to efficient photoexcited carrier separation and strong redox capacity. This research could provide a new strategy to design high-efficiency dual S-scheme photocatalyst for environmental remediation. © 2023 Elsevier B.V."
"Albeit general object detection has made impressive progress in the last decade, as a significant subfield, small object detection still performs far from satisfactorily, which is impeded by several challenges, such as small size, severe occlusion and variant scales. To tackle these challenges, we propose a coarse-to-fine small object detection method leveraging density-aware scale adaptation. Firstly, we employ global sketchy prediction via a coarse network in large scenes and generate adaptively scaled block regions with potential targets. Subsequently, we perform local accurate detection by a fine network for instances in densely packed areas with approximately unified scales. In particular, a density map with object distribution information is utilised to provide a scene classification auxiliary to instruct scale transformation. Extensive experiments on the popular remote sensing benchmark AI-TOD and representative small object datasets VisDrone and UAVDT demonstrate the superiority of our method for small object detection, achieving an improvement of 2.9% mAP-vt and 2.1% mAP on AI-TOD, and outperforming the state-of-the-art methods on VisDrone and UAVDT with an enhancement of 1.7% mAP and 2.0% mAP50, respectively. © 2023 Remote Sensing and Photogrammetry Society and John Wiley & Sons Ltd."
"Neural networks (NN) have shown promising performance in point cloud segmentation (PCS). However, the measured points are too numerous to be used as model input at once. It results in a long inference time and high computational cost due to iterative sampling and inference. This study proposes Probability Propagation (PP) as a stochastic upsampling method. PP propagates the predicted probability of a sampled part of a point cloud into the other unpredicted points by considering proximity. By replacing the iterative inference of NN with PP, large point clouds can be dealt with quickly and efficiently. We investigated the effectiveness of PP using the ShapeNet benchmark on various settings: sampling methods (random, farthest point, and Poisson disk sampling) with sampling ratios (5%, 10%, 20%, 39%, and 78%) for NN and the stochastic mapping conditions (uniform, linear, cosine, Gaussian, and exponential distributions) for PP. Using NN with PP achieved higher performance and faster inference speed than when using NN alone. For the farthest point sampling method of 5% sampling ratio, NN+PP improved the instance mIoU by 2.457%p with 102 times faster speed compared to that when using NN alone. The result indicates that PP can significantly contribute to the improvement of performance and efficiency in PCS when used in edge AI systems. © 2023"
"Background: With increased need for vascular surgery trainees to gain endovascular surgery proficiency, current models of case-numbers and subjective visual assessment are inadequate in capturing the skills required in endovascular surgery. We explored the use of high-fidelity simulators in (1) assessing endovascular surgical competence; (2) clinical decision making; and (3) the reliability of an artificial intelligence (AI) assessor. Methods: Registrars, fellows and consultants from vascular surgery, interventional radiology and general surgery performed identical procedures on a high-fidelity simulator. Performance was independently assessed using a modified Reznick scale. Scores were compared to raw metric data extracted from the simulator, objective scores extracted from the recordings and analysed by AI. Results: 22 participants were enrolled from vascular surgery (n = 6, 27.3%), interventional radiology (n = 10, 45.5%) and general surgery (n = 6, 27.3%). There were 12 trainees, 2 fellows and 8 consultants. Significant correlations between raw metric data and all categories of the modified Reznick scale except ‘respect for tissue’ were found. An AI demonstrated positive reliability in all categories, with some predictions being moderately correlated. Conclusion: The use of high-fidelity simulators to assess endovascular surgical competence has comparable correlations to the traditional assessment methods with global rating scales, which can be used in formative assessment. AI demonstrates an ability to support assessment but requires further research. © 2023 Royal Australasian College of Surgeons."
"To explore the possibilities of artificial intelligence (AI) text-to-picture system, DALL·E 2 was used to generated clinical photographs for medical and plastic surgery education. Generic English text was used to guide AI in three categories: subcutaneous tumor, wound and skin tumor. The most clinically accurate images were chosen for the article or for further editing. AI-generated images with variating clinical accuracy in different categories. The most accurate images were the soft-tissue tumors and the least accurate wounds. This study showed that AI text-to-picture system might be worthy tool for medical education. © 2023 British Association of Plastic, Reconstructive and Aesthetic Surgeons"
"Objectives: ChatGPT, a tool based on natural language processing (NLP), is on everyone's mind, and several potential applications in healthcare have been already proposed. However, since the ability of this tool to interpret laboratory test results has not yet been tested, the EFLM Working group on Artificial Intelligence (WG-AI) has set itself the task of closing this gap with a systematic approach. Methods: WG-AI members generated 10 simulated laboratory reports of common parameters, which were then passed to ChatGPT for interpretation, according to reference intervals (RI) and units, using an optimized prompt. The results were subsequently evaluated independently by all WG-AI members with respect to relevance, correctness, helpfulness and safety. Results: ChatGPT recognized all laboratory tests, it could detect if they deviated from the RI and gave a test-by-test as well as an overall interpretation. The interpretations were rather superficial, not always correct, and, only in some cases, judged coherently. The magnitude of the deviation from the RI seldom plays a role in the interpretation of laboratory tests, and artificial intelligence (AI) did not make any meaningful suggestion regarding follow-up diagnostics or further procedures in general. Conclusions: ChatGPT in its current form, being not specifically trained on medical data or laboratory data in particular, may only be considered a tool capable of interpreting a laboratory report on a test-by-test basis at best, but not on the interpretation of an overall diagnostic picture. Future generations of similar AIs with medical ground truth training data might surely revolutionize current processes in healthcare, despite this implementation is not ready yet.  © 2023 Walter de Gruyter GmbH, Berlin/Boston."
"Climatic changes, sudden or gradual, influence the structural health of buildings and bridges due to variations in temperature and humidity. Risk and disaster management plays a vital role in the decision-making process for safeguarding structures. Data analytics from sensors systems in smart structures aid in taking appropriate action in securing buildings during natural calamities. The correlation between climate and structural measuring responses can be further improved using artificial intelligence (AI)- machine learning (ML) algorithms to monitor and predict structural health and take any precautionary steps before the event of a casualty. Linear regression is an efficient tool for analyzing structural health. The proposed work's objective is to monitor and predict the structural health and inform the concerned authorities in the event of a failure in advance, using AI-ML approaches. We have analyzed various sensor data sets to predict the health of a structure based on the crack developed. From the data obtained for experimentation, mean width of the crack is observed as 2.38 cm and mean length of the crack is 63.36 cm. © 2023 The Authors"
"This study examined the modelling and optimisation of the electrocoagulation-flocculation (ECF) recovery of aquaculture effluent (AQE) using aluminium electrodes. The response surface methodology (RSM), artificial neural network (ANN), and adaptive neuro-fuzzy inference system (ANFIS) were used for the modelling, while the optimisation tools were the numerical RSM and genetic algorithm (GA). Furthermore, the kinetics of the ECF process was studied to provide insight into the mechanism governing the ECF of AQE. The experimental design was performed using the central composite design (CCD) of the RSM. The ANFIS modelling was accomplished via the Grid Partition (GP) of the data set, while the ANN used the multi-layer perceptron (MLP) based feed-forward system. Statistically, the prediction accuracy of the models followed the order: ANFIS (R 2: 0.9990), ANN (R 2: 0.9807), and RSM (R 2: 0.9790). The process optimisation gave optimal turbidity (TD) removal efficiencies of 98.98, 97.81, and 96.01% for ANFIS-GA, ANN-GA, and RSM optimisation techniques, respectively. The ANFIS-GA gave the best optimization result at optimum conditions of pH 4, current intensity (3 A), electrolysis time (7.2 min), settling time (23 min), and temperature (43.8 °C). In the kinetics study, the experimental data was analysed using pseudo-first-order (0.8787), pseudo-second-order (0.9395), and Elovich (R 2: 0.9979) kinetic models; the Elovich model gave the best correlation with the experimental data showing that the process is governed by electrostatic interaction mechanism. This study effectively demonstrated that ECF recovery of AQE can effectively be modelled using RSM, ANN, and ANFIS and be optimised using RSM, ANN-GA, and ANFIS-GA techniques, and the order of performance is ANFIS > ANN > RSM and ANFIS-GA > ANN-GA > RSM, respectively. Graphical abstract: [Figure not available: see fulltext.] © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Background: Current treatments for lung cancer have their own deficiencies, such as severe adverse effect. Therefore, more safe and effective drugs are needed. Purpose: Fuzheng Kang-Ai (FZKA for short) has been applied as an adjuvant treatment in advanced Non-Small Cell Lung Cancer (NSCLC) patients for decades in China, showing a definitive effect with minimal toxicities. However, the underlying mechanism is yet to be identified. Study design: Both in vitro and in vivo experiments were performed in this study to identify the exact mechanism by which FZKA inhibits NSCLC cell proliferation. Methods: MTT and CCK-8 assays were used to detect cell viability. Xenograft model was performed for in vivo experiments. CircRNA and miRNA sequencing were used to find the differentially expressed circRNAs and miRNAs, respectively. qRT-PCR was performed to check the expression levels of circRNA, miRNA and mRNA. BaseScope was carried out to observe the expression of circRNA in situ. Actinomycin D and RNase R experiments were done to show the stability of circRNA. Nuclear-cytoplasmic fractionation and FISH were used to identify the localization of circRNA and miRNA. Pull-down, RIP, and luciferase activity assays were performed to show the biding ability of circRNA, miRNA and target proteins. Flow cytometry was done to observe cell apoptosis. Western blot and IHC were done to detect the protein expression. TCGA database was used to analyze the survival rate. Results: FZKA inhibits NSCLC cell proliferation both in vitro and in vivo. Hsa_circ_0048091 and hsa-miR-378g were the most differentially expressed circRNA and miRNA, respectively, after FZKA treatment. Silencing hsa_circ_0048091 and overexpressing hsa-miR-378g promoted cell proliferation and reversed the inhibition effect of FZKA on NSCLC, respectively. Hsa-miR-378g was sponged by hsa_circ_0048091, and the overexpression of miR-378g reversed the inhibition effect of hsa_ circ_0048091 on NSCLC. ARRDC3, as a target of hsa-miR-378g, was increased by FZKA treatment. Silencing ARRDC3 reversed both the inhibition effect of FZKA and miR-378g inhibitor on NSCLC. Conclusion: This study, for the first time, has established the function of hsa_circ_0048091, hsa- miR-378g, and ARRDC3 in lung cancer. It also shows that FZKA inhibits NSCLC cell proliferation through hsa_circ_0048091/hsa-miR-378g/ARRDC3 pathway, uncovering a novel mechanism by which FZKA controls human NSCLC cell growth. © 2023 The Authors"
"This paper reports on the first international competition on AI for the traveling salesman problem (TSP) at the International Joint Conference on Artificial Intelligence 2021 (IJCAI-21). The TSP is one of the classical combinatorial optimization problems, with many variants inspired by real-world applications. This first competition asked the participants to develop algorithms to solve an orienteering problem with stochastic weights and time windows (OPSWTW). It focused on two learning approaches: surrogate-based optimization and deep reinforcement learning. In this paper, we describe the problem, the competition setup, and the winning methods, and give an overview of the results. The winning methods described in this work have advanced the state-of-the-art in using AI for stochastic routing problems. Overall, by organizing this competition we have introduced routing problems as an interesting problem setting for AI researchers. The simulator of the problem has been made open-source and can be used by other researchers as a benchmark for new learning-based methods. The instances and code for the competition are available at https://github.com/paulorocosta/ai-for-tsp-competition. © 2023 The Author(s)"
"Let p be an odd prime. If an integer g generates a subgroup of index t in (Z/ pZ) ∗, then we say that g is a t-near primitive root modulo p. In this paper, for a subset { a1, a2, ⋯ , an} of Z\ { - 1 , 0 , 1 } , we prove each coprime residue class contains a positive density of primes p not having ai as a t-near primitive root and with the ai satisfying a prescribed residue pattern modulo p, for 1 ≤ i≤ n. We also prove a more refined variant of it. © 2023, Indian Academy of Sciences."
"How does ChatGPT, and other forms of Generative Artificial Intelligence (GenAI) affect the way we have been conducting—and evaluating—academic research, teaching, and business practice? What are the implications for the theory and practice of marketing? What are the opportunities and threats, and what are some interesting avenues for future research? This editorial aims to kick off an initial discussion and stimulate research that will help us better understand how the marketing field can fully exploit the potential of GenAI and effectively cope with its challenges. © 2023 The Author(s)"
"Coronavirus Disease 2019 (COVID-19), which is caused by Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-COV-2), surprised the world in December 2019 and has threatened the lives of millions of people. Countries all over the world closed worship places and shops, prevented gatherings, and implemented curfews to stand against the spread of COVID-19. Deep Learning (DL) and Artificial Intelligence (AI) can have a great role in detecting and fighting this disease. Deep learning can be used to detect COVID-19 symptoms and signs from different imaging modalities, such as X-Ray, Computed Tomography (CT), and Ultrasound Images (US). This could help in identifying COVID-19 cases as a first step to curing them. In this paper, we reviewed the research studies conducted from January 2020 to September 2022 about deep learning models that were used in COVID-19 detection. This paper clarified the three most common imaging modalities (X-Ray, CT, and US) in addition to the DL approaches that are used in this detection and compared these approaches. This paper also provided the future directions of this field to fight COVID-19 disease. © 2023, The Author(s)."
"While the applicable legal framework is often identified as one of the key factors in the success or failure of open government data (OGD), the concrete impact of ‘OGD law’ on actual practices of OGD is often overlooked or hardly addressed in-depth. This contribution therefore aims to disentangle this legal impact based on an AI-driven systematic literature review combining legal and public administration (PA) publications. First, the review shows that OGD law has many faces and cannot be reduced to one single piece of ‘OGD legislation’. Instead, OGD law covers a wide range of topics, dealing with access to information, re-use of information, and conflicting interests (e.g. privacy or copyright). Secondly, the article identifies three main dimensions that structure the assessment of the impact of OGD law on OGD practices: topics of OGD law, sources of OGD law, and levels of OGD law. Finally, the review shows that there is no clear and univocal evidence to answer the question of what regulatory constellation is successful in fostering OGD practices and what is not, partly due to a lack of available empirical research. At the same time, the literature reveals some promising avenues for future research on OGD law in action. © 2023"
"Gray-level co-occurrence matrix (GLCM) and discrete wavelet transform (DWT) analyses are two contemporary computational methods that can identify discrete changes in cell and tissue textural features. Previous research has indicated that these methods may be applicable in the pathology for identification and classification of various types of cancers. In this study, we present findings that squamous epithelial cells in laryngeal carcinoma, which appear morphologically intact during conventional pathohistological evaluation, have distinct nuclear GLCM and DWT features. The average values of nuclear GLCM indicators of these cells, such as angular second moment, inverse difference moment, and textural contrast, substantially differ when compared to those in noncancerous tissue. In this work, we also propose machine learning models based on random forests and support vector machine that can be successfully trained to separate the cells using GLCM and DWT quantifiers as input data. We show that, based on a limited cell sample, these models have relatively good classification accuracy and discriminatory power, which makes them suitable candidates for future development of AI-based sensors potentially applicable in laryngeal carcinoma diagnostic protocols.  © 2023 The Author(s). Published by Oxford University Press on behalf of the Microscopy Society of America. All rights reserved."
"Each year, one million Cryptococcal infections occur among people living with HIV/AIDS, resulting in nearly 625,000 fatalities due to low efficiency of clinical diagnostic procedures and misdiagnosis caused by faulty subjective judgment of manually enumerated imaging-based infection assays. In order to improve the diagnostic efficiency of non-immunodeficient patients infected with Cryptococcus, the present study developed an intelligent diagnostic system combined with a switch-controllable nanocatcher (MNP@PNIPAMAA-CAS) with high capture performance for Cryptococcus and convolutional neural network (CNN)-based artificial intelligence (AI), which was utilized to analyze the collected pathological images for subsequent diagnosis. This system benefitted from the high adsorption efficiency of the MNP@PNIPAMAA-CAS and the objective data processing ability of AI. The system's detection limit was 1–4 cells /mL, the specificity was 97%, and the detection time was only 6 min. The intelligent diagnostic system will provide significant benefit to the diagnosis of clinical fungi due to its high sensitivity, high specificity, and superfast detection speed. © 2023 Elsevier B.V."
"In this article, we argue two points in relation to the challenge to human distinctiveness emerging as artificial intelligence systems and humanlike robots simulate various human capabilities. First, that, in the context of theological anthropology, it is advisable to respond to this challenge by turning toward the human body. Second, following this point, we propose the responsive body hypothesis, suggesting that what makes us distinct from androids are capacities that rise from and depend on our responsive bodies. © 2023 The Authors. Zygon® published by Wiley Periodicals LLC on behalf of Joint Publication Board of Zygon."
"The outbreak of COVID-19 (also known as Coronavirus) has put the entire world at risk. The disease first appears in Wuhan, China, and later spread to other countries, taking a form of a pandemic. In this paper, we try to build an artificial intelligence (AI) powered framework called Flu-Net to identify flu-like symptoms (which is also an important symptom of Covid-19) in people, and limit the spread of infection. Our approach is based on the application of human action recognition in surveillance systems, where videos captured by closed-circuit television (CCTV) cameras are processed through state-of-the-art deep learning techniques to recognize different activities like coughing, sneezing, etc. The proposed framework has three major steps. First, to suppress irrelevant background details in an input video, a frame difference operation is performed to extract foreground motion information. Second, a two-stream heterogeneous network based on 2D and 3D Convolutional Neural Networks (ConvNets) is trained using the RGB frame differences. And third, the features extracted from both the streams are combined using Grey Wolf Optimization (GWO) based feature selection technique. The experiments conducted on BII Sneeze-Cough (BIISC) video dataset show that our framework can 70% accuracy, outperforming the baseline results by more than 8%. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Hormone-free (HF) reproduction in dairy sheep is a way to meet current societal demands, but it requires being prepared for collateral impacts on related system components. The efficiency of HF practices (e.g., using the male effect for estrus induction and synchronization) is uncertain compared with hormonal treatment (HT). For example, these practices can lead to higher variability in the flock physiological stage patterns throughout the year, which has direct consequences for feeding regimens. The objective of this work was to simulate the impacts of HF reproduction management, including artificial insemination (AI), on the temporal distribution of productive performance and nutritional requirements of a conventional dairy sheep flock. Using the REPROsheep2.0 model, 6 scenarios were compared over one typical production season for the same flock (n = 597 Lacaune ewes) intensively reared in the Roquefort region of France. These scenarios depicted reproduction with HT and AI in mid-May (Early); HT and AI in July (Summer Late); HT and AI in November (Autumn Late); and their HF versions (HF-Early; HF-Summer Late, and HF-Autumn Late, respectively). In all HF scenarios, a reduction in the number of ewes lambing and consequently in the annual milk production of the farm was observed (−1 to −7%). This affected annual performance with a subsequent decrease of total annual nutritional requirements (−2 to −6%). The HF scenarios resulted in a staggering of lambing events with a 7- to 14-d shift in the appearance of milk production peaks and related nutritional requirements compared with the HT scenarios. Transitioning from conventional to HF reproduction management, while preserving AI, would increase farm workload, lengthen milking period operations, and necessitate a readjustment of feeding management strategies with regard to available feed resources. Depending on the production season, the observed delay in the distribution of nutritional requirements could be either an attractive or an unfavorable outcome for farmers. The delay may be concordant, for example, with the recently observed impacts of climate change on seasonal forage availability in Mediterranean regions (less spring herbage production and warmer temperatures) that are affecting farmers' decision-making about the most efficient use of forage and feed resources. © 2023 American Dairy Science Association"
[No abstract available]
Background The thrombolytic effect for ischemic stroke（IS） is affected by complex factors，such as acute onset of stroke，short therapeutic time window，various individual patient factors，treatment model，types and doses of medicines as well as mode of administration. To identify the influencing factors of thrombolytic effect，most existing studies adopt statistical methods，while rare studies use artificial intelligence（AI）-based algorithms. Objective To establish models using AI-based algorithms for IS patients based on the real-world data including general patient characteristics，medication model and recovery effects，to achieve precise individualized thrombolytic treatment and provide data support for clinical prescription decisions. Methods A retrospective design was used. The clinical information of IS patients （n=55 621） was extracted from the Yidu Cloud scientific research big data server system of the Second Affiliated Hospital of Dalian Medical University from January 1，2001 to December 31，2021，among whom 1 855 with complete information were enrolled according to the inclusion criteria. Thrombolysis effect was evaluated by comparing the National Institutes of Health Stroke Scale（NIHSS） score measured at admission and discharge，and those with an improvement in the NIHSS score by ≥ 4 points and <4 points were assigned to neurological improvement group（n=1 236），and control group（n=619），respectively. Factors possibly associated with post-IS thrombolytic effect（including general patient characteristics，medication indicators，examination indicators，test indicators，and treatment methods） were obtained by summarizing the factors suggested separately by three neurology experts with a senior title，and reviewing relevant guidelines and literature，then were screened using univariate analysis，and the identified ones were treated by dimensionality reduction using principal component analysis（PCA）. Models of Logistic，support vector machine（SVM），C5.0 decision tree arithmetic，classification and regression tree（CART），deep neural network （DNN），and Wide&Deep，were built and compared to find the one with the best performance in predicting thrombolytic effect，then to determine its parameters. Then by use of two randomly generated two numbers，7 and 11，the 1 855 patients were randomly assigned to three datasets，training（n=1 113，for building and practicing models to discover rules），validation（n=371，for adjusting model parameters），and test （n=371，for evaluating the generalization ability of the final model）. Feature engineering was used to construct a simplified model and evaluate its accuracy. The clinical information of IS patients（n=3 925）was extracted from the Yidu Cloud scientific research big data server system of Dalian Central Hospital for external verification of the model. Results Twenty-six patients characteristics associated with thrombolytic effect were included for establishing models. The dimensionalities were reduced to two principal components by PCA，explaining 93.1% of the total variance. Comparison analysis revealed that the Wide&Deep model had the best predictive performance with an accuracy of 0.815，and an F-index of 0.871. Furthermore，the values of the area under the receiver operating characteristic（AUC） curve of the Wide&Deep model in predicting the thrombolytic effect in patients in the training set and test set were 0.753 and 0.793，respectively. The number of hidden layers and neurons in each layer of the model was 7 and 15，respectively. Using sigmoid as the activation function showed that the model parameters were optimal. The feature-engineering analysis of factors influencing the improvement of neurological function showed that the importance of medication type，administration mode and dosage ranked high，and the importance ranking in a descending order was：cerebrovascular disease history，type of medication，mode of administration，single dose，atherosclerosis，therapeutic time window of thrombolytic therapy，prevalence of use of anticoagulant drugs and drugs for promoting blood circulation and removing blood stasis. After simplifying the independent variables of the model，the accuracy of the Wide&Deep model was 0.819，and its accuracy was 0.801 suggested by the external verification after model simplification，indicating good predictive performance and generalizability. Conclusion The Wide&Deep model has proven to have excellent evaluation indicators. The importance of influencing factors of thrombolytic effect in a descending order is：cerebrovascular disease history，type of medication，administration mode，single dose，atherosclerosis，therapeutic time window of thrombolytic therapy，prevalence of use of anticoagulants and blood-activating and stasis-removing drugs. It provides clinicians with timely and effective thrombolysis treatment support involving thrombolysis related factors and individualized administration using AI-based algorithms. © 2023 Chinese General Practice. All rights reserved.
"Background and purpose: A medical AI system's generalizability describes the continuity of its performance acquired from varying geographic, historical, and methodologic settings. Previous literature on this topic has mostly focused on “how” to achieve high generalizability (e.g., via larger datasets, transfer learning, data augmentation, model regularization schemes), with limited success. Instead, we aim to understand “when” the generalizability is achieved: Our study presents a medical AI system that could estimate its generalizability status for unseen data on-the-fly. Materials and methods: We introduce a latent space mapping (LSM) approach utilizing Fréchet distance loss to force the underlying training data distribution into a multivariate normal distribution. During the deployment, a given test data's LSM distribution is processed to detect its deviation from the forced distribution; hence, the AI system could predict its generalizability status for any previously unseen data set. If low model generalizability is detected, then the user is informed by a warning message integrated into a sample deployment workflow. While the approach is applicable for most classification deep neural networks (DNNs), we demonstrate its application to a brain metastases (BM) detector for T1-weighted contrast-enhanced (T1c) 3D MRI. The BM detection model was trained using 175 T1c studies acquired internally (from the authors' institution) and tested using (1) 42 internally acquired exams and (2) 72 externally acquired exams from the publicly distributed Brain Mets dataset provided by the Stanford University School of Medicine. Generalizability scores, false positive (FP) rates, and sensitivities of the BM detector were computed for the test datasets. Results and conclusion: The model predicted its generalizability to be low for 31% of the testing data (i.e., two of the internally and 33 of the externally acquired exams), where it produced (1) ∼13.5 false positives (FPs) at 76.1% BM detection sensitivity for the low and (2) ∼10.5 FPs at 89.2% BM detection sensitivity for the high generalizability groups respectively. These results suggest that the proposed formulation enables a model to predict its generalizability for unseen data. © 2023 Elsevier Ltd"
"Purpose: Fetal cardiac magnetic resonance imaging (FCMR) can be used as an imaging modality in fetal cardiovascular evaluation as studied in recent years. We aimed to evaluate cardiovascular morphology using FCMR and to observe the development of cardiovascular structures according to gestational age (GA) in pregnant women. Method: In our prospective study, 120 pregnant women between 19 and 37 weeks of gestation in whom absence of cardiac anomaly could not be excluded by ultrasonography (US) or, who were referred to us for magnetic resonance imaging (MRI) for suspected non-cardiovascular system pathology, were included. According to the axis of the fetal heart, axial, coronal, and sagittal multiplanar steady-state free precession (SSFP) and 'real time' untriggered SSFP sequence, respectively, were obtained. The morphology of the cardiovascular structures and their relationships with each other were evaluated, and their sizes were measured. Results: Seven cases (6.3%) contained motion artefacts that did not allow the assessment and measurement of cardiovascular morphology, and three (2.9%) cases with cardiac pathology in the analysed images were excluded from the study. The study included a total of 100 cases. Cardiac chamber diameter, heart diameter, heart length, heart area, thoracic diameter, and thoracic area were measured in all fetuses. The diameters of the aorta ascendens (Aa), aortic isthmus (Ai), aorta descendens (Ad), main pulmonary artery (MPA), ductus arteriosus (DA, superior vena cava (SVC), and inferior vena cava (IVC) were measured in all fetuses. The left pulmonary artery (LPA) was visualised in 89 patients (89%). The right PA (RPA) was visualised in 99 (99%) cases. Four pulmonary veins (PVs) were seen in 49 (49%) cases, three in 33 (33%), and two in 18 (18%). High correlation values were found for all diameter measurements performed with GW. Conclusion: In cases where US cannot achieve adequate image quality, FCMR can contribute to diagnosis. The very short acquisition time and parallel imaging technique with the SSFP sequence allow for adequate image quality without maternal or fetal sedation. © 2023 Elsevier B.V."
"The use of assisted-reproduction technologies such as in vitro fertilization (IVF) is increasing, particularly in dairy cattle. The question of consequences in later life has not yet been directly addressed by studies on large animal populations. Studies on rodents and early data from humans and cattle suggest that in vitro manipulation of gametes and embryos could result in long-term alteration of metabolism, growth, and fertility. Our goal was to better describe these presumed consequences in the population of dairy cows produced by IVF in Québec (Canada) and to compare them to animals conceived by artificial insemination (AI) or multiple ovulation embryo transfer (MOET). To do so, we leveraged a large phenotypic database (2.5 million animals and 4.5 million lactations) from milk records in Québec aggregated by Lactanet (Sainte-Anne-de-Bellevue, QC, Canada) and spanning 2012 to 2019. We identified 304,163, 12,993, and 732 cows conceived by AI, MOET, and IVF, respectively, for a total of 317,888 Holstein animals from which we retrieved information for 576,448, 24,192, and 1,299 lactations (total = 601,939), respectively. Genetic energy-corrected milk yield (GECM) and Lifetime Performance Index (LPI) of the parents of cows were used to normalize for genetic potential across animals. When compared with the general Holstein population, MOET and IVF cows outperformed AI cows. However, when comparing those same MOET and IVF cows with only herdmates and accounting for their higher GECM in the models, we found no statistical difference between the conception methods for milk production across the first 3 lactations. We also found that the rate of Lifetime Performance Index improvement of the IVF population during the 2012 to 2019 period was less than the rate observed in the AI population. Fertility analysis revealed that MOET and IVF cows also scored 1 point lower than their parents on the daughter fertility index and had a longer interval from first service to conception, with an average of 35.52 d compared with 32.45 for MOET and 31.87 for AI animals. These results highlight the challenges of elite genetic improvement while attesting to the progress the industry has made in minimizing epigenetic disturbance during embryo production. Nonetheless, additional work is required to ensure that IVF animals can maintain their performance and fertility potential. © 2023 American Dairy Science Association"
"Water quality strongly influences sustainable growth of a healthy society and green environment. According to the International Initiative on Water Quality (IIWQ) of the UNESCO Intergovernmental Hydrological Programme (IHP), it is essential to address water-quality issues holistically in developed and developing countries. Due to rapid urbanization and industrialization in many developing countries, groundwater - one of the major sources of drinking is getting highly affected. The traditional laboratory-based chemical testing process with conventional statistical methods is often used to analyze water quality. However, it is time-consuming. Recently, Artificial Intelligence (AI) based approaches have proven to be a better alternative for analysis and prediction of the quality of water, provided with its chemical components’ data. In this paper, we present research focusing on groundwater quality analysis using Artificial Intelligence (AI) in a case study of Odisha, an eastern- state of India and the data acquired from the Northern delta, the North Central Coast of Vietnam. The dataset in Vietnam is collected by the Ministry of Natural Resources and Environment, providing technical regulations on water resources monitoring. The Central Groundwater Board and the Government of India collect the dataset from India. The target problem is formulated as a multi-class classification problem to predict groundwater quality for drinking suitability by WHO standards. AI methodologies such as logistic regression, K-NN, Support Vector Machine (SVM) variants, decision tree, AdaBoost and XGBoost are used. Prediction results have demonstrated that Adaboost, the XGBoost and the Polynomial SVM model accurately classified the Water Quality Classes with an accuracy of 92% and 98%, respectively. It would help decision-makers effectively choose the best source of water for drinking. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Background and purpose: Artificial intelligence (AI) is present in many areas of our lives. Much of the digital data generated in health care can be used for building automated systems to bring improvements to existing workflows and create a more personalised healthcare experience for patients. This review outlines select current and potential AI applications in medical imaging practice and provides a view of how diagnostic imaging suites will operate in the future. Challenges associated with potential applications will be discussed and healthcare staff considerations necessary to benefit from AI-enabled solutions will be outlined. Methods: Several electronic databases, including PubMed, ScienceDirect, Google Scholar, and University College Dublin Library Database, were used to identify relevant articles with a Boolean search strategy. Textbooks, government sources and vendor websites were also considered. Results/Discussion: Many AI-enabled solutions in radiographic practice are available with more automation on the horizon. Traditional workflow will become faster, more effective, and more user friendly. AI can handle administrative or technical types of work, meaning it is applicable across all aspects of medical imaging practice. Conclusion: AI offers significant potential to automate most of the manual tasks, ensure service consistency, and improve patient care. Radiographers, radiation therapists, and clinicians should ensure they have adequate understanding of the technology to enable ethical oversight of its implementation. © 2023"
"Artificial Intelligence (AI) permeates every aspect of our daily lives and is no longer a subject reserved for a select few in higher education but is essential knowledge that our youth need for the future. Much is unknown about the level of AI knowledge that is age and developmentally appropriate for high school, let alone about how to teach AI to even younger learners. In this theoretical paper, we discuss the design of a game-based learning environment for high school AI education, drawing upon insights gained from a prior cognitive interview study at a STEM focused private high school. We argue that game-based learning is an excellent fit for AI education due to the commonality of problem solving in both game playing and AI. © 2023, The Author(s)."
"The research focuses on the application of AI in the field of urban morphology. The research takes urban blocks as a case study and treats urban block-related high-resolution images as data. The aim is to train an AI model to automatically detect urban block to conduct further quantitative analysis. © 2023, The Author(s)."
"Protein–protein interactions (PPIs) drive biological processes, and disruption of PPIs can cause disease. With recent breakthroughs in structure prediction and a deluge of genomic sequence data, computational methods to predict PPIs and model spatial structures of protein complexes are now approaching the accuracy of experimental approaches for permanent interactions and show promise for elucidating transient interactions. As we describe here, the key to this success is rich evolutionary information deciphered from thousands of homologous sequences that coevolve in interacting partners. This covariation signal, revealed by sophisticated statistical and machine learning (ML) algorithms, predicts physiological interactions. Accurate artificial intelligence (AI)-based modeling of protein structures promises to provide accurate 3D models of PPIs at a proteome-wide scale. © 2023 Elsevier Ltd"
"Context: Ridge-furrow with plastic film mulching (RPM) has been promoted to improve water use efficiency and crop productivity. However, limited information exists regarding the effects of substituting adding irrigation (AI) with the RPM on economic benefit, carbon footprint (CF) and the sustainability of wheat (Triticum aestivum L.) production, especially under optimizing nitrogen (N) fertilization on the Loess Plateau of China. Objectives: An eight-year field experiment was conducted to evaluate the economic feasibility, soil organic carbon (SOC) sequestration and the CF for wheat production under shifting the adding irrigation with a higher N application rate to the water–efficient field management strategies with a lower N application rate, and to assess the sustainability of wheat production under different N application rates and planting patterns. Methods: A factorial experiment was arranged in a split-plot design with four replications. The main plots were two N levels (i.e., 120 and 240 kg N ha−1), while four planting patterns [i.e., conventional rainfed flat planting (CRF), AI, RPM, and straw mulching (SM)] were assigned to subplots. After measurement of plant-derived biomass and SOC analysis, we evaluated wheat grain yields and economic performance, and calculated SOC sequestration rate (Cseq rate) and the CF during the eight-year experiment periods. Finally, considering the aspects of crop yields, economic benefit, SOC concentration, and GHG emissions, the integrated sustainable evaluation index (SEI) of each field management strategy was calculated. Results: Reducing the N application rate significantly decreased grain yields (4.3%), Cseq rate (31.6%), the intensity of CO2-eq emissions produced per unit of grain yields (CFGY, 26.6%) and per unit of economic profit (CFEP, 26.4%), while had no significant effect on economic profit and SEI. Compared with the CRF, the RPM significantly enhanced grain yields (11.2%) and Cseq rate (48.1%), and SEI (10.6%), while exerting no significant effect on economic profit, CFGY and CFEP. The RPM significantly reduced grain yields (7.2%), CFGY (13.9%) and CFEP (26.7%), but increased the SEI (8.5%) relative to the AI. The SEI value was also enhanced by 7.2% under RPM than that under SM. Conclusions: The integrated strategies of plastic film mulching in combination with a low N application rate might enhance grain yields, economic profit, and soil C sequestration, ultimately achieving sustained wheat production in the studied region. Implications: The integrated strategies of plastic film mulching in combination with a low N application rate represented an economically and C-friendly optimal field management practice for wheat production on the Loess Plateau of China or other regions with similar environmental conditions in the world. © 2023"
"More devices are being introduced to the cellular network every day, and it is time to think about how the next generation will deal with them. The 6G will bring new ways to think about how to share information and will not rely just on communication with a fixed Base Station (BS). A post-disaster scenario may damage the network infrastructure; the devices need to create an ad-hoc network to communicate in this situation. The ad-hoc network must achieve some of the 6G's requirements, such as communication frequency, data rate, and latency, to communicate with the 6G networks when available. The increase in the frequency used to achieve such data rate has the downside of decreasing the signal range, i.e., for two nodes to communicate, they must be closer to each other. Academia is developing alternatives to increase the network coverage area, allowing devices to further the ad-hoc Access Point (AP) to overcome this drawback. Thus, with this work, we propose CAIN, an 6G routing protocol that uses Artificial Intelligence (AI) techniques to increase the network coverage area. Depending on the node's connectivity to an AP, i.e., if the node is connected to an AP or not, CAIN uses different messages to make nodes further the AP to communicate with it, increasing the network coverage area. Hence, if there is no connected node to an AP, it chooses a neighbor with a connection to one to forward the message. Also, if there is no connected neighbor to an AP, the node chooses a neighbor to flood the message until it reaches an AP. To improve its performance, we enhance CAIN with AI techniques, such as Reinforcement Learning (RL), Federated Learning (FL), and Deep Neural Network (DNN). These techniques allow the node to make a better neighbor choice and increase the network coverage area even more. To the best of our knowledge, this is the first work that proposes a routing protocol for the 6G scenario that uses RL techniques with FL and DNN to increase the network coverage. With these characteristics, CAIN makes devices further the APs to communicate with it using less energy and storage resources, producing less delay compared to other protocols. © 2023 Elsevier B.V."
"Background & Aims: Microscopic inflammation has significant prognostic value in ulcerative colitis (UC); however, its assessment is complex with high interobserver variability. We aimed to develop and validate an artificial intelligence (AI) computer-aided diagnosis system to evaluate UC biopsies and predict prognosis. Methods: A total of 535 digitalized biopsies (273 patients) were graded according to the PICaSSO Histologic Remission Index (PHRI), Robarts, and Nancy Histological Index. A convolutional neural network classifier was trained to distinguish remission from activity on a subset of 118 biopsies, calibrated on 42 and tested on 375. The model was additionally tested to predict the corresponding endoscopic assessment and occurrence of flares at 12 months. The system output was compared with human assessment. Diagnostic performance was reported as sensitivity, specificity, prognostic prediction through Kaplan-Meier, and hazard ratios of flares between active and remission groups. We externally validated the model in 154 biopsies (58 patients) with similar characteristics but more histologically active patients. Results: The system distinguished histological activity/remission with sensitivity and specificity of 89% and 85% (PHRI), 94% and 76% (Robarts Histological Index), and 89% and 79% (Nancy Histological Index). The model predicted the corresponding endoscopic remission/activity with 79% and 82% accuracy for UC endoscopic index of severity and Paddington International virtual ChromoendoScopy ScOre, respectively. The hazard ratio for disease flare-up between histological activity/remission groups according to pathologist-assessed PHRI was 3.56, and 4.64 for AI-assessed PHRI. Both histology and outcome prediction were confirmed in the external validation cohort. Conclusion: We developed and validated an AI model that distinguishes histologic remission/activity in biopsies of UC and predicts flare-ups. This can expedite, standardize, and enhance histologic assessment in practice and trials. © 2023 The Authors"
"Cognitive agents such as humans and robots perceive their environment through an abundance of sensors producing streams of data that need to be processed to generate intelligent behavior. A key question of cognition-enabled and AI-driven robotics is how to organize and manage such data and knowledge efficiently in a cognitive robot control architecture. We argue, that memory is a central active component of such architectures that mediates between semantic and sensorimotor representations, orchestrates the flow of data streams and events between different processes and provides the components of a cognitive architecture with data-driven services for learning semantics from sensorimotor data, the parametrization of symbolic plans for execution and prediction of action effects. Based on related work, and the experience gained in developing our ARMAR humanoid robot systems, we identified conceptual and technical requirements of a memory system as central component of cognitive robot control architecture that facilitate the realization of high-level cognitive abilities such as explaining, reasoning, prospection, simulation and augmentation. Conceptually, a memory should be active, support multi-modal data representations, associate knowledge, be introspective, and have an inherently episodic structure. Technically, the memory should support a distributed design, be access-efficient and capable of long-term data storage. We introduce the memory system for our cognitive robot control architecture and its implementation in the robot software framework ArmarX. We evaluate the efficiency of the memory system with respect to transfer speeds, compression, reproduction and prediction capabilities. © 2023"
"Objective: To evaluate Optical Coherence Tomography Angiography (OCT-A) findings in patients with Ocular Hypertension (OHT) and compare them with healthy individuals. Methods: Thirty-four patients with ocular hypertension (OHT) and 22 healthy individuals were included in the study. Foveal thickness, retinal vascular density in superficial and deep capillary plexus and choriocapillaris, foveal avascular zone (FAZ), acircularity index (AI), foveal vessel density (FD), non-flow area, capillary and all vessel densities in the peripapillary area and the disc were automatically measured using the Angiovue software of OCT-A and compared between groups. Results: The comparison of the macular OCT-A findings did not reveal a significant difference between the two groups in terms of central macular thickness, superficial and deep capillary plexus vessel density (p>0.05). The foveal avascular zone width was significantly higher in OHT subjects compared to the control group (0.30±0.08 µ and 0.25±0.11 µ, respectively; p = 0.04). The comparison of optic nerve OCT-A findings revealed that the whole-field vessel density (wVD) (p = 0.007), peripapillary vessel density (pVD) (p = 0.001), inferior, superior and temporal radial peripapillary capillary plexus vessel density (p = 0.006, p = 0.008, p = 0.02) and the mean retinal nerve fiber layer thickness (p = 0.02) were significantly lower in the OHT group. Conclusions: Our findings suggest that the decrement in the optic disc vascular density and foveal avascular zone width was significantly higher in OHT subjects. The possible effect or role of these microvascular changes in terms of glaucoma development should be examined through further studies. © 2023"
"Colorectal cancer (CRC) is currently the third most common cancer in the world. Due to the development of treatment resistance, the efficacy of current chemotherapeutic agents against CRC has reached a plateau. Drug activity depends on the entire physiological response; therefore, drug-dose parameters cannot be designed efficiently by using conventional prediction-based methodologies. In this work, the AI-PRS (artificial intelligence-based phenotypic response surface) platform is successfully applied to find optimal drug-dose combinations in vitro from a pool of ten approved drugs. The AI-PRS platform optimizes effective drug-dose combinations without reference to molecular pathways or drug interaction data. With the aid of AI-PRS platform, efficient one, two, three, and four drug-dose combinations from in vitro studies are found. Of hundreds of combinations, regorafenib (R)/gemcitabine (G)/cetuximab (C)/5-fluorouracil (U) drug-dose combination exhibits the best activity on four CRC cell lines, two circulating tumor cells (CTCs), and one patient derived xenografts (PDX) cell lines. The three-drug combination of R/G/U shows the highest toxicity (70%) in the PDX cell line. Four-drug combination of R/G/C/U displays the best toxicity (80%) in in vitro cultured CTCs. The findings from the present derived cells reveal the prospective validation of the AI-PRS platform, which may help identify customized and highly efficient drug-dose combinations for future CRC treatment. © 2023 Wiley-VCH GmbH."
"The objective of this study was to compare the late embryo mortality (LEM) rate (losses approximately between 32 and 53 days of gestation) and Pregnancy Specific Protein B (PSPB) and progesterone (P4) concentrations on day 32 post AI in Holstein cows bred with either Holstein or Limousine semen. A sample size of 1082 cows per group diagnosed pregnant between 28- and 35-days post breeding was calculated. The study consisted of evaluating LEM (%) in a cohort of Holstein cows bred with Holstein semen (HO × HO) or Limousine semen (HO × LM), to compare pregnancy loss from 28 to 35 days post breeding to 50–57 days post breeding. A logistic regression model to compare embryo losses was developed considering as main explanatory variable the cohort (HO × HO embryo vs. HO × LM embryo), correcting by lactation number, breeding season, days to breeding and AI technician. HO × HO embryos had greater LEM (15.16%) than HO × LM embryos (9.79%). Cows bred in summertime had higher LEM (15.23%) than cows bred in no-summertime (9.88%). There were no differences among AI technicians. Within summertime there was no difference in LEM (%) between groups within each lactation number; yet, within no-summertime, LEM (%) was higher in HO × HO than HO × LM within each lactation number. Pregnancy SPB optical densities were significantly greater in the HO × HO than in the HO × LM (p =.023) group; yet, the concentration of P4 was not different between groups (p >.05). © 2023 Wiley-VCH GmbH. Published by John Wiley & Sons Ltd."
"With crew scheduling and Artificial Intelligence (AI) gaining attention in the past few years, there has been growing interest in algorithms that could effectively handle the crew scheduling problem (CSP) in railway transport. Despite AI becoming pervasive in most engineering domains, there is a lack of methods that can provide high-quality crew scheduling in the railway industry. To fill this gap, this study designs a railway crew scheduling model of mixed integer linear programming (MILP) problem utilizing the bacterial foraging algorithm (BFA) and evaluates the model's advantages and limitations. BFA is a novel class of biologically inspired stochastic global search methodology that is based on E. coli bacteria's foraging behavior. Using the Taiwan Railways dataset, we compare the performance of the proposed BFA-based method for railway crew scheduling optimization (BFARCSO) against two benchmark methods, the genetic algorithm (GA) and the particle swarm optimization algorithm (PSO) and showcase its advantages in terms of solution quality and computation time. Finally, a series of computational testing and validation highlights the efficiency and superiority of BFARCSO. It demonstrates that this approach significantly improves the large-scale railway crew scheduling problem over typical approaches, making it well-suited to practice real-time decision support in railway crew scheduling. © 2023 Elsevier Ltd"
"Transportation electrification has been fueled by recent advancements in the technology and manufacturing of battery systems, but the industry yet is facing serious challenges that could be addressed using cutting-edge digital technologies. One such novel technology is based on the digital twining of battery systems. Digital twins (DTs) of batteries utilize advanced multi-layer models, artificial intelligence, advanced sensing units, Internet-of-Things technologies, and cloud computing techniques to provide a virtual live representation of the real battery system (the physical twin) to improve the performance, safety, and cost-effectiveness. Furthermore, they orchestrate the operation of the entire battery value chain offering great advantages, such as improving the economy of manufacturing, re-purposing, and recycling processes. In this context, various studies have been carried out discussing the DT applications and use cases from cloud-enabled battery management systems to the digitalization of battery testing. This work provides a comprehensive review of different possible use cases, key enabling technologies, and requirements for battery DTs. The review inclusively discusses the use cases, development/integration platforms, as well as hardware and software requirements for implementation of the battery DTs, including electrical topics related to the modeling and algorithmic approaches, software architectures, and digital platforms for DT development and integration. The existing challenges are identified and circumstances that will create enough value to justify these challenges, such as the added costs, are discussed. © 2023 The Authors"
"Background and objective: Breast cancer is the world's most prevalent form of cancer. The survival rates have increased in the last years mainly due to factors such as screening programs for early detection, new insights on the disease mechanisms as well as personalised treatments. Microcalcifications are the only first detectable sign of breast cancer and diagnosis timing is strongly related to the chances of survival. Nevertheless microcalcifications detection and classification as benign or malignant lesions is still a challenging clinical task and their malignancy can only be proven after a biopsy procedure. We propose DeepMiCa, a fully automated and visually explainable deep-learning based pipeline for the analysis of raw mammograms with microcalcifications. Our aim is to propose a reliable decision support system able to guide the diagnosis and help the clinicians to better inspect borderline difficult cases. Methods: DeepMiCa is composed by three main steps: (1) Preprocessing of the raw scans (2) Automatic patch-based Semantic Segmentation using a UNet based network with a custom loss function appositely designed to deal with extremely small lesions (3) Classification of the detected lesions with a deep transfer-learning approach. Finally, state-of-the-art explainable AI methods are used to produce maps for a visual interpretation of the classification results. Each step of DeepMiCa is designed to address the main limitations of the previous proposed works resulting in a novel automated and accurate pipeline easily customisable to meet radiologists’ needs. Results: The proposed segmentation and classification algorithms achieve an area under the ROC curve of 0.95% and 0.89% respectively. Compared to previously proposed works, this method does not require high performance computational resources and provides a visual explanation of the final classification results. Conclusion: To conclude, we designed a novel fully automated pipeline for detection and classification of breast microcalcifications. We believe that the proposed system has the potential to provide a second opinion in the diagnosis process giving the clinicians the opportunity to quickly visualise and inspect relevant imaging characteristics. In the clinical practice the proposed decision support system could help reduce the rate of misclassified lesions and consequently the number of unnecessary biopsies. © 2023 Elsevier B.V."
"Identifying Alzheimer's disease (AD) involves a deliberate diagnostic process owing to its innate traits of irreversibility with subtle and gradual progression. These characteristics make AD biomarker identification from structural brain imaging (e.g., structural MRI) scans quite challenging. Using clinically-guided prototype learning, we propose a novel deep-learning approach through eXplainable AD Likelihood Map Estimation (XADLiME) for AD progression modeling over 3D sMRIs. Specifically, we establish a set of topologically-aware prototypes onto the clusters of latent clinical features, uncovering an AD spectrum manifold. Considering this pseudo map as an enriched reference, we employ an estimating network to approximate the AD likelihood map over a 3D sMRI scan. Additionally, we promote the explainability of such a likelihood map by revealing a comprehensible overview from clinical and morphological perspectives. During the inference, this estimated likelihood map served as a substitute for unseen sMRI scans for effectively conducting the downstream task while providing thorough explainable states. © 2023"
"Purpose: To evaluate the sensitivity of artificial intelligence (AI)-powered software in detecting liver metastases, especially those overlooked by radiologists. Methods: Records of 746 patients diagnosed with liver metastases (November 2010–September 2017) were reviewed. Images from when radiologists first diagnosed liver metastases were reviewed, and prior contrast-enhanced CT (CECT) images were checked for availability. Two abdominal radiologists classified the lesions into overlooked lesions (all metastases missed by radiologists on prior CECT) and detected lesions (all metastases if any of them were correctly identified and invisible on prior CECT or those with no prior CECT). Finally, images from 137 patients were identified, 68 of which were classified as “overlooked cases.” The same radiologists created the ground truth for these lesions and compared them with the software's output at 2-month intervals. The primary endpoint was the sensitivity in detecting all liver lesion types, liver metastases, and liver metastases overlooked by radiologists. Results: The software successfully processed images from 135 patients. The per-lesion sensitivity for all liver lesion types, liver metastases, and liver metastases overlooked by radiologists was 70.1%, 70.8%, and 55.0%, respectively. The software detected liver metastases in 92.7% and 53.7% of patients in detected and overlooked cases, respectively. The average number of false positives was 0.48 per patient. Conclusion: The AI-powered software detected more than half of liver metastases overlooked by radiologists while maintaining a relatively low number of false positives. Our results suggest the potential of AI-powered software in reducing the frequency of overlooked liver metastases when used in conjunction with the radiologists’ clinical interpretation. © 2023 Elsevier B.V."
"For high-risk early-stage hormone-receptor-positive, HER2-negative breast cancer (HR + /HER2 − EBC), short- and long-term recurrence risks remain substantial despite local control with surgery and radiation and systemic treatment with chemotherapy and endocrine therapy (ET). Recent trials have provided new strategies for reducing recurrence. The monarchE trial demonstrated that adding 2 years of adjuvant abemaciclib to ET improves invasive disease-free survival (iDFS) and distant recurrence-free survival (DRFS). In the OlympiA trial for high-risk disease in patients with germline BRCA1/BRCA2 mutations, adding 1 year of olaparib to ET improved iDFS, DRFS, and overall survival (OS). In addition, for premenopausal women with high-risk tumors, long-term follow-up of the SOFT, ASTRRA, TEXT, ABCSG-12, and HOBOE trials supports the role of ovarian function suppression (OFS), in combination with adjuvant tamoxifen or aromatase inhibition (AI). For postmenopausal women with high-risk tumors, extended-duration AI for at least 7 years should be used with zoledronic acid. Given the remaining recurrence risk even with these interventions and with the ongoing development of new strategies for HR + disease, patients with high-risk EBC should be encouraged to participate in clinical trials, such as trials of immunotherapy, novel oral estrogen receptor alpha (ERα)-targeting agents, antibody–drug conjugates (ADCs), and trials guided by measurements of minimal residual disease (MRD). © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Along with the rapid development of artificial intelligence (AI) technology, scientific research enters a new era of AI. Topology optimization (TO) and AI technology are recently showing a growing trend of cross development, which has received continuous attentions from relative researchers. In this paper, we introduce a concept of Implicit Neural Representations from AI into TO field and establish a novel TO framework which is named as TOINR. In TOINR, the Topology Description Function is the inherent combination factor, which is constructed by a Neural Network (NN) and determines the topology statuses of structural materials in the design domain. We adopt sine as a periodic activation function combined with MLP as the architecture of the NN. The inputs of NN are a predefined set of spatial points’ coordinates, while the outputs are the implicit representations of corresponding topological boundaries. Along with updating NN's parameters (i.e., design variables), the structural topologies iteratively evolve according to the responses analysis results and optimization functions. A boundary-adaptive multi-resolution finite element analysis method is developed to improve the physical response accuracy. At each step of TOINR, we ensure the computational differentiability. Thus, the automatic differentiation is used in sensitivity analysis. The multi-penalty function approach is applied to handle objects with constraints. Besides, we design an adaptive adjustment scheme for learning rates to enhance the stability of the optimization process. Numerical examples illustrate that TOINR can stably obtain optimized structures for different problems with high performance and robustness. © 2023 Elsevier B.V."
"Definition of the problem: Rapid developments in the field of artificial intelligence (AI) and robotics pose new challenges not only to ethics but also to law, especially in the field of medicine and nursing. In principle, the use of AI has the potential to facilitate, if not improve, both curative treatments and adequate handling in the context of care. Administrative tasks, the monitoring of vital functions and their parameters, and the examination of tissue samples, for example, could run autonomously. In diagnostics and therapy, such systems can support the attending physician. Intelligent beds make it possible to mobilize patients early while at the same time reducing the need for personnel. Nevertheless, the use of these systems poses legal challenges. For example, there is a risk of injury to the people involved. Unlike conventional technologies, AI “suffers” from the “black box problem”: the results generated by the systems are no longer fully predictable and comprehensible. Its use entails unknown and incalculable risks, with particular implications for civil liability and criminal responsibility. Arguments: To whom the decisions of the systems are normatively attributable is a core question of legal discourse. The obvious choice, for reasons of practicability, of attributing the behaviour of an AI system to the human being who makes the final decisions is not convincing in all cases, but often degrades the human being to a symbolic “liability servant” and imposes the risks of the technologies on the human being in a one-sided manner. Furthermore, in the field of medicine and care, questions arise regarding the approval of AI systems, since the machines continue to learn during use and thus continuously change their structural design. Since the systems require any amount of reliable data for training and later use—especially through further learning—adequate handling of personal data is also necessary with regard to data protection law in the area of care and medicine. Conclusions: It is therefore advisable to address the potential for conflict from an ethical and legal perspective at an early stage in order to prevent a potential social fear of such systems and to create a practical framework for action. Orientation towards the guiding principle of “meaningful human control” offers the potential to solve these challenges. © 2023, The Author(s)."
"ATP-binding cassette transporter A1 (ABCA1) plays a crucial role in atherosclerotic formation through mediated cholesterol efflux in macrophage-derived foam cells. In this study, a scavenger receptors AI (SR-AI) targeted theranostic nanoparticles was constructed for atherosclerosis regression via ABCA1 activation in foam cells. ABCA1-upregulator 5242331 and IR780 were encapsulated in PLGA-PEG micelles which were conjugated with SR-AI targeting peptide (PP1) to formulate the nanoparticles (SAU-NPs). Immunostaining revealed that SR-AI was highly expressed both in macrophage foam cells and in atherosclerotic plaque of ApoE−/− mice. The SAU-NPs have shown more active targeting to plaque lesion with higher stability compared with non-SR-AI targeted nanoparticles. The transformation from macrophage to foam cells was inhibited by SAU-NPs carried 5242331. Cholesterol deposition was effectively reduced in foam cells by SAU-NPs through activating the LXRα-ABCA1/ABCG1/SR-BI pathway. In conclusion, theranostic SAU-NPs which carried ABCA1-upregulator 5242331 exert beneficial effects on atherosclerosis regression via LXRα activation. © 2023"
"Recently years, we have seen the exponential upgrowth of the Industrial Internet of Things (IIoT), which brings significant benefits to our daily lives, industry, and society. The common idea behind the IIoT is to connect digital devices and services with physical systems using sensors and actuators. The IIoT output a very enormous amount of data through resource-constrained devices (sensors). However, due to the heterogeneity of these devices and their limited resources, they are exposed to a multifariousness of threats that jeopardize IIoT's ability to provide seamless operations to enterprises. Therefore, there is an urgent need to develop efficient security approaches to combat these threats and protect IIoT systems. To this end, some intrusion detection systems (IDS) have been enhanced in recent years. With the advent of intelligent approaches (IAs), most IDSs are based on Machine Learning (ML) or Deep Learning approaches and institutions are incorporating accurate IAs techniques in their real-world applications. Therefore, we survey recent efforts in the literature related to intrusion detection in IIoT, focusing on ML algorithms. We divide them into three categories: Agent Placement Strategy, Detection Method, and Security Problem. We pose a number of open questions and suggest some research directions. © 2023 Elsevier Ltd"
"Aim: The aim of this study is to demonstrate the added value of three-dimensional (3D) reconstruction models and artificial intelligence for preoperative planning in complex perianal Crohn's disease. MRI is the gold standard for diagnosis of complex perianal fistulas and abscess due to its high sensitivity, but it lacks high specificity values. This creates the need for better diagnostic models such as 3D image processing and reconstruction (3D-IPR) with artificial intelligence (AI) algorithms. Method: This is a prospective study evaluating the utility of 3D reconstruction models from MRI in four patients with perineal Crohn's disease (pCD). Results: Four pCD patients had 3D reconstruction models made from pelvic MRI. This provided a more visual representation of perianal disease and made possible location of the internal fistula orifice, seton placement in fistula tracts and abscess drainage. Conclusion: Three-dimensional reconstruction in CD-associated complex perianal fistulas can facilitate disease interpretation, anatomy and surgical strategy, potentially improving preoperative planning as well as intraoperative assistance. This could probably result in better surgical outcomes to control perianal sepsis and reduce the number of surgical procedures required in these patients. © 2023 The Authors. Colorectal Disease published by John Wiley & Sons Ltd on behalf of Association of Coloproctology of Great Britain and Ireland."
"Purpose: To determine whether combinations of devices with different measuring principles, supported by artificial intelligence (AI), can improve the diagnosis of keratoconus (KC). Methods: Scheimpflug tomography, spectral-domain optical coherence tomography (SD-OCT), and air-puff tonometry were performed in all eyes. The most relevant machine-derived parameters to diagnose KC were determined using feature selection. The normal and forme fruste KC (FFKC) eyes were divided into training and validation datasets. The selected features from a single device or different combinations of devices were used to develop models based on random forest (RF) or neural networks (NN) trained to distinguish FFKC from normal eyes. The accuracy was determined using receiver operating characteristic (ROC) curves, area under the curve (AUC), sensitivity, and specificity. Results: 271 normal eyes, 84 FFKC eyes, 85 early KC eyes, and 159 advanced KC eyes were included. A total of 14 models were built. Air-puff tonometry had the highest AUC for detecting FFKC using a single device (AUC = 0.801). Among all two-device combinations, the highest AUC was accomplished using RF applied to selected features from SD-OCT and air-puff tonometry (AUC = 0.902), followed by the three-device combination with RF (AUC = 0.871) with the best accuracy. Conclusion: Existing parameters can precisely diagnose early and advanced KC, but their diagnostic ability for FFKC could be optimized. Applying an AI algorithm to a combination of air-puff tonometry with Scheimpflug tomography or SD-OCT could improve FFKC diagnostic ability. The improvement in diagnostic ability by combining three devices is modest. © 2023"
"ChatGPT has been a frequent topic of discussion lately. All over the Internet, from YouTube to blogs, there have been reports about how ChatGPT is able to plan people's daily activities, even for a whole month. However, what matters is what activities ChatGPT recommends. When ChatGPT was trained on a vast amount of data from the Internet, we wondered if it would suggest activities that can lead to addiction. In our test, not once did ChatGPT recommend an activity related to alcohol, drug use, or any other activity that can lead to addiction with serious health consequences. Suggestions seemed more like self-improvement posts on blogs than discussion forums where people might mention drinking in the evenings. Thus, if a person were to use ChatGPT as a personal lifestyle advisor, it does not appear on the basis of this test that ChatGPT would recommend activities that would be fundamentally detrimental to their health. However, more detailed long-term testing of similar tools is needed before recommendations for use in practice can be made. © 2023, The Author(s) under exclusive licence to Biomedical Engineering Society."
"This letter proposes a novel Cloud Radio Access Network (C-RAN) traffic analysis and management model that estimates probable RAN traffic congestion and mitigate its effect by adopting a suitable handling mechanism. A computation approach is introduced to classify heterogeneous RAN traffic into distinct traffic states based on bandwidth consumption and execution time of various job requests. Further, a cloud-based traffic management is employed to schedule and allocate resources among user job requests according to the associated traffic states to minimize latency and maximize bandwidth utilization. The experimental evaluation and comparison of the proposed model with state-of-the-art methods reveal that it is effective in minimizing the worse effect of traffic congestion and improves bandwidth utilization and reduces job execution latency up to 17.07% and 18%, respectively. © 2012 IEEE."
"This study aimed to compare milk production and reproductive performance in high yield Holstein cows that lose BCS early and late in the postpartum period. Lactating dairy cows (n = 76) were received first timed AI at 60 to 75 DIM using the farm-managed estradiol-progesterone-GnRH–based timed AI protocol. The BCS of all cows was daily evaluated by automated BCS cameras. Aiming to evaluate the effect of the days in milk (DIM) in which a cow reached the nadir BCS on the reproductive parameters, cows were separated into two groups: early BCS loss (n = 42), cows that reached the nadir BCS ≤ 34 DIM, and late BCS loss (n = 34), cows that reached the nadir BCS > 34 DIM. The optimal cut-off point for determining the relationship between days to nadir BCS and pregnancy by 150 DIM (P150) was calculated using the receiver operating characteristic (ROC) curve. From the ROC analysis, the cut-off was 34 DIM (Se, 80.9%; Sp, 66.7%; AUC, 0.74; P < 0.01). No differences (P>0.05) were detected between groups on the BCS and milk production. The average of milk production in both groups was 46.65 ± 6.15 Kg/day. Cows that reached the nadir BCS early postpartum presented lower (P < 0.01) calving interval and greater (P < 0.01) pregnancy at first AI and P150. In summary, cows that lost BCS early had better reproductive performance and had similar milk yield compared with cows that lost BCS late in the postpartum period. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Healthcare artificial intelligence (AI) holds the potential to increase patient safety, augment efficiency and improve patient outcomes, yet research is often limited by data access, cohort curation, and tools for analysis. Collection and translation of electronic health record data, live data, and real-time high-resolution device data can be challenging and time-consuming. The development of clinically relevant AI tools requires overcoming challenges in data acquisition, scarce hospital resources, and requirements for data governance. These bottlenecks may result in resource-heavy needs and long delays in research and development of AI systems. We present a system and methodology to accelerate data acquisition, dataset development and analysis, and AI model development. We created an interactive platform that relies on a scalable microservice architecture. This system can ingest 15,000 patient records per hour, where each record represents thousands of multimodal measurements, text notes, and high-resolution data. Collectively, these records can approach a terabyte of data. The platform can further perform cohort generation and preliminary dataset analysis in 2-5 minutes. As a result, multiple users can collaborate simultaneously to iterate on datasets and models in real time. We anticipate that this approach will accelerate clinical AI model development, and, in the long run, meaningfully improve healthcare delivery.  © 2013 IEEE."
[No abstract available]
"Background:The retina is a key focus in the search for biomarkers of Alzheimer's disease (AD) because of its accessibility and shared development with the brain. The pathological hallmarks of AD, amyloid beta (Aβ), and hyperphosphorylated tau (pTau) have been identified in the retina, although histopathologic findings have been mixed. Several imaging-based approaches have been developed to detect retinal AD pathology in vivo. Here, we review the research related to imaging AD-related pathology in the retina and implications for future biomarker research.Evidence Acquisition:Electronic searches of published literature were conducted using PubMed and Google Scholar.Results:Curcumin fluorescence and hyperspectral imaging are both promising methods for detecting retinal Aβ, although both require validation in larger cohorts. Challenges remain in distinguishing curcumin-labeled Aβ from background fluorescence and standardization of dosing and quantification methods. Hyperspectral imaging is limited by confounding signals from other retinal features and variability in reflectance spectra between individuals. To date, evidence of tau aggregation in the retina is limited to histopathologic studies. New avenues of research are on the horizon, including near-infrared fluorescence imaging, novel Aβ labeling techniques, and small molecule retinal tau tracers. Artificial intelligence (AI) approaches, including machine learning models and deep learning-based image analysis, are active areas of investigation.Conclusions:Although the histopathological evidence seems promising, methods for imaging retinal Aβ require further validation, and in vivo imaging of retinal tau remains elusive. AI approaches may hold the greatest promise for the discovery of a characteristic retinal imaging profile of AD. Elucidating the role of Aβ and pTau in the retina will provide key insights into the complex processes involved in aging and in neurodegenerative disease. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"Purpose: To test the changes of meibomian gland (MG) morphology using an artificial intelligence (AI) analytic system in asymptomatic children wearing overnight orthokeratology (OOK) and soft contact lens (SCL). Methods: A retrospective study was conducted including 89 participants treated with OOK and 70 participants with SCL. Tear meniscus height (TMH), noninvasive tear breakup time (NIBUT), and meibography were obtained using Keratograph 5 M. MG tortuosity, height, width, density, and vagueness value were measured using an artificial intelligence (AI) analytic system. Results: In an average of 20.80 ± 10.83 months follow-up, MG width of the upper eyelid significantly increased and MG vagueness value significantly decreased after OOK and SCL treatment (all P < 0.05). MG tortuosity of the upper eyelid significantly increased after OOK treatment (P < 0.05). TMH and NIBUT did not differ significantly pre- and post- OOK and SCL treatment (all P > 0.05). The results from the GEE model demonstrated that OOK treatment positively affected MG tortuosity of both upper and lower eyelids (P < 0.001; P = 0.041, respectively) and MG width of the upper eyelid (P = 0.038), while it negatively affected MG density of the upper eyelid (P = 0.036) and MG vagueness value of both upper and lower eyelids (P < 0.001; P < 0.001, respectively). SCL treatment positively affected MG width of both upper and lower eyelids (P < 0.001; P = 0.049, respectively) as well as MG height of the lower eyelid (P = 0.009) and tortuosity of the upper eyelid, (P = 0.034) while it negatively affected MG vagueness value of both upper and lower eyelids (P < 0.001; P < 0.001, respectively). However, no significant relationship was found between the treatment duration and TMH, NIBUT, MG morphological parameters in OOK group. SCL treatment duration negatively affected MG height of the lower eyelid (P = 0.002). Conclusions: OOK and SCL treatment in asymptomatic children can influence MG morphology. The AI analytic system may be an effective method to facilitate the quantitative detection of MG morphological changes. © 2023 British Contact Lens Association"
"This paper provides a systematic account of how artificial intelligence (AI) technologies could harm nonhuman animals and explains why animal harms, often neglected in AI ethics, should be better recognised. After giving reasons for caring about animals and outlining the nature of animal harm, interests, and wellbeing, the paper develops a comprehensive ‘harms framework’ which draws on scientist David Fraser’s influential mapping of human activities that impact on sentient animals. The harms framework is fleshed out with examples inspired by both scholarly literature and media reports. This systematic account and framework should help inform ethical analyses of AI’s impact on animals and serve as a comprehensive and clear basis for the development and regulation of AI technologies to prevent and mitigate harm to nonhumans. © 2023, The Author(s)."
"In this work we have used as a source of information a large sample of the press articles published during 2021 about the eruption of the Cumbre Vieja volcano in the island of La Palma (Canary Islands). In contraposition, the scientific papers evaluating different facets of natural disasters have preferentially used social networks as a source of information. Herein we have shown how the emotions and sentiments expressed in press media can be efficiently analyzed via AI techniques to better assess the social impact of a disaster at the time it takes place. We have also gauged the usefulness of different classifiers combining sentiment analysis with multivariate statistical analysis and machine learning techniques. By applying this methodology, we were able to classify a newspaper article within a certain time frame of the eruption, and we observed significant differences between local news published in Spanish and those of foreign newspapers written in English. We also found different emotional trajectories of articles by applying the Fourier transform onto the inner “valence” progress along each article narrative time. In addition, there appeared a significant relationship between the surface area occupied by lava and the emotions and sentiments expressed in the articles—many other correlations and causalities could be explored too. The main findings of this research may constitute a helpful resource for a better understanding of the way press media react to volcanic activity, and may guide in public decision-making under different temporal horizons, including the design of improved strategies in the risk reduction domain. © 2023"
"The integration of technologies has made it possible to develop optimal operating conditions at reduced costs, which results in a more sustainable energy transition away from fossil fuels and a step closer towards net-zero emission buildings (NZEB) for sustainable development. In recent years, ground source heat pump has gained recognition as an established thermal technology that can be integrated into smart energy systems to support the sudden rise in energy demand, flatten the quick changes in the supply side, and lower energy costs. On the user side, low-temperature heating and high-temperature cooling borehole coupled heat pumps (BCHP) have gained popularity due to its excellent performance in terms of energy efficiency, sustainability, and simplicity of integration with renewable resources. As a green solution for building space heating/cooling, BCHP systems have the potential to considerably contribute to the CO2 reduction milestones, but they are still underutilized, mostly because of their high initial investment costs. The applications of automation, data retrieval, smart decision making, control optimization, modeling and monitoring, are recent areas where data driven AI algorithms are becoming increasingly significant. AI approaches can help BCHP become more intelligent and offer new opportunities for studying heating and cooling systems. While much research is conducted to improve the design of borehole heat exchanger (BHEx) based heat pump, an efficient control approach is equally essential to achieving long-term performance and a shorter payback period. The objective of the current study is to identify the potential of most recent innovations in the field of data driven machine learning techniques to enhance BCHP operations and performance predictions to meet NZEBs. The explicit implementation challenges linked with BCHPs modeling are pointed out and the requirements needed for setting BCHP control algorithms are presented. Various methods found in the literature studies to come up with more accurate modeling and optimized control for BCHPs, with a special interest for the ones based on data driven machine learning algorithms such as artificial neural networks (ANN), are reviewed, categorized, and their advantages along with limitations are addressed. The latest developments in machine learning algorithms and how they have been utilized in heating/cooling applications are reviewed critically and their significance for Hybrid-BCHP control optimization is presented. Opportunities and limitations associated with their physical implementation on real-time heating/cooling systems are also discussed. In summary, data driven machine learning algorithms can not only be implemented for modeling of BHEx performance, but also can be applied to design and optimization of operational control of Hybrid-BCHP system, while each component's aspect might be intricately related. © 2023 International Energy Initiative"
"Sensors are regarded as a fundamental vector for sustainable development of future advanced civilization. To satisfy the demands of future generations, fabrication of advanced sensor systems integrated with artificial intelligence (AI), fifth generation (5G) connectivity, machine learning (ML), and internet of things (IoTs) is growing very fast. Incorporation of two-dimensional (2D) nanomaterials (NMs) with IoTs/5G/AI/ML technologies has transformed wide range of sensor applications in healthcare, wearable electronics for, safety, environment, military, space, and agriculture sectors. Finally, to operate those sensors we need powerful energy storage devices (ESDs) and hence advance 2D NMs. Since the discovery of MXenes NMs in 2011, and 2D boron nanosheets (NSs) (borophene) on Ag substrates (2015) their research has been accelerated in the domains of advanced nanotechnological world. Borophene and MXenes NMs have came out as an outstanding 2D NMs to construct next generation novel sensors and ESDs due to their novel physicochemical properties and surface functions. By lowering costs, requiring fewer resources (including labor), and minimizing contamination, ML/AI based theoretical simulation has effectively directed the study and manufacturing of improved 2D NMs based sensors/ESDs applications on large scale industrial level. Modern 2D NMs based flexible sensors and ESDs can fundamentally alter the traditional sensing/ESDs technologies since they are adaptable, wearable, intelligent, portable, biocompatible, energy-efficient, self-sustaining, point-of-care, affordable etc. this review summarized the MXenes and borophene NMs synthesis with corresponding achievements, and there advancement, limitations, and challenges in sensors/ESDs technological applications. © 2023"
"Background and objective:This paper focuses on nutritional recommendation systems (RS), i.e. AI-powered automatic systems providing users with suggestions about what to eat to pursue their weight/body shape goals. A trade-off among (potentially) conflictual requirements must be taken into account when designing these kinds of systems, there including: (i) adherence to experts’ prescriptions, (ii) adherence to users’ tastes and preferences, (iii) explainability of the whole recommendation process. Accordingly, in this paper we propose a novel approach to the engineering of nutritional RS, combining machine learning and symbolic knowledge extraction to profile users—hence harmonising the aforementioned requirements. MethodsOur contribution focuses on the data processing workflow. Stemming from neural networks (NN) trained to predict user preferences, we use CART Breiman et al.(1984) to extract symbolic rules in Prolog Körner et al.(2022) form, and we combine them with expert prescriptions brought in similar form. We can then query the resulting symbolic knowledge base via logic solvers, to draw explainable recommendations. ResultsExperiments are performed involving a publicly available dataset of 45,723 recipes, plus 12 synthetic datasets about as many imaginary users, and 6 experts’ prescriptions. Fully-connected 4-layered NN are trained on those datasets, reaching ∼86% test-set accuracy, on average. Extracted rules, in turn, have ∼80% fidelity w.r.t. those NN. The resulting recommendation system has a test-set precision of ∼74%. The symbolic approach makes it possible to devise how the system draws recommendations. ConclusionsThanks to our approach, intelligent agents may learn users’ preferences from data, convert them into symbolic form, and extend them with experts’ goal-directed prescriptions. The resulting recommendations are then simultaneously acceptable for the end user and adequate under a nutritional perspective, while the whole process of recommendation generation is made explainable. © 2023 The Authors"
"Aim: To develop a high-accuracy low-dose computed tomography (LDCT) lung nodule diagnosis system by combining artificial intelligence (AI) technology with the Lung CT Screening Reporting and Data System (Lung-RADS), which can be used in the future AI-aided diagnosis of pulmonary nodules. Materials and Methods: The study comprised the following steps: (1) the best deep-learning segmentation method for pulmonary nodules was compared and selected objectively; (2) the Image Biomarker Standardization Initiative (IBSI) was used for feature extraction and to determine the best feature reduction method; and (3) a principal component analysis (PCA) and three machine learning methods were used to analyse the extracted features, and the best method was determined. The Lung Nodule Analysis 16 dataset was applied to train and test the established system in this study. Results: The competition performance metric (CPM) score of the nodule segmentation reached 0.83, the accuracy of nodule classification was 92%, the kappa coefficient with the ground truth was 0.68, and the overall diagnostic accuracy (calculated by the nodules) was 0.75. Conclusion: This paper summarises a more efficient AI-assisted diagnosis process of pulmonary nodules, and has better performance compared with the previous literature. In addition, this method will be validated in a future external clinical study. © 2023"
"Applications such as autonomous driving or assistive robotics heavily rely on the usage of Deep Neural Networks. In particular, Convolutional Neural Networks (CNNs) provide precise and reliable results in image processing tasks like camera-based object detection or semantic segmentation. However, to achieve even better results, CNNs are becoming more and more complex. Deploying these networks in distributed embedded systems thereby imposes new challenges, due to additional constraints regarding performance and energy consumption in the near-sensor compute platforms, i.e. the sensor nodes. Processing all data in the central node, however, is disadvantageous since raw data of camera consumes large bandwidth and running CNN inference of multiple tasks requires certain performance. Moreover, sending raw data over the interconnect is not advisable for privacy reasons. Hence, offloading CNN workload to the sensor nodes in the system can lead to reduced traffic on the link and a higher level of data security. However, due to the limited hardware-resources on the sensor nodes, partitioning CNNs has to be done carefully to meet overall latency requirements and energy constraints. Therefore, we present CNNParted, an open-source framework for efficient, hardware-aware CNN inference partitioning targeting embedded AI applications. It automatically searches for potential partitioning points in the CNN to find a beneficial workload distribution between sensor nodes and a central edge node. Thereby, CNNParted not only analyzes the CNN architecture but also takes hardware components, such as dedicated hardware accelerators and memories, into consideration to evaluate inference partitioning regarding latency and energy consumption. Exemplary, we apply CNNParted to three commonly used feed forward CNNs in embedded systems. Thereby, the framework first searches for several potential partitioning points and then evaluates the latter regarding inference latency and energy consumption. Based on the results, beneficial partitioning points can be identified depending on the system constraints. Using the framework, we are able to find and evaluate 10 potential partitioning points for FCN ResNet-50, 13 partitioning points for GoogLeNet, and 8 partitioning points for SqueezeNet V1.1 within 520 s, 330 s, and 140 s, respectively, on an AMD EPYC 7702P running 8 concurrent threads. For GoogLeNet, we determine two partitioning points that provide a good trade-off between required bandwidth, latency and energy consumption. We also provide insights into further interesting findings that can be derived from the evaluation results. © 2023 Elsevier B.V."
"Advances in AI research have brought increasingly sophisticated capabilities to AI systems and heightened the societal consequences of their use. Researchers and industry professionals have responded by contemplating responsible principles and practices for AI system design. At the same time, defense institutions are contemplating ethical guidelines and requirements for the development and use of AI for warfare. However, varying ethical and procedural approaches to technological development, research emphasis on offensive uses of AI, and lack of appropriate venues for multistakeholder dialogue have led to differing operationalization of responsible AI principles and practices among civilian and defense entities. We argue that the disconnect between civilian and defense responsible development and use practices leads to underutilization of responsible AI research and hinders the implementation of responsible AI principles in both communities. We propose a research roadmap and recommendations for dialogue to increase exchange of responsible AI development and use practices for AI systems between civilian and defense communities. We argue that generating more opportunities for exchange will stimulate global progress in the implementation of responsible AI principles. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Large-scale data transport for data-intensive sciences is a complex multidimensional challenge. The challenge includes optimizing the end-to-end Big Data movement performance in real-time, supporting direct remote data access using NVMe over Fabrics (NVMeoF) and deploying to existing research platforms. AIDTN is the first effort to provide a unique AI system designed to incorporate NVMe over Fabrics (NVMeoF) and optimize coordination among multiple components supporting large-scale, multi-domain Wide Area Network (WAN) data-intensive science. AIDTN's research objective is to integrate next-generation storage architecture using NVMeoF, specialized network design using high-performance network appliances, Data Transfer Nodes (DTNs), catalysts in driving data transport, and a unique AI system explicitly designed for high-performance data movement challenges. AIDTN is the first system that uses network and system features to predict the end-to-end performance of high-performance data movement and further extends the model with NVMe-specific features for NVMeoF remote data access. As a result, AIDTN improves data movement performance by up to 284% while minimizing packet loss compared to other heuristics approaches. It also has a prediction error rate as low as 0.16 compared to AI models with the only network (error rate = 0.29) or network and system features (error rate = 0.19). © 1990-2012 IEEE."
"New technological advancements in wireless networks have enlarged the number of connected devices. The unprecedented surge of data volume in wireless systems empowered by artificial intelligence (AI) opens up new horizons for providing ubiquitous data-driven intelligent services. Traditional cloud-centric machine learning (ML)-based services are implemented by centrally collecting datasets and training models. However, this conventional training technique encompasses two challenges: (i) high communication and energy cost and (ii) threatened data privacy. In this article, we introduce a comprehensive survey of the fundamentals and enabling technologies of federated learning (FL), a newly emerging technique coined to bring ML to the edge of wireless networks. Moreover, an extensive study is presented detailing various applications of FL in wireless networks and highlighting their challenges and limitations. The efficacy of FL is further explored with emerging prospective beyond fifth-generation (B5G) and sixth-generation (6G) communication systems. This survey aims to provide an overview of the state-of-the-art FL applications in key wireless technologies that will serve as a foundation to establish a firm understanding of the topic. Lastly, we offer a road forward for future research directions. © 2017 IEEE."
"Purpose: This retrospective observational study compares how different classes of blastocyst genotypes from egg donor cycles differentially blastulate and expand using a standard assay. Methods: Quantitative measurements of expansion utilized a customized neural network that segments all sequential time-lapse images during the first 10 h of expansion. Results: Analyses were performed using two developmental time perspectives using time-lapse imaging. The first was the time to blastocyst formation (tB), which broadly reflects variations in developmental rate. Euploidy peaked at 100–115 h from fertilization. In contrast, aneuploidy peaks flanked this interval bi-modally. These distributions limit ploidy discrimination based upon traditional standard grading features when assessed in real time. In contrast, from the second perspective of progressive blastocyst expansion that is normalized to each individual blastocyst’s tB time, euploidy was significantly increased at expansion values > 20,000µ2 across all tB intervals studied. A Cartesian coordinate plot graphically summarizes information useful to rank order blastocysts within cohorts for transfer. Defined aneuploidy subgroups, distinguished by the number and complexity of chromosomes involved, also showed distributive differences from both euploids and from each other. A small subset of clinically significant trisomies did not show discriminating features separating them from other euploids. Conclusion: A standard assay of blastocyst expansion normalized to each individual blastocyst’s time of blastocyst formation more usefully discriminates euploidy from aneuploidy than real-time expansion comparisons using absolute developmental time from fertilization. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"This study aimed to determine the effect of the addition of rumen-protected palm oil making up 3% of the ration on lipid health indices and milk fatty acid composition of Kivircik ewes’. Kivircik ewes at two years of age, the same parity, lactation stage, and the same bodyweight (52.57 ± 5.80 kg) were chosen for this purpose. Two groups were formed, in which the control group was fed a basal diet without feed supplementation, whereas the treatment group received rumen-protected palm oil which corresponded to 3% of the ration. In order to protect palm oil, it was coated with calcium salts. Treatment increased the palmitic acid (C16:0) content of milk compared to the control group (P < 0.05) and tended to increase saturated fatty acids (SFA) and monounsaturated fatty acids (MUFA) (P = 0.14). An increase in SFA and MUFA was attributed to an increase in palmitic acid and oleic acid (C18:1), respectively (P < 0.05). Results indicated that the omega-6/omega-3 ratio (n-6/n-3) ranged between 0.61 and 2.63. The inclusion of palm oil in the diet tended to increase desirable fatty acids (DFAs) regardless of the week of milk sampled (P = 0.42). Treatment did not improve the atherogenicity index (AI), thrombogenicity index (TI), health-promoting index (HPI), and hypocholesterolemic/hypercholesterolemic (h/H) ratio. Results showed that adding rumen-protected palm oil is a plausible method to meet the energy intake of ewes required during lactation without negatively affecting lipid health indices. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Artificial intelligence (AI) in healthcare has now begun to make its contributions to real-world patient care with varying degrees of both public and clinical acceptability around it. The heavy investment from governments, industry and academia needed to reach this point has helped to surface different perspectives on AI. As clinical AI applications become a reality, however, there is an increasing need to harness and integrate patient perspectives, which address the distinct needs of different populations, healthcare systems and clinical problems more closely. Despite this need, patient perspectives on AI implementation have little presence in academic literature and within implementation science and are not sufficiently considered throughout the MedTech and eHealthtech product development cycle, which brings its own challenges and opportunities. This joint patient expert/clinician commentary aims to briefly summarise views on AI. It reflects upon recommendations on how stakeholders such as clinicians and Health & MedTech small and medium-sized enterprises (SMEs) can make practical usage of these views. The recommendations of the authors centre around how to work better with patients to enable both product centric and patient centric innovation and person-centred care. © 2023, The Author(s)."
"This dataset is a time series of tropical cyclones simulated using the high-resolution Nonhydrostatic Icosahedral Atmospheric Model (NICAM). By tracking tropical cyclones from 30 years of simulation data, 2,463 tracks that include the life stages of precursors (pre-TCs), tropical cyclones (TCs), and post-tropical cyclones (post-TCs), if any, were extracted. Each track data includes the time, latitude, longitude, maximum wind speed, minimum pressure, elapsed time since onset, and life-stage label of the tropical cyclone. The numbers of steps (6 h) for pre-TCs, TCs, and post-TCs were 45,288, 55,206, and 37,312, respectively. The dataset for each step also consists of atmospheric field data of multiple physical quantities, such as outgoing longwave radiation at the top-of-the-atmosphere, sea level pressure, sea surface temperature, specific humidity at 600 hPa, and zonal and meridional winds at 850 and 200 hPa over a 1000 km2 area that includes a tropical cyclone at its center. This dataset can be used to develop machine-learning models for the detection, intensity prediction, and cyclogenesis prediction of tropical cyclones. © 2023 The Author(s)"
"Context:: Many software systems can be tuned for multiple objectives (e.g., faster runtime, less required memory, less network traffic or energy consumption, etc.). Such systems can suffer from “disagreement” where different models have different (or even opposite) insights and tactics on how to optimize a system. For configuration problems, we show that (a) model disagreement is rampant; yet (b) prior to this paper, it has barely been explored. Objective:: We aim at helping practitioners and researchers better solve multi-objective configuration optimization problems, by resolving model disagreement. Method:: We propose a dimension reduction method called VEER that builds a useful one-dimensional approximation to the original N-objective space. Traditional model-based optimizers use Pareto search to locate Pareto-optimal solutions to a multi-objective problem, which is computationally heavy on large-scale systems. VEER builds a surrogate that can replace the Pareto sorting step after deployment. Results:: Compared to the prior state-of-the-art, for 11 configurable systems, VEER significantly reduces disagreement and execution time, without compromising the optimization performance in most cases. For our largest problem (with tens of thousands of possible configurations), optimizing with VEER finds as good or better optimizations with zero model disagreements, three orders of magnitude faster. Conclusion:: When employing model-based optimizers for multi-objective optimization, we recommend to apply VEER, which not only improves the execution time, but also resolves the potential model disagreement problem. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Recently, reinforcement learning (RL) is often used for learning the strategy of peg-in-hole tasks. However, traditional state representation of PiH RL might be either redundant or abstract, which leads to unnecessary learning steps and incompatibility with mathematical training optimization. To issue these problems, a geometric-feature (GF) state representation method for peg-in-hole DDPG (Deep Deterministic Policy Gradient) RL is proposed to get rid of the redundant states. Also, a network pre-training method for peg-in-hole DDPG based on GF representation is provided to help DDPG acquire basic assembly skills before training. Finally, we executed the simulation experiments in the environment with Open AI Gym + Pybullet to test the performance of the proposed GF representation based pre-training method. We also executed the lenrned strategy on the real-world Panda robot.  © 2016 IEEE."
"Background: Low attenuation volume percentage (LAV%) has been identified as a quantitative imaging biomarker for emphysema with good correlation with spirometry. The influence of intravenous contrast agent on LAV% and its correlation with spirometry is not well known. Purpose: To evaluate the influence of intravenous contrast agent on artificial intelligence (AI)-based LAV% in correlation with spirometric Tiffeneau-Pinelli Index (TI). Material and Methods: In a retrospective study, two groups of 47 patients (mean age 68.04 ± 12.64 and 67.89 ± 11.54 years) with either non-enhanced chest computed tomography (CT) or contrast-enhanced CT were compared. Using an AI-based software, LAV% was quantified using a threshold <−950 HU. TI was calculated from spirometry and pathologic airway obstruction was considered with a TI <70. The effect of contrast agent on LAV% and the relationship between TI and LAV% was analyzed. Correlation coefficients between TI and LAV% were compared for both groups. Results: Patients with non-enhanced CT had a mean LAV% of 9.07 ± 7.53. Of them, 22 patients had a TI <70% and 25 patients a TI ≥70%. Patients with contrast-enhanced CT had a mean LAV% of 6.54 ± 4.62. Of them, 20 patients had a TI <70% and 27 patients had a TI ≥70%. Contrast agent did not show a major effect on LAV% (P = 0.099) and the relationship between TI and LAV% (P = 0.88). In both groups, a significant correlation between TI and LAV% was found (ρ = −0.317 for non-enhanced CT; ρ = −0.514 for contrast-enhanced CT). Difference between correlation coefficients was insignificant. Conclusion: Our findings suggest that contrast agent does not influence LAV% nor its correlation with TI. © The Foundation Acta Radiologica 2023."
"INTRODUCTION:Acute infectious gastroenteritis (AGE) is a common reason for outpatient visits and hospitalizations in the United States. This study aimed to understand the demographic and clinical characteristics, common pathogens detected, health care resource utilization (HRU), and cost among adult outpatients with AGE visiting US health systems.METHODS:A retrospective cohort study was conducted using one of the largest hospital discharge databases (PINC AI Healthcare Database) in the United States. Adult patients (aged ≥18 years) with a principal diagnosis of AGE during an outpatient visit between January 1, 2016, and June 30, 2021, were included. Pathogen detection analysis was performed in those with microbiology data available.RESULTS:Among 248,896 patients, the mean age was 44.3 years (range 18-89+ years), 62.9% were female, and 68.5% were White. More than half (62.0%) of the patients did not have any preexisting comorbidity, and only 18.3% underwent stool workup at the hospital. Most patients (84.7%) were seen in the emergency department, and most (96.4%) were discharged home. Within 30 days of discharge, 1.0% were hospitalized, and 2.8% had another outpatient visit due to AGE. The mean cost of the index visit plus 30-day AGE-related follow-up was $1,338 per patient, amounting to $333,060,182 for the total study population. Among patients with microbiology data available (n = 12,469), common pathogens detected were Clostridioides difficile (32.2%), norovirus (6.3%), and Campylobacter spp. (4.0%).DISCUSSION:AGE is a common and costly disease affecting adults of all ages and more females than males, including individuals with or without baseline conditions in a hospital-based outpatient setting. C. difficile was the most common pathogen detected. © 2023 Wolters Kluwer Health. All rights reserved."
"Accurate identification of plant disease is of great significance for intelligent agriculture. Currently, plant species, disease, and severity are considered as joint categories in most disease classification methods, which will increase the number of categories and decrease the generalization ability of these models. Compared to disease and severity, information on plant species is less important because different species may suffer from the same disease, and such information is usually known to users. Given this, this paper proposed a novel triple-branch Swin Transformer classification (TSTC) network for classification of disease and severity simultaneously and separately. The TSTC network consists of a multitask feature extraction module, a feature fusion module and a deep supervision module. Firstly, preliminary features are extracted using a triple-branch network, which is built based on Swin Transformer backbone under the multitask classification strategy (i.e., one for disease classification, one for severity classification and the other for deep supervision). After that, these features are fused using compact bilinear pooling technique to enhance the feature extractor's learning ability and thus more discriminative features can be extracted. Finally, the deep supervision module combines losses from both hidden layers and the last layers of the TSTC so that it can be trained in the direction where all layers can work efficiently for disease and severity classifications. Compared to five widely used classification networks, experiments with the AI Challenger 2018 dataset shows that our proposed TSTC network achieves the highest accuracy with an overall accuracy of 99.00% for disease classification and 88.73% for severity classification. © 2023 Elsevier B.V."
[No abstract available]
"Purpose: To present a novel protocol for authentic three-dimensional (3D) planning of dental implants, using artificial intelligence (AI) and augmented reality (AR). Methods: The novel protocol consists of (1) 3D data acquisition, with an intraoral scanner (IOS) and cone-beam computed tomography (CBCT); (2) application of AI for CBCT segmentation to obtain standard tessellation language (STL) models and automatic alignment with IOS models; (3) loading of selected STL models within the AR system and surgical planning with holograms; (4) surgical guide design with open-source computer-assisted-design (CAD) software; and (5) surgery on the patient. Results: This novel protocol is effective and time-efficient when used for planning simple cases of static guided implant surgery in the partially edentulous patient. The clinician can plan the implants in an authentic 3D environment, without using any radiological guided surgery software. The precision of implant placement looks clinically acceptable, with minor deviations. Conclusions: AI and AR technologies can be successfully used in guided implant surgery for authentic 3D planning that may replace conventional software. However, further clinical studies are needed to validate this protocol. Statement of clinical relevance: The combined use of AI and AR may change the perspectives of modern guided implant surgery for authentic 3D planning that may replace conventional software. © 2023 Elsevier Ltd"
"Background and objective: The value of implementing artificial intelligence (AI) on ultrasound screening for thyroid cancer has been acknowledged, with numerous early studies confirming AI might help physicians acquire more accurate diagnoses. However, the black box nature of AI's decision-making process makes it difficult for users to grasp the foundation of AI's predictions. Furthermore, explainability is not only related to AI performance, but also responsibility and risk in medical diagnosis. In this paper, we offer Explainer, an intrinsically explainable framework that can categorize images and create heatmaps highlighting the regions on which its prediction is based. Methods: A dataset of 19341 thyroid ultrasound images with pathological results and physician-annotated TI-RADS features is used to train and test the robustness of the proposed framework. Then we conducted a benign-malignant classification study to determine whether physicians perform better with the assistance of an explainer than they do alone or with Gradient-weighted Class Activation Mapping (Grad-CAM). Results: Reader studies show that the Explainer can achieve a more accurate diagnosis while explaining heatmaps, and that physicians’ performances are improved when assisted by the Explainer. Case study results confirm that the Explainer is capable of locating more reasonable and feature-related regions than the Grad-CAM. Conclusions: The Explainer offers physicians a tool to understand the basis of AI predictions and evaluate their reliability, which has the potential to unbox the ""black box"" of medical imaging AI. © 2023 Elsevier B.V."
"Introduction: Artificial intelligence (AI), by means of computer vision in machine learning, is a promising tool for cholangiocarcinoma (CCA) diagnosis. The aim of this study was to provide a comprehensive overview of AI in medical imaging for CCA diagnosis. Methods: A systematic review with scientometric analysis was conducted to analyze and visualize the state-of-the-art of medical imaging to diagnosis CCA. Results: Fifty relevant articles, published by 232 authors and affiliated with 68 organizations and 10 countries, were reviewed in depth. The country with the highest number of publications was China, followed by the United States. Collaboration was noted for 51 (22.0%) of the 232 authors forming five clusters. Deep learning algorithms with convolutional neural networks (CNN) were the most frequently used classifiers. The highest performance metrics were observed with CNN-cholangioscopy for diagnosis of extrahepatic CCA (accuracy 94.9%; sensitivity 94.7%; and specificity 92.1%). However, some of the values for CNN in CT imaging for diagnosis of intrahepatic CCA were low (AUC 0.72 and sensitivity 44%). Conclusion: Our results suggest that there is increasing evidence to support the role of AI in the diagnosis of CCA. CNN-based computer vision of cholangioscopy images appears to be the most promising modality for extrahepatic CCA diagnosis. Our social network analysis highlighted an Asian and American predominance in the research relational network of AI in CCA diagnosis. This discrepancy presents an opportunity for coordination and increased collaboration, especially with institutions located in high CCA burdened countries. © 2023 The Authors. Journal of Gastroenterology and Hepatology published by Journal of Gastroenterology and Hepatology Foundation and John Wiley & Sons Australia, Ltd."
"Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders’ views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic. © 2023 The Authors"
"In Dec 2020, the results of AlphaFold version 2 were presented at CASP14, sparking a revolution in the field of protein structure predictions. For the first time, a purely computational method could challenge experimental accuracy for structure prediction of single protein domains. The code of AlphaFold v2 was released in the summer of 2021, and since then, it has been shown that it can be used to accurately predict the structure of most ordered proteins and many protein–protein interactions. It has also sparked an explosion of development in the field, improving AI-based methods to predict protein complexes, disordered regions, and protein design. Here I will review some of the inventions sparked by the release of AlphaFold. © 2023 The Author(s)"
"Background: The effect of disease modifying therapies (DMTs) on brain atrophy in persons with multiple sclerosis (pwMS) is typically investigated in highly standardized clinical trial settings or single-center academic institutions. We aimed at utilizing artificial intelligence (AI)-based volumetric analysis on routine unstandardized T2-FLAIR scans in determining the effect of DMTs on lateral ventricular volume (LVV) and thalamic volume (TV) changes in pwMS. Methods: The DeepGRAI (Deep Gray Rating via Artificial Intelligence) registry is a multi-center, longitudinal, observational, real-word study with a convenience sample of 1002 relapsing-remitting (RR) pwMS from 30 United States sites. Brain MRI exams acquired as part of the routine clinical management were collected at baseline and on average at 2.6-years follow-up. The MRI scans were acquired either on 1.5T or 3T scanners with no prior harmonization. TV was determined using the DeepGRAI tool and lateral ventricular volume LVV was measured using NeuroSTREAM software. Results: After propensity matching based on baseline age, disability and time of follow-up, untreated pwRRMS had significantly greater TV change when compared to treated pwRRMS (-1.2% vs. -0.3%, p = 0.044). PwRRMS treated with high-efficacy DMTs had significant and two-fold lower% LVV change when compared to pwRRMS treated on moderate-efficacy DMTs (3.5% vs. 7.0%, p = 0.001). PwRRMS who stopped DMT during the follow-up had significantly greater annualized% TV change compared to pwRRMS who remained on their DMT (-0.73% vs. -0.14%, p = 0.012) and significantly greater annualized% LVV change (3.4% vs. 1.7%, p = 0.047). These findings were also observed in a propensity analysis that additionally incorporated matching for scanner model at both baseline and follow-up visits. Conclusions: LVV and TV measured on T2-FLAIR scans can detect treatment-induced short-term neurodegenerative changes measured in a real-word unstandardized, multicenter, clinical routine setting. © 2023"
"Smart automobiles have become popular in recent years, facilitating the expansion of the Internet of Vehicles (IoV) networks. The Internet of Vehicles (IoV) is a network of automobiles with the ability to exchange and analyse data in real-time, necessitating a well-organized and effective data transmission method. Key problems in identifying an optimal path among the cars are cluster stability and dynamic topology change in IoV. The novelty of this manuscript lies in the route optimization method dependent on grid size, orientation, velocity, node number, and range. The proposed approach for creating and evaluating the best cluster head (CH) is derived from Harris Hawks' Optimization for Intelligent Route Clustering, for the optimal discovery of routes amongst the vehicles in the Internet of Vehicles networks. To analyse and validate the proposed method, other cutting-edge techniques are analysed. Considering the constraints such as the number of clusters and network, variable communication ranges, and vehicle quantity, our results suggest that the proposed method performs better than other techniques in the literature. Further experimentations have been performed considering Packet Delivery Ratio (PDR), bandwidth utilization, and latency which shows supremacy over other existing approaches. Furthermore, statistical analysis shows improvement in cluster optimization (by 80%) and increase stability of cluster (by 90.6 R-squared). © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"A materials informatics framework to explore a large number of candidate van der Waals (vdW) materials is developed. In particular, in this study a large space of monolayer transition metal halides is investigated by combining high-throughput density functional theory calculations and artificial intelligence (AI) to accelerate the discovery of stable materials and the prediction of their magnetic properties. The formation energy is used as a proxy for chemical stability. Semi-supervised learning is harnessed to mitigate the challenges of sparsely labeled materials data in order to improve the performance of AI models. This approach creates avenues for the rapid discovery of chemically stable vdW magnets by leveraging the ability of AI to recognize patterns in data, to learn mathematical representations of materials from data and to predict materials properties. Using this approach, previously unexplored vdW magnetic materials with potential applications in data storage and spintronics are identified. © 2023 Wiley-VCH GmbH."
"The current technological developments have paved the path for various fields to thrive, explore and enrich their current applications with the help of technology such as Artificial Intelligence (AI), Internet Technology, Wireless Technology, and the Internet Of Things (IoT). This research focused on providing energy-efficient software and IoT applications for sustainability. Sustainability is integrating the environmental, social, and economic resources to thrive healthy and different requirements to fulfill human needs simultaneously. It can also be referred to as the maintenance of the environment, especially the natural resources maintaining social equality and economic stability to create a healthy community for this generation and the next generation. For sustainability, this paper proposed a Region-based clustering and cluster-head election model to improve the Energy efficiency of IoT networks deployed in the Agriculture environment (REAN). The proposed methodology uses the Shortest Routing and Less Cost algorithm (SRLC) and Region Clustering and Cluster Head Selection (RCHS) algorithm to provide energy-efficient software and IoT application. The experimental results are used to verify the performance of IoT-based sustainable applications in a real-time environment. © 2023 The Author(s)"
"Objective: The aim of this study is to automatically assess the positional relationship between lower third molars (M3i) and the mandibular canal (MC) based on the panoramic radiograph(s) (PR(s)). Material and methods: A total of 1444 M3s were manually annotated and labeled on 863 PRs as a reference. A deep-learning approach, based on MobileNet-V2 combination with a skeletonization algorithm and a signed distance method, was trained and validated on 733 PRs with 1227 M3s to classify the positional relationship between M3i and MC into three categories. Subsequently, the trained algorithm was applied to a test set consisting of 130 PRs (217 M3s). Accuracy, precision, sensitivity, specificity, negative predictive value, and F1-score were calculated. Results: The proposed method achieved a weighted accuracy of 0.951, precision of 0.943, sensitivity of 0.941, specificity of 0.800, negative predictive value of 0.865 and an F1-score of 0.938. Conclusion: AI-enhanced assessment of PRs can objectively, accurately, and reproducibly determine the positional relationship between M3i and MC. Clinical significance: The use of such an explainable AI system can assist clinicians in the intuitive positional assessment of lower third molars and mandibular canals. Further research is required to automatically assess the risk of alveolar nerve injury on panoramic radiographs. © 2023 The Author(s)"
"A high-stakes event is an extreme risk with a low probability of occurring, but severe consequences (e.g., life-threatening conditions or economic collapse). The accompanying lack of information is a source of high-stress pressure and anxiety for emergency medical services authorities. Deciding on the best proactive plan and action in this environment is a complicated process, which calls for intelligent agents to automatically produce knowledge in the manner of human-like intelligence. Research in high-stakes decision-making systems has increasingly focused on eXplainable Artificial Intelligence (XAI), but recent developments in prediction systems give little prominence to explanations based on human-like intelligence. This work investigates XAI based on cause-and-effect interpretations for supporting high-stakes decisions. We review recent applications in the first aid and medical emergency fields based on three perspectives: available data, desirable knowledge, and the use of intelligence. We identify the limitations of recent AI, and discuss the potential of XAI for dealing with such limitations. We propose an architecture for high-stakes decision-making driven by XAI, and highlight likely future trends and directions. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Treatment of gout involves two basic approaches: reducing the serum uric acid mainly by xanthine oxidase inhibitors (XOIs) and alleviating the intensity of the accompanying acute arthritic inflammation using non-steroidal anti-inflammatory drugs (NSAIDs). Febuxostat (FEB) is the first non-purine XOI approved for the treatment of hyperuricemia and gout. The present study aims at combining the hypouricemic effect of FEB and the anti‐inflammatory (AI) properties of NSAIDs in a single entity by adopting the “mutual prodrug” approach. Accordingly, a series of seven ester prodrugs comprising basically FEB together with different NSAIDs namely, diclofenac (4), ibuprofen (5), ketoprofen (6), indomethacin (7), naproxen (8), ketorolac (9) and etodolac (10) was synthesized. All the investigated seven prodrugs (4–10) were equipotent or even superior to their corresponding parent drugs in the hypouricemic and AI activities, together with a gastrointestinal (GI) safety profile. Among this series, the prodrug FEB-DIC (4) showed excellent dual in vivo hypouricemic and anti-inflammatory activity (43.60 % and 15.96 %, respectively) when compared to the parent drugs FEB and diclofenac (36.82 % and 12.10 %, respectively) and its physical mixture (37.28 % and 12.41 %, respectively). Investigation of the in vitro chemical stability and hydrolysis of the prodrug (4) in aqueous and biological samples using a developed HPLC method confirmed its stability in various pHs, whereas rapid hydrolysis to the parent drugs in liver homogenate and human plasma was proven. Finally, it is concluded that the mutual prodrug approach could be successfully used in drug design and development for overcoming undesirable difficulties without losing the desired activities of the parent drugs. © 2023 Elsevier Inc."
"Purpose of Review: To discuss how advanced breast imaging modalities can supplement standard breast imaging with mammography, ultrasound, and MRI. Recent Findings: For the last 40–50 years, the primary breast cancer screening examination has been mammography which has undergone many changes from xeromammography, film-screen analog, to digital mammography techniques. Digital breast tomosynthesis (DBT) and contrast-enhanced mammography (CEM) are the most recent advances to digital mammography. Molecular breast imaging (MBI) and positron emission mammography (PEM) are nuclear medicine breast examinations that utilize mammographic positioning and are useful supplements to standard breast imaging examination. Summary: Advances in mammographic techniques have improved the sensitivity and specificity of mammography in detecting breast cancer, demonstrating the extent of disease, and evaluating the response to systemic treatments. Understanding how advanced mammographic techniques fit in with other available breast imaging examinations helps ensure optimal evaluation to help guide treatment decisions. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Modular construction (MC) is an increasingly important construction technique. However, it also requires the use of sophisticated scheduling algorithms. A comprehensive literature review of different scheduling systems used for prefabricated construction, was conducted using the PRISMA methodology. Over 500 relevant papers were analysed and 59 critical applications of production scheduling metaheuristics were closely examined. However, very few of these were for modular construction. This paper provides a deep analysis of GA applications in scheduling and the newer techniques using PSO, SA and ACO. The results of the review suggest 6 directions for future research namely, (i) Consideration of added complexities, (ii) Responsiveness of the scheduling system to new tenders or work, (iii) Integrating production scheduling with Manufacturing Execution and Control Systems, (iv) Dynamic scheduling, (v) Simulation-based optimization techniques for MC, (vi) Use of AI and machine learning concepts for MC. This paper will inform better production scheduling of MC projects. © 2023 Elsevier B.V."
"Background: Artificial neural networks (ANNs) can be a powerful tool for spectroscopic data analysis. Their ability to detect and model complex relations in the data may lead to outstanding predictive capabilities, but the predictions themselves are difficult to interpret due to the lack of understanding of the black box ANN models. ANNs and linear methods can be combined by first fitting a linear model to the data followed by a non-linear fitting of the linear model residuals using an ANN. This paper explores the use of residual modelling in high-dimensional data using modern neural network architectures. Results: By combining linear- and ANN modelling, we demonstrate that it is possible to achieve both good model performance while retaining interpretations from the linear part of the model. The proposed residual modelling approach is evaluated on four high-dimensional datasets, representing two regression and two classification problems. Additionally, a demonstration of possible interpretation techniques are included for all datasets. The study concludes that if the modelling problem contains sufficiently complex data (i.e., non-linearities), the residual modelling can in fact improve the performance of a linear model and achieve similar performance as pure ANN models while retaining valuable interpretations for a large proportion of the variance accounted for. Significance and novelty: The paper presents a residual modelling scheme using modern neural network architectures. Furthermore, two novel extensions of residual modelling for classification tasks are proposed. The study is seen as a step towards explainable AI, with the aim of making data modelling using artificial neural networks more transparent. © 2023 The Author(s)"
"Background: Explainable artificial intelligence (XAI) is a technology that can enhance trust in mental state classifications by providing explanations for the reasoning behind artificial intelligence (AI) models outputs, especially for high-dimensional and highly-correlated brain signals. Feature importance and counterfactual explanations are two common approaches to generate these explanations, but both have drawbacks. While feature importance methods, such as shapley additive explanations (SHAP), can be computationally expensive and sensitive to feature correlation, counterfactual explanations only explain a single outcome instead of the entire model. Methods: To overcome these limitations, we propose a new procedure for computing global feature importance that involves aggregating local counterfactual explanations. This approach is specifically tailored to fMRI signals and is based on the hypothesis that instances close to the decision boundary and their counterfactuals mainly differ in the features identified as most important for the downstream classification task. We refer to this proposed feature importance measure as Boundary Crossing Solo Ratio (BoCSoR), since it quantifies the frequency with which a change in each feature in isolation leads to a change in classification outcome, i.e., the crossing of the model's decision boundary. Results and Conclusions: Experimental results on synthetic data and real publicly available fMRI data from the Human Connect project show that the proposed BoCSoR measure is more robust to feature correlation and less computationally expensive than state-of-the-art methods. Additionally, it is equally effective in providing an explanation for the behavior of any AI model for brain signals. These properties are crucial for medical decision support systems, where many different features are often extracted from the same physiological measures and a gold standard is absent. Consequently, computing feature importance may become computationally expensive, and there may be a high probability of mutual correlation among features, leading to unreliable results from state-of-the-art XAI methods. © 2023 The Author(s)"
"Artificial intelligence (AI) has been applied to a wide spectrum of industrial sectors, but manual operations remain indispensable in most manufacturing systems due to high complexity or costs involving in automation. A more practical approach is enabling humans to collaborate with machines complementary to each other. One critical issue in human-robot collaboration (HRC) is to assure the operator's productivity, safety, and trust while interacting with the robot. This paper presents an experimental study on user interface design in augmented reality (AR) for human-robot collaborative assembly in a shared workspace. The experiment aims to verify and cross-compare the effectiveness of visual and haptic cues in various forms that convey the robot intent to human. Analysis of the work performance and gazing behavior of participants shows that both cues can reduce their visual attention on the moving robot during the collaboration. The interface that provides the proximity of robot using visual cues is considered most useful. It is not intuitive to recognize complex information by vibration on different parts of the human hand in the experiment. Finally, human trust to robot has a higher correlation on the usability of a user interface than the work performance assisted by the interface. These findings may work as design guidelines for AR assisted human-robot interaction in smart manufacturing. © 2023 The Society of Manufacturing Engineers"
"Objectives of this experiment were to study the effect of infusing utero-pathogenic bacteria to induce endometrial inflammation on productive performance in early lactation and subsequent reproduction. Although endometritis is associated with perturbed reproduction, numerous factors may contribute to the observed association. It was hypothesized that induced endometrial inflammation, resulting in localized and systemic inflammatory responses, compromises production and reproduction. Holstein cows without clinical disease and with less than 18% polymorphonuclear leukocytes (PMN) in endometrial cytology on d 31 ± 3 postpartum had their estrous cycle synchronized. Cows were blocked by parity and genomic breeding value for cow conception rate and, within block, assigned randomly to remain as untreated controls (CON; n = 37) or to receive an intrauterine infusion of 5.19 × 108 cfu Escherichia coli and 4.34 × 108 cfu Trueperella pyogenes during the luteal phase to induce endometrial inflammation (INF; n = 48). Endometrial cytology was taken on d 2 and 7 after treatment to evaluate the proportion of PMN. Rectal temperature, dry matter intake, and yields of milk and components were measured in the first 7 d after treatment. Blood serum was analyzed for concentration of haptoglobin. Leukocytes were isolated from blood on d 2 and 7 after treatment and on d 19 after artificial insemination (AI) and mRNA was quantified for a select group of genes. Cows received AI and reproduction was followed for 300 d postpartum. Bacterial infusion induced endometrial inflammation with increased proportions of PMN in the endometrial cytology on d 2 (4.4 ± 0.7 vs. 26.3 ± 2.8%) and 7 (10.9 ± 1.7 vs. 17.4 ± 2.1%) after treatment, resulting in increased mean prevalence of subclinical endometritis (>10% PMN; 23.3 ± 6.3 vs. 80.9 ± 5.1%). Rectal temperature did not differ between CON and INF, but the concentration of haptoglobin in serum tended to increase in INF compared with CON (113 ± 14 vs. 150 ± 16 µg/mL). Induced endometrial inflammation reduced yields of milk (44.9 ± 0.8 vs. 41.6 ± 0.8 kg/d), protein (1.19 ± 0.03 vs. 1.12 ± 0.03 kg/d), and lactose (2.17 ± 0.04 vs. 2.03 ± 0.04 kg/d) and tended to reduce dry matter intake (20.7 ± 0.5 vs. 19.4 ± 0.6 kg/d) in the first 7 d after treatment. Indeed, the reduction in milk yield lasted 4 wk. However, treatment did not affect yields of energy-corrected milk or fat because treatment with INF increased the concentration of fat in milk (3.54 ± 0.10 vs. 3.84 ± 0.10%). Induced endometrial inflammation reduced pregnancy per AI at all inseminations (33.4 ± 5.1 vs. 21.6 ± 3.7%) and the hazard of pregnancy (0.61; 95% CI = 0.36–1.04), which extended the median days open by 24 d. Blood leukocytes from INF cows had increased mRNA expression of the pro-inflammatory gene IL1B on d 2 and 7 after treatment, but reduced expression of the IFN-stimulated genes ISG15 and MX2 on d 19 after AI. Induced endometrial inflammation depressed production and caused long-term negative effects on reproduction in lactating dairy cows. © 2023 American Dairy Science Association"
"During global outbreaks such as COVID-19, regular nucleic acid amplification tests (NAATs) have posed unprecedented burden on hospital resources. Data of traditional NAATs are manually analyzed post assay. Integration of artificial intelligence (AI) with on-chip assays give rise to novel analytical platforms via data-driven models. Here, we combined paper microfluidics, portable optoelectronic system with deep learning for SARS-CoV-2 detection. The system was quite streamlined with low power dissipation. Pixel by pixel signals reflecting amplification of synthesized SARS-CoV-2 templates (containing ORF1ab, N and E genes) can be real-time processed. Then, the data were synchronously fed to the neural networks for early prediction analysis. Instead of the quantification cycle (Cq) based analytics, reaction dynamics hidden at the early stage of amplification curve were utilized by neural networks for predicting subsequent data. Qualitative and quantitative analysis of the 40-cycle NAATs can be achieved at the end of 22nd cycle, reducing time cost by 45%. In particular, the attention mechanism based deep learning model trained by microfluidics-generated data can be seamlessly adapted to multiple clinical datasets including readouts of SARS-CoV-2 detection. Accuracy, sensitivity and specificity of the prediction can reach up to 98.1%, 97.6% and 98.6%, respectively. The approach can be compatible with the most advanced sensing technologies and AI algorithms to inspire ample innovations in fields of fundamental research and clinical settings. © 2023"
"Deep learning (DL) models find increasing application in mental state decoding, where researchers seek to understand the mapping between mental states (e.g., experiencing anger or joy) and brain activity by identifying those spatial and temporal features of brain activity that allow to accurately identify (i.e., decode) these states. Once a DL model has been trained to accurately decode a set of mental states, neuroimaging researchers often make use of methods from explainable artificial intelligence research to understand the model's learned mappings between mental states and brain activity. Here, we benchmark prominent explanation methods in a mental state decoding analysis of multiple functional Magnetic Resonance Imaging (fMRI) datasets. Our findings demonstrate a gradient between two key characteristics of an explanation in mental state decoding, namely, its faithfulness and its alignment with other empirical evidence on the mapping between brain activity and decoded mental state: explanation methods with high explanation faithfulness, which capture the model's decision process well, generally provide explanations that align less well with other empirical evidence than the explanations of methods with less faithfulness. Based on our findings, we provide guidance for neuroimaging researchers on how to choose an explanation method to gain insight into the mental state decoding decisions of DL models. © 2023"
"The xylem is the water transport tissue of vascular plants, and its structure is closely related to the efficiency and safety of water transport. Studying how xylem anatomical traits and related functional traits vary with environmental gradients is important for understanding plant responses to environmental changes and ecological adaptation strategies. In this study, ten populations of the dominant shrub Reaumuria soongarica were selected along a west-east transect using an aridity index (AI, the ratio of annual precipitation to annual potential evapotranspiration) ranging from 0.02 to 0.09 in the desert region of northwest China. Based on AI, the sampling sites were divided into arid regions (0.05 ≤ AI < 0.2) and hyper-arid regions (AI < 0.05). Pearson correlation analysis, linear mixed models, and plant trait networks were used to analyze the spatial variation characteristics and climatic factors of 12 traits in three categories, including xylem anatomical traits, hydraulic functional traits, and mechanical strength. The results were as follows: (1) Vessel diameter contributing 95% hydraulic conductivity (D95) and wood density (WD) significantly differed between arid and hyper-arid regions, whereas the other traits were not significantly different. (2) Mean temperature of the driest quarter was one of the most important climatic factors driving the most trait variation. With the increase in AI and the precipitation of the driest quarter, D95 and the vessel grouping index (Vg) significantly increased, but not WD. The mean temperature of the driest quarter was significantly negatively correlated with D95 and WD but not significantly correlated with Vg. (3) Correlations between most traits were significant, but Vg was not significantly correlated with all other traits. Theoretical hydraulic conductivity was significantly positively correlated with the vulnerability index, indicating a trade-off between the efficiency and safety of the xylem hydraulic system. (4) Coordination among multiple traits differed at the transect and regional climate levels, and the number of edges of the trait network of hyper-arid regions was more than that of arid regions. Moreover, network analyses showed that mean vessel diameter and theoretical hydraulic conductivity are central traits in arid and hyper-arid regions, respectively, because of the number and strength of the relationships with other traits, which could be the important predictor of related hydraulic functions and associations. In conclusion, the xylem anatomical structure of R. soongarica can adapt to changing environments by different combinations of functional traits. Plant trait networks provide a novel approach for exploring the adaptation strategies of desert shrubs along environmental gradients with future climate change. © 2023 Elsevier B.V."
"Archaeologists and computer scientists have both studied board games since the early days of their fields. Early archaeologists had an interest in identifying ways of playing the games of antiquity, and they applied diffusionist models fashionable at the time to trace the development of games from antiquity to the games played in nineteenth century Europe and North America. In time, a huge amount of data on ancient games was collected, and in the last thirty years archaeologists have studied games as they relate to social processes. In parallel to this, artificial intelligence (AI) research has utilized board games, primarily as testbeds for developing AI techniques, but also as an application domain. Archaeological and AI methods are combined in the Digital Ludeme Project, which documents the preserved knowledge of ancient games and uses computational techniques to evaluate research questions that can be addressed through AI playouts of proposed rulesets for games. © 2023 The Author(s)"
"Purpose of Review: The purpose of this review is to summarize the current research and critically examine artificial intelligence (AI) technologies and their applicability to the daily practice of anesthesiologists. Recent Findings: Novel AI tools are developed using data from electronic health records, imaging, waveforms, clinical notes, and wearables. These tools can accurately predict the perioperative risk for adverse outcomes, the need for blood transfusion, and the risk of difficult intubation. Intraoperatively, AI models can assist with technical skill augmentation, patient monitoring, and management. Postoperatively, AI technology can aid in preventing complications and discharge planning. While further prospective validation is needed, these early applications demonstrate promise in every area of perioperative care. Summary: The practice of anesthesiology is at a precipice fueled by technological innovation. The clinical AI implementation would enable personalized and safer patient care by offering actionable insights from the wealth of perioperative data. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"This Commentary discusses the current developments, potential applications, and challenges of artificial intelligence in attention-deficit hyperactivity disorder treatment. © ©The Author(s) 2023."
"Background: Post-operative cardiac complications occur infrequently but contribute to mortality after liver transplantation (LT). Artificial intelligence-based algorithms based on electrocardiogram (AI-ECG) are attractive for use during pre-operative evaluation to screen for risk of post-operative cardiac complications, but their use for this purpose is unknown. Aims: The aim of this study was to evaluate the performance of an AI-ECG algorithm in predicting cardiac factors such as asymptomatic left ventricular systolic dysfunction or potential for developing post-operative atrial fibrillation (AF) in cohorts of patients with end-stage liver disease either undergoing evaluation for transplant or receiving a liver transplant. Methods: A retrospective study was performed in two consecutive adult cohorts of patients who were either evaluated for LT or underwent LT at a single center between 2017 and 2019. ECG were analyzed using an AI-ECG trained to recognize patterns from a standard 12-lead ECG which could identify the presence of left ventricular systolic dysfunction (LVEF < 50%) or subsequent atrial fibrillation. Results: The performance of AI-ECG in patients undergoing LT evaluation is similar to that in a general population but was lower in the presence of prolonged QTc. AI-ECG analysis on ECG in sinus rhythm had an AUROC of 0.69 for prediction of de novo post-transplant AF. Although post-transplant cardiac dysfunction occurred in only 2.3% of patients in the study cohorts, AI-ECG had an AUROC of 0.69 for prediction of subsequent low left ventricular ejection fraction. Conclusions: A positive screen for low EF or AF on AI-ECG can alert to risk of post-operative cardiac dysfunction or predict new onset atrial fibrillation after LT. The use of an AI-ECG can be a useful adjunct in persons undergoing transplant evaluation that can be readily implemented in clinical practice. Graphical Abstract: [Figure not available: see fulltext.]. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Smart replies, writing enhancements, and virtual assistants powered by artificial intelligence (AI) language technologies are becoming part of consumer products and everyday experiences. This study explores the opportunities and risks of using language-generating AI systems in politics to increase legislative responsiveness. Legislators receive a large volume of constituent communication and often cannot devote individual consideration and timely response to each. Here, AI language technologies may allow legislators to process constituent communication more efficiently. For example, AI writing tools can suggest reply snippets when a staffer responds to a common concern. However, legislative human-AI collaboration could reduce constituent trust or undermine the representative process. In two experiments, we compared constituents' impressions of human-written legislative correspondence to correspondences partially or fully generated by GPT-3, a state-of-the-art language model. Our results suggest that legislative correspondence generated by AI with human oversight may be received favorably and increase constituent trust compared to generic auto-responses that busy legislators may employ. However, poorly performing AI language technologies may damage confidence in the legislator. Our findings highlight the potential and risks of introducing AI-mediated communication to the representation process. We discuss the importance of disclosure, transparency, and maintaining human-in-the-loop accountability for political deployments of AI language technologies. © 2023 Elsevier Inc."
[No abstract available]
"The discovery of functional protein complex and the interrogation of the complex structure-function relationship (SFR) play crucial roles in the understanding and intervention of biological processes. Affinity purification-mass spectrometry (AP-MS) has been proved as a powerful tool in the discovery of protein complexes. However, validation of these novel protein complexes as well as elucidation of their molecular interaction mechanisms are still challenging. Recently, native top-down MS (nTDMS) is rapidly developed for the structural analysis of protein complexes. In this review, we discuss the integration of AP-MS and nTDMS in the discovery and structural characterization of functional protein complexes. Further, we think the emerging artificial intelligence (AI)-based protein structure prediction is highly complementary to nTDMS and can promote each other. We expect the hybridization of integrated structural MS with AI prediction to be a powerful workflow in the discovery and SFR investigation of functional protein complexes. © 2023 Elsevier Ltd"
"Over the past month, a new AI model called Chatbot Generative Pre-trained Transformer (ChatGPT), has received enormous attention in the media and scientific communities due to its ability to process and respond to commands in a humanistic fashion. As reported, five days after its launch, the number of registered users of ChatGPT exceeded one million, and its monthly active users had exceeded 100 million two months later, making it the most rapidly growing consumer application in history. The advent of ChatGPT has further brought about new ideas and challenges in the realm of infectious disease. In view of this, in order to evaluate the potential use of ChatGPT in clinical practice and scientific research of infectious disease, we conducted a brief online survey by using the publicly available ChatGPT webpage. Also, the present study also talks about the relevant social and ethical issues related to this program. Graphical Abstract: [Figure not available: see fulltext.] © 2023, The Author(s) under exclusive licence to Biomedical Engineering Society."
"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications. © 2023 by the authors."
"The paper examines the open problems that experts of space law shall increasingly address over the next few years, according to four different sets of legal issues. Such differentiation sheds light on what is old and what is new with today’s troubles of space law, e.g., the privatization of space, vis-à-vis the challenges that AI raises in this field. Some AI challenges depend on its unique features, e.g., autonomy and opacity, and how they affect pillars of the law, whether on Earth or in space missions. The paper insists on a further class of legal issues that AI systems raise, however, only in outer space. We shall never overlook the constraints of a hazardous and hostile environment, such as on a mission between Mars and the Moon. The aim of this paper is to illustrate what is still mostly unexplored or in its infancy in this kind of research, namely, the fourfold ways in which the uniqueness of AI and that of outer space impact both ethical and legal standards. Such standards shall provide for thresholds of evaluation according to which courts and legislators evaluate the pros and cons of technology. Our claim is that a new generation of sui generis standards of space law, stricter or more flexible standards for AI systems in outer space, down to the “principle of equality” between human standards and robotic standards, will follow as a result of this twofold uniqueness of AI and of outer space. © 2023, The Author(s)."
"Facial recognition technology has been adopted for the better delivery of m-government services, but the ease and frequency with which users' personal information is being accessed and misused have inevitably increased. This has recently triggered a new phenomenon of privacy fatigue in e-governance, which makes people feel powerless to protect their privacy. A comprehensive understanding of users' privacy fatigue is important, since negative user perceptions may lead to disengagement and the distrust of the government. Drawing on the Person-Environment fit theory and user agency perspective, this study examines the antecedents of privacy fatigue, manifested as emotional exhaustion and cynicism. A large-scale telephone survey allowed data to be collected from 3, 436 users of facial recognition-based m-Gov services in China. The Partial Least Squares (PLS) results reveal that: (1) privacy fatigue indeed occurs among the users of facial recognition-based m-Gov services in China; (2) the perceptions of privacy control is the core antecedent of users' privacy fatigue, which influences two dimension—emotional exhaustion and cynicism—in different directions; (3) the effectiveness of both privacy self-efficacy and government legislation positively influences perceived privacy control, while the effect of the effectiveness of privacy policies on it is insignificant. This study contributes to the research by empirically testing the notion and antecedents of privacy fatigue in the context of facial recognition-based m-Gov services, and provides a mechanism analysis with an overarching theoretical framework. In addition, the findings may generate new research avenues related to privacy fatigue under an AI-enabled government. © 2023 Elsevier Inc."
"Understanding the importance of data is crucial for realizing the full potential of AI in architectural design. Satellite images are extremely numerous, continuous, high resolution, and accessible, allowing nuanced experimentation through dataset curation. Combining deep learning with remote-sensing technologies, this study poses the following questions. Do newly available datasets uncover ideas about the city previously hidden because urban theory is predominantly Eurocentric? Do extensive and continuous datasets promise a more refined examination of datasets’ effects on outcomes? Generative adversarial networks can endlessly generate new designs based on a curated dataset, but architectural evaluation has been questionable. We employ quantitative and qualitative assessment metrics to investigate human collaboration with AI, producing results that contribute to understanding AI-based urban design models and the significance of dataset curation. © The Author(s) 2023."
"In this paper, we discuss the relation between recent philosophical discussions about meaning in life (from authors like Susan Wolf, Thaddeus Metz, and others) and the ethics of artificial intelligence (AI). Our goal is twofold, namely, to argue that considering the axiological category of meaningfulness can enrich AI ethics, on the one hand, and to portray and evaluate the small, but growing literature that already exists on the relation between meaning in life and AI ethics, on the other hand. We start out our review by clarifying the basic assumptions of the meaning in life discourse and how it understands the term ‘meaningfulness’. After that, we offer five general arguments for relating philosophical questions about meaning in life to questions about the role of AI in human life. For example, we formulate a worry about a possible meaningfulness gap related to AI on analogy with the idea of responsibility gaps created by AI, a prominent topic within the AI ethics literature. We then consider three specific types of contributions that have been made in the AI ethics literature so far: contributions related to self-development, the future of work, and relationships. As we discuss those three topics, we highlight what has already been done, but we also point out gaps in the existing literature. We end with an outlook regarding where we think the discussion of this topic should go next. © 2023, The Author(s)."
"Artificial intelligence (AI) has received great attention since the concept was proposed, and it has developed rapidly in recent years with applications in many fields. Meanwhile, newer iterations of smartphone hardware technologies which have excellent data processing capabilities have leveraged on AI capabilities. Based on the desirability for portable detection, researchers have been investigating intelligent analysis by combining smartphones with AI algorithms. Various examples of the application of AI algorithm-based smartphone detection and analysis have been developed. In this review, we give an overview of this field, with a particular focus on bioanalytical detection applications. The applications are presented in terms of hardware design, software algorithms, and specific application areas. We also discuss the existing limitations of AI-based smartphone detection and analytical approaches, and their future prospects. The take-home message of our review is that the application of AI in the field of detection analysis is restricted by the limitations of the smartphone's hardware as well as the model building of AI for detection targets with insufficient data. Nevertheless, at this juncture, while bioanalytical diagnostics and health monitoring have set the pace for AI-based smartphone applicability, the future should see the technology making greater inroads into other fields. In relation to the latter, it is likely that the ordinary or average person will play a greater participatory role. © 2023 Elsevier B.V."
"Manual screening of Ziehl-Neelsen (ZN)-stained slides that are negative or contain rare acid-fast mycobacteria (AFB) is labor-intensive and requires repetitive refocusing to visualize AFB candidates under the microscope. Whole slide image (WSI) scanners have enabled implementation of AI to classify digital ZN-stained slides as AFB+ or AFB-. By default, these scanners acquire a single-layer WSI. However, some scanners can acquire a multilayer WSI with a z-stack and an extended focus image layer embedded. We developed a parameterized WSI classification pipeline to assess whether multilayer imaging improves ZN-stained slide classification accuracy. A CNN built into the pipeline classified tiles in each image layer to form an AFB probability score heatmap. Features extracted from the heatmap were then entered into a WSI classifier. 46 AFB+ and 88 AFB- single-layer WSIs were used for the classifier training. 15 AFB+ (with rare microorganisms) and 5 AFB- multilayer WSIs comprised the test set. Parameters in the pipeline included: (a) a WSI representation: z-stack of image layers, middle image layer (a single image layer equivalent) or an extended focus image layer, (b) 4 methods of aggregating AFB probability scores across the z-stack, (c) 3 classifiers, (d) 3 AFB probability thresholds, and (e) 9 feature vector types extracted from the aggregated AFB probability heatmaps. Balanced accuracy (BACC) was used to measure the pipeline performance for all parameter combinations. Analysis of Covariance (ANCOVA) was used to statistically evaluate the effect of each parameter on the BACC. After adjusting for other factors, a significant effect of the WSI representation (p-value < 1.99E-76), classifier type (p-value < 1.73E-21), and AFB threshold (p-value = 0.003) was observed on the BACC. The feature type had no significant effect (p-value = 0.459) on the BACC. WSIs represented by the middle layer, extended focus layer and the z-stack followed by the weighted averaging of AFB probability scores were classified with the average BACC of 58.80%, 68.64%, and 77.28%, respectively. The multilayer WSIs represented by the z-stack with the weighted averaging of AFB probability scores were classified by a Random Forest classifier with the average BACC of 83.32%. Low classification accuracy of WSIs represented by the middle layer suggests that they contain fewer features permitting identification of AFB than the multilayer WSIs. Our results indicate that single-layer acquisition can introduce a bias (sampling error) into the WSI. This bias can be mitigated by the multilayer or the extended focus acquisitions. © 2023 Elsevier B.V."
"Artificial intelligence (AI) and machine learning (ML) methods have touched practically all aspects of our life. Their utility ranges from separating different quality agricultural produce to facial recognition to guiding us through most steps in our day-to-day life. In this perspective article, we demonstrate the utility of artificial neural network (ANN) method in fitting potential energy curves and surfaces and point out the potential applications to predicting and analyzing dynamical observables. Although the regression methods seem to be successful in fitting potential energy surfaces using limited ab initio data, the ANN method yields accurate fits of surfaces when enough number of ab initio points on the potential energy surface become available. The possibility of utilizing the ANN method for fitting excitation function data is pointed out and the implications are discussed. Graphical abstract: This perspective article illustrates how the artificial neural network can be used to interpolate accurately potential energy curves and surfaces for molecular systems and how the method can be extended to systems with avoided crossing of potential energy curves and to multidimensional excitation function data.[Figure not available: see fulltext.] © 2023, Indian Academy of Sciences."
[No abstract available]
"Wearable sensors enable down range data collection of physiological and cognitive performance of the warfighter. However, autonomous teams may find the sensor data impractical to interpret and hence influence real-time decisions without the support of subject matter experts. Decision support tools can reduce the burden of interpreting physiological data in the field and incorporate a systems perspective where noisy field data can contain useful additional signals. We present a methodology of how artificial intelligence can be used for modeling human performance with decision-making to achieve actionable decision support. We provide a framework for systems design and advancing from the laboratory to real world environments. The result is a validated measure of down-range human performance with a low burden of operation. © 2023 Sports Medicine Australia"
"The transistor celebrated its 75th birthday in 2022. The continued scaling of the transistor defined by Moore's law continues, albeit at a slower pace. Meanwhile, computing demands and energy consumption required by modern artificial intelligence (AI) algorithms have skyrocketed. As an alternative to scaling transistors for general-purpose computing, the integration of transistors with unconventional technologies has emerged as a promising path for domain-specific computing. In this article, we provide a full-stack review of probabilistic computing with p-bits as a representative example of the energy-efficient and domain-specific computing movement. We argue that p-bits could be used to build energy-efficient probabilistic systems, tailored for probabilistic algorithms and applications. From hardware, architecture, and algorithmic perspectives, we outline the main applications of probabilistic computers ranging from probabilistic machine learning (ML) and AI to combinatorial optimization and quantum simulation. Combining emerging nanodevices with the existing CMOS ecosystem will lead to probabilistic computers with orders of magnitude improvements in energy efficiency and probabilistic sampling, potentially unlocking previously unexplored regimes for powerful probabilistic algorithms.  © 2014 IEEE."
"Microwave discharge is effective in improving microwave energy conversion efficiency due to its significant thermal effect. An in-depth analysis of the thermal effect is the key to understanding the nature of microwave discharge. However, the mechanism of thermal effect of microwave discharge is still unclear because of the deficiency of decoupling research on the thermal effect in the process of microwave discharge. Consequently, the spherical carbon-based dielectric is used for igniting microwave discharge, and the total heat generated in the discharge process is stripped into the electromagnetic conversion heat and the additional heat generated by the discharge. On this basis, a model matching microwave-induced discharge with the spherical carbon-based dielectric is established. Furthermore, the heat transfer mode of discharge additional heat source is discussed and its heat generation rate is calculated. The results show that the errors between the temperature rise curves obtained by this model and the real measured curves are kept within ±5%. The heat generation rate of discharge additional heat source is a function of the average temperature of the dielectric gaps, following a relation of qDA,i(T) = Ai + BiT + CiT2. The work is of great significance to reveal the mechanism of thermal effect in process of microwave discharge. © 2023 Energy Institute"
"Quadcopters are popular UAVs owing to their compact size and maneuverability. Quadcopters are unmanned aircraft guided by remote control, and the demand for them is increasing due to their widespread surveillance, goods delivery, aerial photography, and defense applications. Nonlinear quadcopter operation makes control system implementation very challenging. In this paper, based on artificial intelligence (AI), we train a feedforward neural network (FFNN) controller of a traditional proportional integral derivative (PID). The conventional (PID) is generally tuned to improve the quadcopter control and performance. FFNN can perform offline learning between the inputs and outputs of the controller to learn its behavior. Once the learning is complete, we replace the PID controller with the neural network controller, to get a controller that can maintain system stabil-ity, and overcome the limitations of hardware implementation problems caused by the classical PID controller. © 2023, Institute of Advanced Engineering and Science. All rights reserved."
"Militairy technology is developing at a rapid pace and we are seeing a growing number of weapons with increasing levels of autonomy being developed and deployed. This raises various legal, ethical, and security concerns. The absence of clear international rules setting limits and governing the use of autonomous weapons is extremely concerning. There is an urgent need for the international community to work together towards a treaty not only to safeguard ethical and legal norms, but also for our shared security. This article explains why a treaty on autonomous weapons is needed and achievable. It goes into what a treaty could consist of to establish an international norm and set rules and limits on autonomy in weapon systems. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"The objective of this observational study was to evaluate the association of transition cow health and estrous expression, detected by an automated activity monitoring system (Smarttag Neck, Nedap Livestock Management), with reproductive performance in lactating Holstein cows. A total of 3,750 lactating Holstein cows (1,563 primiparous cows and 2,187 multiparous cows) from a commercial dairy farm in Slovakia calving from January 2020 until July 2021 were enrolled on an ongoing basis. Activity data were recorded from d 7 until d 60 postpartum. Within this observational period, cows were classified into 3 categories: (1) no estrus event (Estrus0), (2) 1 estrus event (Estrus1), or (3) 2 or more estrus events (Estrus2+). Transition cow health was assessed by farm personnel within the first 30 d in milk (DIM) using standard operating procedures. Generalized linear mixed models were used to analyze continuous and categorical data. Cox proportional hazard models were used for time to event data. The overall prevalence of anestrus was 20.8%. Multiparous cows had a greater risk for anestrus compared with primiparous cows [odds ratio (OR) = 1.4]. Cows with stillbirth (OR = 1.76), retained placenta (OR = 2.19), puerperal metritis (OR = 1.48), or subclinical ketosis (OR = 1.51) had a greater risk for anestrus. In addition, cows calving in summer (OR = 0.82), autumn (OR = 0.38), or winter (OR = 0.56) had a higher incidence of anestrus than cows calving in spring. Estrous expression from d 7 until d 60 postpartum was associated with estrous duration (DU) and estrous intensity at first artificial insemination (AI). Cows in Estrus0 had the shortest DU at first postpartum AI (9.4 ± 0.18 h) compared with cows in Estrus1 (10.5 ± 0.13 h) and Estrus2+ (11.4 ± 0.12 h). Cows in Estrus2+ had a longer DU at first postpartum AI compared with cows in Estrus1. For Estrus0, Estrus1, and Estrus2+ cows, pregnancy per AI at first service was 42.5%, 50.9%, and 55.4%, respectively. Estrous expression from d 7 until d 60 postpartum was associated with time to first AI and time to pregnancy. Compared with Estrus0 cows, Estrus1 [hazard ratio (HR) = 1.43] and Estrus2+ cows (HR = 1.62) had an increased hazard of being inseminated within 100 DIM. Compared with Estrus2+, Estrus1 cows had a reduced hazard of being inseminated within 100 DIM (HR = 0.89). Compared with Estrus0 cows, Estrus1 (HR = 1.24) and Estrus2+ cows (HR = 1.46) had an increased hazard of becoming pregnant within 200 DIM. Median DIM to pregnancy were 121, 96, and 92 for Estrus0, Estrus1, and Estrus2+ cows, respectively. In conclusion, cows with transition cow disorders (i.e., stillbirth, retained placenta, puerperal metritis, or subclinical ketosis) had a greater chance for anestrus compared with healthy cows. Cows in Estrus0 had reduced estrous expression at first AI and inferior reproductive performance compared with cows that displayed estrous activity from d 7 until d 60. © 2023 American Dairy Science Association"
"Many sectors have been fundamentally altered by the entrance of the 5G era due to the rapid advancement of information technology and computer technology. A fresh wave of digital media art (DMA) creation and invention has taken place in the current context. DMA is a brand-new field of study that brings together art and digital technology in a powerful way. It is a modern, multidisciplinary, and versatile art topic that is merged with other art themes. Humans used to be the primary means of creating digital media like animation. The labor of creating media content from raw sources is progressively being replaced by computers with advancements in artificial intelligence (AI) technology. Virtual reality (VR) technology's popularity has rapidly spread beyond the computer area to other parts of life, and it has also evolved into a new approach to DMA. Art will surely be highly influenced in the AI age, but we must also recognize the new developments brought to art by technical advancement in the 5G and AI. Hence, in this work, we examine the properties of virtual reality technology as well as the two most widely utilized approaches, artificial intelligence-based image recognition technology (AI-IRT) and artificial intelligence-based speech recognition technology (AI-SRT). We investigate and practice them in detail. We also compared these technologies to conventional teaching methods and discovered that visuals and pictures were considerably more responsive and enlightening to students than traditional teaching methods. © 2023 The Author(s)"
"The ability to predict the maintenance needs of machines is generating increasing interest in a wide range of industries as it contributes to diminishing machine downtime and costs while increasing efficiency when compared to traditional maintenance approaches. Predictive maintenance (PdM) methods, based on state-of-the-art Internet of Things (IoT) systems and Artificial Intelligence (AI) techniques, are heavily dependent on data to create analytical models capable of identifying certain patterns which can represent a malfunction or deterioration in the monitored machines. Therefore, a realistic and representative dataset is paramount for creating, training, and validating PdM techniques. This paper introduces a new dataset, which integrates real-world data from home appliances, such as refrigerators and washing machines, suitable for the development and testing of PdM algorithms. The data was collected on various home appliances at a repair center and included readings of electrical current and vibration at low (1 Hz) and high (2048 Hz) sampling frequencies. The dataset samples are filtered and tagged with both normal and malfunction types. An extracted features dataset, corresponding to the collected working cycles is also made available. This dataset could benefit research and development of AI systems for home appliances' predictive maintenance tasks and outlier detection analysis. The dataset can also be repurposed for smart-grid or smart-home applications, predicting the consumption patterns of such home appliances. © 2023 The Author(s)"
"In this mini review, we capture the latest progress of applying artificial intelligence (AI) techniques based on deep learning architectures to molecular de novo design with a focus on integration with experimental validation. We will cover the progress and experimental validation of novel generative algorithms, the validation of QSAR models and how AI-based molecular de novo design is starting to become connected with chemistry automation. While progress has been made in the last few years, it is still early days. The experimental validations conducted thus far should be considered proof-of-principle, providing confidence that the field is moving in the right direction. © 2023 Elsevier Ltd"
"Objective: The aim of this study was to test the plausibility of using the ΦX174 bacteriophage as a tracer of viral aerosols spreading in a dental aerosol-generating procedure (AGP) model. Methods: ΦX174 bacteriophage (~ 108 plaque-forming units (PFU)/mL) was added into instrument irrigation reservoirs and aerosolized during class-IV cavity preparations followed by composite fillings on natural upper-anterior teeth (n = 3) in a phantom head. Droplets/aerosols were sampled through a passive approach that consisted of Escherichia coli strain C600 cultures immersed in a LB top agar layer in Petri dishes (PDs) in a double-layer technique. In addition, an active approach consisted of E coli C600 on PDs sets mounted in a six-stage cascade Andersen impactor (AI) (simulating human inhalation). The AI was located at 30 cm from the mannequin during AGP and afterwards at 1.5 m. After collection PDs were incubated overnight (18 h at 37 °C) and bacterial lysis was quantified. Results: The passive approach disclosed PFUs mainly concentrated over the dental practitioner, on the mannequin’s chest and shoulder and up to 90 cm apart, facing the opposite side of the AGP’s source (around the spittoon). The maximum aerosol spreading distance was 1.5 m in front of the mannequin’s mouth. The active approach disclosed collection of PFUs corresponding to stages (and aerodynamic diameters) 5 (1.1–2.1 µm) and 6 (0.65–1.1 µm), mimicking access to the lower respiratory airways. Conclusion: The ΦX174 bacteriophage can be used as a traceable viral surrogate in simulated studies contributing to understand dental bioaerosol’s behavior, its spreading, and its potential threat for upper and lower respiratory tract. Clinical relevance: The probability to find infectious virus during AGPs is high. This suggests the need to continue characterizing the spreading viral agents in different clinical settings through combination of passive and active approaches. In addition, subsequent identification and implementation of virus-related mitigation strategies is relevant to avoid occupational virus infections. © 2023, The Author(s)."
"Cancer immunotherapy is a method of controlling and eliminating tumors by reactivating the body's cancer-immunity cycle and restoring its antitumor immune response. The increased availability of data, combined with advancements in high-performance computing and innovative artificial intelligence (AI) technology, has resulted in a rise in the use of AI in oncology research. State-of-the-art AI models for functional classification and prediction in immunotherapy research are increasingly used to support laboratory-based experiments. This review offers a glimpse of the current AI applications in immunotherapy, including neoantigen recognition, antibody design, and prediction of immunotherapy response. Advancing in this direction will result in more robust predictive models for developing better targets, drugs, and treatments, and these advancements will eventually make their way into the clinical setting, pushing AI forward in the field of precision oncology. © 2023 Elsevier Ltd"
"Component reliability plays a pivotal role in industrial systems, which are evolving with larger complexity and higher dimensionality of data. It is insufficient to ensure reliability and prevent failure based only on empiri- cal and parametric assumptions. Driven by huge amount of historical data, data- and statistics-based approaches aided by artificial intelligence (AI) are emerging as promising solutions. Especially, with the introduction to deep learning technology, the powerful ability of hierarchy representation is re- markably enhanced with deep cascaded layers. Furthermore, the demand for AI technology is high, and the applicability of the model in securing reliability, failure prediction and prevention in the industrial system is still nontrivial. Yet, there hardly exists such a systematic review of the AI-based approaches. In this survey, we provide a comprehensive overview of the AI- aided approaches to failure analysis in industrial systems, with sufficient or insufficient data, and imbalanced issues. We provide a concise introduction to the popular AI algorithms, classify the application scenarios of industrial systems into homogeneous or heterogeneous data-based scenarios, and review them respectively. We also summarize the resolved issues, challenges and promising directions. © 2023 Elsevier Ltd"
"This research study presents the design and simulation of a hybrid active power filter (HAPF) for reducing harmonics. The reference currents have been determined using the synchronous reference frame technique. To achieve its goals, the proposed HAPF employs AI algorithm known as particle swarm optimization (PSO) to fine-tune the proportional-integral PI controller's parameters. With the help of PI-PSO controller the DC link voltage is regulated in the HAPF-inverter. A non-linear current control strategy based on hysteresis employed here to construct the pulse gate by comparing the retrieved reference and real currents necessitated by the HAPF. Simulations were carried out in MATLAB and shown that the proposed method is extremely adaptable and efficient in reducing harmonic currents caused by non-linear loads. © 2023, Institute of Advanced Engineering and Science. All rights reserved."
"Objectives: The role of startups has been growing in healthcare delivery, particularly in telehealth and telemedicine. Yet, little has been published about their role in evolving digital healthcare ecosystem. This study aimed to review the literature on telehealth startups to understand their roles, challenges, business models, and directions for sustainable innovation and commercialization. Methods: Ten databases were screened: PubMed, Scopus, Web of Science, IEEE Xplore, ACM digital library, EBSCOhost, Embase, Medline, Cochrane review, and PsycINFO. The articles were shortlisted based on pre-determined screening criteria, and qualitative synthesis was performed. The quality of included studies was assessed using the Mixed Methods Appraisal Tool. Cohen's K was calculated to ensure the reliability of the authors scoring on the quality appraisal test and qualitative synthesis. Results: 26 articles were included in the review. Findings are clubbed under five themes: remote and on-demand healthcare; healthcare data management; digital therapeutics; high-tech driven personalized care; and information integration and exchange. Technical infrastructure, regulation, and revenue generation were identified as major challenges for telehealth start-ups. Osterwalder business canvas was the predominantly used model. Value perspectives were recognized for a sustainable telehealth innovation and its commercialization. Conclusion: Telehealth startups are evolving to meet digital healthcare needs and playing a significant role in teleconsultations, telemonitoring, and electronic health record solutions. Recently, their focus has shifted towards smartphone-enabled AI-driven personalized care, including digital therapeutics and wearable device innovation. They have significant technical and operational challenges in innovation and commercialization to optimize their role. The review also provides researchers with a new understanding of telehealth startups’ sustainable innovation and commercialization through the systematic direction of value proposition, creation, and capture. © 2023 The Author(s)"
"Context: Colorectal cancer (CRC) has a high mortality rate and a large financial burden. Therefore, it is imperative to screen appropriately for this disease. By evaluating trends in different CRC screening methods and evaluating screening methods based on sex and race, improvements in screening can be made. Objectives: By analyzing data from the Behavioral Risk Factor Surveillance System (BRFSS), our primary objective was to evaluate trends in CRC screening methods from 2018 through 2020. Our secondary objectives were to investigate deviations in screening rates by sex and race/ethnicity. Methods: A cross-sectional design was utilized to analyze trends in CRC screening methods utilizing data from the BRFSS for the years 2018 through 2020. Sex and race were also analyzed to evaluate for deviations in screening rates. Results: All race/ethnicity groups most often completed colonoscopies, with all but individuals identifying as Hispanic having higher than 56% completion rates. Individuals reporting as Hispanic received more blood stool tests than other races at 23.4%. Average CRC screening among all methods showed that 89.7% of individuals who reported as being White completed screening, with 91.3% of individuals reporting as Black, and 81.9% with race not listed, completed screening. Individuals identifying as Asian (74.4%), American Indian/Alaska Native (AI/AN [79.2%]) and Hispanic (78.1%) had lower rates of screening overall. Conclusions: Our study found that trends in CRC screening were similar across years for individuals who reported as being White or Black. We also found that those identifying as Asian, AI/AN, Hispanic, and those whose identifying race was not listed deviated across years. These latter groups were also less likely to have received colonoscopies, the gold standard of screening. Because CRC is oftentimes a preventable disease, the importance of appropriate screening cannot be emphasized enough. © 2023 the author(s), published by De Gruyter, Berlin/Boston."
"We study means of geometric type of quasi-Toeplitz matrices, that are semi-infinite matrices A=(ai,j)i,j=1,2,… of the form A= T(a) + E, where E represents a compact operator, and T(a) is a semi-infinite Toeplitz matrix associated with the function a, with Fourier series ∑k=-∞∞akeikt, in the sense that (T(a)) i,j= aj-i. If a is real valued and essentially bounded, then these matrices represent bounded self-adjoint operators on ℓ2. We prove that if a1, … , ap are continuous and positive functions, or are in the Wiener algebra with some further conditions, then matrix geometric means, such as the ALM, the NBMP and the weighted mean of quasi-Toeplitz positive definite matrices associated with a1, … , ap, are quasi-Toeplitz matrices associated with the function (a1⋯ap)1p, which differ only by the compact correction. We introduce numerical algorithms for their computation and show by numerical tests that these operator means can be effectively approximated numerically. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Dependable and accurate monitoring of elderly at home becomes crucial to limit both the costs and human efforts of following up elderly for establishing a healthy care system. Human Activity Recognition (HAR) tools, based on sensors installed in smart homes, will become an important tool to provide useful information to the caregiver when something happens in the house of an elderly and care is required. The current available detection tools either exist out of interpretable knowledge-driven techniques or scalable data-driven ones. In this paper, a hybrid methodology that combines both approaches is designed and evaluated to Track Activities by Linking Knowledge (TALK). Both sensor data and their link to the relevant domain knowledge about where those sensors are installed, the performed activities that occur, and how the household is constructed, are generalized in a specific knowledge graph (KG) structure to represent continuous events. The interpretable knowledge graph embedding technique Instance Neighboring using Knowledge (INK) is then used to transform these events inside the KG to a tabular format, which can be used by any traditional machine learning classifier to create a HAR tool. The TALK methodology is evaluated on two HAR datasets and shows (a) that TALK outperforms both traditional automated data-driven as well as knowledge-driven techniques in terms of predictive performance, and (b) how TALK can be easily used in a more out of lab environment. All these results and the interpretable aspects show that TALK can become an important tool to monitor elderly in their homes efficiently, effectively and with less intrusive techniques. © 2023 Elsevier Ltd"
"Background: AI/ML CAD tools can potentially improve outcomes in the high-stakes, high-volume model of trauma radiology. No prior scoping review has been undertaken to comprehensively assess tools in this subspecialty. Purpose: To map the evolution and current state of trauma radiology CAD tools along key dimensions of technology readiness. Methods: Following a search of databases, abstract screening, and full-text document review, CAD tool maturity was charted using elements of data curation, performance validation, outcomes research, explainability, user acceptance, and funding patterns. Descriptive statistics were used to illustrate key trends. Results: A total of 4052 records were screened, and 233 full-text articles were selected for content analysis. Twenty-one papers described FDA-approved commercial tools, and 212 reported algorithm prototypes. Works ranged from foundational research to multi-reader multi-case trials with heterogeneous external data. Scalable convolutional neural network–based implementations increased steeply after 2016 and were used in all commercial products; however, options for explainability were narrow. Of FDA-approved tools, 9/10 performed detection tasks. Dataset sizes ranged from < 100 to > 500,000 patients, and commercialization coincided with public dataset availability. Cross-sectional torso datasets were uniformly small. Data curation methods with ground truth labeling by independent readers were uncommon. No papers assessed user acceptance, and no method included human–computer interaction. The USA and China had the highest research output and frequency of research funding. Conclusions: Trauma imaging CAD tools are likely to improve patient care but are currently in an early stage of maturity, with few FDA-approved products for a limited number of uses. The scarcity of high-quality annotated data remains a major barrier. © 2023, The Author(s), under exclusive licence to American Society of Emergency Radiology (ASER)."
"Purpose: There is a growing body of diagnostic performance studies for emergency radiology-related artificial intelligence/machine learning (AI/ML) tools; however, little is known about user preferences, concerns, experiences, expectations, and the degree of penetration of AI tools in emergency radiology. Our aim is to conduct a survey of the current trends, perceptions, and expectations regarding AI among American Society of Emergency Radiology (ASER) members. Methods: An anonymous and voluntary online survey questionnaire was e-mailed to all ASER members, followed by two reminder e-mails. A descriptive analysis of the data was conducted, and results summarized. Results: A total of 113 members responded (response rate 12%). The majority were attending radiologists (90%) with greater than 10 years’ experience (80%) and from an academic practice (65%). Most (55%) reported use of commercial AI CAD tools in their practice. Workflow prioritization based on pathology detection, injury or disease severity grading and classification, quantitative visualization, and auto-population of structured reports were identified as high-value tasks. Respondents overwhelmingly indicated a need for explainable and verifiable tools (87%) and the need for transparency in the development process (80%). Most respondents did not feel that AI would reduce the need for emergency radiologists in the next two decades (72%) or diminish interest in fellowship programs (58%). Negative perceptions pertained to potential for automation bias (23%), over-diagnosis (16%), poor generalizability (15%), negative impact on training (11%), and impediments to workflow (10%). Conclusion: ASER member respondents are in general optimistic about the impact of AI in the practice of emergency radiology and its impact on the popularity of emergency radiology as a subspecialty. The majority expect to see transparent and explainable AI models with the radiologist as the decision-maker. © 2023, The Author(s), under exclusive licence to American Society of Emergency Radiology (ASER)."
"Although immunotherapy (IO) has changed the paradigm for the treatment of patients with advanced non-small cell lung cancers (aNSCLC), only around 30% to 50% of treated patients experience a long-term benefit from IO. Furthermore, the identification of the 30 to 50% of patients who respond remains a major challenge, as programmed Death-Ligand 1 (PD-L1) is currently the only biomarker used to predict the outcome of IO in NSCLC patients despite its limited efficacy. Considering the dynamic complexity of the immune system-tumor microenvironment (TME) and its interaction with the host's and patient's behavior, it is unlikely that a single biomarker will accurately predict a patient's outcomes. In this scenario, Artificial Intelligence (AI) and Machine Learning (ML) are becoming essential to the development of powerful decision-making tools that are able to deal with this high-complexity and provide individualized predictions to better match treatments to individual patients and thus improve patient outcomes and reduce the economic burden of aNSCLC on healthcare systems. I3LUNG is an international, multicenter, retrospective and prospective, observational study of patients with aNSCLC treated with IO, entirely funded by European Union (EU) under the Horizon 2020 (H2020) program. Using AI-based tools, the aim of this study is to promote individualized treatment in aNSCLC, with the goals of improving survival and quality of life, minimizing or preventing undue toxicity and promoting efficient resource allocation. The final objective of the project is the construction of a novel, integrated, AI-assisted data storage and elaboration platform to guide IO administration in aNSCLC, ensuring easy access and cost-effective use by healthcare providers and patients. © 2023"
"Underwater Robots such as Autonomous Underwater Vehicles (AUVs) and Remotely Operated Vehicles (ROVs) has played an important role in many tasks, such as marine environmental monitoring, underwater resource exploration, oil and gas industries, hydrographic surveys, military missions, etc. Underwater robotic swarm is a team of cooperative underwater robots which focuses on controlling multiple underwater robots to work in an organic group. In contrast to a single underwater robot, underwater robotic swarm represents higher operation efficiency and better stability while executing complex tasks. However, it needs higher intelligence to realize complementary cooperation than a single robot. It is beneficial to researchers to present a comprehensive survey of the state of the art of cooperative research for underwater robotic swarm. We observe that the research of Artificial Intelligence (AI) for multiple underwater robots is still in an early stage. In this paper, we study different collaborative operation mode in detail, such as formation control, task allocation, path planning, obstacle avoidance, flocking control etc. We propose different classification frameworks for these research topics and it also can be used to compare different methods and help engineers choose suitable methods for various applications. To achieve better cooperative performance of underwater robots, there are several key factors, including multi-source heterogeneous sensing, cooperative communication and navigation, information fusion and decision. Moreover, cooperative AI for underwater robotic swarm has different kinds of interesting and helpful applications. Finally, several possible applied AI methods including meta-heuristic algorithms, deep learning method and distributed learning method are accomplishing to cooperation of underwater robotic swarm. © 2023 Elsevier B.V."
"Whenever a numerical method produces accurate results, it creates an interesting functional equation, and because regularities is not assumed, unexpected solutions can emerge. Thus, this paper is mainly devoted to finding solutions to a generalized functional equation constructed in this spirit; namely, we solve the generalized form of the functional equation considered in Fechner and Gselmann (Publ Math Debrecen 80(1–2):143–154, 2012), then considered in Nadhomi et al. (Aequationes Math 95:1095–1117, 2021) and continued in Okeke and Sablik (Results Math 77:125, https://doi.org/10.1007/s00025-022-01664-x, 2022), that is we find the polynomial functions satisfying the following functional equation, ∑i=1nγiF(aix+biy)=∑j=1m(αjx+βjy)f(cjx+djy),for every x, y∈ R, γi, αj, βj∈ R, and ai, bi, cj, dj∈ Q, and its special forms. Thus we continue investigations presented in Nadhomi et al. (Aequationes Math 95:1095–1117, 2021) where we generalized the left hand side of Fechner–Gselmann equation and those from Okeke and Sablik (Results Math 77:125, https://doi.org/10.1007/s00025-022-01664-x, 2022) where the right hand side of the Fechner–Gselmann equation was studied. It turns out that under some assumptions on the parameters involved, the pair (F, f) solving Eq. (0.1) happens to be a pair of polynomial functions. © 2023, The Author(s)."
"The conversation between humans and Artificial Intelligence (AI)-enabled intelligent voice assistants (IVA) can create bonds that go beyond a mere utilitarian purpose. The emotional cues in a Human-AI conversation can lead consumers to feel connected with the AI-agents and even consider such a relationship as cool. Although brand coolness is known to affect consumer behavior, little is known about how consumers perceive a close relationship with IVAs and what the drivers of their use or avoidance are. Therefore, the current paper adds to the literature by analyzing how AI-enabled voice assistant experience affects IVA coolness and customer-brand relationships using the attachment-aversion (A-A) theory. A total of 308 consumers showed that affective, behavioral, and intellectual experiences with the intelligent voice assistant affect IVA coolness. IVA coolness was also found to affect A-A relationships positively, influencing consumers’ motivational strength to adopt, maintain and enhance the relationship in the future. © 2023 The Author(s)"
"In today's era, one of the important applications of Artificial Intelligence (AI) is Human Activity Recognition (HAR). It has a wide range of applicability in health monitoring for patients with chronic diseases, gaming consoles for gesture recognition, etc. Sensor-based HAR systems use signals collected over a period of time to label an activity. When we design an efficient sensor-based HAR system, a model requires learning an optimal association of spatial and temporal features. In this article, we propose a sensor-based HAR technique using the deep learning approach. We present a deep reverse transformer-based attention mechanism to guide the side residual features Unlike the conventional bottom-up approaches for feature fusion, we exploit a top-down feature fusion approach. The reverse attention is self-calibrated throughout the course of learning, which regularizes the attention modules and dynamically adjusts the learning rate. The overall framework outperforms several state-of-the-art methods and is shown to be statistically significant against these methods on five publicly available sensor-based HAR datasets, namely, MHEALTH, USC-HAD, WHARF, UTD-MHAD1, and UTD-MHAD2. Further, we conduct an ablation study to showcase the importance of each of the components of the proposed framework. Source code of this work is available at https://github.com/rishavpramanik/RevTransformerAttentionHAR. © 2023 Elsevier Ltd"
"Based on a historical debate on bipolar logical reasoning, this article surveys the rugged road from negative numbers to quantum intelligence (QI) machinery. The survey leads to a logical distinction of QI from AI. It is shown that, although negative numbers have been forbidden to enter logical formulation, they played major roles in the history of science. With their acceptance into bipolar logical axiomatization, they are expected to play key roles in the advancement of modern science with logically definable causality for quantum superposition/entanglement. It is shown that the advancement can extend truth-based local reality to an equilibrium-based theory of global realism in which logically definable causality can be analytically tested and applied in the development of mind-light-matter unity AI&QI machinery for observing, thinking, and imagination in quantum-digital compatible terms. It is concluded that, without negative numbers, neither quantum mechanics nor analytical quantum computing would be possible. Philosophically, it is shown that negative numbers lead to an objective answer to the Needham puzzle. Logically, as an extension to the Turning question “Can machine think?”, negative numbers lead to the deeper, trickier, and potentially more realistic question: “If AI machine cannot think, can QI machine think?” © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
"Anemia of inflammation (AI) is frequently present in subjects with inflammatory disorders, primarily caused by inflammation-driven iron retention in macrophages. So far, only limited data on qualitative and quantitative estimates of tissue iron retention in AI patients exist. We performed a prospective cohort study analyzing splenic, hepatic, pancreatic, and cardiac iron content with MRI-based R2*-relaxometry in AI patients, including subjects with concomitant true iron deficiency (AI+IDA) hospitalized between 05/2020–01/2022. Control groups were individuals without inflammation. Spleen R2* values in AI patients with ferritin ≤200 μg/L (AI+IDA) were comparable with those found in controls. In AI patients with ferritin >200 μg/L, spleen (47.6 s−1 vs. 19.3 s−1, p <.001) and pancreatic R2* values (32.5 s−1 vs. 24.9 s−1, p =.011) were significantly higher compared with controls, while liver and heart R2*-values did not differ. Higher spleen R2* values were associated with higher ferritin, hepcidin, CRP, and IL-6 concentrations. Spleen R2* values normalized in AI patients after recovery (23.6 s−1 vs. 47.6 s−1, p =.008), while no changes were found in patients with baseline AI+IDA. This is the first study investigating tissue iron distribution in patients with inflammatory anemia and AI with concomitant true iron deficiency. The results support the findings in animal models demonstrating iron retention in macrophages, which are primarily accumulating in the spleen under inflammatory conditions. MRI-related iron measurement may help to better characterize actual iron needs and to define better biomarker thresholds in the diagnosis of true ID in patients with AI. It may qualify as a useful diagnostic method to estimate the need for iron supplementation and to guide therapy. © 2023 The Authors. American Journal of Hematology published by Wiley Periodicals LLC."
"Background: Artificial intelligence (AI)-based algorithms have been developed to facilitate rapid and accurate computed tomography angiography (CTA) assessment in proximal large vessel occlusion (LVO) acute ischemic stroke, including internal carotid artery and M1 occlusions. In clinical practice, however, the detection of medium vessel occlusion (MeVO) represents an ongoing diagnostic challenge in which the added value of AI remains unclear. Purpose: To assess the diagnostic performance of AI platforms for detecting M2 occlusions. Methods: Studies that report the diagnostic performance of AI-based detection of M2 occlusions were screened, and sensitivity and specificity data were extracted using the semi-automated AutoLit software (Nested Knowledge, MN) platform. STATA (version 16 IC; Stata Corporation, College Station, Texas, USA) was used to conduct all analyses. Results: Eight studies with a low risk of bias and significant heterogeneity were included in the quantitative and qualitative synthesis. The pooled estimates of sensitivity and specificity of AI platforms for M2 occlusion detection were 64% (95% CI, 53 to 74%) and 97% (95% CI, 84 to 100%), respectively. The area under the curve (AUC) in the SROC curve was 0.79 (95% CI, 0.74 to 0.83). Conclusion: The current performance of the AI-based algorithm makes it more suitable as an adjunctive confirmatory tool rather than as an independent one for M2 occlusions. With the rapid development of such algorithms, it is anticipated that newer generations will likely perform much better. © 2023 Elsevier Masson SAS"
"The validity and reliability of diagnoses in psychiatry is a challenging topic in mental health. The current mental health categorization is based primarily on symptoms and clinical course and is not biologically validated. Among multiple ongoing efforts, neurological observations alongside clinical evaluations are considered to be potential solutions to address diagnostic problems. The Bipolar-Schizophrenia Network on Intermediate Phenotypes (B-SNIP) has published multiple papers attempting to reclassify psychotic illnesses based on biological rather than symptomatic measures. However, the effort to investigate the relationship between this new categorization approach and other neuroimaging techniques, including resting-state fMRI data, is still limited. This study focused on investigating the relationship between different psychotic disorders categorization methods and resting-state fMRI-based measures called dynamic functional network connectivity (dFNC) using state-of-the-art artificial intelligence (AI) approaches. We applied our method to 613 subjects, including individuals with psychosis and healthy controls, which were classified using both the Diagnostic and Statistical Manual of Mental Disorders (DSM-IV) and the B-SNIP biomarker-based (Biotype) approach. Statistical group differences and cross-validated classifiers were performed within each framework to assess how different categories. Results highlight interesting differences in occupancy in both DSM-IV and Biotype categorizations compared to healthy individuals, which are distributed across specific transient connectivity states. Biotypes tended to show less distinctiveness in occupancy level and included fewer cellwise differences. Classification accuracy obtained by DSM-IV and Biotype categories were both well above chance. Results provided new insights and highlighted the benefits of both DSM-IV and biology-based categories while also emphasizing the importance of future work in this direction, including employing further data types. © 2023 The Authors. Human Brain Mapping published by Wiley Periodicals LLC."
"In order to evaluate the effect of reduced doses of trifluralin in integration with non-chemical management treatments on weeds, essential oil yield and composition of dill (Anethum graveolens L.), a 2-year study was conducted in East Azarbaijan, Iran in 2020–2021. The study was conducted as factorial experiment based on randomized complete block design with four replications. The first factor was trifluralin use at four levels (non-use, 480, 720 and 960 g ai ha−1) and the second factor was non-chemical weed management treatments consisted of using the living mulch of fenugreek (Trigonella foenum-graecum L.) (FLM) and bitter vetch (Vicia ervilia (L.) Willd. (VLM), wheat straw mulch (WSM), one time hand weeding (OHW) and without non-chemical management (control). The results indicated that at all trifluralin doses the weed biomasses in OHW and WSM treatments were lower than those in FLM and VLM. The relative ranking of non-chemical weed management treatments for weed biomass reduction and essential oil yield of A. graveolens L.was OHW > WSM > FLM and VLM. The results of GC–MS analysis indicated that the main chemical constituents of essential oil (carvone, limonene and dill apiole) were influenced differently by trifluralin use and non-chemical weed management treatment. We can conclude that recommended dose of trifluralin could be replaced with reduced doses of trifluralin in integration with non-chemical treatments in order to improve the weed control efficacy and essential oil yield of A. graveolens L. in a sustainable production system. © 2023 Elsevier Ltd"
"Definition of the problem: The umbrella term “explicability” refers to the reduction of opacity of artificial intelligence (AI) systems. These efforts are challenging for medical AI applications because higher accuracy often comes at the cost of increased opacity. This entails ethical tensions because physicians and patients desire to trace how results are produced without compromising the performance of AI systems. The centrality of explicability within the informed consent process for medical AI systems compels an ethical reflection on the trade-offs. Which levels of explicability are needed to obtain informed consent when utilizing medical AI? Arguments: We proceed in five steps: First, we map the terms commonly associated with explicability as described in the ethics and computer science literature, i.e., disclosure, intelligibility, interpretability, and explainability. Second, we conduct a conceptual analysis of the ethical requirements for explicability when it comes to informed consent. Third, we distinguish hurdles for explicability in terms of epistemic and explanatory opacity. Fourth, this then allows to conclude the level of explicability physicians must reach and what patients can expect. In a final step, we show how the identified levels of explicability can technically be met from the perspective of computer science. Throughout our work, we take diagnostic AI systems in radiology as an example. Conclusion: We determined four levels of explicability that need to be distinguished for ethically defensible informed consent processes and showed how developers of medical AI can technically meet these requirements. © 2023, The Author(s)."
"The Architecture, Engineering and Construction (AEC) sector currently exhibits a significant scarcity of systematised information in databases (DB). This characteristic is a relevant obstacle to implementing new methodologies in the sector, which have proven highly successful in other industries. In addition, this scarcity also contrasts with the intrinsic workflow of the AEC sector, which generates a high volume of documentation throughout the construction process. To help solve this issue, the present work focuses on the systematisation of the data related to the contracting and public tendering procedure in Portugal, summarising the steps to obtain and process this information through the use of scraping algorithms, as well as the subsequential translation of the gathered data into English. The contracting and public tendering procedure is one of the most well-documented procedures at the national level, having all its data available as open-access. The resulting DB comprises 5214 unique contracts, characterised by 37 distinct properties. This paper identifies future development opportunities that can be supported by this DB, such as the application of descriptive statistical analysis techniques and/or Artificial Intelligence (AI) algorithms, namely, Machine Learning (ML) and Natural Language Processing (NLP), to improve construction tendering. © 2023"
"In recent years, edge-related smart computing is the way to process and store data via outsourcing environments. Security is the main progressive concept in smart-related Internet of Things (IoT) software denied networks to share and increase the scalability and efficiency in data storage. Because of the rapid growth of different smart services, different security-related problems may appear in resource processing and sharing data in real-time computing systems. To increase the reliability in secure data storage and satisfy the basic requirements related to IoT-related smart computing networks. So propose and implement a Novel Artificial Intelligence based Blockchain Secure Model (NAIBSM) to provide efficient secure data storage in IoT-related smart computing systems. This model flexibly captures each user authentication for the detection of different user-related attacks (i.e. distributed denial-of-service (DDOS)) in the storage of data via applying Artificial Intelligence (AI) calculation method i.e. Leverage Bat algorithm to explore complex features. Build a blockchain at the server side to provide secure communication and reliability of storing data on the terminal of IoT. This approach provides random hash values to each data to ensure blockchain for the integrity of data and uses a weight-based data storage procedure to arrange/store and classify data to each user with secure and unsecured storage in smart computing networks. The experimental results of the proposed model are to explore complex security features and ensure the performance of secure authentication to each user and better security, accuracy, and low communication overhead in IoT-related smart computing data storage and sharing systems. © 2023 The Authors"
"Carotid plaque is a biomarker of generalized atherosclerosis, and may predict ischemic stroke. Carotid intima-media thickness (C-IMT) measurement with ultrasonography imaging could capture the condition of carotid plaque. However, manual measurement of C-IMT is observer-dependent, resulting in observer bias and low reproducibility. In this study, we develop artificial intelligence (AI) framework that could automatically measure the C-IMT, and compared it with C-IMT measured by board of expert. This is a retrospective study done in Dr. Moewardi General Hospital, Surakarta, Indonesia. Carotid B-mode ultrasonography images were measured by panel of expert and by AI. After annotation process on Neurabot platform, AI could detect region of interest (ROI), and would do segmentation on the area to measure C-IMT autonomously. Dependent T-test was used to evaluate validity, and Cronbach’s alpha was used to find the reliability of C-IMT measured by panel of expert and AI. There was strong correlation (r=0.874; p=0.014) on dependent t-test for C-IMT measured by AI with C-IMT measured by board of expert. The internal consistency reliability coefficients (Cronbach’s alpha) were 0.938 and 0.909, for pretest and posttest, respectively. We also analyzed the test-retest reliability by comparing pretest and posttest score with dependent t-test, and we observed strong correlation with r=0.871 (p=0.000). AI developed on Neurabot platform are valid and reliable to measure C-IMT. © 2023, Intelektual Pustaka Media Utama. All rights reserved."
"Artificial intelligence (AI) plays a prominent role in smart cities' development and offers benefits to different services such as finance, healthcare, security, agriculture, transport, education, and manufacturing. Despite the expected benefits, the adoption of AI varies from one smart city to another, due in part to barriers that can inhibit a smart city from adopting AI. The aim of this paper is to provide a comprehensive view of the barriers faced by smart cities. Through a systematic literature review, this study identifies 18 primary and secondary barriers grouped into three main categories — technology, environment, and organization. This research contributes to the literature by developing a typology of AI adoption barriers based on the Technology-Organization-Environment (TOE) perspective. The typology provides a novel mapping of the barriers to AI adoption faced by smart cities and suggests directions for further investigation through a cohesive research agenda. At a practical level, the findings will allow policymakers, planners, and citizens to make more informed decisions about AI adoption. Practical implications are also proposed for guiding smart cities to increase the adoption of AI. © 2023 Elsevier Inc."
"This study addresses two critical research gaps in human-robot interaction (HRI): the limited systematic research on the role of trust in customers’ acceptance of artificially intelligent (AI) robots; and the lack of understanding of robot acceptance under different cultural backgrounds. Drawing on the AIDUA framework, this study examines the impacts of trust and moderating effects of both national (the U.S. and China) and individual culture on customers’ intentions to use AI robots in hospitality services by developing a theoretical model. The model is tested on data collected using online data collection platforms from 491 U.S. and 495 Chinese respondents. PLS-SEM and the bootstrapping method were used to test the hypothesized relationships and analyze the moderating effects of culture, respectively. The findings suggest that trust in interaction with AI robots is a significant higher-order construct that influences the intention of use. Furthermore, uncertainty avoidance, long-term orientation, and power distance have been found to exhibit significant moderation effects. The results of this study extend the theoretical frameworks in HRI and provide detailed guidance to promote AI robot applications across different cultures. © 2023 Elsevier Ltd"
"Objectives: To develop and assess the performance of a novel artificial intelligence (AI)-driven convolutional neural network (CNN)-based tool for automated three-dimensional (3D) maxillary alveolar bone segmentation on cone-beam computed tomography (CBCT) images. Materials and Methods: A total of 141 CBCT scans were collected for performing training (n = 99), validation (n = 12), and testing (n = 30) of the CNN model for automated segmentation of the maxillary alveolar bone and its crestal contour. Following automated segmentation, the 3D models with under- or overestimated segmentations were refined by an expert for generating a refined-AI (R-AI) segmentation. The overall performance of CNN model was assessed. Also, 30% of the testing sample was randomly selected and manually segmented to compare the accuracy of AI and manual segmentation. Additionally, the time required to generate a 3D model was recorded in seconds (s). Results: The accuracy metrics of automated segmentation showed an excellent range of values for all accuracy metrics. However, the manual method (95% HD: 0.20 ± 0.05 mm; IoU: 95% ± 3.0; DSC: 97% ± 2.0) showed slightly better performance than the AI segmentation (95% HD: 0.27 ± 0.03 mm; IoU: 92% ± 1.0; DSC: 96% ± 1.0). There was a statistically significant difference of the time-consumed among the segmentation methods (p <.001). The AI-driven segmentation (51.5 ± 10.9 s) was 116 times faster than the manual segmentation (5973.3 ± 623.6 s). The R-AI method showed intermediate time-consumed (1666.7 ± 588.5 s). Conclusion: Although the manual segmentation showed slightly better performance, the novel CNN-based tool also provided a highly accurate segmentation of the maxillary alveolar bone and its crestal contour consuming 116 times less than the manual approach. © 2023 The Authors. Clinical Oral Implants Research published by John Wiley & Sons Ltd."
"We have developed an AI-aided multiple time stepping (AI-MTS) algorithm and multiscale modeling framework (AI-MSM) and implemented them on the AiMOS supercomputer. AI-MSM is the first of its kind to integrate multi-physics, including intra-platelet, inter-platelet, and fluid-platelet interactions, into one system. It has simulated a record-setting multiscale blood clotting model of 102 million particles, of which 70 flowing and 180 aggregating platelets, under dissipative particle dynamics to coarse-grained molecular dynamics. By adaptively adjusting timestep sizes to match the characteristic time scales of the underlying dynamics, AI-MTS optimally balances speeds and accuracies of the simulations. © 2023 Elsevier B.V."
"Fast and efficient fault monitoring and diagnostics methods are essential for fault diagnosis and prognosis tasks in Health Monitoring Systems. These tasks are even more complicated when facing dynamic systems with multiple operation points. This article introduces a symbiotic solution for fault detection and isolation, based on the integration of two complementary techniques: Possible Conflicts (PCs), a model-based diagnosis technique from the Artificial Intelligence (AI) community, and Principal Component Analysis (PCA), a Multivariate Statistical Process Control (MSPC) technique. Our proposal improves the PCA-based fault detection in systems with multiple operation points and transient states and provides a straightforward fault isolation stage for PCA. At the same time, the proposal increases the robustness for fault detection using PCs through the application of PCA to the residual signals. PCA has the ability to filter out residual deviations caused by model uncertainties that can lead to a high number of false positives. The proposed method has been successfully tested in a real-world plant with accurate fault detection results. The plant has noisy sensors and a system model without the same accuracy at each operation point and transient states. © 2023 The Author(s)"
"Brain-Computer Interaction (BCI) system intelligence has become more dependent on electroencephalogram (EEG)-based emotion recognition because of the numerous applications of emotion classification, such as recommender systems, cognitive load detection, etc. Emotion classification has drawn the recent buzz in Artificial Intelligence (AI)-powered research. In this article, we presented a systematic review of automated emotion recognition from EEG signals using AI. The review process is carried out based on Preferred Reporting Items for Systematic Reviews and Meta-Analyses(PRISMA). After that EEG databases, and EEG preprocessing methods are included in this study. Also included feature extraction and feature selection methods. In addition, the included studies were divided into two types: i)deep learning(DL)-based emotion identification systems and ii) machine learning(ML)-based emotion classification models. The examined systems are analyzed based on their features, classification methodologies, classifiers, types of classified emotions, accuracy, and the datasets they employed. There is also an interesting comparison, a look at feature research trends, and ideas for new areas to study. © 2023 The Author(s)"
"Based on the advantages of revealing the functional status and molecular expression of tumor cells, positron emission tomography (PET) imaging has been performed in numerous types of malignant diseases for diagnosis and monitoring. However, insufficient image quality, the lack of a convincing evaluation tool and intra- and interobserver variation in human work are well-known limitations of nuclear medicine imaging and restrict its clinical application. Artificial intelligence (AI) has gained increasing interest in the field of medical imaging due to its powerful information collection and interpretation ability. The combination of AI and PET imaging potentially provides great assistance to physicians managing patients. Radiomics, an important branch of AI applied in medical imaging, can extract hundreds of abstract mathematical features of images for further analysis. In this review, an overview of the applications of AI in PET imaging is provided, focusing on image enhancement, tumor detection, response and prognosis prediction and correlation analyses with pathology or specific gene mutations in several types of tumors. Our aim is to describe recent clinical applications of AI-based PET imaging in malignant diseases and to focus on the description of possible future developments. © 2023"
"Background: In hospitals, it is crucial to rule out coronavirus disease 2019 (COVID-19) timely and reliably. Artificial intelligence (AI) provides sufficient accuracy to identify chest computed tomography (CT) scans with signs of COVID-19. Purpose: To compare the diagnostic accuracy of radiologists with different levels of experience with and without assistance of AI in CT evaluation for COVID-19 pneumonia and to develop an optimized diagnostic pathway. Material and Methods: The retrospective, single-center, comparative case-control study included 160 consecutive participants who had undergone chest CT scan between March 2020 and May 2021 without or with confirmed diagnosis of COVID-19 pneumonia in a ratio of 1:3. Index tests were chest CT evaluation by five radiological senior residents, five junior residents, and an AI software. Based on the diagnostic accuracy in every group and on comparison of groups, a sequential CT assessment pathway was developed. Results: Areas under receiver operating curves were 0.95 (95% confidence interval [CI]=0.88–0.99), 0.96 (95% CI=0.92–1.0), 0.77 (95% CI=0.68–0.86), and 0.95 (95% CI=0.9–1.0) for junior residents, senior residents, AI, and sequential CT assessment, respectively. Proportions of false negatives were 9%, 3%, 17%, and 2%, respectively. With the developed diagnostic pathway, junior residents evaluated all CT scans with the support of AI. Senior residents were only required as second readers in 26% (41/160) of the CT scans. Conclusion: AI can support junior residents with chest CT evaluation for COVID-19 and reduce the workload of senior residents. A review of selected CT scans by senior residents is mandatory. © The Foundation Acta Radiologica 2023."
"Identifying the dominant acoustic emission (AE) signal attributes acquired under various experimental cutting conditions may provide significant insight to the process. Signal processing methods in time-frequency domain are more appropriate for such analysis due to their capabilities to cover the time and frequency transparency and transient phenomena. However, according to the literature, a lack of study was noticed on the sensitivity of AE signal attributes acquired by time-frequency domain analysis to various cutting conditions in the machining processes. Since milling is among the most widely used machining operations, this investigation aims to acquire adequate knowledge about interactions between cutting parameters and their direct and indirect effects on the obtained AE signal attributes from the milling process. To that end, this study investigates wavelet transform (WT) analysis, one of the most famous analyses in the time-frequency domain. WT signal processing was conducted with five models of mother wavelets, and appropriate decomposition numbers were deployed. The detail and approximate signal attributes obtained from each decomposition were assessed. According to WT analysis and statistical calculations, cutting speed, feed rate, and coating material significantly impacted the variation of AE signal attributes. Also, the most sensitive AE signal attributes and decompositions were rms, std, entropy and energy, and 2nd and 6th decompositions, respectively. The outcome of this research can be integrated into artificial intelligence (AI) methods to implement online monitoring and predictive system. Consequently, it may lead to better process control and optimization. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
"New psychoactive substances (NPS) are a type of abused drug designed to mimic the effects of the currently known illicit drugs, whose structures are constantly changing to escape surveillance. The quick identification of NPS use in the community therefore demands immediate action. This study aimed to develop a target and suspect screening method using LC-HRMS to identify NPS in wastewater samples. An in-house database of 95 traditional and NPS was built using the reference standards, and an analytical method was developed. Wastewater samples were collected from 29 wastewater treatment plants (WWTP) across South Korea, representing 50 % of the total population. The psychoactive substances in waste water samples were screened using in-house database and developed analytical methods. A total of 14 substances were detected in the target analysis, including three NPS (N-methyl-2-AI, 25E-NBOMe, and 25D-NBOMe) and 11 traditional psychoactive substances and their metabolites (zolpidem phenyl-4-COOH, ephedrine, ritalinic acid, tramadol, phenmetrazine, phendimetrazine, phentermine, methamphetamine, codeine, morphine, and ketamine). Out of these, N-methyl-2-AI, zolpidem phenyl-4-COOH, ephedrine, ritalinic acid, tramadol, phenmetrazine, and phendimetrazine were detected with a detection frequency of over 50 %. Primarily, N-methyl-2-Al was detected in all the wastewater samples. Additionally, four NPSs (amphetamine-N-propyl, benzydamine, isoethcathinone, methoxyphenamine) were tentatively identified at level 2b in a suspect screening analysis. This is the most comprehensive study to investigate NPS using target and suspect analysis methods at the national level. This study raises a need for continuous monitoring of NPS in South Korea. © 2023"
"Pipelines are vital infrastructures in oil and gas, water, and sewage transfer, offering a safer and more environmentally friendly option compared to other forms of transportation. The use of in-pipe robots for labor-intensive and hazardous operations, such as inspection, maintenance, and repair of pipelines, has increased in recent years. Some of the major tasks performed by in-pipe robots include the accurate positioning of the robot and associated tools inside the pipe for data collection, inspection, and welding. This paper provides a comprehensive review of various aspects of in-pipe robots, including their geometries, propulsion systems, kinematic designs, and steering mechanisms. The paper also covers localization and navigation methods utilized for in-pipe robotics. A study of the advantages and disadvantages of the above mechanisms is presented, highlighting open research avenues for future advancements in the field. © 2023 Elsevier Ltd"
"Social isolation has become a growing public health concern in older adults and older adults with mild cognitive impairment. Coping strategies must be developed to increase social contact for socially isolated older adults. In this paper, we explored the conversational strategy between trained conversation moderators and socially isolated adults during a conversational engagement clinical trial (Clinicaltrials.gov: NCT02871921). We carried out structural learning and causality analysis to investigate the conversation strategies used by the trained moderators to engage socially isolated adults in the conversation and the causal effects of the strategy on engagement. Causal relations and effects were inferred between participants’ emotions, the dialogue strategies used by moderators, and participants’ following emotions. The results found in this paper may be used to support the development of cost-efficient, trustworthy AI- and/or robot-based platform to promote conversational engagement for older adults to address the challenges in social interaction. © 2023 Elsevier Inc."
"Background and objectives: Pulmonary obstruction diseases produce adventitious sounds in the breathing cycle. With the increased impact of lung diseases, it has become essential for the medical professional to leverage artificial intelligence for faster and more accurate lung auscultation. Initial biomedical signal processing techniques focused on features based on signal amplitude, so accuracy detection depends upon the signal amplitude. The adventitious sounds heard in the respiratory cycle have non-linear characteristics. The present research targets to propose features based on the non-linearity of the adventitious sounds. Also, in this research, SVM-LSTM with the Bayesian optimization model is applied for the first time to test features of adventitious sounds. Methods: The characteristics of adventitious sounds contain non-linearities. Targeting the same, the research proposes two feature sets based on wavelet bi-spectrum and bi-phase (eight each). SVM-LSTM analyzes these features with the Bayesian optimization algorithm model. The research employs the RALEⓇ database, which is the most comprehensive public database of lung sounds. Results: The results are presented in a matrix of 3×10 with parameters as MSE, PSNR, R-value, RMSE, and NRMSE from the confusion matrix for SVM, SVM-LSTM, and SVM-LSTM with BO for each class, i.e., wheeze, crackle, and normal. The results are evaluated using MatlabⓇ 2021b (MathWorksⓇ, Inc.). Results reveal that feature sets achieved an accuracy of 94.086% for SVM, 94.684% for SVM-LSTM, and 95.699% with 95.161% for LSTM Bayesian optimization for WBS and WBP, respectively. Conclusion: The research supports the hypothesis that adventitious sounds have non-linear properties. New features are more effective in detecting lung sounds. Also, combining the LSTM with Bayesian optimization improved each class’s accuracy and statistical parameters. The above model design achieved accurate AI-aided detection of lung diseases for light weighted edge devices. © 2023, The Author(s), under exclusive licence to The Brazilian Society of Biomedical Engineering."
"Unpaid domestic work is vital for human reproduction and enables all other forms of work. In this article, we present first estimates of the impacts of “smart” and “AI” technologies on unpaid work. We ask what the likelihood is of various types of unpaid work being automated, and how this would change the time spent on domestic work and on the gendered division of labour. We adapt three automation likelihood estimates for paid work occupations to estimate the automation likelihood of 19 domestic work tasks. Applying these estimates to Japanese and UK national time use data, we find that 50–60 % of the total time spent on unpaid work could be saved through automation. The savings are unevenly distributed: a Japanese woman aged 20–59 could save up to 3.5 h, a UK woman of the same age could save up to 3 h on an average weekday. A man in the UK could save 1.5 h and a Japanese man only 1 h on an average weekday. Domestic automation could free up to 9.3 % of women in Japan and 5.8 % of women in the UK to take up full- or part-time employment, pointing to substantial potential economic and social gains. © 2023 The Authors"
"Artificial insemination (AI) is commonly used in the equine industry to enhance the genetic value in breeding programs and to effectively utilize ejaculates. Many stallions are used as breeding stallions as well as in high-level sports competitions to improve their market value. The goal of the present study was to investigate whether this dual use of stallions influences the animals´ stress levels and/or the quality of their ejaculates. For this purpose, 18 stallions were grouped into two categories: breeding stallions with (BSC = breeding stallion competition), and breeding stallions without secondary use in competitions (BS = breeding stallion). Two ejaculates were collected at a one-week interval and analysed with an extended spectrum of spermatological methods. Furthermore, saliva, as well as seminal plasma samples were taken, and the concentration of cortisol therein was determined. Additionally, dehydroepiandrosterone (DHEA) and the cortisol/DHEA ratio were analysed and calculated for seminal plasma. After statistical analysis of the correlations and interdependences between the two groups, the results showed that the BSC group had significantly higher saliva cortisol levels (p =.027) and tendentially higher DHEA concentrations in their seminal plasma (p =.056). No difference between BS and BSC could be found in regard to the sperm quality parameters and the cortisol concentration in seminal plasma samples. It can be concluded that while active participation in competitions represents a stress factor, the dual use of stallions in breeding programs and sports competitions is possible without negative effects on their sperm quality. © 2023 Wiley-VCH GmbH. Published by John Wiley & Sons Ltd."
"Thanks to the tremendous progress in data, computing power and algorithms, AI-based material mining and design have gained much attention. However, building high-performance AI models requires efficient material structure representation. In this work, we propose a structural characterization method based on the neighborhood path complex for the first time. Specifically, we use persistent neighborhood path homology to obtain the structural features by introducing a filtration. This approach preserves more elemental information, as well as the corresponding physicochemical information, through the directed edges of the neighborhood digraph. To validate our model, we perform cross-validation with the carborane structures. The Pearson coefficient for stability prediction is as high as 0.903, which is a 15.5% improvement compared to the traditional persistent homology method. In addition, we constructed a prediction model based on the neighborhood path complex, and the Pearson coefficients for the prediction of carboranes' HOMO, LUMO, and HOMO-LUMO gaps were 0.915, 0.946, and 0.941, respectively. The results show that our proposed method can effectively extract structural information and achieve accurate material property prediction.  © 2023 World Scientific Publishing Company."
"Aims: There are several options for treating anal incontinence (AI), with limited success rate in long-term follow-up. Patients' selection is important to avoid unnecessary investigations and therapies. The aim of this review is to assess the utility of pelvic floor investigations to predict success from conservative treatment in AI. Methods: Baseline demographics, severity scores, and pelvic floor investigations of 490 patients with AI symptoms were retrospectively reviewed. Patient-reported outcomes were used to define success of conservative treatment. Results: Bivariate analysis showed that gender, St Mark's incontinence score, Bowel continence and quality of life domains of International Consultation on Incontinence Modular Questionnaire–Bowel symptoms score, Bristol stool chart, anal squeeze pressure, enterocoele, leak of contrast at rest, and dyssynergia in defecography were associated with patient's outcomes from conservative treatment (p < 0.05). Multivariate analysis showed that only the Bowel continence score was an independent predictor of patient's success with treatment. Conclusions: Pelvic floor investigations are of limited value to predict success of conservative treatment and they should be reserved for patients who fail noninvasive management and might require surgical intervention. © 2023 Wiley Periodicals LLC."
"Diabetes mellitus, often known as diabetes, is an endocrine disorder that has a wide global impact today. Here is a requirement for an effective model that able prognoses diabetes and its types with more accurateness as early. Given the breadth and depth of existing studies, there is a pressing need for accurate and timely illness forecasting in the healthcare sector. Current circumstances need the creation and design of systems that are quicker to respond, more accurate, more durable, and more generalizable. For increasing the accurateness of prediction with best effectiveness innovative Artificial Intelligence and Machine Learning Model is proposed. This model predicts the diabetes class using the symptoms located into the data-set which is having the row as one rule of the system & this rule are need to understand and compile using feature. © 2023 The Authors"
"Understanding the acting wear mechanisms in many cases is key for predicting lifetime, developing models describing component behavior or for the improvement of performance of components under tribological loading. Conventionally Scanning-Electron-Microscopy (SEM) and sometimes additional analytical techniques are performed in order to analyze wear appearances, i.e. grooves, pittings, surface films, and others. In addition, experience is required in order to draw the correct and relevant conclusions on the acting damage and wear mechanisms from the obtained analytical data. E.g. differences in the degree of plastic deformation or chemical changes in the surface material are sometimes challenging to characterize and observe, but may have a distinctive influence not only on wear, but also on the acting friction. Until now, different types of wear mechanisms are classified by experts examining the damage patterns manually. In addition to this approach based on expert knowledge, the use of artificial intelligence (AI) represents a promising alternative. Here, no expert knowledge is required, instead the classification is done by a purely data-driven model. In this contribution, artificial neural networks are used to classify the wear mechanisms on different alloys after lubricated sliding wear based on SEM images. The content of this contribution is the investigation of the performance of different AI-based models for automated classification of wear mechanisms. Besides state of the art image classifiers adapted by transfer learning, a self-designed artificial neural network based on a hyperparameter optimization is evaluated and differences in classification accuracy of the models are discussed. While the models prove the feasibility of classifying wear mechanisms by AI, the main challenge remains the high number of wear appearances found on metals, and the correspondingly large database required to obtain a high classification accuracy. © 2023 Elsevier B.V."
"Background and objectives: Parkinson's Disease (PD) is a devastating chronic neurological condition. Machine learning (ML) techniques have been used in the early prediction of PD progression. Fusion of heterogeneous data modalities proved its capability to improve the performance of ML models. Time series data fusion supports the tracking of the disease over time. In addition, the trustworthiness of the resulting models is improved by adding model explainability features. The literature on PD has not sufficiently explored these three points. Methods: In this work, we proposed an ML pipeline for predicting the progression of PD that is both accurate and explainable. We explore the fusion of different combinations of five time series modalities from the Parkinson's Progression Markers Initiative (PPMI) real-world dataset, including patient characteristics, biosamples, medication history, motor, and non-motor function data. Each patient has six visits. The problem has been formulated in two ways: ❶ a three-class based progression prediction with 953 patients in each time series modality, and ❷ a four-class based progression prediction with 1,060 patients in each time series modality. The statistical features of these six visits were calculated from each modality and diverse feature selection methods were applied to select the most informative feature sets. The extracted features were used to train a set of well-known ML models including Support vector machines (SVM), random forests (RF), extra tree classifier (ETC), light gradient boosting machines (LGBM), and stochastic gradient descent (SGD). We examined a number of data-balancing strategies in the pipeline with different combinations of modalities. ML models have been optimized using the Bayesian optimizer. A comprehensive evaluation of various ML methods has been conducted, and the best models have been extended to provide different explainability features. Results: We compare the performance of ML models before and after optimization and using and without using feature selection. In the three-class experiment and with various modality fusions, the LGBM model produced the most accurate results with a 10-fold cross-validation (10-CV) accuracy of 90.73% using non-motor function modality. RF produced the best results in the four-class experiment with various modality fusions with a 10-CV accuracy of 94.57% using non-motor modality. With the fused dataset of non-motor and motor function modalities, the LGBM model outperformed the other ML models in both the 3-class and 4-class experiments (i.e., 10-CV accuracy of 94.89% and 93.73%, respectively). Using the Shapely Additive Explanations (SHAP) framework, we employed global and instance-based explanations to explain the behavior of each ML classifier. Moreover, we extended the explainability by implementing the LIME and SHAPASH local explainers. The consistency of these explainers has been explored. The resultant classifiers were accurate, explainable, and thus medically more relevant and applicable. Conclusions: The select modalities and feature sets were confirmed by the literature and medical experts. The various explainers suggest that the bradykinesia (NP3BRADY) feature was the most dominant and consistent. By providing thorough insights into the influence of multiple modalities on the disease risk, the suggested approach is expected to help improve the clinical knowledge of PD progression processes. © 2023 Elsevier B.V."
"Initially, Artificial Intelligence (AI) focused on diagnostics during the 70s and 80s. Unfortunately, it did not gain trust and few industries embraced it, mostly due to the extensive manual programming effort that AI required for interpreting data and act. In addition, the computer capacity, for handling the amounts of data necessary to train AI, was lacking the disc dimensions we are used to today, which made it go slowly. Not until the 2000 s confidence in AI was established in parallel with the introduction of new tools that was paving the way for PLS, PCA, ANN and soft sensors. Year 2011, IBM Watson (an AI application) was developed and won over the jeopardy champion. Today's machine learning (ML) such as “deep learning” and artificial neural networks (ANN) have created interesting use cases. AI has therefore regained confidence and industries are beginning to embrace where they see appropriate uses. Simultaneously, Internet of Things (IoT) tools have been introduced and made it possible to develop new capabilities such as virtual reality (VR), augmented reality (AR), mixed reality (MR) and extended reality (XR). These technologies are maturing and could be used in several application areas for the industries and form part of their digitalization journey. Furthermore, it is not only the industries that could benefit from introducing these technologies. Studies also show several areas and use cases where augmented reality has a positive impact, such as on students' learning ability. Yet few teachers know or use this technology. This paper evaluates and analyze AR, remote assistance tool for industrial purposes. The potential of the tool is discussed for frequent maintenance cases in the mining industry. Further on, if we look into the future, it is not surprising if we will be able to see that today's concepts of reality tools have evolved to become smarter by being trained by multimedia recognition and from people who have thus created an AI expert. Where the AI expert will support customers and be able to solve simple errors but also those that occur rarely and thus be a natural part of the solution for future completely autonomous processes for the industry. The article demonstrates a framework for creating smarter tools by combining AR, ML and AI and forms part of the basis creating the smarter industry of the future. Natural Language Processing (NLP) toolbox has been utilized to train and test an AI expert to give suitable resolutions to a specific maintenance request. The motivation for AR is the possible energy savings and reduction of CO₂ emissions in the maintenance field for all business trips that can be avoided. At the same time saving money for the industries and expert manhours that are spent on traveling and finally enhancing the productivity for the industries. Tests cases have verified that with AR, the resolution time could be significantly reduced, minimizing production stoppages by more than 50% of the time, which ultimately has a positive effect on a country's GDP. How much energy can be saved is predicted by the fact that 50% of all the world's business flights are replaced by one of the reality concepts and are estimated to amount to at least 50 Mton CO₂ per year. This figure is probably slightly higher as business trips also take place by other means of transport such as trains, buses, and cars. With today's volatile employees changing jobs more frequently, industry experts are becoming fewer and fewer. Since new employee stays for a maximum of 3–5 years per workplace, they will not stay long enough to become experts. Introducing an AI expert trained by today's experts, there is a chance that this knowledge can be maintained. © 2023 Elsevier Ltd"
"Recent research in transport safety focuses on the processing of large amounts of available data by means of intelligent systems, in order to decrease the number of accidents for transportation users. Several Machine Learning (ML) and Artificial Intelligence (AI) applications have been developed to address safety problems and improve efficiency of transportation systems. However exchange of knowledge between transport modes has been limited. This paper reviews the ML and AI methods used in different transport modes (road, rail, maritime and aviation) to address safety problems, in order to identify good practices and experiences that can be transferable between transport modes. The methods examined include statistical and econometric methods, algorithmic approaches, classification and clustering methods, artificial neural networks (ANN) as well as optimization and dimension reduction techniques. Our research reveals the increasing interest of transportation researchers and practitioners in AI applications for crash prediction, incident/failure detection, pattern identification, driver/operator or route assistance, as well as optimization problems. The most popular and efficient methods used in all transport modes are ANN, SVM, Hidden Markov Models and Bayesian models. The type of the analytical technique is mainly driven by the purpose of the safety analysis performed. Finally, a wider variety of AI and ML methodologies is observed in road transport mode, which also appears to concentrate a higher, and constantly increasing, number of studies compared to the other modes. © 2023 The Authors"
"The IT literature suggests that information technology capability (ITC) enhances firm performance. Yet studies have challenged this positive relationship. This study proposes an inverted U-shaped ITC–performance relationship and investigates how artificial intelligence-infused operations capability (AIOP) moderates this inverted U-shaped relationship. We collect data from 259 U.S. and 279 Chinese companies to test the hypotheses and find some interesting, but counterintuitive, results. This study extends the IT and strategic management literatures by examining how AIOP moderate an inverted U-shaped ITC–performance relationship. The study also reveals some cross-national similarities and differences. © 2023 Elsevier Inc."
"The effect of heat transmission on the absolute instability (AI) and convective instability (CI) of axisymmetrical disturbances in a viscoelastic liquid jet that falls under gravity is investigated. In general, when heat is included to the interface of a viscoelastic jet, it can be used to process droplet sizes and breakup lengths even more. We describe the jet's dynamics mathematically using the Upper-Convected Maxwell (UCM) model. On the basis of the jet's slenderness, an asymptotic approach is used to simplify the problem and obtain solutions of steady basic flow, which are then linearly analyzed for absolute and convective instability. When traveling wave modes are considered, a dispersion relation between the wavenumber and the growth rate of viscoelastic jets is derived, which can then be solved numerically using the Newton–Raphson method. The impact of varying some non-dimensional parameters is shown on absolute and convective instability. In this work, absolute instability is explored by employing a mapping technique called the cusp map method. For a variety of parameter regimes, the convective-to-absolute instability boundary (CAIB) is determined. We have found that absolute-to-convective transitions occur at lower critical Weber numbers when heating is included to the interface of the viscoelastic jets. © 2023 The Author(s)"
"Conventional sidewalk studies focused on quantitative analysis of sidewalk walkability at a large scale which cannot capture the dynamic interactions between the environment and individual factors. Embracing the idea of Tech for Social Good, Urban Digital Twins seek AI-empowered approaches to bridge humans with digitally-mediated technologies to enhance their prediction ability. We employ GraphSAGE-LSTM, a geo-spatial artificial intelligence (GeoAI) framework on crowdsourced data and computer vision to predict human comfort on the sidewalks. Conceptualising the pedestrians and their interactions with surrounding built and unbuilt environments as human-centric dynamic graphs, our model captures such spatio-temporal variations given by the sequential movements of human walking, enabling the GraphSAGE-LSTM to be spatio-temporal-explicit. Our experiments suggest that the proposed model provides higher accuracy by more than 20% than a traditional machine learning model and two state-of-art deep learning frameworks, thus, enhancing the prediction power of Urban Digital Twin. The source code for the model is shared openly on GitHub. © 2023 Elsevier Ltd"
"Workplace Artificial Intelligence (AI) helps organisations increase operational efficiency, enable faster-informed decisions, and innovate products and services. While there is a plethora of information about how AI may provide value to workplaces, research on how workers and AI can coexist in workplaces is evolving. It is critical to explore emerging themes and research agendas to understand the trajectory of scholarly research in this area. This study's overarching research question is how workers will coexist with AI in workplaces. A search protocol was employed to find relevant articles in Scopus, ProQuest, and Web of Science databases based on appropriate and specific keywords and article inclusion and exclusion criteria. We identified four themes: (1) Workers' distrust in workplace AI stems from perceiving it as a job threat, (2) Workplace AI entices worker-AI interactions by offering to augment worker abilities, (3) AI and worker coexistence require workers' technical, human, and conceptual skills, and (4) Workers need ongoing reskilling and upskilling to contribute to a symbiotic relationship with workplace AI. We then developed four propositions with relevant research questions for future research. This review makes four contributions: (1) it argues that an existential argument better explains workers' distrust in AI, (2) it gathers the required skills for worker and AI coexistence and groups them into technical, human, and conceptual skills, (3) it suggests that technical skills benefit coexistence but cannot outweigh human and conceptual skills, and (4) it offers 20 evidence-informed research questions to guide future scholarly inquiries. © 2023 The Authors"
"After 200 years of their birth, synthetic polymers are present in over 60 primary forms. Many are largely known and spread, others less common and investigated, such as Polyphenylsulfone (PPSU) which, on the contrary, offers multiple applications. Among them, in particular, many concerns the use of the PPSU at (cold or hot) temperature. But experiments with its temperature-dependent characterization are very few and limited to specific formulations. Machine learning algorithms can fill the gap providing accurate predictions. Specifically, an unsupervised classification was here used for clustering material data with the scope to recognize patterns between PPSU up to the selection of similarities respect to a polymer as reference. Then, a supervised regression was used to predict temperature-dependent tensile properties. A high level of accuracy was achieved, up to 99% in terms of coefficient of determination. This is probably the first time that data regarding the mechanical behavior of PPSU were derived from an approach based on artificial intelligence and machine learning. © 2023 Elsevier Ltd"
"The advent of generative artificial intelligence (AI) offers transformative potential in the field of education. The study explores three main areas: (1) How did ChatGPT answer questions related to science education? (2) What are some ways educators could utilise ChatGPT in their science pedagogy? and (3) How has ChatGPT been utilised in this study, and what are my reflections about its use as a research tool? This exploratory research applies a self-study methodology to investigate the technology. Impressively, ChatGPT’s output often aligned with key themes in the research. However, as it currently stands, ChatGPT runs the risk of positioning itself as the ultimate epistemic authority, where a single truth is assumed without a proper grounding in evidence or presented with sufficient qualifications. Key ethical concerns associated with AI include its potential environmental impact, issues related to content moderation, and the risk of copyright infringement. It is important for educators to model responsible use of ChatGPT, prioritise critical thinking, and be clear about expectations. ChatGPT is likely to be a useful tool for educators designing science units, rubrics, and quizzes. Educators should critically evaluate any AI-generated resource and adapt it to their specific teaching contexts. ChatGPT was used as a research tool for assistance with editing and to experiment with making the research narrative clearer. The intention of the paper is to act as a catalyst for a broader conversation about the use of generative AI in science education. © 2023, The Author(s)."
"Block ramps are among the environmentally friendly hydraulic structures used for energy dissipation in rivers and waterways. Modeling the energy dissipation on these structures is ever-challenging in hydraulic engineering. The primary goal of the current study is to propose a novel metaheuristic-based artificial intelligence (AI) framework for energy dissipation prediction on block ramp structures. An improved African Vultures Optimization Algorithm (AVOA) is used to optimize the Adaptive Neuro-Fuzzy Inference System (ANFIS) in this investigation for accurate prediction of the energy dissipation on the block ramps. The performance of the hybrid ANFIS-IAVOA model is compared with an ANFIS and its hybrid versions using original AVOA, honey badger algorithm (ANFIS-HBA), grey wolf optimizer (ANFIS-GWO), monarch butterfly optimization (ANFIS-MBO), and black widow optimization (ANFIS-BWO) models. A dataset of 210 experiments measured at Shahid Chamran University of Ahvaz and 241 experiments collected from literature are used to construct the proposed hybrid models. The results demonstrate the better efficiency of hybrid ANFIS-IAVOA with RMSE of 0.018–0.020 and R2 of 0.98–0.98 compared to ANFIS-AVOA (RMSE ~ 0.023–0.25 and R2 ~ 0.97–0.97), ANFIS-HBA (RMSE ~ 0.021–0.025 and R2 ~ 0.97–0.97), ANFIS-MBO (RMSE ~ 0.022–0.023 and R2 ~ 0.97–0.97), ANFIS-GWO (RMSE ~ 0.022–0.024 and R2 ~ 0.97–0.97), ANFIS-BWO (RMSE ~ 0.027–0.028 and R2 ~ 0.96–0.96), and ANFIS (RMSE ~ 0.029–0.033 and R2 ~ 0.954 − 0.951). The statistical measures show that the proposed ANFIS-IAVOA performs better than the other metaheuristic-based and standalone ANFIS-developed models. The impressiveness of the proposed hybrid model demonstrates that it can be used for further investigations on the probabilistic assessment of the block ramp hydraulic structures. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Machine learning (ML) is the core of Artificial Intelligence (AI), and it is the fundamental way to make computer have intelligence. ML is a technology that uses algorithms to parse data, constantly learn, and make judgements and predictions about what happens. With the continuous development of ML technology, using ML algorithms to analyze the security of physical hardware has gradually become one of the hot spots in the research field. In the field of hardware security, post quantum cryptography is one of the research hotspots, e.g., multivariate cryptography. However, analyzing post-quantum signatures based on ML is still in the early stage. As substitutions of current used signatures, post-quantum signatures should fully consider side channel attack based on ML techniques so that they can be used in reality. In order to address such challenges, we present ML techniques to exploit the measurement of side channel attacks to post-quantum signatures. We propose a ML model for the measurement of side channel attacks. The efficiency of the proposed model is measured and it can be extended to analyze other similar signatures. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Machine learning for design and analysis of steel frame halls. The use of digital tools in design and construction processes increases efficiency, especially for recurring tasks and processes, such as the dimensioning of components or structures, which follow similar patterns. This paper develops an artificial intelligence (AI)-based assistant for use in the pre-design and design of steel frame halls as a prototypical co-pilot. For this purpose, the necessary database of steel frame halls is first compiled from project archives of TRAGRAUM Partnerschaft Beratender Ingenieure mbB. This will be examined in the context of data analyses according to the decisive description parameters and similarities, so that finally suitable AI surrogate models for the prediction of the hall parameters are trained by means of regression analyses. Furthermore, an interface to the finite element program SOFiSTiK is implemented, which passes the predictions of the AI regarding the parameters of the steel frame halls to a parameterized input file and triggers the calculation and design. In the course of the calibration of the AI substitute models, in addition to the approximation qualities, possible safety margins to be provided are also discussed in the context of this article. This article shows that the prototypical co-pilot for the preliminary design of steel frame halls could be successfully implemented. Furthermore the AI newly generated hall frames can be classified as stable with respect to the validation considerations. An outlook on future further developments of this approach as well as the transfer to other applications complete this paper. © 2023, Ernst und Sohn. All rights reserved."
"Artificial intelligence (AI) based clinical decision support systems (CDSS) are becoming ever more widespread in healthcare and could play an important role in diagnostic and treatment processes. For this reason, AI-based CDSS has an impact on the doctor–patient relationship, shaping their decisions with its suggestions. We may be on the verge of a paradigm shift, where the doctor–patient relationship is no longer a dual relationship, but a triad. This paper analyses the role of AI-based CDSS for shared decision-making to better comprehend its promises and associated ethical issues. Moreover, it investigates how certain AI implementations may instead foster the inappropriate paradigm of paternalism. Understanding how AI relates to doctors and influences doctor–patient communication is essential to promote more ethical medical practice. Both doctors' and patients' autonomy need to be considered in the light of AI. © 2023 The Authors. Bioethics published by John Wiley & Sons Ltd."
[No abstract available]
"The exponential proliferation of fake news in recent years has emphasized the demand for automated fake news detection. Several techniques for detecting fake news have yielded encouraging results. However, these detection systems lack explainability i.e., providing the reason for their prediction. The critical advantage of explainability is the identification of bias and discrimination in detection algorithms. There are very few surveys conducted on the area of explainable AI applied to fake news detection. All of theses surveys summarize the existing methods in this area. Most of them are limited to the discussion of specific topics like datasets, evaluation methods, and potential future applications. In contrast, this survey looks at existing explainable AI methods and highlights the current state of the art in explainable fake news detection. We identify and enumerate a few open research problems based on our review of the existing explainable fake news detection techniques. We group the existing work in this area, by viewing it from four different perspectives: features used for the classification, explanation type, explainee type, and the metric used for explainability evaluation. The potential research topics in the above four groups which are unexplored so far and which need attention are also listed in this paper. © 2023 Elsevier Ltd"
"Reproductive failure of replacement breeding animals is one of the leading causes of loss to the beef production industry. The losses are further increased due to the inability to diagnose the reproductive potential of the beef heifer prior to the breeding season until the pregnancy outcome. To overcome this problem, a system to discriminate beef heifers with varying reproductive potential as early and accurately as possible is demanded. The omics technologies, such as transcriptomics, could predict the future reproductive potential of beef heifers. Therefore, this manuscript provides the gene expression profile dataset using RNA-Seq identified from peripheral white blood cells (PWBC) of beef heifers at weaning. To accomplish this, the blood samples were collected at the time of weaning, processed to extract the PWBC pellet and stored at – 80 °C until further processing. After the breeding protocol (artificial insemination (AI) followed by natural bull service) and pregnancy diagnosis, the heifers that were pregnant to AI (n = 8) or remained open (n = 7) were utilized for this study. Total RNA was extracted from PWBC collected at the time of weaning from these samples and subjected to sequencing using the Illumina Nova-Seq platform. High-quality sequencing data was analyzed using a bioinformatic workflow based on FastQC and MultiQC for quality control, STAR for read alignment, and DESeq2 for differential expression analysis. Genes were considered significantly differentially expressed after adjustment with Bonferroni correction (padj ≤ 0.05) and absolute (log2 fold change) ≥ 0.5. Raw and processed RNA-Seq data were deposited and made publicly available on the gene expression omnibus database (GEO; GSE221903). To our knowledge, this is the first dataset investigating the change in the gene expression level as early as weaning to predict the future reproductive outcome in beef heifers. Interpretation of the main findings based on this data is reported in a research article titled “mRNA Signatures in Peripheral White Blood Cells Predicts Reproductive Potential in Beef Heifers at Weaning” [1]. © 2023"
"Ethical and human rights issues of artificial intelligence (AI) are a prominent topic of research and innovation policy as well as societal and scientific debate. It is broadly recognised that AI-related technologies have properties that can give rise to ethical and human rights concerns, such as privacy, bias and discrimination, safety and security, economic distribution, political participation or the changing nature of warfare. Numerous ways of addressing these issues have been suggested. In light of the complexity of this discussion, we undertook a Delphi study with experts in the field to determine the most pressing issues and prioritise appropriate mitigation strategies. The results of the study demonstrate the difficulty of defining clear priorities. Our findings suggest that the debate around ethics and human rights of AI would benefit from being reframed and more strongly emphasising the systems nature of AI ecosystems. © 2023 The Authors"
"Smart manufacturing processes, building upon machine learning (ML) models could potentially reduce the pre-production testing and validation time for new processes. Beyond calculating accurate and reliable models, one critical challenge would be for users of these models (plant operators, engineers and technicians) to trust these models’ outputs. We propose to apply explainable AI methods to create trustworthy AI-based manufacturing systems. Consequently, these systems will be enriched with capabilities to explain their reasoning processes and outputs (e.g., predictions) automatically. This paper applies explainable AI methods to two problems in manufacturing: ultrasonic weld (USW) quality prediction and body-in-white (BIW) dimensional variability reduction. Class activation maps were computed to explain the effect of input signals and their patterns on the quality predictions of an ultrasonic weld yield by a neural network (good or bad). Contrastive gradient based saliency maps were also created to assess the robustness of this classifier. Furthermore, we explain a given connectionist network that predicts the dimensional quality of body-in-white framer points based on deviations in underbody points. Explaining these predictions help engineers understand which underbody points have more influence on deviations in the framer points. These two applications highlight the importance of explainable AI methods in the modern manufacturing industry. © 2023 Elsevier Inc."
"We tested to see how Ruben’s copy of “The Battle of Anghiari” by Leonardo da Vinci would be interpreted by AI in a neuroanatomical aspect. We used WOMBO Dream, an artificial intelligence (AI)-based algorithm that creates images based on words and figures. The keyword we provided for the algorithm was “brain” and the reference image was Ruben’s drawing. AI interpreted the whole drawing as a representation of the brain. The image generated by the algorithm was similar to our interpretation of the same painting. © The Author(s) 2023."
"In this paper, OpenAI's ChatGPT (Generative Pre-trained Transformer), also known as GPT-3, a machine-learning model that has the ability to generate human-like text, was employed as an interviewee instead of a human subject. The scope of the interview was the impacts of OpenAI's GPT on higher education and academic publishing. Particularly, several questions about the impacts of OpenAI's ChatGPT and other AI-based machine learning models on the hospitality and tourism industry and education were asked. The originality of this paper derives from having the ChatGPT as an interviewee. ChatGPT stated that its use helps instructors delegate monotonous tasks such as grading and focus on more intellectual tasks, and students may utilize ChatGPT to brainstorm ideas. ChatGPT confesses the risk of diminishing critical thinking for students in the case of over-reliance on ChatGPT as well as educational inequalities. For academic work, ChatGPT addressed it cannot be a substitute for human creativity and intellectuality because originality and novelty lack in outputs generated by ChatGPT. The tourism and hospitality industry can benefit from ChatGPT for certain things such as personalized services, content creation, and many more. © 2023 The Author(s)."
"Digital twin (DT) has been moving progressively from concept to practice for bridge operation and maintenance (O&M), but its issues of data synchronization and fault tolerance remain problematic. This paper investigates the time delay of bridge DT services according to communication and computation complexity, revealing the distinct impact of their sequence, and proposes an AIoT-informed DT communication framework to solve the above issues. The information hierarchy and two-way communication can be leveraged to minimize communication complexity in the framework. Meanwhile, the data flow and resilience of the proposed framework are demonstrated using a Petri net. Moreover, the framework is developed into a prototypical DT through cross-platform integration and validated with different cases. The results demonstrate that compared with other existing bridge DTs, the proposed framework has high efficiency, low-latency, and excellent fault tolerance, which can contribute to the efficiency and safety of bridge O&M, especially under communication-constraint circumstances. The framework is also promising for federated learning to protect the AI-model privacy of different stakeholders and has the potential to support agent-based intelligent bridge management in the future with little human intervention. © 2023 The Authors"
"Dissolved gas analysis (DGA) is a traditional approach for power transformer fault diagnostics based on the measurement of gas contamination. Hydrocarbon gases generated and dissolved in transformer oil during operation can increase in density as fault conditions predominate. Critical determination of gas concentration changes and assessment trending of dissolved gases for fault prediction and prevention of transformer damage is essential. In this article, a dynamic fault prediction approach is proposed using a long short-term memory (LSTM) model with intelligent classification to determine the running state of a transformer for prediction and avoidance of potential transformer damage. In the article, the LSTM model processed DGA data collected from real on-site transformer field measurements and predicts future dissolved gas concentrations in time sequence. Four artificial intelligence (AI) diagnostic models [support vector machine (SVM), k -nearest neighbors (KNN), decision tree, and artificial neural network (ANN)] were rendered and used for comparative fault prediction assessment. By comparing experimental results from the different LSTM-based models, this article asserts that the LSTM-KNN model provides the highest and most reliable prediction accuracy for power transformers. © 1994-2012 IEEE."
"Ambient intelligence (AmI) systems aim to provide users with context-aware assistance services intended to improve the quality of their lives in terms of autonomy, safety, and well-being. Taking the uncertainty and partial observability of these environments into account is of major importance for context recognition and, more specifically, to detect and solve context abnormalities such as those related to the user's behavior or those related to context attribute prediction. In this paper, an ontology-based framework integrating machine learning and probabilistic planning within commonsense reasoning is proposed to recognize the user's context and abnormalities associated with it. The reasoning is performed using event calculus in answer set programming (ECASP); ECASP allows for abductive and temporal reasoning, which results in an eXplainable AI (XAI) approach. A context ontology is proposed to axiomatize the reasoning and introduce the notion of probabilistic fluents into the EC formalism in order to perform probabilistic reasoning. The reasoning incorporates probabilistic planning based on a partially observable Markov decision process (POMDP) to solve knowledge incompleteness. To evaluate the proposed framework, real-life scenarios, based on the Orange4Home and SIMADL public datasets are implemented and discussed. © 2023 Elsevier Inc."
"A growing body of literature shows that despite the significant benefits of artificial intelligence (AI), its adoption has many unknowns and challenges. However, theoretical studies dominate this topic. Completing the recent works, this article aims to identify challenges faced by public organizations when adopting AI based on the PRISMA Framework and an empirical assessment of these challenges in the opinion of public managers using survey research. The adopted research procedure is also an added value because it could be replicated in other context scenarios. To achieve this paper's aim, the Systematic Literature Review (SLR) and survey research among authorities in 414 Polish cities were carried out. As a result, a list of 15 challenges and preventive activities proposed by researchers to prevent these challenges have been identified. Empirical verification of identified challenges allows us to determine which of them limit AI adoption to the greatest extent in public managers' opinion. These include a lack of strategy or plans to initial adoption / continued usage of AI; no ensuring that AI is used in line with human values; employees' insufficient knowledge of how to use AI; insufficient AI policies, laws, and regulations; and different expectations of stakeholders and partners about AI. These findings could help practitioners to prioritize AI adoption activities and add value to digital government theory. © 2023 Elsevier Inc."
"The aim of this study was to compare the association of the triglycerides and glucose (TyG) index and homeostatic model assessment of insulin resistance (HOMA-IR) with lipoprotein(a) (lp[a]), apolipoprotein AI (apoAI), and apoliprotein B (apoB) concentrations in children with normal-weight. Children with normal weight aged 6–10 years and Tanner 1 stage were included in a cross-sectional study. Underweight, overweight, obesity, smoking, alcohol intake, pregnancy, acute or chronic illnesses, and any kind of pharmacological treatment were exclusion criteria. According to the lp(a) levels, children were allocated into the groups with elevated concentrations and normal values. A total of 181 children with normal weight and an average age of 8.4 ± 1.4 years were enrolled in the study. The TyG index showed a positive correlation with lp(a) and apoB in the overall population (r = 0.161 and r = 0.351, respectively) and boys (r = 0.320 and r = 0.401, respectively), but only with apoB in the girls (r = 0.294); while the HOMA-IR had a positive correlation with lp(a) levels in the overall population (r = 0.213) and boys (r = 0.328). The linear regression analysis showed that the TyG index is associated with lp(a) and apoB in the overall population (B = 20.72; 95%CI 2.03–39.41 and B = 27.25; 95%CI 16.51–37.98, respectively) and boys (B = 40.19; 95%CI 14.50–65.7 and B = 29.60; 95%CI 15.03–44.17, respectively), but only with apoB in the girls (B = 24.22; 95%CI 7.90–40.53). The HOMA-IR is associated with lp(a) in the overall population (B = 5.37; 95%CI 1.74–9.00) and boys (B = 9.63; 95%CI 3.65–15.61). Conclusion: The TyG index is associated with both lp(a) and apoB in children with normal-weight.What is Known:• The triglycerides and glucose index has been positively associated with an increased risk of cardiovascular disease in adults.What is New:• The triglycerides and glucose index is strongly associated with lipoprotein(a) and apolipoprotein B in children with normal-weight.• The triglycerides and glucose index may be a useful tool to identify cardiovascular risk in children with normal-weight. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Background: Yishen Tongbi (YSTB) decoction is a patented herbal formula that is used in China to treat rheumatoid arthritis (RA); however, the exact mechanism of its anti-synovial hyperplasia efficacy has not been fully elucidated. Purpose: Based on our previous proteomics study, we aimed to reveal whether YSTB inhibits the proliferation and migration of RA-FLSs through the SLC3A2/integrin β3 pathway in vivo and in vitro. Study design: The study design consists of three parts, a comparison of the expression of SLC3A2 and integrin β3 in synovial tissues of RA and OA patients; an animal experiment to verify the pharmacodynamic effect of YSTB, and in vitro experiment to elucidate the specific mechanism of YSTB. Methods: The expression of SLC3A2 and integrin β3 in the synovial tissues of patients with RA and osteoarthritis (OA) patients were detected by immunohistochemistry (IHC). In vitro, firstly, the proliferation and migration abilities of HFLS (human fibroblast-like synoviocytes) and HFLS-RA (human fibroblast-like synoviocytes-RA) cells were compared by EdU staining and wound healing assays, respectively, and the differences in the expression and localization of SLC3A2, integrin β3, p-FAK and p-Src between HFLS and HFLS-RA cells were detected by IF and WB. In vivo, DBA/1 mice were injected with bovine collagen II to construct a CIA mouse model. Paw swelling, body weight and the arthritis index (AI) were used as basic treatment evaluation indicators for YSTB. Micro-CT and histopathological analyses of the knee and ankle joints were also performed. In addition, the expression of SLC3A2, integrin β3, p-FAK and p-Src in the synovial tissue of mice was detected by IHC. Subsequently, CCK-8 was used to screen for suitable concentrations of YSTB for use in HFLS-RA cells. EdU staining and transwell migration assays were performed to evaluate the inhibitory effect of YSTB on cell proliferation and migration, and WB was conducted to assess whether YSTB inhibited HFLS-RA migration through downregulation of the SLC3A2/integrin β3 pathways. Results: IHC showed that the expression of SLC3A2 and integrin β3 was higher in RA synovial tissues than in OA tissues. In vivo experiments showed that YSTB inhibited synovial hyperplasia, prevented bone destruction, and reduced the expression of SLC3A2, integrin β3, p-FAK and p-Src. In vitro experiments showed that YSTB inhibited HFLS-RA migration and proliferation by inhibiting the expression of SLC3A2/integrin β3 and downstream signaling molecules. Conclusion: YSTB inhibits the proliferation and migration of synovial fibroblasts in RA by downregulating the SLC3A2/integrin β3 pathways. © 2023"
"The morphological and mechanical characteristics of red blood cells (RBCs) largely vary depending on the occurrence of hematologic disorders. Variations in the rheological properties of RBCs affect the dynamic motions of RBCs, especially their rotational behavior. However, conventional techniques for measuring the orientation of biconcave-shaped RBCs still have some technical limitations, including complicated optical setups, complex post data processing, and low throughput. In this study, we propose a novel image-based technique for measuring 3D position and orientation of normal RBCs using digital in-line holographic microscopy (DIHM) and artificial intelligence (AI). Formaldehyde-fixed RBCs are immobilized in coagulated polydimethylsiloxane (PDMS). Holographic images of RBCs positioned at various out-of-plane angles are acquired by precisely manipulating the PDMS-trapped RBC sample attached to a 4-axis optical stage. With the aid of deep learning algorithms for data augmentation and regression analysis, the out-of-plane angle of RBCs is directly predicted from the captured holographic images. The 3D position and in-plane angle of RBCs are acquired by employing numerical reconstruction and ellipse detection methods. Combining these digital image processing techniques, the 3D positional and orientational information of each RBC recorded in a single holographic image is measured within 23.5 and 3.07 s, respectively. The proposed AI-based DIHM technique that can extract the 3D position, orientation, and morphology of individual RBCs would be utilized to analyze the dynamic translational and rotational motions of abnormal RBCs with hematologic disorders in shear flows through further research. © 2023 Elsevier B.V."
"Smart and innovative education is an integral part of the development of sustainable entrepreneurship practices. Due to this, UNCED included it on its agenda. The primary goal of smart education is to teach young people to be responsible members of society in the years to come. The youth of this generation should have the opportunity to play an active role in shaping our future. For the sake of future generations, they should be taught to accept personal responsibility for their own well-being and the well-being of future generations. However, with traditional teaching techniques, this goal is not achieved; therefore, there is a requirement for the integration of cutting-edge technologies such as IoT, cloud computing, AI, and machine learning with the education process for the development of sustainable entrepreneurship practices in order to achieve a better future. In this context, we analyze the current smart education techniques proposed by different researchers that lead to the development of sustainable entrepreneurship practices. In this investigation, we also highlight the various limitations and challenges of the current smart education system. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Rationale and objectives: An annual survey of chief residents in accredited North American radiology programs is conducted by the American Alliance of Academic Chief Residents in Radiology (A3CR2). The purpose of this study is to summarize the 2020 A3CR2 chief resident survey. Materials and methods: An online survey was distributed to chief residents from 194 Accreditation Council on Graduate Medical Education–accredited radiology residencies. Questions were designed to gather information about residency program practices, benefits, fellowship or advanced interventional radiology (IR) training choices, and the integration of IR training. Subsets of questions focused on the perception of corporatization, non-physician providers (NPPs), and artificial intelligence (AI) in radiology and their relationship to the radiology job market. Results: 174 individual responses from 94 programs were provided, yielding a 48 % program response rate. Extended emergency department coverage has steadily decreased over the last 5 years (2016–2020), however only 52 % of programs have independent overnight call (without attending coverage). Regarding the impact of new integrated IR residencies on training, 42 % indicated there was no appreciable impact on their DR or IR training, while 20 % indicated DR training for IR residents suffered and 19 % indicated IR training for DR residents suffered. Corporatization in radiology was perceived as the biggest potential threat to the future job market. Conclusions: Integration of IR residency did not detrimentally affect DR or IR training in most programs. Radiology resident perception of corporatization, NPPs, and AI may help residency programs shape educational content. © 2023 Elsevier Inc."
"A medical analysis of diagnosing rare genetic diseases has rapidly become the most expensive and time-consuming component for doctors. By combining predictive methods with growing knowledge of genetic disease, artificial intelligence (AI) has the potential to simplify and accelerate genome interpretation greatly. In this paper, multiple machine-learning models like support vector machine, Gaussian Naïve Bayes, KNN, Decision Tree, Gradient Boosting, logistic regression, light gradient boosting classifier, Random Forest, extreme gradient boosting classifier, and cat-boost are applied to the genetic disorder as well as genetic disorder sub-classes datasets. The dataset has been initially pre-processed to check for NAN values, which are graphically represented in various categories like genetic disorder, genetic disorder subclasses, five samples of symptoms, genes inherited from mother’s and father’s side, birth defects etc. to study their pattern. Later, the features have been selected using standardization technique on which the machine learning models are applied and later evaluated using accuracy, loss, recall, precision, root mean square error, and F1 score. Furthermore, the confusion matrix is also generated to compute false negative, true positive, false positive and true negative values for the classes drawn from both datasets. It has been found that the highest accuracy has been calculated by decision tree, random forest, gradient boosting, LGBM classifier, XGB classifier, and CatBoost by 99.9% for genetic disorder while as only the random forest, decision tree, LGBM classifier, and CatBoost, on the other hand, achieved 99.9% accuracy for genetic disorder sub-classes. © 2023, The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE)."
"This study aimed to decrease the activity of phytohaemagglutinins and kidney bean α-amylase activity in some selected stable foods. Kidney bean α-amylase inhibitors may inhibit mammalian α-amylase, reducing the estimated glycaemic index (eGI). The effect of temperature, pH and protease pretreatment on PHA activity in the crude extract was investigated. Besides, the inhibitory effect of kidney bean extract on α-amylase activity and properties was assessed. Results showed that the PHA activity was decreased to 6.97% from the aqueous extract of kidney beans, while the α-AI activity was retained by 85.89% by acid protease treatment. The extract's α-amylase inhibitory activity remained at 87.37% following simulation in a gastric environment, whereas no effect was observed in a simulated intestinal environment. Adding 3% of the extract decreased the eGI of bread, rice, steamed bread, and steamed corn bread from 65.00, 71.65, 68.17, and 63.41 to 30.69, 39.51, 22.30, and 19.69, respectively. The processing method of crude kidney bean extract examined in this study has significant reference value for the development of kidney bean α-amylase inhibitors and can enhance the intensive processing level and industrial development value of kidney beans worldwide. This discovery suggests that kidney bean extracts can be used to develop foods with a low GI. © 2023 Institute of Food Science and Technology."
"Quorum sensing (QS) is an inter- and intracellular communication mechanism that regulates gene expression in response to population size. Autoinducer-2 (AI-2) signaling is a QS signaling molecule common to both Gram-negative and Gram-positive bacteria. Enterococcus faecalis is one of the leading causes of nosocomial infections worldwide. There has been an increasing interest in controlling infectious diseases through targeting the QS mechanism using natural compounds. This study aimed to investigate the effect of nisin and p-coumaric acid (pCA), on biofilm formation and AI-2 signaling in E. faecalis. Their effect on the expression of the QS-regulated virulence encoding gene sprE was also investigated. Nisin exhibited a MIC ranging from 0.25 to 0.5 mg/mL, while the MIC of pCA was 1 mg/mL. The luminescence-based response of the reporter strain Vibrio harveyi BB170 was used to determine AI-2 activity in E. faecalis strains. Nisin was not effective in inhibiting AI-2 activity, while pCA reduced AI-2 activity by ≥ 60%. Moreover, pCA and nisin combination showed higher inhibitory effect on biofilm formation of E. faecalis, compared to the treatment of pCA or nisin alone. qRT-PCR analysis showed that nisin alone and the combination of nisin and pCA, at their MIC values, led to a 32.78- and 40.22-fold decrease in sprE gene expression, respectively, while pCA alone did not have a significant effect. Considering the demand to explore new therapeutic avenues for infectious bacteria, this study was the first to report that pCA can act like a quorum sensing inhibitor (QSI) against AI-2 signaling in E. faecalis. © 2023, The Author(s) under exclusive licence to Sociedade Brasileira de Microbiologia."
"Food waste (FW) is a severe environmental and social concern that today's civilization is facing. Therefore, it is necessary to have an efficient and sustainable solution for managing FW bioprocessing. Emerging technologies like the Internet of Things (IoT), Artificial Intelligence (AI), and Machine Learning (ML) are critical to achieving this, in which IoT sensors' data is analyzed using AI and ML techniques, enabling real-time decision-making and process optimization. This work describes recent developments in valorizing FW using novel tactics such as the IoT, AI, and ML. It could be concluded that combining IoT, AI, and ML approaches could enhance bioprocess monitoring and management for generating value-added products and chemicals from FW, contributing to improving environmental sustainability and food security. Generally, a comprehensive strategy of applying intelligent techniques in conjunction with government backing can minimize FW and maximize the role of FW in the circular economy toward a more sustainable future. © 2023 Elsevier Ltd"
"Social media has created new opportunities to map cultural ecosystem services (CES) related to biodiversity at large scales. However, using these novel data to understand people's preferences in relation to these CES remains a challenge. To address this, we trained a deep learning model to capture people's interactions with selected flora and fauna on Flickr as a cultural service related to biodiversity and compared this with citizen science data on iNaturalist, with photos of individual species considered as human–species interactions. After mapping the distribution of people's interactions in Great Britain on Flickr and iNaturalist, we find significant spatial differences in people's preferences on the two platforms. Using a second, pretrained deep learning model, we were also able to identify different preferences for species groups such as birds on social media versus citizen science. To better understand people's preferences, we also compared people's interactions with species richness and abundance for a group of 36 bird species, sometimes finding large differences between people's interactions and these ecological measures. Our findings demonstrate that social media can be used to include a wider range of preferences in CES assessments along-side citizen science data. However, these preferences reflect only a limited first-hand experience of biodiversity. Read the free Plain Language Summary for this article on the Journal blog. © 2023 The Authors. People and Nature published by John Wiley & Sons Ltd on behalf of British Ecological Society."
"This paper provides a new approach to understanding bankers' risk-taking behavior. We build upon prior studies that suggest artificial intelligence algorithms are an effective approach to obtaining this understanding. Our approach uses behavioral finance and a unique decision-making model. Although the decision-making literature is replete with descriptions and explanations of creditors and investors' perceptions and judgment, it does not provide an algorithmic model that incorporates a more flexible approach to how creditors subjectively valuate risky projects. Specifically, a model is presented where 33 corporate bankers realized ex ante that they were unable to accurately model the underlying uncertainty that characterizes a company's need for a loan. The results indicate that bankers' risk assessments result in different evaluations of financial information regarding loans. This approach depicts an integrative algorithmic modelling process, whereby limits in the amount of historical conditional information prohibit the use of more complex econometric techniques. © 2023 The Authors"
"Incorporating artificial intelligence and machine learning (AI/ML) methods within the 5G wireless standard promises autonomous network behavior and ultra-low-latency reconfiguration. However, the effort so far has purely focused on learning from radio frequency (RF) signals. Future standards and next-generation (nextG) networks beyond 5G will have two significant evolution over the state-of-the-art 5G implementations: (i) massive number of antenna elements, scaling up to hundreds-to-thousands in number, and (ii) inclusion of AI/ML in the critical path of the network reconfiguration process that can access sensor feeds from a variety of RF and non-RF sources. While the former allows unprecedented flexibility in ‘beamforming’, where signals combine constructively at a target receiver, the latter enables the network with enhanced situation awareness not captured by a single and isolated data modality. This survey presents a thorough analysis of the different approaches used for beamforming today, focusing on mmWave bands, and then proceeds to make a compelling case for considering non-RF sensor data from multiple modalities, such as LiDAR, Radar, and GPS for increasing beamforming directional accuracy and reducing processing time. This so called idea of multimodal beamforming will require deep learning based fusion techniques, which will serve to augment the current RF-only and classical signal processing methods that do not scale well for massive antenna arrays. The survey describes relevant deep learning architectures for multimodal beamforming, identifies computational challenges and the role of edge computing in this process, dataset generation tools, and finally, lists open research challenges that the community should tackle to realize this transformative vision of the future of beamforming. © 2023 Elsevier B.V."
"World Health Organization (WHO) proclaimed the Corona virus (COVID-19) as a pandemic, since it contaminated billions of individuals and killed lakhs. The spread along with the severity of the disease plays a key role in early detection and classification to reduce the rapid spread as the variants are changing. COVID-19 could be categorized as a pneumonia infection. Bacterial pneumonia, fungal pneumonia, viral pneumonia, etc., are the classifications of several forms of pneumonia, which are subcategorized into more than 20 forms and COVID-19 will come under viral pneumonia. The wrong prediction of any of these can mislead humans into improper treatment, which leads to a matter of life. From the radiograph that is X-ray images, diagnosis of all these forms can be possible. For detecting these disease classes, the proposed method will employ a deep learning (DL) technique. Early detection of the COVID-19 is possible with this model; hence, the spread of the disease is minimized by isolating the patients. For execution, a graphical user interface (GUI) provides more flexibility. The proposed model, which is a GUI approach, is trained with 21 types of pneumonia radiographs by a convolutional neural network (CNN) trained on Image Net and adjusts them to act as feature extractors for the Radiograph images. Next, the CNNs are combined with united AI strategies. For the classification of COVID-19 detection, several approaches are proposed in which those approaches are concerned with COVID-19, pneumonia, and healthy patients only. In classifying more than 20 types of pneumonia infections, the proposed model attained an accuracy of 92%. Likewise, COVID-19 images are effectively distinguished from the other pneumonia images of radiographs. © 2023, The Author(s), under exclusive licence to The Japanese Society for Artificial Intelligence and Springer Nature Japan KK, part of Springer Nature."
"Industrial health monitoring in factories is essential for quality assurance, energy and cost reduction, and health and safety. In aluminum factories, anode furnace pits’ flue walls deform over time due to cyclic heating and cooling. They are inspected and classified using manually acquired measurements in a process that takes several hours and is done under high temperatures using specialized equipment. We propose an end-to-end AI-powered system for automated inspection using drones. We fly a drone carrying color and depth cameras to film and navigate a 50-pit furnace floor autonomously in a GPS-denied environment. We then mosaic the recorded videos to produce color and depth mosaics using frame-to-frame motion parameters estimated using the color videos and applied to both. We finish the mosaics using depth-to-color mosaic registration based on maximizing mutual information on gradients. We extract pit images from the mosaics using a YOLOv5 object detection initially trained using a physical floor model with a proposed data augmentation scheme and then fine-tuned for the on-site environment. We achieve a mean average precision of 94.7%. Once pit images are detected and initially classified, we propose a dual-stage segmentation algorithm using the Hough transform and a semantic segmentation network trained using probabilistic feature images with a precision of 96.67%. Segmented pits with depth information allow us to produce 3D models of pits to aid in temporal monitoring and diagnosis confirmation. Our system is cost-effective and reduces inspection downtime by 87%, eliminating the need for human intervention. © 2023 The Authors"
"Artificial intelligence (AI) has reformed the healthcare system with its compelling capabilities of processing biomedical data for disease diagnosis, prediction, and individualized management. The eye, as a non-invasive observation window for many systemic diseases, can be used to detect the signs of chronic kidney diseases, and other diseases like hypertension and type 2 diabetes mellitus, based on specific manifestations of retinal images. Recent advances using AI technology have posed a great potential of using retinal images for rapid mass screening and prognosis prediction of kidney diseases. Herein, we outlined the key applications of AI in ophthalmology and the detection of systemic diseases based on retinal imaging, especially the current progress of retinal image-based AI models for the detection and prediction of kidney diseases. We hope to shed light on the current opportunities and future challenges in this field to provide suggestions for further improvement and applications. © 2023 The Authors. VIEW published by Shanghai Fuji Technology Consulting Co., Ltd, authorized by Professional Community of Experimental Medicine, National Association of Health Industry and Enterprise Management (PCEM) and John Wiley & Sons Australia, Ltd."
"The COVID-19 pandemic has created an emergency across the globe. The number of corona positive and death cases is still rising worldwide. All countries’ governments are taking various steps to control the infection of COVID-19. One step to control the coronavirus’s spreading is to quarantine. But the number of active cases at the quarantine center is increasing daily. Also, the doctors, nurses, and paramedical staff providing service to the people at the quarantine center are getting infected. This demands the automatic and regular monitoring of people at the quarantine center. This paper proposed a novel and automated method for monitoring people at the quarantine center in two phases. These are the health data transmission phase and health data analysis phase. The health data transmission phase proposed a geographic-based routing that involves components like Network-in-box, Roadside-unit, and vehicles. An effective route is determined using route value to transmit data from the quarantine center to the observation center. The route value depends on the factors such as density, shortest path, delay, vehicular data carrying delay, and attenuation. The performance metrics considered for this phase are E2E delay, number of network gaps, and packet delivery ratio, and the proposed work performs better than the existing routing like geographic source routing, anchor-based street traffic aware routing, Peripheral node based GEographic DIstance Routing. The analysis of health data is done at the observation center. In the health data analysis phase, the health data is classified into multi-class using a support vector machine. There are four categories of health data: normal, low-risk, medium-risk, and high-risk. The parameters used to measure the performance of this phase are precision, recall, accuracy, and F-1 score. The overall testing accuracy is found to be 96.8%, demonstrating strong potential for our technique to be adopted in practice. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"This study strives to analyze a poroelastic media, including water, oil, and gas, of a carbonate formation in southwest Iran. For this purpose, three data sets comprising geochemical, seismic, and geomechanical information from laboratory tests and well-logging operations were employed. In this research, seismic parameters, including P & S—wave velocity, velocity ratio (VP/VS), acoustic impedance (AI) and shear impedance (SI), were directly measured by well-logging data. Dynamic elastic moduli were calculated using the bulk density and seismic velocities. Uniaxial compressive strength (UCS) was calculated by empirical relationships and calibrated with a several laboratory tests. Initially, 41 un-contaminated core samples were tested for the Rock–Eval Pyrolysis (REP) chemical experiment. Simultaneous use of logs and chemical analysis identified three zones (A, B, C), including gas, water, and oil, for the evaluations, proving that hydrocarbon in zone C was migrated from other intervals. The results show that water saturation increases compressional wave velocity and decreases shear wave velocity through porous media. Elasticity is conjugately affected by both shear wave and compressional wave velocity. Accordingly, Young's modulus in the hydrocarbon zones increases while it experiences a decreasing trend in the water zone. On the other hand, the results exhibit a reduction in shear modulus in hydrocarbon zones compared with the water zone. In addition, in hydrocarbon zones, the shear impedance increases while the acoustic impedance decreases, in contrast to the water zone. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"The objective of this observational study was to evaluate the association between increased physical activity at first artificial insemination (AI) and subsequent pregnancy per AI (P/AI) in lactating Holstein cows following spontaneous estrus or a timed AI (TAI) protocol. We also wanted to identify factors associated with the intensity of activity increase (PA) captured by automated activity monitors (AAM) and fertility. Two experiments were conducted, in which cows either were inseminated based on the alert of the AAM system (AAM cows) or received TAI following a 7-d Ovsynch protocol (TAI cows) if not inseminated within a farm-specific period after calving. Experiment 1 included 2,698 AI services from AAM cows and 1,042 AI services from TAI cows equipped with the Smarttag Neck (Nedap Livestock Management) from a dairy farm in Slovakia (farm 1). In the second experiment, 6,517 AI services from AAM cows and 1,226 AI services from TAI cows fitted with Heatime (Heatime Pro; SCR Engineers Ltd.) from 8 dairy farms in Germany (farms 2–9) were included. Pregnancy diagnosis was performed on a weekly basis by transrectal ultrasound (farms 1, 3, 7, 8) or by transrectal palpation (farms 2, 4–6, 9). Estrous intensity was represented by the peak value of the change in activity. In experiment 1, PA was categorized into low (x-factor 0–20) and high (x-factor 21–100) PA, and in experiment 2 into low (activity change = 35–89) and high (activity change = 90–100) PA. In TAI cows from both experiments, PA was additionally categorized into cows with no AAM alert. Data were analyzed separately for AAM and TAI cows using multinomial logistic regression models for PA in TAI cows and logistic regression models for PA in AAM cows and P/AI in both groups. In experiment 1, P/AI of AAM cows was greater for AI services performed with conventional frozen semen (57.6%) compared with sexed semen (47.2%), whereas type of semen only tended to be associated with P/AI in TAI cows (54.4% conventional frozen semen vs. 48.9% sexed semen). In experiment 2, P/AI was greater for fresh semen (AAM cows: 44.4% vs. TAI cows: 44.2%) compared with conventional frozen semen (AAM cows: 37.6% vs. TAI cows: 34.6%). In both experiments, pregnancy outcomes were associated with PA. In experiment 1, AAM cows with high PA (55.1%) had greater P/AI than cows with low PA (49.8%). Within TAI cows, cows with no alert (38.8%) had reduced P/AI compared with cows with low (54.2%) or high PA (61.8%). In experiment 2, AAM cows with high PA (45.8%) had greater P/AI compared with cows with low PA (36.4%). Timed AI cows with no alert (27.4%) had decreased P/AI compared with cows with low (41.1%) or high (50.8%) PA. The greatest risk factors for high PA were parity (experiment 1) and season of AI (except for TAI cows from experiment 1). We conclude that high PA at the time of AI is associated with greater odds of pregnancy for both AAM and TAI cows. In both experiments, about 2 thirds of AAM cows (experiment 1: 69.9% and experiment 2: 70.7%) reached high PA, whereas only approximately one-third or less of TAI cows (experiment 1: 37.3% and experiment 2: 23.6%) showed high PA. Although we observed similar results using 2 different AAM systems for the most part, risk factors for high PA might differ between farms and insemination type (i.e., AAM vs. TAI). © 2023 American Dairy Science Association"
"Both band gap and stability of halide perovskites are prerequisites for deployable photovoltaic devices; however, many machine learning researches focus on one target output and a systematic machine learning workflow for achieving multiple targets is desirable. In this manuscript, we employ machine learning (ML) coupled with high-throughput density functional theory (DFT) calculation to predict potential two-dimensional lead-free halide perovskite materials with appropriate band gap and stability for solar cell applications. This is realized by the construction of two machine learning models based on the random forest algorithm with each targeting on band gap or formation energy, followed by the candidate intersection for the materials screening. The multi-objective DFT+ML framework predicts three possible lead-free two-dimensional halide perovskite materials with suitable stability and band gap, which are further evaluated via the molecular dynamics to evaluate their thermodynamic stability. Their spectroscopic limited maximum efficiencies (SLMEs) are calculated to confirm their photovoltaic capabilities. In order to comprehensively evaluate the features, new descriptors for the halide perovskite materials with better correlation with the target output are automatically formulated via symbolic regression based on genetic algorithms, and an alternative feature analysis method based on the literature textual data and natural language processing (NLP) is proposed. Post hoc analysis is performed via DFT and molecular dynamics to provide more detailed information on the materials prediction. This study highlights the developments multi-objective machine learning workflow for inverse materials design and analysis. © 2023 Elsevier Ltd"
"The changes in both the grain boundary area and grain boundary energy that occur during grain growth have been measured in polycrystalline Ni using high energy diffraction microscopy. In addition to the reduction of grain boundary area, the average grain boundary energy decreases as higher energy grain boundaries are replaced by lower energy grain boundaries. This energy dissipation mechanism influences grain boundary migration and might explain the absence of a correlation between grain boundary curvature and migration velocity. Classical studies of isotropic grain growth in polycrystals have been based on the idea that grain boundary (GB) migration is driven by the product of the GB energy and curvature. [1,2] While support for this foundational concept is certainly found in studies of bicrystals [3] and simulations [4], recent experimental evidence contradicts this idea. When measured curvatures and velocities were compared in Ni [5] and SrTiO3, [6] they were found to be uncorrelated. Similar observations were reported for α-Fe, [7] where the measured constant of proportionality between curvature and velocity did not behave in expected ways. The purpose of this letter is to present evidence that, during grain growth, the replacement of higher energy GBs by lower energy boundaries dissipates energy by a mechanism that is not related to curvature. This reduction of the average GB energy represents an additional driving force for grain growth. For a GB network comprised of N triangular mesh elements of area ai, the total free energy (F) is: © 2023 The Authors"
"Let A be an infinite sequence of positive integers and let m={(k1,m1),⋯,(kl,ml)} be a set of integer tuples such that gcd⁡(m1,m2,⋯,ml)>1. Denote by Rm(A,n) the number of different solutions of the equation k1(a1,1+⋯+a1,m)+⋯+kl(al,1+⋯+al,m)≤n with ai,j∈A. In 2013, Rué proved that for every ε>0 the function Rm(A,n)=cn+O(n1/4−ε) cannot hold for any constant c>0. Recently, we improved this bound to o(n1/4). In this paper, we obtain a stronger version about this result. © 2023 Elsevier B.V."
"Diabetic retinopathy (DR), a leading cause of preventable blindness, is expected to remain a growing health burden worldwide. Screening to detect early sight-threatening lesions of DR can reduce the burden of vision loss; nevertheless, the process requires intensive manual labor and extensive resources to accommodate the increasing number of patients with diabetes. Artificial intelligence (AI) has been shown to be an effective tool which can potentially lower the burden of screening DR and vision loss. In this article, we review the use of AI for DR screening on color retinal photographs in different phases of application, ranging from development to deployment. Early studies of machine learning (ML)-based algorithms using feature extraction to detect DR achieved a high sensitivity but relatively lower specificity. Robust sensitivity and specificity were achieved with the application of deep learning (DL), although ML is still used in some tasks. Public datasets were utilized in retrospective validations of the developmental phases in most algorithms, which require a large number of photographs. Large prospective clinical validation studies led to the approval of DL for autonomous screening of DR although the semi-autonomous approach may be preferable in some real-world settings. There have been few reports on real-world implementations of DL for DR screening. It is possible that AI may improve some real-world indicators for eye care in DR, such as increased screening uptake and referral adherence, but this has not been proven. The challenges in deployment may include workflow issues, such as mydriasis to lower ungradable cases; technical issues, such as integration into electronic health record systems and integration into existing camera systems; ethical issues, such as data privacy and security; acceptance of personnel and patients; and health-economic issues, such as the need to conduct health economic evaluations of using AI in the context of the country. The deployment of AI for DR screening should follow the governance model for AI in healthcare which outlines four main components: fairness, transparency, trustworthiness, and accountability. © 2023, The Author(s)."
"Background: Visualization of key anatomical landmarks is required during surgical Trans Abdominal Pre Peritoneal repair (TAPP) of inguinal hernia. The Critical View of the MyoPectineal Orifice (CVMPO) was proposed to ensure correct dissection. An artificial intelligence (AI) system that automatically validates the presence of key and marks during the procedure is a critical step towards automatic dissection quality assessment and video-based competency evaluation. The aim of this study was to develop an AI system that automatically recognizes the TAPP key CVMPO landmarks in hernia repair videos. Methods: Surgical videos of 160 TAPP procedures were used in this single-center study. A deep neural network-based object detector was developed to automatically recognize the pubic symphysis, direct hernia orifice, Cooper’s ligament, the iliac vein, triangle of Doom, deep inguinal ring, and iliopsoas muscle. The system was trained using 130 videos, annotated and verified by two board-certified surgeons. Performance was evaluated in 30 videos of new patients excluded from the training data. Results: Performance was validated in 2 ways: first, single-image validation where the AI model detected landmarks in a single laparoscopic image (mean average precision (MAP) of 51.2%). The second validation is video evaluation where the model detected landmarks throughout the myopectineal orifice visual inspection phase (mean accuracy and F-score of 77.1 and 75.4% respectively). Annotation objectivity was assessed between 2 surgeons in video evaluation, showing a high agreement of 88.3%. Conclusion: This study establishes the first AI-based automated recognition of critical structures in TAPP surgical videos, and a major step towards automatic CVMPO validation with AI. Strong performance was achieved in the video evaluation. The high inter-rater agreement confirms annotation quality and task objectivity. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"The EU-funded project BASTA (Boost Applied munition detection through Smart data inTegration and AI workflows, http://www.basta-munition.eu) aimed at improving underwater unexploded ordnance (UXO) detection approaches and advancing data acquisition techniques. One aspect of the project was performing autonomous underwater vehicle (AUV)-based magnetic measurements. In this paper, we present the first results of integrating three submersible fluxgate magnetometers to a Girona 500 AUV in the context of underwater UXO detection. The hovering capabilities of these AUVs allow them to maintain a fixed position or to precisely navigate at very low velocities and altitudes. The magnetic sensors are rigidly attached to the nose of the AUV at a lateral distance of 2 m and are arranged in the shape of a vertical triangle, thereby allowing for the calculation of three spatial magnetic gradients. A series of surveys was performed when visiting several munitions dumpsites in the German Baltic Sea. Furthermore, we successfully conducted a test survey with surrogate objects of known magnetic moments in a naval port basin in Kiel, Germany. With a noise floor of approximately 2 nT, the system is capable of reliably detecting munitions similar in size to 81 mm shells from altitudes of 1 m above the seafloor. For ground-truthing purposes and for a concluding confirmation or rejection of a UXO suspicion, the AUV is equipped with a high-resolution camera system. This newly developed system aims at improving the industry standard's technical potentials of autonomously discriminating between hazardous UXO and anthropogenic debris or rocks and therefore reducing the number of target points before underwater UXO clearance campaigns. © 2023 The Authors. Journal of Field Robotics published by Wiley Periodicals LLC."
"Phishing scams are increasing as the technical skills and costs of phishing attacks diminish, emphasizing the need for rapid, precise, and low-cost prevention measures. Based on a character-level convolutional neural network (CNN), we present CNN-Fusion, an effective and lightweight phishing URL detection method. Our basic idea is to deploy multiple variants of one-layer CNN with various-sized kernels in parallel to extract multi-level features. Observing that differences between phishing and benign URLs might exhibit a strong spatial correlation, we choose SpatialDropout1D, making the model more robust and preventing it from memorizing the training data. To further reduce the probability of errors that may cause by irrelevant or noisy features, we apply a max-over time pooling technique over the feature map to pick only the most important feature. Finally, the model is evaluated using five publicly available datasets containing 1.85 million phishing and benign URLs. Other than that, we assess the model against AI adversarial attacks, known as “Offensive AI.” Compared to existing methods, experiments demonstrate that our approach enjoys advantages in 5 times less training time and much more in memory consumption, achieving an average accuracy above 99% on five different datasets as well as on AI-generated malicious attacks. © 2023 Elsevier Inc."
"In recent years, the use of artificial intelligence (AI) for image and video-based crime detection has gained significant attention from law enforcement agencies and security experts. Indeed, deep learning (DL) models can learn complex patterns from data and help law enforcement agencies save time and resources by automatically identifying and tracking potential criminals. This contributes to make deep investigations and better steer their targets’ searches. Among others, handheld firearms and bladed weapons are the most frequent objects encountered at crime scenes. In this paper, we propose a DL-based surveillance system that can detect the presence of tracked objects, such as handheld firearms and bladed weapons, as well as may proceed to alert authorities regarding eventual threats before an incident occurs. After making a comparison of different DL-based object detection techniques, such as you only look once (YOLO), single shot multibox detector (SSD), or faster region-based convolutional neural networks (R-CNN), YOLO achieves the optimal balance of mean average precision (mAP) and inference speed for real-time prediction. Thus, we retain YOLOv5 for the implementation of our solution. © 2023, Institute of Advanced Engineering and Science. All rights reserved."
"Basal cell carcinoma (BCC) is one of the most common types of cancer. The growing incidence worldwide and the need for fast, reliable and less invasive diagnostic techniques make a strong case for the application of different artificial intelligence techniques for detecting and classifying BCC and its subtypes. We report on the current evidence regarding the application of handcrafted and deep radiomics models used for the detection and classification of BCC in dermoscopy, optical coherence tomography and reflectance confocal microscopy. We reviewed all the articles that were published in the last 10 years in PubMed, Web of Science and EMBASE, and we found 15 articles that met the inclusion criteria. We included articles that are original, written in English, focussing on automated BCC detection in our target modalities and published within the last 10 years in the field of dermatology. The outcomes from the selected publications are presented in three categories depending on the imaging modality and to allow for comparison. The majority of articles (n = 12) presented different AI solutions for the detection and/or classification of BCC in dermoscopy images. The rest of the publications presented AI solutions in OCT images (n = 2) and RCM (n = 1). In addition, we provide future directions for the application of these techniques for the detection of BCC. In conclusion, the reviewed publications demonstrate the potential benefit of AI in the detection of BCC in dermoscopy, OCT and RCM. © 2023 The Authors. Journal of the European Academy of Dermatology and Venereology published by John Wiley & Sons Ltd on behalf of European Academy of Dermatology and Venereology."
"During crises, there is a need for a large amount of information in a short period. Such need creates the base for misinformation to spread within and outside the affected community. This may result in misinformation harms that can generate serious short term or long-term consequences. In such situations, there is a need for a joint human-machine effort to mitigate misinformation. Though there has been research in the area of management of AI in the recent past, there has been scarce work in examining situations where machines and humans interact for mitigating misinformation. In order to systematically analyze misinformation and suggest mechanisms for mitigation, we draw on Activity Theory to conceptualize a suitable framework. Such a framework will enable investigating human-machine interactions through loops of “misinformation generation” and “misinformation mitigation” activities for mitigating misinformation harms. The paper also validates the framework using three different target audiences, undergraduates, graduates and professionals. © 2023"
"In this work, we present a cloud-based system for noncontact, real-time recognition, and monitoring of physical activities and walking periods within a domestic environment. The proposed system employs standalone Internet of Things (IoT)-based millimeter wave radar devices and deep learning models to enable autonomous, free-living activity recognition, and gait analysis. To train deep learning models, we utilize range-Doppler maps generated from a data set of real-life in-home activities. The performance of several deep learning models is evaluated based on accuracy and prediction time, with the gated recurrent network [gated recurrent unit (GRU)] model selected for real-time deployment due to its balance of speed and accuracy compared to 2-D convolutional neural network long short-term memory (2D-CNNLSTM) and long short-term memory (LSTM) models. The overall accuracy of the GRU model for classifying in-home physical activities of trained subjects is 93%, with 86% accuracy for a new subject. In addition to recognizing and differentiating various activities and walking periods, the system also records the subject's activity level over time, washroom use frequency, sleep/sedentary/active/out-of-home durations, current state, and gait parameters. Importantly, the system maintains privacy by not requiring the subject to wear or carry any additional devices.  © 2014 IEEE."
"In low-rank matrix recovery, many kinds of measurements fail to meet the standard restricted isometry property (RIP), such as rank-one measurements, that is, [A(X)]i=〈Ai,X〉 with rank(Ai)=1, i=1,...,m. Historical iterative hard thresholding sequence for low-rank matrix recovery and rank-one measurements was taken as Xn+1=Ps(Xn−μnPt(A⁎sign(A(Xn)−y))), which introduced the “tail” and “head” approximations Ps and Pt, respectively. In this paper, we remove the term Pt and provide a new iterative hard thresholding algorithm with adaptive step size (abbreviated as AIHT). The linear convergence analysis and stability results on AIHT are established under the ℓ1/ℓ2-RIP. Particularly, we discuss the rank-one Gaussian measurements under the tight upper and lower bounds on E‖A(X)‖1, and provide better convergence rate and sampling complexity. Besides, several empirical experiments are provided to show that AIHT performs better than the historical rank-one iterative hard thresholding method. © 2023 Elsevier Inc."
"Mines are one of the important energy sources in the world. Due to mining areas are often affected by adverse weather and environmental conditions (sand, dust, extreme cold, heavy snow, etc.), the efficiency and security of mining are low. As a new, efficient, prospective mode, Metaverse has been studied and successfully applied in various industries. However, there is still no research about its usage in mines. In this article, we apply Metaverse to mining and propose MetaMining. We present its definition and analyze the functions it could perform. Furthermore, we propose its development phases and architecture. Specifically, MetaMining is a mode, which aims to achieve high-efficiency, high-security mining in the physical world through the interaction between the physical world and the virtual world. Its development phases involve three steps: 1) digital twins; 2) digital natives; and 3) surreality. Its architecture consists of three components: 1) human world; 2) virtual mining system; and 3) physical mining system. In addition, we analyze key technologies and possible challenges in the construction of MetaMining.  © 2013 IEEE."
"Background: During endoscopic ultrasound-guided fine needle aspiration (EUS-FNA), cytopathology with rapid on-site evaluation (ROSE) can improve diagnostic yield and accuracy. However, ROSE is unavailable in most Asian and European institutions because of the shortage of cytopathologists. Therefore, developing computer-assisted diagnostic tools to replace manual ROSE is crucial. Herein, we reported the validation of an artificial intelligence (AI)-based model (ROSE-AI model) to substitute manual ROSE during EUS-FNA. Methods: A total of 467 digitized images from Diff-Quik (D&F)-stained EUS-FNA slides were divided into training (3642 tiles from 367 images) and internal validation (916 tiles from 100 images) datasets. The ROSE-AI model was trained and validated using training and internal validation datasets, respectively. The specificity was emphasized while developing the model. Then, we evaluated the AI model on a 693-image external dataset. We assessed the performance of the AI model to detect cancer cells (CCs) regarding the accuracy, sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV). Results: The ROSE-AI model achieved an accuracy of 83.4% in the internal validation dataset and 88.7% in the external test dataset. The sensitivity and PPV were 79.1% and 71.7% in internal validation dataset and 78.0% and 60.7% in external test dataset, respectively. Conclusion: We provided a proof of concept that AI can be used to replace manual ROSE during EUS-FNA. The ROSE-AI model can address the shortage of cytopathologists and make ROSE available in more institutes. © 2022 Journal of Gastroenterology and Hepatology Foundation and John Wiley & Sons Australia, Ltd."
"Background: Although radical gastrectomy with lymph node dissection is the standard treatment for gastric cancer, the complication rate remains high. Thus, estimation of surgical complexity is required for safety. We aim to investigate the association between the surgical process and complexity, such as a risk of complications in robotic distal gastrectomy (RDG), to establish an artificial intelligence (AI)-based automated surgical phase recognition by analyzing robotic surgical videos, and to investigate the predictability of surgical complexity by AI. Method: This study assessed clinical data and robotic surgical videos for 56 patients who underwent RDG for gastric cancer. We investigated (1) the relationship between surgical complexity and perioperative factors (patient characteristics, surgical process); (2) AI training for automated phase recognition and model performance was assessed by comparing predictions to the surgeon-annotated reference; (3) AI model predictability for surgical complexity was calculated by the area under the curve. Result: Surgical complexity score comprised extended total surgical duration, bleeding, and complications and was strongly associated with the intraoperative surgical process, especially in the beginning phases (area under the curve 0.913). We established an AI model that can recognize surgical phases from video with 87% accuracy; AI can determine intraoperative surgical complexity by calculating the duration of beginning phases from phases 1–3 (area under the curve 0.859). Conclusion: Surgical complexity, as a surrogate of short-term outcomes, can be predicted by the surgical process, especially in the extended duration of beginning phases. Surgical complexity can also be evaluated with automation using our artificial intelligence-based model. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Artificial intelligence (AI) products play a significant role in achieving environmental sustainability. These products can save various resources (e.g., energy, water), achieve cost savings, and manage waste better. However, understanding the determinants affecting the use of AI products and their impact on environmental sustainability is relatively low, specifically in developing countries. To fill this gap in the literature, this study develops a theoretical model by integrating two well-known theories, UTAUT and PMT, to explain the determinants influencing Generation Z use of AI products and their impact on environmental sustainability. The developed model was then evaluated using the PLS-SEM approach based on data collected from 562 respondents in Malaysia and Turkey. Although effort expectancy, performance expectancy, social influence, perceived severity, response efficacy, and response costs are significant drivers of green behavior among Malaysian individuals, effort expectancy, facilitating conditions, perceived severity, response efficacy, and response costs are essential determinants among Turkish individuals. Interestingly, there is no significant difference between the importance of coping appraisals (i.e., self-efficacy, response efficacy, and response costs) among these two populations. The outcomes provide several contributions to the literature on AI and environmental sustainability and offer valuable insights for the practitioners, policymakers, and AI product developers. © 2023 Elsevier Ltd"
"The ongoing COVID-19 pandemic caused by the SARS-CoV-2 virus has already resulted in 6.6 million deaths with more than 637 million people infected after only 30 months since the first occurrences of the disease in December 2019. Hence, rapid and accurate detection and diagnosis of the disease is the first priority all over the world. Researchers have been working on various methods for COVID-19 detection and as the disease infects lungs, lung image analysis has become a popular research area for detecting the presence of the disease. Medical images from chest X-rays (CXR), computed tomography (CT) images, and lung ultrasound images have been used by automated image analysis systems in artificial intelligence (AI)- and machine learning (ML)-based approaches. Various existing and novel ML, deep learning (DL), transfer learning (TL), and hybrid models have been applied for detecting and classifying COVID-19, segmentation of infected regions, assessing the severity, and tracking patient progress from medical images of COVID-19 patients. In this paper, a comprehensive review of some recent approaches on COVID-19-based image analyses is provided surveying the contributions of existing research efforts, the available image datasets, and the performance metrics used in recent works. The challenges and future research scopes to address the progress of the fight against COVID-19 from the AI perspective are also discussed. The main objective of this paper is therefore to provide a summary of the research works done in COVID detection and analysis from medical image datasets using ML, DL, and TL models by analyzing their novelty and efficiency while mentioning other COVID-19-based review/survey researches to deliver a brief overview on the maximum amount of information on COVID-19-based existing researches. [Figure not available: see fulltext.] © 2023, International Federation for Medical and Biological Engineering."
"Today, Artificial Intelligence is achieving prodigious real-time performance, thanks to growing computational data and power capacities. However, there is little knowledge about what system results convey; thus, they are at risk of being susceptible to bias, and with the roots of Artificial Intelligence (“AI”) in almost every territory, even a minuscule bias can result in excessive damage. Efforts towards making AI interpretable have been made to address fairness, accountability, and transparency concerns. This paper proposes two unique methods to understand the system’s decisions aided by visualizing the results. For this study, interpretability has been implemented on Natural Language Processing-based sentiment analysis using data from various social media sites like Twitter, Facebook, and Reddit. With Valence Aware Dictionary for Sentiment Reasoning (“VADER”), heatmaps are generated, which account for visual justification of the result, increasing comprehensibility. Furthermore, Locally Interpretable Model-Agnostic Explanations (“LIME”) have been used to provide in-depth insight into the predictions. It has been found experimentally that the proposed system can surpass several contemporary systems designed to attempt interpretability. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"There has been an international surge towards online, digital, and telehealth mental health services, further amplified during COVID-19. Implementation and integration of technological innovations, including artificial intelligence (AI), have increased with the intention to improve clinical, governance, and administrative decision-making. Mental health nurses (MHN) should consider the ramifications of these changes and reflect on their engagement with AI. It is time for mental health nurses to demonstrate leadership in the AI mental health discourse and to meaningfully advocate that safety and inclusion of end users' of mental health service interests are prioritized. To date, very little literature exists about this topic, revealing limited engagement by MHNs overall. The aim of this article is to provide an overview of AI in the mental health context and to stimulate discussion about the rapidity and trustworthiness of AI related to the MHN profession. Despite the pace of progress, and personal life experiences with AI, a lack of MHN leadership about AI exists. MHNs have a professional obligation to advocate for access and equity in health service distribution and provision, and this applies to digital and physical domains. Trustworthiness of AI supports access and equity, and for this reason, it is of concern to MHNs. MHN advocacy and leadership are required to ensure that misogynist, racist, discriminatory biases are not favoured in the development of decisional support systems and training sets that strengthens AI algorithms. The absence of MHNs in designing technological innovation is a risk related to the adequacy of the generation of services that are beneficial for vulnerable people such as tailored, precise, and streamlined mental healthcare provision. AI developers are interested to focus on person-like solutions; however, collaborations with MHNs are required to ensure a person-centred approach for future mental healthcare is not overlooked. © 2023 The Authors. International Journal of Mental Health Nursing published by John Wiley & Sons Australia, Ltd."
"It is well recognized that historical biases exist in training data against a certain sensitive group (e.g., non-White, women) which are socially unacceptable, and these unfair biases are inherited in trained artificial intelligence (AI) models. Various learning algorithms have been proposed to remove or alleviate unfair biases in trained AI models. In this paper, we consider another type of bias in training data so-called covariate shift in view of fair AI. Here, covariate shift means that training data do not represent the population of interest well. Covariate shift occurs when special sampling designs (e.g., stratified sampling) are used when collecting training data, or the population where training data are collected is different from the population of interest. When covariate shift exists, fair AI models on training data may not be fair in test data. To ensure fairness on test data, we develop computationally efficient learning algorithms robust to covariate shifts. In particular, we propose a robust fairness constraint based on the Lq norm which is a generic algorithm to be applied to various fairness AI problems without much hampering. By analyzing multiple benchmark datasets, we show that our proposed robust fairness AI algorithm improves existing fair AI algorithms much in terms of the fairness-accuracy tradeoff to covariate shift and has significant computational advantages compared to other robust fair AI algorithms. © 2023 Wiley Periodicals LLC."
"Since memristors have shown great application prospects in non-volatile memory, neural synapse, brain-like chips, artificial intelligence (AI) and quantum computers, the research of memristors has received extensive attention. In this work, the resistive switching (RS) device with Ag/SrTiO3 (STO)/Ti structure was fabricated, and it can significantly improve the RS performance of device by doping Mn ions into STO to replace part of Ti ions. That is to say, the Ag/Mn-doped SrTiO3 (SMTO)/Ti device exhibits larger resistance window and better retention at room temperature. Finally, through in-depth mechanistic analysis, a physical model of the conducting filament and Schottky emission was established to understand the charge transport mechanism and RS behavior of the device. In particular, it provides favorable factors for the migration of ions and electrons through the Mn doping because the substitution of Mn for Ti ions reduces the volume of the STO crystal while distorting its crystal shape since the ionic radius of Mn is smaller than that of Ti. This work provides an important way to improve the RS properties of ABO3-type perovskite-based resistive random access memory (RRAM), and helps to explore the cutting-edge applications of high-performance RS devices in information processing and AI. © 2023 Elsevier Ltd"
"Agricultural metaverse (AgriVerse) aims to optimize the production chain by saving costs, increasing efficiencies, and breaking information silos, in order to achieve sustainable agriculture. While AgriVerse is featured by the virtual-real interaction of the agriculture-related processes based on heterogeneous data, knowledge, and models, the link between AgriVerse and the intensively studied plant modeling is vague. This article presents briefly the research contents of plant modeling, analyzes the ongoing transition at the age of artificial intelligence (AI), and envisions future AgriVerse with the support of the agricultural foundation model, the decentralized agricultural organization (DAO) and the decentralized science (DeSci) of the plant model. Three AgriVerse application scenarios are presented. The opportunities and challenges of AgriVerse are discussed. This work is expected to identify the key research issues of AgriVerse and bring practitioners of diverse backgrounds together into the AgriVerse community.  © 2013 IEEE."
"Recent advances in technology have accelerated digitalization and intelligence in modern business. Particularly, the increasing use of Artificial Intelligence (AI) in managerial accounting is expected to accurately measure corporate performance, provide intelligent analyses, and predict the future of a company. However, along with the benefits, ethical concerns of using AI also arise, such as deprofessionalization, data breach, and isolation among accountants. This paper explores the ethical impact of AI in managerial accounting at both pre- and post-adoption stages. Based on 47 interviews conducted with companies, an AI system vendor, and regulators, we found that data security, privacy, and misuse; accountability; accessibility; benefits and challenges; and transparency and trust of AI are among the most common ethical risks in the development and use of AI in managerial accounting. Unique ethical impacts on four types of stakeholders: developers, managers in charge of AI adoption, managerial accountants, and regulators, were also discovered. © 2023 Elsevier Inc."
"Background: Congenital anomalies (CAs) with or without intellectual disability (ID)/developmental delay (DD) comprise a heterogeneous spectrum of diseases that affect approximately 3% of live births worldwide. Recently, whole-exome sequencing (WES) demonstrated the highly heterogeneous genetic causes of CAs. The purpose of this study was to evaluate a referral system to increase the yield of WES for CAs. Methods: From August 2018 to July 2019, patients with CAs, with or without ID/DD, after excluding gross chromosomal aberrations, were referred to geneticists in two medical centers. Variant prioritization was conducted with an AI-assisted tool for whole exomes or a CA-related gene panel. Results: Forty patients (27 males and 13 females) with CAs were enrolled in the study with a mean age of 4.71 years (range, 0.01–18.2). Pathogenic variants in 14 genes were discovered in 16 patients (three patients with CHD7 and 13 patients with one gene each of ATP6V1B2, TAF6, COL4A3BP, ANKH, BMP2, SMARCA4, CUL4B, PGAP3, SOX11, FBN2, PTPN11, SOS1, or PROKR2), with a positive diagnostic rate of 40%. Among the 16 positive cases, 13 (81%) also had ID/DD. The inheritance was autosomal dominant in 13 (81%), autosomal recessive in two (13%), and X-linked in one (6%). Only five patients received a correct clinical diagnosis before WES. The analyses of patients with a negative genetic diagnosis revealed a phenotype and gene mutation load similar to those of the positive-finding patients but with a lower percentage of ID/DD. Conclusions: The careful selection of patients by experienced geneticists and the exclusion of chromosomal aberrations raises the positive rate of the molecular diagnosis for CAs to 40%. However, more than half of the patients with CAs still do not have a genetic diagnosis by current technologies. © 2023 The Authors. Molecular Genetics & Genomic Medicine published by Wiley Periodicals LLC."
"Algorithmic discrimination has rapidly become a topic of intense public and academic interest. This article explores three issues raised by algorithmic discrimination: (1) the distinction between direct and indirect discrimination, (2) the notion of disadvantageous treatment, and (3) the moral badness of discriminatory automated decision-making. It argues that some conventional distinctions between direct and indirect discrimination appear not to apply to algorithmic discrimination, that algorithmic discrimination may often be discrimination between groups, as opposed to against groups, and that it is not necessarily the case that morally bad algorithmic discrimination gives us reason to not use automated decision-making. For each of the three issues, the article explores implications for algorithmic discrimination, suggests some alternative answers, and clarifies how we may want to think of discrimination more broadly in light of lessons drawn from the context of algorithmic discrimination. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"The goal of smart cities is to improve efficiencies, enhance sustainability, advance quality of life and reduce energy consumption. One of the key factors to accomplish a smart city involves the use of IoT and information technology infrastructure which can be described as the foundation of a smart city. Its effective implementation will allow the city to meet its wide range of requirements while being able to respond to innovations, such as advanced sensors, analytic tools, measurement, and artificial intelligent based solutions. This paper investigates and compares between the use of low and high resolution infrared sensors as part of the Internet of Things (IoT) to estimate crowds in cities to enhance and optimise the efficiency of the transportation process and other public services for low density scenarios. A case study was conducted in Nottingham city at one of the tram stops. An experimental methodology is used where different number of people are captured and the results are compared using different image processing techniques. The findings show that both technologies are useful in the estimation of crowd density, however, the high resolution camera has been found to be more accurate in estimating the number of people albeit it is more expensive for the integration into infrastructures. The practical implication is that low-cost and low resolution infrared cameras could provide reasonable results. However, for higher accuracy, high resolution infrared cameras will be needed; and they are potentially more expensive. So a compromise might be needed between cost and performance to encourage the installation of more IoT systems using infrared technologies. © 2022 THE AUTHORS"
[No abstract available]
"Four species of Giardia out of nine have been identified in rodents based on molecular data: G. muris, G. microti, G. cricetidarum, and G. duodenalis. A total of seven G. duodenalis assemblages (A, B, C, D, E, F, G) have been identified in rodents to date. The zoonotic assemblages A and B are responsible for 74.88% (480/641) of the total identified genotypes in rodents by statistic. For sub-assemblage A in humans, AII is responsible for 71.02% (1397/1967) of the identified sub-assemblages, followed by AI with 26.39% (519/1967) and AIII with 1.17% (23/1967), indicating a significantly greater zoonotic potential for G. duodenalis infections in humans originating from animals. For sub-assemblages of type A in rodents, AI was identified in 86.89% (53/61), and AII in 4.92% (3/61). For assemblage B, 60.84% (390/641) were identified in rodents as having zoonotic potential to humans. In environmental samples, the zoonotic assemblages A and B were responsible for 83.81% (533/636) in water samples, 86.96% (140/161) in fresh produce samples, and 100% (8/8) in soil samples. The same zoonotic potential assemblage A or B simultaneously identified in humans, rodents, and environment samples had potential zoonotic transmission between humans and animals via a synanthropic environment. The infections and zoonotic potential for G. duodenalis were higher in farmed rodents and pet rodents than that in zoo, lab, and wild rodents. In conclusion, the role of rodents in zoonotic transmission of giardiasis should be noticed. In addition to rodents, dogs, cats, wild animals, and livestock could be involved in the zoonotic transmission cycle. This study aims to explore the current situation of giardiasis in rodents and seeks to delineate the role of rodents in the zoonotic transmission of giardiasis from the One Health perspective. © 2023"
"Natural language processing (NLP) is at the forefront of great advances in contemporary AI, and it is arguably one of the most challenging areas of the field. At the same time, in the area of quantum computing (QC), with the steady growth of quantum hardware and notable improvements towards implementations of quantum algorithms, we are approaching an era when quantum computers perform tasks that cannot be done on classical computers with a reasonable amount of resources. This provides an new range of opportunities for AI, and for NLP specifically. In this work, we work with the Categorical Distributional Compositional (DisCoCat) model of natural language meaning, whose underlying mathematical underpinnings make it amenable to quantum instantiations. Earlier work on fault-tolerant quantum algorithms has already demonstrated potential quantum advantage for NLP, notably employing DisCoCat. In this work, we focus on the capabilities of noisy intermediate-scale quantum (NISQ) hardware and perform the first implementation of an NLP task on a NISQ processor, using the DisCoCat framework. Sentences are instantiated as parameterised quantum circuits; word-meanings are embedded in quantum states using parameterised quantum-circuits and the sentence’s grammatical structure faithfully manifests as a pattern of entangling operations which compose the word-circuits into a sentence-circuit. The circuits’ parameters are trained using a classical optimiser in a supervised NLP task of binary classification. Our novel QNLP model shows concrete promise for scalability as the quality of the quantum hardware improves in the near future and solidifies a novel branch of experimental research at the intersection of QC and AI. © 2023, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
"Aims: Breast cancer (BC) risk stratification is critical for predicting behaviour and guiding management decision-making. Despite the well-established prognostic value of cellular proliferation in BC, the interplay between proliferation and apoptosis remains to be defined. In this study, we hypothesised that the combined proliferation and apoptosis indices can provide a more accurate in-vivo growth rate measure and a precise prognostic predictor. Methods and results: Apoptotic and mitotic figures were counted in whole slide images (WSI) generated from haematoxylin and eosin-stained sections of 1545 BC cases derived from two well-defined BC cohorts. Counts were carried out visually within defined areas. There was a significant correlation between mitosis and apoptosis scores. High apoptotic counts were associated with features of aggressive behaviour, including high grade, high pleomorphism score and hormonal receptor negativity. Although the mitotic index (MI) and apoptotic index (AI) were independent prognostic indicators, the prognostic value was synergistically higher when combined. BC patients with a high combined AI and MI had the shortest survival. Replacing the mitosis score with the mitosis–apoptosis index in the Nottingham grading system revealed that the modified grade with the new score had a higher significant association with BC-specific survival with a higher hazard ratio. Conclusion: Apoptotic figures count provides additional prognostic value in BC when combined with MI; such a combination can be implemented to assess the behaviour of BC and provides an accurate prognostic indicator. This can be considered when using artificial intelligence algorithms to assess proliferation in BC. © 2023 The Authors. Histopathology published by John Wiley & Sons Ltd."
"This paper proposes a Human Intelligence (HI)-based Computational Intelligence (CI) and Artificial Intelligence (AI) Fuzzy Markup Language (CI&AI-FML) Metaverse as an educational environment for co-learning of students and machines. The HI-based CI&AI-FML Metaverse is based on the spirit of the Heart Sutra that equips the environment with teaching principles and cognitive intelligence of ancient words of wisdom. There are four stages of the Metaverse: preparation and collection of learning data, data preprocessing, data analysis, and data evaluation. During the data preparation stage, the domain experts construct a learning dictionary with fuzzy concept sets describing different terms and concepts related to the course domains. Then, the students and teachers use the developed CI&AI-FML learning tools to interact with machines and learn together. Once the teachers prepare relevant material, students provide their inputs/texts representing their levels of understanding of the learned concepts. A Natural Language Processing (NLP) tool, Chinese Knowledge Information Processing (CKIP), is used to process data/text generated by students. A focus is put on speech tagging, word sense disambiguation, and named entity recognition. Following that, the quantitative and qualitative data analysis is performed. Finally, the students’ learning progress, measured using progress metrics, is evaluated and analyzed. The experimental results reveal that the proposed HI-based CI&AI-FML Metaverse can foster students' motivation to learn and improve their performance. It has been shown in the case of young students studying Software Engineering and learning English. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"As an imperative method of investigating the internal mechanism of femtosecond lasers, traditional femtosecond laser modeling relies on the split-step Fourier method (SSFM) to iteratively resolve the nonlinear Schrödinger equation suffering from the large computation complexity. To realize inverse design and optimization of femtosecond lasers, numerous simulations of mode-locked fiber lasers with different cavity settings are required, further highlighting the time-consuming problem induced by the large computation complexity. Here, a recurrent neural network is proposed to realize fast and accurate femtosecond mode-locked fiber laser modeling. The generalization over different cavity settings is achieved via the proposed prior information feeding method. With the acceleration of GPU, the mean time of the artificial intelligence (AI) model inferring 500 roundtrips is less than 0.1 s, which is ≈146 times faster than the SSFM running on a CPU. The proposed AI-enabled method is promising to become a standard approach to femtosecond laser modeling. © 2023 Wiley-VCH GmbH."
"Aphasia is a language disorder which impairs people's ability to comprehend or produce words. The mechanisms behind this disorder are not yet fully understood mainly because of the challenge of interpreting them through large-scale quantitative models. To this aim, we use artificial intelligence and knowledge graphs to investigate picture naming in people affected by anomic aphasia. Our knowledge graphs encode four aspects of associative knowledge: free word associations, synonyms, generalisations and phonological similarities. We then use these networks to compute features of target words and mistakes in producing them as recorded in a psychological mega-study with 31700 utterances. Adopting a human-centric AI approach, we train an artificial general intelligence (AGI) to predict the type of mistake (formal, semantic or mixed) according to network distances and individual level psychological norms. Our results reveal some key relationships between the multiplex structure and the errors made: (i) Network distance is found to be predictive of the error type but only when considered independently across each layer (accuracy: 73.3%, precision: 74.1%, recall 70.9%); (ii) The most predictive model is achieved when closeness centrality, rather than other psychological norms is added to the four network distances (accuracy: 80.9%, precision: 80.1%, recall 79.7%). We find that the ability to predict different types of mistakes crucially depends on the presence or absence of different aspects of knowledge in the network. In particular, removing free associations damages predictions of all mistakes the most. This indicates the importance of free associations in driving picture naming. © 2023"
"Monitoring structural damage is essential for preserving and sustaining civil and mechanical systems' structural service lifecycle. Successful monitoring provides valuable information on structural health, integrity, and safety. Maintaining continuous performance highly depends on monitoring damage's occurrence, formation, and propagation. Damage may accumulate on a structure due to surrounding conditions or human-induced factors. Although structural health monitoring (SHM) technology is becoming more mature and is being adopted across a wide range of civil engineering applications (CEAs), the difficulty of capturing subtle damage from structural vibration response (SVR) is still challenging. The SVR is almost nonstationary and complex. In addition, there is no generic robust, intelligent algorithm for extracting sensitive features from massive collected data that can estimate and predict different structural integrity conditions. Thus, this study introduces a technique to derive informative damage-sensitive features (DSFs) and develop a pattern, recognition-based statistical model. The extracted DSFs differ from the prior one in some significant respect, accurately represent various damage features, and then are integrated with an AI network for pattern recognition. The wavelet energy as a damage feature is used to classify structural damage states. Experimental data of a six-story frame are used to validate the computational model and demonstrate its efficiency and accuracy. The proposed algorithm can determine the structural integrity state of large complex systems with a noisy measurement under arbitrary dynamic excitation. © 2023, Springer-Verlag GmbH Germany, part of Springer Nature."
"Objectives: This study investigated Korean dental hygiene students’ perceptions and attitudes toward artificial intelligence (AI) and aimed to identify needs for education to strengthen professional competencies. Methods: A 24-question online survey was conducted to the dental hygiene students from four Korean schools in 2021. The questionnaire included seven questions on basic characteristics and 17 AI-related questions on the student's attitudes toward AI, the confidence in AI, predictions about AI, and its future prospects. Responses were analyzed according to the frequencies and correlations between the participants’ subjective level of knowledge about AI and questions using chi-square test. Results: Invitations were sent out to 1310 students and 800 (61.1%) participated. Note that 44.2% of participants were interested in AI, and 93.1% accessed AI-related information through the internet. Participants expressed lower confidence in AI's diagnosis (14.8%) and judgment (8.1%) than in those of humans, and 21.9% believed AI would replace their job. The proportions of participants with positive perceptions of the usefulness and the potential for improvement of AI in dentistry were 65.5% and 55.4%, respectively. Participants from schools who had existing AI knowledge expressed higher demands for AI-related content as compared to those who did not (p < 0.05). Conclusion: Although dental hygiene students expressed low level of confidence in AI, they were interested in AI and had positive views of its application and potential for improvement. However, the fact they had little AI-related information from dental hygiene curriculum strongly suggests the need for AI-related lectures in schools to prepare for the future. © 2023 American Dental Education Association."
"Background and aim: Since hepatocytes produce majority of serum proteins, patients with cirrhosis display substantial alterations in the serum proteome. The aim of the current study was to characterize these changes and to study the prognostic utility of hepatocellular proteins available in routine clinical testing. Methods: Sera from 29 healthy controls and 43 patients with cirrhosis were subjected to untargeted proteomic analysis. Unsupervised hierarchical clustering was performed with Perseus software and R. Ingenuity pathway analysis (IPA) suggested upstream regulators that were validated in liver tissues. The behavior and prognostic usefulness of selected biomarkers was investigated in 61 controls and 285 subjects with decompensated cirrhosis. Results: Proteomics uncovered 65 and 16 hepatocellular serum proteins that are significantly downregulated or upregulated in patients with cirrhosis vs. controls. Hierarchical clustering revealed two main clusters and six sub-clusters. IPA identified HNF4α and IL-6 as the two major upstream regulators that were confirmed by hepatic gene expression analyses. Among pseudocholinesterase, transferrin, transthyretin, albumin, and apolipoprotein AI (Apo-AI), Apo-AI was the best predictor of 90-days transplant-free survival (AUROC 0.678; p = 0.0001) and remained an independent predictor in multivariable Cox independently of the presence of acute-on-chronic liver failure. Conclusion: Our study reveals cirrhosis-associated changes in hepatocellular serum proteins and underlying transcription factors. Serum apolipoprotein AI may constitute a useful prognostic adjunct in patients with decompensated cirrhosis. Graphical abstract: [Figure not available: see fulltext.] © 2023, The Author(s)."
"Machine learning and deep learning have become the most useful and powerful tools in the last years to mine information from large datasets. Despite the successful application to many research fields, it is widely known that some of these solutions based on artificial intelligence are considered black-box models, meaning that most experts find difficult to explain and interpret the models and why they generate such outputs. In this context, explainable artificial intelligence is emerging with the aim of providing black-box models with sufficient interpretability. Thus, models could be easily understood and further applied. This work proposes a novel method to explain black-box models, by using numeric association rules to explain and interpret multi-step time series forecasting models. Thus, a multi-objective algorithm is used to discover quantitative association rules from the target model. Then, visual explanation techniques are applied to make the rules more interpretable. Data from Spanish electricity energy consumption has been used to assess the suitability of the proposal. © 2023 Elsevier B.V."
"With its rapid development and significant benefits, increasingly more organizations have adopted artificial intelligence (AI) and taken various ways to promote employees’ AI usage and AI-related support behaviors. As a primary way to promote AI usage and AI-related supportive behaviors, building an AI identity is widely recognized as having beneficial effects on employees’ work attitudes and outcomes. Drawing upon role identity theory, we challenge this general conclusion by identifying a potential dark side of AI identity and investigating how and when AI identity promotes unethical behavior. Based on an experiment and a multi-wave field study, we found that AI identity had a positive indirect effect on unethical behavior via psychological entitlement. Furthermore, perceived rarity of AI identity moderated the observed effects—that is, when perceived rarity of AI identity was high, employees with AI identity were more likely to have psychological entitlement, which increased unethical behavior. Taken together, our findings provide new insights into the consequences of AI identity as well as reveal the importance of the rarity of AI identity and psychological entitlement in this process. © 2023 Elsevier Ltd"
"Speech recognition is one of the important applications of artificial intelligence (AI). Speech recognition aims to recognize spoken words regardless of who is speaking to them. The process of voice recognition involves extracting meaningful features from spoken words and then classifying these features into their classes. This paper presents a neural network classification system for Arabic letters. The paper will study the effect of changing the multi-layer perceptron (MLP) artificial neural network (ANN) properties to obtain an optimized performance. The proposed system consists of two main stages; first, the recorded spoken letters are transformed from the time domain into the frequency domain using fast Fourier transform (FFT), and features are extracted using mel frequency cepstral coefficients (MFCC). Second, the extracted features are then classified using the MLP ANN with back-propagation (BP) learning algorithm. The obtained results show that the proposed system along with the extracted features can classify Arabic spoken letters using two neural network hidden layers with an accuracy of around 86%. © 2023 Institute of Advanced Engineering and Science. All rights reserved."
"The coronavirus is considered this century's most disruptive catastrophe and global concern. This disease has prompted extreme social, psychological and economic impacts affecting millions of people around the globe. COVID-19 is transmitted from one infected person's body to another through respiratory droplets. This virus proliferates when people breathe in air-contaminated space with droplets and microscopic airborne particles. This research aims to analyze automatic COVID-19 detection using machine learning techniques to build an intelligent web application. The dataset has been preprocessed by dropping null values, feature engineering, and synthetic oversampling (SMOTE) techniques. Next, we trained and evaluated different classifiers, i.e., logistic regression, random forest, decision tree, k-nearest neighbor, support vector machine (SVM), ensemble models (adaptive boosting and extreme gradient boosting) and deep learning (artificial neural network, convolutional neural network and long short-term memory) techniques. Explainable AI with the LIME framework has been applied to interpret the prediction results. The hybrid CNN-LSTM algorithm with the SMOTE approach performed better than the other models on the employed open-source dataset obtained from the Israeli Ministry of Health website, with 96.34% accuracy and a 0.98 F1 score. Finally, this model was chosen to deploy the proposed prediction system to a website, where users may acquire an instantaneous COVID-19 prognosis based on their symptoms. © 2023 The Authors"
"Implementation of neural networks in case of hardware helps us to understand the different parts of the human brain operation, using artificial intelligence (AI). This paper presents a new model of the Hindmarsh–Rose (HR) Neuron that is based on basic polynomial functions called Nyquist-look up table-Hindmarsh–Rose (N-LUT-HR) based on an accurate sampling of the original model. The proposed approach is investigated in terms of its digital realization feasibility. According to high matching between the original and proposed terms, it is showed that the new modified model can follow all spiking patterns of primary model with low-error computations. In hardware case, the proposed and original models are implemented on Xilinx FPGA XC2VP30 chip to validate different aspects of the simulation results. Hardware results demonstrate that our model regenerates the desired patterns in low-cost and high-frequency (speed-up) in comparison with the other similar works. Overall saving in FPGA resources show that this new model is capable of being used in large-scale networks in case of minimum required resources (FPGA costs). In addition, the analysis of hardware indicates that the new circuits can work in a maximum frequency of 123 MHz with 98.25% saving in FPGA costs (resources utilization of FPGA). © 2023 John Wiley & Sons Ltd."
"We review publications in automated scientific discovery using deep learning, with the aim of shedding light on problems with strong connections to philosophy of science, of physics in particular. We show that core issues of philosophy of science, related, notably, to the nature of scientific theories; the nature of unification; and of causation loom large in scientific deep learning. Therefore, advances in deep learning could, and ideally should, have impact on philosophy of science, and vice versa. We suggest lines of further research, and highlight the role ‘theory-based’ AI could have in future developments of the field. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
"A technique for the separation of rarefied gas mixtures based on microelectromechanical systems (MEMS) has recently attracted the attention of researchers. The mechanism of gas separation is the thermally induced flow caused by inhomogeneous temperature gradients within the microchannel, which distinguishes the velocity of different gas species. In this paper, the effects of Knudsen number Kn and equilibrium temperature T0 on the flow properties of each gas species within the gas mixture, such as velocity, molar fraction, streamlines, temperature, and pressure, are investigated by the Direct simulation Monte Carlo (DSMC) method. The flow of the gas mixture is numerically simulated using a quantum scattering-based ab initio (AI) potential and compared with the results of numerical simulations based on the variable soft sphere (VSS) model. The results show that the gas velocity accelerates with increasing temperature, but decreases with increasing Knudsen number. The streamlines are very sensitive to the Knudsen number, yet they are only affected by the temperature in the transitional flow. When the conditions are the same, He flows faster than Ne and the streamlines are smoother. The gas separation factor increases with equilibrium temperature at T0<100K and decreases slightly with equilibrium temperature at T0≥100K. The gas separation efficiency is best in the transitional flow. The temperature discrepancy for the different gas species is less affected by the equilibrium temperature and Knudsen number, with the maximum discrepancy rate being only 2.727% for He and Ne. The pressure discrepancy is more affected by the equilibrium temperature and Knudsen number, with the maximum rate of discrepancy reaching 39.228% for He and Ne. At T0=10K and Kn=1, the discrepancy rates of the gas separation factor, temperature, and pressure obtained based on the AI potential and the VSS model reach maximum values of 44.802%, 4.675%, and 52.716%, respectively. Compared to the VSS model, the AI potential based on quantum scattering is applicable to a wider range of temperatures and is more accurate for both low and high temperatures. Furthermore, the AI potential is more accurate than the VSS model in the transitional flow. The results obtained from the AI potential based on quantum scattering will be more desirable when performing gas separation studies based on thermally induced flows in rarefied gases. © 2023 Elsevier Ltd"
"This article explores views about older people and aging underpinning practices and perceptions of development and implementation of Artificial Intelligence (AI) in long-term care homes (LTC). Drawing on semi-structured interviews with seven AI developers, seven LTC staff, and four LTC advocates, we analyzed how AI technologies for later life are imagined, designed, deployed, and resisted. Using the concepts of “promissory discourse” and “aging anxieties”, we investigated manifestations of ageism in accounts of AI applications in LTC. Despite positive intentions, both AI developers and LTC staff/advocates engaged in simplistic scripts about aging, care, and the technological capacity of older people. We further uncovered what we termed sociotechnical ageism—a form that is not merely digital but rests on interacting pre-conceptions about the inability or lack of interest of older people to use emerging technologies coupled with social assumptions about aging, LTC, and technological innovation. © The Author(s) 2023."
"Recently introduced into the archival sphere, ‘paradata’ is a conceptual framework for defining the character of information resource processing. (Davet J, Hamidzadeh B, Franks P, Bunn J (2022) Tracking the functions of AI as paradata & pursuing archival accountability. In: Archiving 2022: Final Programs and Proceedings, 7-10 June 2022. Society for imaging science and technology, Springfield, VA, USA, pp 83–88) While it need not be applied exclusively to artificial intelligence-based automated systems, paradata can be, should be, and is currently used to explicate the function of AI in the archives. The use of paradata in relation to AI can help ensure that archival ethical principles continue to be honored in an age of increasing automation. This paper summarizes the theoretical background of paradata, documentation created during recent experiments with machine learning-based AI which gestures toward the varieties of paradata already being collected by researchers, and some of the factors which condition paradata’s use. A few currently available data structures for the representation and display of paradata are evaluated, with an eye toward their suitability for different stakeholder groups. Future directions for theoretical and practical research are suggested. © 2023, The Author(s), under exclusive licence to Springer Nature B.V."
"Educators in large-scale online courses tend to lack the necessary resources to generate and provide adequate feedback for all students, especially when students’ learning outcomes are evaluated through student writing. As a result, students welcome peer feedback and sometimes generate self-feedback to widen their perspectives and obtain feedback, but often lack the support to do so. This study, as part of a larger project, sought to address this prevalent problem in large-scale courses by allowing students to write essays as an expression of their opinions and response to others, conduct peer and self-evaluation, using provided rubric and Artificial Intelligence (AI)-enabled evaluation to aid the giving and receiving of feedback. A total of 605 undergraduate students were part of a large-scale online course and contributed over 2500 short essays during a semester. The research design uses a mixed-methods approach, consisting qualitative measures used during essay coding, and quantitative methods from the application of machine learning algorithms. With limited instructors and resources, students first use instructor-developed rubric to conduct peer and self-assessment, while instructors qualitatively code a subset of essays that are used as inputs for training a machine learning model, which is subsequently used to provide automated scores and an accuracy rate for the remaining essays. With AI-enabled evaluation, the provision of feedback can become a sustainable process with students receiving and using meaningful feedback for their work, entailing shared responsibility from teachers and students, and becoming more effective. © 2023 Elsevier Ltd"
"Unmanned aerial vehicles (UAVs), also known as drones, communicate, collaborate, and form flying ad hoc networks (FANETs) to perform many different missions, ranging from delivery tasks to agriculture applications. Recently, FANETs have been integrated with different technologies, such as artificial intelligence (AI), virtual reality, and Internet of Things. Such new avenues for the use of UAVs directly impact the research on FANETs and cause some major challenges, such as security and physical layer issues, resource management, and UAV positioning issues that need to be addressed. Several researchers have been working for the last few years to propose AI and machine learning (ML)-based solutions for different use cases in UAV-based networks. They present the limitations of the existing research work and highlight some possible future works on FANETs. However, exhibiting the trends in the UAV research papers in a quantitative manner is still required to motivate researchers to rethink the research on FANETs. Therefore, this study covers more than 170 scientific publications extracted from five trusted academic databases published from 2013 to 2021 to provide a thorough overview of the main research and development statistics in the area of FANETs, the open challenges existing in this area and the ML-based solutions to solve these challenges. In addition, the investigation of emerging technologies integrated with FANETs, as well as the simulation tools employed for evaluating FANETs' performance are discussed. Moreover, the future research directions in the area of FANETs are considered within a prospective vision discussion. © 2023 Wiley Periodicals LLC."
"The letter presents a patent of lateral coupled inductor structure and its applications in single-phase and multiphase dc-dc converters. The central processing units (CPUs), graphics processing units (GPUs), and field-programming gate arrays (FPGAs) in the latest servers and artificial intelligence (AI) systems are powered by multiphase dc-dc switching converters and require high current, small size, fast transient response and high efficiency. This single-turn lateral coupled-inductor structure reduces magnetic core size through inverse coupling and minimizes inductor direct current resistor power loss for high-current applications. The coupled inductor can be embedded inside a motherboard or a power module. The letter illustrates different implementations of the patent and provides solutions to the AI systems that require high current and small volume of the power converters.  © 1986-2012 IEEE."
"Bitcoin is a type of Cryptocurrency that relies on Blockchain technology and its growing popularity is leading to its acceptance as an alternative investment. However, the future value of Bitcoin is difficult to predict due to its significant volatility and speculative behavior. Considering this, the key objective of this research is to assess Bitcoins' explosive behavior during 2013-2022 including the most volatile COVID-19 pandemic and Russia-Ukraine war period and to forecast its price by comparing the predictive abilities offive different econometric, machine learning and artificial Intelligence methods namely, ARIMA, Decision Tree, Random Forest, SVM, and Artificial Intelligence Long Short-Term Memory Network (AI-LSTM). The precision of such methodologies has been assessed using root mean square error (RMSE) and mean average per cent error (MAPE) values. The findings confirmed that the AI-LSTM model performs better than other forecast models in predicting Bitcoins' opening price on the following working day. Therefore, Bitcoin traders, policymakers, and financial institutions can use the model effectively to better forecast the next day's opening price.  © 2023 World Scientific Publishing Company."
"Deep learning has emerged recently as a type of artificial intelligence (AI) and machine learning (ML), it usually imitates the human way in gaining a particular knowledge type. Deep learning is considered an essential data science element, which comprises predictive modeling and statistics. Deep learning makes the processes of collecting, interpreting, and analyzing big data easier and faster. Deep neural networks are kind of ML models, where the non-linear processing units are layered for the purpose of extracting particular features from the inputs. Actually, the training process of similar networks is very expensive and it also depends on the used optimization method, hence optimal results may not be provided. The techniques of deep learning are also vulnerable to data noise. For these reasons, fuzzy systems are used to improve the performance of deep learning algorithms, especially in combination with neural networks. Fuzzy systems are used to improve the representation accuracy of deep learning models. This survey paper reviews some of the deep learning based fuzzy logic models and techniques that were presented and proposed in the previous studies, where fuzzy logic is used to improve deep learning performance. The approaches are divided into two categories based on how both of the samples are combined. Furthermore, the models' practicality in the actual world is revealed. © 2023 Institute of Advanced Engineering and Science. All rights reserved."
"Data intensive applications such as AI (Artificial Intelligence) and IoT (Internet of Things) demand high performance and highly reliable non-volatile memories (NVM). FeFET offers several exciting features such as faster write/read speed, high density, low power, non-destructive readout, random access and high endurance compared with existing mainstream flash memories. Large memory window (MW) and robust device reliability in terms of endurance and retention time are the key requirements for FeFETs in practical NVM applications. Ferroelectric materials, channel materials, gate electrode materials and seed layers can significantly affect the memory performance of FeFETs. Therefore, this article critically reviews the memory performance FeFETs over the last 50 years, various FeFET architectures, the role of 2D-materials in FeFETs, emergence of organic materials for the development of organic FeFETs, role of oxides as channel materials, advancements in fabrication process and reliability concerns and emerging applications of FeFETs. © 2023 Elsevier Ltd"
"The stock market is an exciting field of interest to many people regardless of their occupational background. It is a market where individuals with adequate knowledge can join and earn an additional income. Nowadays, life expenses have increased. Hence, the number of people investing in stocks is increasing dramatically. Anyone may indeed start participating in the stock market at any time, yet it is not ensured that they will profit from this investment. The stock market is a risky field of investment, given that it is unknown whether the stock will rise or fall. Stock market prediction using Artificial Intelligence techniques is a possible way to help people anticipate stock market directions. Current research showed that many factors aid in changing the stock market value in general and specifically in the Saudi stock market. To our knowledge, most research studies only consider historical data in predicting stock market trends. However, this research aims to enhance the accuracy of the daily closing price for three Saudi stock market sectors by considering historical and sentimental data. Several intelligent algorithms are considered, and their performance indicators are discussed and contrasted against each other. This research concluded that more accurate stock market prediction models could be produced by employing historical and sentimental data.  © 2023 World Scientific Publishing Co."
"As the demand for automatic video interviews powered by artificial intelligence (AI) increases among employers in the postpandemic era, so do concerns over job applicants' trust in the technology. There are various forms of AI-based video interviews with and without the features of tangibility, immediacy, and transparency used for preemployment screening, and these features may distinctively influence applicants' trust in the technology and whether they engage in or disengage from the hiring process accordingly. This field study involved designing a test of the effect of various forms of AI-based video interviews on interviewees' cognitive and affective trust based on the self-reporting of 152 real job applicants. The study found that AI used in asynchronous video interviews (AI-AVI) increased applicants' cognitive trust from that in the non-AI condition. Moreover, when the AI-AVI had features of tangibility and transparency, the applicants’ cognitive and affective trust increased. However, the feature of immediacy did not have a statistically significant impact. Contrary to concern over the potential negative effects caused by AI and its features, no statistically significant impacts were found in this study. © 2023 Elsevier Ltd"
"Precise permanent fault localization is an important task for fast power restoration in underground HV cable systems. Among online fault localization methods, the fault localization accuracy strongly relies on the type and volume of measurement, and there is always a tradeoff between localization accuracy and measurement cost. To balance the tradeoff, a data-efficient HV cable fault localization framework is proposed in this paper. First, the fault characteristics of sheath currents are analyzed in modal mode, and compared with the conventional core conductor measurements. It is discerned that the sum of three phases sheath currents has similar characteristics, which can be measured by fewer sensors in a lower rating as the replacement of the conventional core conductor measurements. Second, the challenges of recognizing the wavefront arrival in the sheath are presented, and a Convolution Neural Network is introduced for the localization purpose. The proposed approach can realize high localization accuracy with low-cost measurement, and keep consistent performance under various scenarios through limited training datasets. A case study has been carried out using PSCAD/EMTDC platform to validate the effectiveness and feasibility of the proposed approach, followed by a discussion on the results.  © 1986-2012 IEEE."
"In this paper, an extensive review for objects and drones (AUVs) detection and tracking is presented. The article presents state of the art methods used in detection and tracking of drones with adequate analysis and comparisons summarizing the findings of the most recent research material in that field. The most famous technique used in drones tracking is Kalman Filters (KFs) in its different forms. The paper presents analysis and comparisons for drones tracking based on Linear Kalman Filters (LKF) compared to tracking using Nonlinear Polynomial Regression (NPR) techniques. Interesting findings reflect the need for both methods at different circumstances depending on the noise conditions of the measurements. On the other hand, many new methods such as Artificial Intelligence (AI) based techniques are recently used in drones detection and recognition. Detection methods could come separate or combined with tracking techniques. The work presents broad and deep literature review with critical analysis of most famous methods used in drones detection and tracking. © 2023, The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE)."
"In today's digital world, saturated with data flows, universal multifunctional systems are developing, capable of solving various problems related to optimizing the use of available computing resources. A distinctive feature of such systems is the heterogeneity of incoming flows of user requests due to the multifunctionality of modern information systems, expressed in supporting various multimedia services on a single platform. Data heterogeneity and large volumes of data create many problems related to the speed of digital systems and data storage security. The solutions can be found in artificial intelligence (AI) technologies, particularly machine learning. Therefore, development and implementation of digital telecommunication complexes for storing, processing, and forming a dynamic flow of multiformat data using AI technologies are becoming more relevant. This paper aims to identify trends and prospects for developing these complexes, and develop proposals on their perspective characteristics. The authors focused on review the experience of Russian organizations developing multi-object analytics systems and analyze the technical and functional characteristics of existing systems. The result of the review and analysis is a table with a comparison of the technical characteristics of existing complexes and proposals for characteristics that are promising for further implementation. © 2023, Institute of Advanced Engineering and Science. All rights reserved."
"Objectives: Only few published artificial intelligence (AI) studies for COVID-19 imaging have been externally validated. Assessing the generalizability of developed models is essential, especially when considering clinical implementation. We report the development of the International Consortium for COVID-19 Imaging AI (ICOVAI) model and perform independent external validation. Methods: The ICOVAI model was developed using multicenter data (n = 1286 CT scans) to quantify disease extent and assess COVID-19 likelihood using the COVID-19 Reporting and Data System (CO-RADS). A ResUNet model was modified to automatically delineate lung contours and infectious lung opacities on CT scans, after which a random forest predicted the CO-RADS score. After internal testing, the model was externally validated on a multicenter dataset (n = 400) by independent researchers. CO-RADS classification performance was calculated using linearly weighted Cohen’s kappa and segmentation performance using Dice Similarity Coefficient (DSC). Results: Regarding internal versus external testing, segmentation performance of lung contours was equally excellent (DSC = 0.97 vs. DSC = 0.97, p = 0.97). Lung opacities segmentation performance was adequate internally (DSC = 0.76), but significantly worse on external validation (DSC = 0.59, p < 0.0001). For CO-RADS classification, agreement with radiologists on the internal set was substantial (kappa = 0.78), but significantly lower on the external set (kappa = 0.62, p < 0.0001). Conclusion: In this multicenter study, a model developed for CO-RADS score prediction and quantification of COVID-19 disease extent was found to have a significant reduction in performance on independent external validation versus internal testing. The limited reproducibility of the model restricted its potential for clinical use. The study demonstrates the importance of independent external validation of AI models. Key Points: • The ICOVAI model for prediction of CO-RADS and quantification of disease extent on chest CT of COVID-19 patients was developed using a large sample of multicenter data. • There was substantial performance on internal testing; however, performance was significantly reduced on external validation, performed by independent researchers. The limited generalizability of the model restricts its potential for clinical use. • Results of AI models for COVID-19 imaging on internal tests may not generalize well to external data, demonstrating the importance of independent external validation. © 2023, The Author(s)."
"Abstract: Serial MRI is an essential assessment tool in prostate cancer (PCa) patients enrolled on active surveillance (AS). However, it has only moderate sensitivity for predicting histopathological tumour progression at follow-up, which is in part due to the subjective nature of its clinical reporting and variation among centres and readers. In this study, we used a long short-term memory (LSTM) recurrent neural network (RNN) to develop a time series radiomics (TSR) predictive model that analysed longitudinal changes in tumour-derived radiomic features across 297 scans from 76 AS patients, 28 with histopathological PCa progression and 48 with stable disease. Using leave-one-out cross-validation (LOOCV), we found that an LSTM-based model combining TSR and serial PSA density (AUC 0.86 [95% CI: 0.78–0.94]) significantly outperformed a model combining conventional delta-radiomics and delta-PSA density (0.75 [0.64–0.87]; p = 0.048) and achieved comparable performance to expert-performed serial MRI analysis using the Prostate Cancer Radiologic Estimation of Change in Sequential Evaluation (PRECISE) scoring system (0.84 [0.76–0.93]; p = 0.710). The proposed TSR framework, therefore, offers a feasible quantitative tool for standardising serial MRI assessment in PCa AS. It also presents a novel methodological approach to serial image analysis that can be used to support clinical decision-making in multiple scenarios, from continuous disease monitoring to treatment response evaluation. Key Points: •LSTM RNN can be used to predict the outcome of PCa AS using time series changes in tumour-derived radiomic features and PSA density. •Using all available TSR features and serial PSA density yields a significantly better predictive performance compared to using just two time points within the delta-radiomics framework. •The concept of TSR can be applied to other clinical scenarios involving serial imaging, setting out a new field in AI-driven radiology research. © 2023, The Author(s)."
"The outbreak of the COVID-19 pandemic has transpired the global media to gallop with reports and news on the novel Coronavirus. The intensity of the news chatter on various aspects of the pandemic, in conjunction with the sentiment of the same, accounts for the uncertainty of investors linked to financial markets. In this research, Artificial Intelligence (AI) driven frameworks have been propounded to gauge the proliferation of COVID-19 news towards Indian stock markets through the lens of predictive modelling. Two hybrid predictive frameworks, UMAP-LSTM and ISOMAP-GBR, have been constructed to accurately forecast the daily stock prices of 10 Indian companies of different industry verticals using several systematic media chatter indices related to the COVID-19 pandemic alongside several orthodox technical indicators and macroeconomic variables. The outcome of the rigorous predictive exercise rationalizes the utility of monitoring relevant media news worldwide and in India. Additional model interpretation using Explainable AI (XAI) methodologies indicates that a high quantum of overall media hype, media coverage, fake news, etc., leads to bearish market regimes. © 2023 The Author(s)"
"In this work, we propose a condition-aware analytical framework - KEdge - for health condition recommendation in Internet of Things (IoT) based mobile healthcare systems. Procuring data from multiple sensors and making a singular assessment from them is a challenging task. KEdge overcomes such an issue by determining the severity of the patient by determining a condition index (CI) using a two-step analytical framework and a multiple rule fuzzy inference system (FIS). In the first step, KEdge detects the heart severity condition using a convolutional neural network model and, in the second step, it detects the respiratory condition using a random forest classification model. KEdge also utilizes auscultation sounds from SkopEdge (a digital stethoscope) for assessing the heartbeats. Through extensive experiments, we observe that KEdge identifies the arrhythmia condition with an accuracy of 98.53% and respiratory condition by 98.68%. KEdge considers the analytical predictions and analysis from SkopEdge to evaluate the CI for recommending the overall health condition using Mamdani FIS. We observe that KEdge is suitable for resource-constrained IoT devices providing memory consumption of 6.6%. On offloading the same to the fog nodes, we observe improved CPU utilization with data upload rates in the order of 25 kb/s (KEdge) and 5 Mb/s (SkopEdge).  © 2007-2012 IEEE."
"The Internet of Things (IoT) uses embedded sensors and networks to intelligently connect our surrounded objects and improve quality of life. A huge number of review research studies have been published that review previous IoT related research and more recent advances in a wide number of different IoT technologies and applications. However, as yet, there has not been any meta-review conducted to comprehensively analyze all current IoT review publications. The main objective of this survey is to present the research community with an overview and analysis of IoT review studies that have been published so far, and what challenges still remain that need to be addressed in this field. This paper also attempts to report the enabling factors that were identified, as well as the weaknesses and the risk factors in the current published survey papers. First, IoT review publications were classified into eight categories: IoT applications, types, security, data, communication networks, development, protocol standards, and technologies. Second, the bibliometric indicators were used to identify the leading trends in the IoT field. Third, a graphical/spatial mapping of the bibliographic material, journals and author co-citation networks, the bibliometric coupling of countries, and the co-occurrence of keyword network maps were developed from the IoT reviews. The results indicate that IoT applications, technologies, and security categories had the highest number of publications, whereas the IoT data and network categories had the lowest amount of publications, each of them had less than 5.5% of the total. The USA had the highest h-index at 56, the leading country in this field. © 2023"
"Social media is a more common and powerful platform for communication to share views about any topic or article, which consequently leads to unstructured toxic, and hateful conversations. Curbing hate speeches has emerged as a critical challenge globally. In this regard, Social media platforms are using modern statistical tools of AI technologies to process and eliminate toxic data to minimize hate crimes globally. Demanding the dire need, machine and deep learning-based techniques are getting more attention in analyzing these kinds of data. This survey presents a comprehensive analysis of hate speech definitions along with the motivation for detection and standard textual analysis methods that play a crucial role in identifying hate speech. State-of-the-art hate speech identification methods are also discussed, highlighting handcrafted feature-based and deep learning-based algorithms by considering multimodal and multilingual inputs and stating the pros and cons of each. Survey also presents popular benchmark datasets of hate speech/offensive language detection specifying their challenges, the methods for achieving top classification scores, and dataset characteristics such as the number of samples, modalities, language(s), number of classes, etc. Additionally, performance metrics are described, and classification scores of popular hate speech methods are mentioned. The conclusion and future research directions are presented at the end of the survey. Compared with earlier surveys, this paper gives a better presentation of multimodal and multilingual hate speech detection through well-organized comparisons, challenges, and the latest evaluation techniques, along with their best performances. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Climate change is a major global challenge that requires the integration of many different scientific disciplines, including atmospheric science, oceanography, and ecology. The complexity and scale of the problem require sophisticated tools and techniques to understand, model, and project future climate conditions. Artificial intelligence and natural language processing technologies, such as ChatGPT, have the potential to play a critical role in advancing our understanding of climate change and improving the accuracy of climate projections. ChatGPT can be used in a variety of ways to aid climate research, including in model parameterization, data analysis and interpretation, scenario generation, and model evaluation. This technology provides researchers and policy-makers with a powerful tool for generating and analyzing different climate scenarios based on a wide range of data inputs, and for improving the accuracy of climate projections. The author acknowledges asking chatGPT questions regarding its uses for Climate Change Research. Some of the uses that it states are possible now and some are potentials for the future. The author has analyzed and edited the replies of chat GPT. © 2023, The Author(s) under exclusive licence to Biomedical Engineering Society."
"Global competition and increased variety in products have created challenges for manufacturing companies. One solution to handle the variety in production is to use reconfigurable manufacturing systems (RMS). These are modular systems where machines can be rearranged depending on what is being manufactured. However, implementing a rearrangeable system drastically increases complexity, among which one challenge with RMS is how to design a new layout for a customized product in a highly autonomous and responsive fashion, known as the layout design problem. In this paper, we combine several Industry 4.0 technologies, i.e., IIoT, digital twin, simulation, advanced robotics, and artificial intelligence (AI), together with optimization to create a smart layout design system for RMS. The system automates the layout design process of RMS and removes the need for humans to design a new layout of the system. © 2023 The Author(s)"
"The stories told by expert activists about the relationship between AI and inequality are the focus of this article. It explores internet governance discourse in two fora - RightsCon and Sweden's Internet Days - which, it is argued, comprise a communicative space that is both global and liminal. Narrative analysis is used to map how 30 expert activists from around the world, whose engagement is bound neither to state nor corporate interests, talk about how AI can be understood as a boon or a bane to inequality, both social and communicative. While common themes are in evidence (such as the need to safeguard people's right to own their own data), some noteworthy dissonances are also discernible (such as whether such people should be envisaged as individuals or collectivities). The narratives are critical in that they resist the impetus of rapid, and in some cases unfettered, technological advancement while at the same time pushing back against the apocalyptic AI narratives familiar from popular culture. The study contributes to an understanding of the socio-technico imaginaries of a category of actors who merit more attention than they have been paid by scholars to date. Their expertise grants them authority, and the stories they tell speak of agency. © 2022 The Authors"
"The combination of Artificial Intelligence and the Internet of Things (AIoT) is enabling the next economic revolution in which data and immediacy are at the key players. Agriculture is one of the sectors that can benefit most from the use of AIoT to optimise resources and reduce its environmental footprint. However, this convergence requires computational resources that enable the execution of AI workloads, and in the context of agriculture, ensuring autonomous operation and low energy consumption. In this work, we evaluate TinyML and edge computing platforms to predict the indoor temperature of an operational greenhouse in situ. In particular, the computational/energy trade-off of these platforms is assessed to analyse whether their use in this context is feasible. Two artificial neural networks are adapted to these platforms to predict the indoor temperature of the greenhouse. Our results show that the microcontroller-based devices can offer a competitive and energy-efficient computational alternative to more traditional edge computing approaches for lightweight ML workloads. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Domain experts utilize a decision-support system depending on an artificial intelligence (AI) algorithm. Likewise, researchers in brain-computer interface (BCI) have recently employed deep learning (DL) algorithms for decoding and analyzing neural signals. Despite its outstanding performance, the BCI technology with the DLs has pointed out that it has a potential problem of low transparency due to algorithmic complexity of the models. On this problem, explainable artificial intelligence (XAI) can be a solution to make an AI algorithm and its decisions more interpretable. However, the explanation from the XAI has been emphasized that it should be designed corresponding with the user's different expectations which are contextually variable. Thus, our study aims to propose an explanation interface for the BCI expert under Pragmatism structuralizing an explanation with scientific knowledge in a contrastive manner. For this work, we conduct a contextual design process with five BCI experts, specifically conducting a contextual inquiry and work modeling to extract design requirements from their expertise in their work environment; next, designing and evaluating an interactive prototype of the explanation interface. The results indicated that our prototype has the advantages of increasing contextual understanding and intuitive interface design. Yet, there were also challenges on the explanation for novice users without prior knowledge on the XAI and objective understanding of the AI model with enough interpretability. This study contributes to providing a theoretical framework based on Pragmatism and designing a user-centered XAI system for domain experts in a specific context. © 2023 Elsevier Ltd"
"Purpose: Dyssynchrony may cause lung injury and is associated with worse outcomes in mechanically ventilated patients. Reverse triggering (RT) is a common type of dyssynchrony presenting with several phenotypes which may directly cause lung injury and be difficult to identify. Due to these challenges, automated software to assist in identification is needed. Materials and methods: This was a prospective observational study using a training set of 15 patients and a validation dataset of 13 patients. RT events were manually identified and compared with “rules-based” programs (with and without esophageal manometry and reverse triggering with breath stacking), and were used to train a neural network artificial intelligence (AI) program. RT phenotypes were identified using previously defined rules. Performance of the programs was compared via sensitivity, specificity, positive predictive value (PPV) and F1 score. Results: 33,244 breaths were manually analyzed, with 8718 manually identified as reverse-triggers. The rules-based and AI programs yielded excellent specificity (>95% in all programs) and F1 score (>75% in all programs). RT with breath stacking (24.4%) and mid-cycle RT (37.8%) were the most common phenotypes. Conclusions: Automated detection of RT demonstrated good performance, with the potential application of these programs for research and clinical care. © 2023 Elsevier Inc."
"Background: Surgical video recording provides the opportunity to acquire intraoperative data that can subsequently be used for a variety of quality improvement, research, and educational applications. Various recording devices are available for standard operating room camera systems. Some allow for collateral data acquisition including activities of the OR staff, kinematic measurements (motion of surgical instruments), and recording of the endoscopic video streams. Additional analysis through computer vision (CV), which allows software to understand and perform predictive tasks on images, can allow for automatic phase segmentation, instrument tracking, and derivative performance-geared metrics. With this survey, we summarize available surgical video acquisition technologies and associated performance analysis platforms. Methods: In an effort promoted by the SAGES Artificial Intelligence Task Force, we surveyed the available video recording technology companies. Of thirteen companies approached, nine were interviewed, each over an hour-long video conference. A standard set of 17 questions was administered. Questions spanned from data acquisition capacity, quality, and synchronization of video with other data, availability of analytic tools, privacy, and access. Results: Most platforms (89%) store video in full-HD (1080p) resolution at a frame rate of 30 fps. Most (67%) of available platforms store data in a Cloud-based databank as opposed to institutional hard drives. CV powered analysis is featured in some platforms: phase segmentation in 44% platforms, out of body blurring or tool tracking in 33%, and suture time in 11%. Kinematic data are provided by 22% and perfusion imaging in one device. Conclusion: Video acquisition platforms on the market allow for in depth performance analysis through manual and automated review. Most of these devices will be integrated in upcoming robotic surgical platforms. Platform analytic supplementation, including CV, may allow for more refined performance analysis to surgeons and trainees. Most current AI features are related to phase segmentation, instrument tracking, and video blurring. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"We consider the influence of two types of contextual information, background information available to users and users’ goals, on users’ views and preferences regarding textual explanations generated for the outcomes predicted by Decision Trees (DTs). To investigate the influence of background information, we generate contrastive explanations that address potential conflicts between aspects of DT predictions and plausible expectations licensed by background information. We define four types of conflicts, operationalize their identification, and specify explanatory schemas that address them. To investigate the influence of users’ goals, we employ an interactive setting where given a goal and an initial explanation for a predicted outcome, users select follow-up questions, and assess the explanations that answer these questions. Here, we offer algorithms to generate explanations that address six types of follow-up questions. The main result from both user studies is that explanations which have a contrastive aspect about a predicted class are generally preferred by users. In addition, the results from the first study indicate that these explanations are deemed especially valuable when users expectations differ from predicted outcomes; and the results from the second study indicate that contrastive explanations which describe how to change a predicted outcome are particularly well regarded in terms of helping users achieve this goal, and they are also popular in terms of helping users achieve other goals. © 2023 Elsevier Ltd"
"Battery energy storage systems (BESSs) have the potential to become providers of frequency regulation services in future energy systems. In this case, the reliable operation of BESS has a significant impact on system stability. One of the tools that ensures the secure BESS operation is applying a digital twin to forecast the BESS state of charge (SOC). It allows to predict BESS behavior and identify potential failures or cyberattacks in BESS. In this article, we compare multiple data-driven approaches to forecast the SOC of a BESS based on realistic dataset and real battery operation. Recurrent neural networks, e.g., feedforward neural network, gated recurrent unit, long short-term memory, support vector regression, random forest, and AdaBoost methods are applied to evaluate each method's performance. We compare the methods based on the maximum and average forecast errors, the required computational capacity, as well as expected training speed. To validate the results, in addition to the data gathered from simulations, we used experimental setup with a BESS of 79 kWh providing containment reserve for normal operation, which is a BESS service with a high economical potential in Nordic Region. Furthermore, we provide recommendations for the methods that are suitable to be applied for modeling the digital twin of a utility-scale battery digital twin.  © 2007-2012 IEEE."
"Asphaltenes create severe problems in light crude oils (LO) and heavy crude oils (HO) production; therefore, understanding the proper asphaltenes adsorption is a demanding topic to circumvent asphaltene deposition and reconfigure asphaltene viscoelastic networks. The aim of this work is to develop several artificial intelligence (AI) agents that accurately predict the asphaltene adsorption produced by different types of nanoparticles. More than 35 experimental data points were used including different types of crude oils (LO, HO, and extraheavy oils) combined with different types of nanoparticles including silica and alumina. This work presents a general AI agent that predicts the adsorption isotherms of asphaltene exclusively for silica and alumina nanoparticles. Copyright © 2023 Society of Petroleum Engineers."
"With the advent of the Big Data era, neuromorphic computing (NC) (also known as brain-inspired computing) has gained a lot of research interest. Spintronic devices are the emerging candidates for implementing the NC due to their intrinsic nonvolatility, extremely high endurance, low-power consumption, and complementary metal-oxide compatibility. Many research groups have proposed various NC architectures based on spintronic devices. Herein, a collective survey of different spintronic-based approaches is given for NC. The reviewed approaches include the progress of stochastic magnetic tunnel junction (MTJ) devices, spin-torque nano-oscillator, spin-Hall nano-oscillator, domain walls, and skyrmion devices. In all of these approaches, spin–orbit torque (SOT)-based magnetization control, which is achieved via spintronics heterostructures, plays a significant role. Various heterostructures of heavy metal and ferromagnetic layers that have been proposed are reviewed for generating SOT. In addition, the phenomena and materials involved in the generation of orbital torque are summarized due to the orbital Hall effect (OHE), which has recently gained researchers’ attention. Finally, an outlook on the opportunities and challenges for spintronic-based NC hardware is provided, shedding light on its great potential for artificial intelligence (AI) applications. © 2023 Wiley-VCH GmbH."
"The US is promoting a new vision of a “Good AI Society” through its recent AI Bill of Rights. This offers a promising vision of community-oriented equity unique amongst peer countries. However, it leaves the door open for potential rights violations. Furthermore, it may have some federal impact, but it is non-binding, and without concrete legislation, the private sector is likely to ignore it. © 2023, The Author(s)."
"In the current integrated circuit (IC) design process, research on its design process is relatively backward. This paper mainly adopts 5G + artificial intelligence technology, and conducts an in-depth discussion on the structure and algorithm of wireless network integrated circuits. The article analyzes the development of IC industry and the application of EDA software, and puts forward some problems that China's IC industry is currently facing. The article discusses the realization of IC design flow, which is realized from five aspects: design input, function simulation, layout realization, physical verification, and post-parasitic simulation. Then, in view of the problems existing in the current IC design process, specific solutions for improvement and strengthening are given. Four parts are designed in detail, namely multi-mode simulation, realization of fully automated layout, feature extraction and modeling, IR Drop and EM analysis. Combined with the widely used characteristics of IP design reuse technology, this paper conducts a targeted research on the process of IP design reuse. From the IP circuit transplantation and IP layout transplantation, the specific discussion is carried out. From the perspective of IP design reuse, the design process of the wireless network integrated circuit is further improved. This paper turns to the design of RF IC from the perspective of IC design process, giving the flow chart of RF chip. The test results showed that over the full temperature range, the total current was about 2.45 mA, which met the design requirements. When the load resistance is 4KΩ, the ascending propagation delay is 92tns and the descending propagation delay is 65tns, both of which increase significantly. The article can provide further reference for the design of wireless network integrated circuits. © 2023 Elsevier B.V."
"Background: Bone mineral density (BMD) lacks sensitivity in individual fracture risk assessment in early breast cancer (EBC) patients treated with aromatase inhibitors (AIs). New dual-energy X-ray absorptiometry (DXA) based risk factors are needed. Methods: Trabecular bone score (TBS), bone strain index (BSI) and DXA parameters of bone geometry were evaluated in postmenopausal women diagnosed with EBC. The aim was to explore their association with morphometric vertebral fractures (VFs). Subjects were categorized in 3 groups in order to evaluate the impact of AIs and denosumab on bone geometry: AI-naive, AI-treated minus (AIDen-) or plus (AIDen+) denosumab. Results: A total of 610 EBC patients entered the study: 305 were AI-naive, 187 AIDen-, and 118 AIDen+. In the AI-naive group, the presence of VFs was associated with lower total hip BMD and T-score and higher femoral BSI. As regards as bone geometry parameters, AI-naive fractured patients reported a significant increase in femoral narrow neck (NN) endocortical width, femoral NN subperiosteal width, intertrochanteric buckling ratio (BR), intertrochanteric endocortical width, femoral shaft (FS) BR and endocortical width, as compared to non-fractured patients. Intertrochanteric BR and intertrochanteric cortical thickness significantly increased in the presence of VFs in AIDen- patients, not in AIDen+ ones. An increase in cross-sectional area and cross-sectional moment of inertia, both intertrochanteric and at FS, significantly correlated with VFs only in AIDen+. No association with VFs was found for either lumbar BSI or TBS in all groups. Conclusions: Bone geometry parameters are variably associated with VFs in EBC patients, either AI-naive or AI treated in combination with denosumab. These data suggest a tailored choice of fracture risk parameters in the 3 subgroups of EBC patients. © 2023"
"This paper proposes novel improvements to existing Design Space Exploration (DSE) workflows that enable detailed performance-analysis at the speed of design for all projects at little cost. The authors refer to this methodology as Universal Design Space Exploration (UDSE). Rather than apply DSE to a project-specific challenge, UDSE enables a single pre-simulated design space to be applicable across many projects. The novel scalability of these “universal” design spaces justify investment in ML-powered apps that make pre-simulated analysis instantly accessible, affordable, and impactful across multiple projects. This research showcases the feasibility and potential benefit of UDSE by applying it to the challenge of early conceptual energy modeling. First, a group of experts crafts the input parameters and output metrics of a massive Design Space so that it encompasses the common problem. Then an automated parametric simulation workflow is developed to model and simulate any combination of input parameters. Several hundred thousand iterations are then simulated and analyzed. The result of this analysis guides the design of a prototype app which is powered by an AI surrogate model that allows users to receive instantaneous analysis about any design contained within the design space. This research shows that it is feasible to simulate the massive design spaces required by UDSE using currently available computational resources. We show that the surrogate modeling process is capable of accurately extending relatively limited simulation data to fully map the design space. We also show that these surrogate models can be effectively integrated into custom apps that can automate advanced DSE analysis and deliver insights to design teams in real-time. This paper concludes that UDSE offers a novel and scalable approach to early conceptual performance analysis. © 2023 Elsevier Ltd"
"Robust optimization is an ideal solution for enhancing safety in tunnel construction in the presence of unpredictable soil conditions, especially in large-diameter tunnel construction, since it requires the least amount of information about uncertainties. However, the application of robust optimization to real-world projects is greatly hampered by its dependence on mathematical models. To address this issue, this study builds a pipeline machine learning model to forecast tunnel-induced damage that can be addressed using the robust optimization (RO) algorithm with high accuracy. The optimization process is integrated into a building information modeling (BIM) platform and analyzed using the Shapley Additive ExPlanations (SHAP) technique, allowing the designer to understand and interact with the algorithm. The average improvement of testing samples using an ellipsoidal uncertainty set with a size of 0.05 is 23.8 and 4.9% on the two selected criteria, which is more conservative than using deterministic optimization (DO) and stochastic optimization (SO). This study establishes an interactive and explainable optimization platform that enables designers to make judgments under the most unfavorable soil conditions with the least amount of accessible information about the uncertainties during tunneling. © 2023"
"We explore using body gestures for hidden emotional state analysis. As an important non-verbal communicative fashion, human body gestures are capable of conveying emotional information during social communication. In previous works, efforts have been made mainly on facial expressions, speech, or expressive body gestures to interpret classical expressive emotions. Differently, we focus on a specific group of body gestures, called micro-gestures (MGs), used in the psychology research field to interpret inner human feelings. MGs are subtle and spontaneous body movements that are proven, together with micro-expressions, to be more reliable than normal facial expressions for conveying hidden emotional information. In this work, a comprehensive study of MGs is presented from the computer vision aspect, including a novel spontaneous micro-gesture (SMG) dataset with two emotional stress states and a comprehensive statistical analysis indicating the correlations between MGs and emotional states. Novel frameworks are further presented together with various state-of-the-art methods as benchmarks for automatic classification, online recognition of MGs, and emotional stress state recognition. The dataset and methods presented could inspire a new way of utilizing body gestures for human emotion understanding and bring a new direction to the emotion AI community. The source code and dataset are made available: https://github.com/mikecheninoulu/SMG. © 2023, The Author(s)."
"Background: Readability metrics provide us with an objective and efficient way to assess the quality of educational texts. We can use the readability measures for finding assessment items that are difficult to read for a given grade level. Hard-to-read math word problems can put some students at a disadvantage if they are behind in their literacy learning. Despite their math abilities, these students can perform poorly on difficult-to-read word problems because of their poor reading skills. Less readable math tests can create equity issues for students who are relatively new to the language of assessment. Less readable test items can also affect the assessment's construct validity by partially measuring reading comprehension. Objectives: This study shows how large language models help us improve the readability of math assessment items. Methods: We analysed 250 test items from grades 3 to 5 of EngageNY, an open-source curriculum. We used the GPT-3 AI system to simplify the text of these math word problems. We used text prompts and the few-shot learning method for the simplification task. Results and Conclusions: On average, GPT-3 AI produced output passages that showed improvements in readability metrics, but the outputs had a large amount of noise and were often unrelated to the input. We used thresholds over text similarity metrics and changes in readability measures to filter out the noise. We found meaningful simplifications that can be given to item authors as suggestions for improvement. Takeaways: GPT-3 AI is capable of simplifying hard-to-read math word problems. The model generates noisy simplifications using text prompts or few-shot learning methods. The noise can be filtered using text similarity and readability measures. The meaningful simplifications AI produces are sound but not ready to be used as a direct replacement for the original items. To improve test quality, simplifications can be suggested to item authors at the time of digital question authoring. © 2023 John Wiley & Sons Ltd."
"We argue that recent advances of artificial intelligence (AI) in the domain of art (e.g., music, painting) pose a profound ontological threat to anthropocentric worldviews because they challenge one of the last frontiers of the human uniqueness narrative: artistic creativity. Four experiments (N = 1708), including a high-powered preregistered experiment, consistently reveal a pervasive bias against AI-made artworks and shed light on its psychological underpinnings. The same artwork is preferred less when labeled as AI-made (vs. human-made) because it is perceived as less creative and subsequently induces less awe, an emotional response typically associated with the aesthetic appreciation of art. These effects are more pronounced among people with stronger anthropocentric creativity beliefs (i.e., who believe that creativity is a uniquely human characteristic). Systematic depreciation of AI-made art (assignment of lower creative value, suppression of emotional reactions) appears to serve a shaken anthropocentric worldview whereby creativity is exclusively reserved for humans. © 2023 The Authors"
"Background: We previously reported that endoscopic response evaluation can preoperatively predict the prognosis and distribution of residual tumors after neoadjuvant chemotherapy (NAC). In this study, we developed artificial intelligence (AI)-guided endoscopic response evaluation using a deep neural network to discriminate endoscopic responders (ERs) in patients with esophageal squamous cell carcinoma (ESCC) after NAC. Method: Surgically resectable ESCC patients who underwent esophagectomy following NAC were retrospectively analyzed in this study. Endoscopic images of the tumors were analyzed using a deep neural network. The model was validated with a test data set using 10 newly collected ERs and 10 newly collected non-ER images. The sensitivity, specificity, positive predictive value (PPV), and negative predictive value (NPV) of the endoscopic response evaluation by AI and endoscopists were calculated and compared. Results: Of 193 patients, 40 (21%) were diagnosed as ERs. The median sensitivity, specificity, PPV, and NPV values for ER detection in 10 models were 60%, 100%, 100%, and 71%, respectively. Similarly, the median values by the endoscopist were 80%, 80%, 81%, and 81%, respectively. Conclusion: This proof-of-concept study using a deep learning algorithm demonstrated that the constructed AI-guided endoscopic response evaluation after NAC could identify ER with high specificity and PPV. It would appropriately guide an individualized treatment strategy that includes an organ preservation approach in ESCC patients. © 2023, Society of Surgical Oncology."
"Peptides have shown increasing advantages and significant clinical value in drug discovery and development. With the development of high-throughput technologies and artificial intelligence (AI), machine learning (ML) methods for discovering new lead peptides have been expanded and incorporated into rational drug design. Predictions of peptide–protein interactions (PepPIs) and protein–protein interactions (PPIs) are both opportunities and challenges in computational biology, which will help to better understand the mechanisms of disease and provide the impetus for the discovery of lead peptides. This paper comprehensively reviews computational models for PepPI and PPI predictions. It begins with an introduction of various databases of peptide ligands and target proteins. Then it discusses data formats and feature representations for proteins and peptides. Furthermore, classical ML methods and emerging deep learning (DL) methods that can be used to train prediction models of PepPI and PPI are classified into four categories, and their advantages and disadvantages are analyzed. To assess the relative performance of different models, different validation protocols and evaluation indexes are discussed. The goal of this review is to help researchers quickly get started to develop computational frameworks using these integrated resources and eventually promote the discovery of lead peptides. © 2023 Wiley-VCH GmbH."
"Deep learning (DL) has been proposed to automate image segmentation and provide accuracy, consistency, and efficiency. Accurate segmentation of lipomatous tumors (LTs) is critical for correct tumor radiomics analysis and localization. The major challenge of this task is data heterogeneity, including tumor morphological characteristics and multicenter scanning protocols. To mitigate the issue, we aimed to develop a DL-based Super Learner (SL) ensemble framework with different data correction and normalization methods. Pathologically proven LTs on pre-operative T1-weighted/proton-density MR images of 185 patients were manually segmented. The LTs were categorized by tumor locations as distal upper limb (DUL), distal lower limb (DLL), proximal upper limb (PUL), proximal lower limb (PLL), or Trunk (T) and grouped by 80%/9%/11% for training, validation and testing. Six configurations of correction/normalization were applied to data for fivefold-cross-validation trainings, resulting in 30 base learners (BLs). A SL was obtained from the BLs by optimizing SL weights. The performance was evaluated by dice-similarity-coefficient (DSC), sensitivity, specificity, and Hausdorff distance (HD95). For predictions of the BLs, the average DSC, sensitivity, and specificity from the testing data were 0.72 ± 0.16, 0.73 ± 0.168, and 0.99 ± 0.012, respectively, while for SL predictions were 0.80 ± 0.184, 0.78 ± 0.193, and 1.00 ± 0.010. The average HD95 of the BLs were 11.5 (DUL), 23.2 (DLL), 25.9 (PUL), 32.1 (PLL), and 47.9 (T) mm, whereas of SL were 1.7, 8.4, 15.9, 2.2, and 36.6 mm, respectively. The proposed method could improve the segmentation accuracy and mitigate the performance instability and data heterogeneity aiding the differential diagnosis of LTs in real clinical situations. © 2023, The Author(s)."
"Customers have since long received service from various machines, and this development is expected to accelerate when AI-powered synthetic agents—such as chatbots and embodied service robots—become more common. Existing research on customers' interactions with service machines is typically focused on perceptions of machine attributes when the machine is busy. However, many machines are idle for a considerable time (i.e., they are not used), and little is known about consumer perceptions of machine idleness—despite the fact that idle machine behavior can contribute to the user experience, too. In the present study, it is assumed that (a) idleness and busyness represent differently valenced states in a human-to-human context (i.e., idleness is more negatively charged than busyness for most humans). It is also assumed that (b) anthropomorphism can occur in relation to a service machine, and that (c) beliefs about idleness and busyness from a human-to-human context can carry over and inform views of machines' minds. Three experiments were conducted to explore these assumptions, and they show that an idle service machine is attributed less positively charged mind states than a busy service machine. The results also show that such attribution activities affect the overall evaluation of the service machine. © 2023 The Authors. Psychology & Marketing published by Wiley Periodicals LLC."
"Renewable energy sources are present copiously in the nature and are good for environmental conservation as they restore themselves and thus have considerable potential in the near future. It is hence important to concentrate on the forecast of these energy sources in order to make effective use of them as soon as possible. This paper is focused primarily on solar energy. There are many approaches that could be applied for the prediction of global solar radiation (GSR). In the field of artificial intelligence (AI), the forecasting of solar resources has moved from conventional mathematical approaches to the use of intelligent techniques. The extent to which data based decisions are made for planning such as judicious and functional for the solar energy sector has been increased to a large extent by this giant step. In modelling challenging and unpredictable connections in between a set of input data and output data along with specific patterns that occur between datasets, AI techniques have demonstrated increasing reliability. In this regard, purpose of this paper is to provide a synopsis of solar energy forecasting methods utilizing machine learning and deep learning approaches to the best of our understanding. © 2023, The Author(s) under exclusive licence to International Center for Numerical Methods in Engineering (CIMNE)."
"The von Neumann computing architecture faces considerable challenges (e.g., high throughput and improving energy efficiency) in developing artificial intelligence (AI) edge devices. In-memory computation (IMC) is a new computing paradigm to improve the energy efficiency and the throughput of dot product operations for AI edge devices. In this paper, a 6T2M hybrid SRAM (HSRAM)-based IMC macro is proposed that supports non-volatile storage and in-memory dot product (IMDP) operation. The HSRAM bit cell is designed using NMOS and memristor devices, which reduces the area overhead and improves the energy efficiency compared to prior SRAM-based IMC macro due to non-volatile storage capability. A 128 x 128 IMC macro based on HSRAM is designed in 65 nm technology. For normal memory operation, the read margin of the proposed HSRAM bit cell is improved by 84.1% compared to 4T2R ReRAM, and the write margin is enhanced by 44.01% compared to 8T SRAM. For IMDP operation, it can compute 128 parallel dot products on binary input and binary weight values with 500 MHz frequency and achieves the energy efficiency of 134.5 TOPS/W at VDD = 1V. According to Monte Carlo simulations, the IMDP operation has a standard deviation of 4.24 percent in accumulation, which equates to a classification accuracy of 96.71% on the MNIST dataset and an 82.51% on the CIFAR-10 dataset. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Quorum sensing (QS) is enabled by the production of signalling molecules called autoinducers, whose presence in the environment modulates the behavior of bacterial populations. Autoinducer-2 (AI-2) is unique as it is recognized by different species of bacteria and fosters signalling across different species. To further understand QS mechanisms and to discover potential novel therapeutics based on the modulation of bacterial behavior many analogs have been synthesized. This review aims to provide a critical analysis of the state-of-the-art of the synthesis of AI-2 and its analogs. It serves as a non-exhaustive consolidation of previous studies of analogs, focusing on the synthetic approaches and the structure-activity relationship conclusions drawn after the analysis of the chemical libraries of the research groups involved in the field. This article is intended to be a strong foundation for the rational planning of synthetic strategies of analogs with specific bioactivities. © 2023 The Authors. Israel Journal of Chemistry published by Wiley-VCH GmbH."
"The deployment of autonomous driving technology is hindered by “corner cases”: unusual nuanced conditions that the self-driving software cannot understand and act fully. We argue that some corner cases originate from a “narrow AI” approach, which lacks the general knowledge that humans exploit when dealing with these cases. We propose an alternative that can be seen as a step toward features of Artificial General Intelligence. We exploit the biological principle of affordance competition in layered control architectures to create an artificial agent that realizes emergent, adaptive, and logical behaviors without programming case-specific rules or algorithms. We give six different examples of simple and complex emergent behaviors. For the case study of merge scenarios, we contrast the approach of this paper with an algorithmic solution of the literature. The ideas presented here (if not the whole agent's sensorimotor organization) could be used to improve the robustness and flexibility of self-driving technology. © 2022 The Author(s)"
"Artificial intelligence (AI) technology is promising in the field of healthcare. With the developments of big data and image-based analysis, AI shows potential value in ophthalmology applications. Recently, machine learning and deep learning algorithms have made significant progress. Emerging evidence has demonstrated the capability of AI in the diagnosis and management of anterior segment diseases. In this review, we provide an overview of AI applications and potential future applications in anterior segment diseases, focusing on cornea, refractive surgery, cataract, anterior chamber angle detection, and refractive error prediction. © 2023, The Author(s)."
"Personalized treatment strategies for cancer frequently rely on the detection of genetic alterations which are determined by molecular biology assays. Historically, these processes typically required single-gene sequencing, next-generation sequencing, or visual inspection of histopathology slides by experienced pathologists in a clinical context. In the past decade, advances in artificial intelligence (AI) technologies have demonstrated remarkable potential in assisting physicians with accurate diagnosis of oncology image-recognition tasks. Meanwhile, AI techniques make it possible to integrate multimodal data such as radiology, histology, and genomics, providing critical guidance for the stratification of patients in the context of precision therapy. Given that the mutation detection is unaffordable and time-consuming for a considerable number of patients, predicting gene mutations based on routine clinical radiological scans or whole-slide images of tissue with AI-based methods has become a hot issue in actual clinical practice. In this review, we synthesized the general framework of multimodal integration (MMI) for molecular intelligent diagnostics beyond standard techniques. Then we summarized the emerging applications of AI in the prediction of mutational and molecular profiles of common cancers (lung, brain, breast, and other tumor types) pertaining to radiology and histology imaging. Furthermore, we concluded that there truly exist multiple challenges of AI techniques in the way of its real-world application in the medical field, including data curation, feature fusion, model interpretability, and practice regulations. Despite these challenges, we still prospect the clinical implementation of AI as a highly potential decision-support tool to aid oncologists in future cancer treatment management. © 2023 The Authors"
"Climate change is one of the challenges that humanity is facing and can have different effects on agricultural production. The effect of climate change is mainly related to the three factors namely increasing carbon dioxide concentration, rising temperature, and changing rainfall patterns. The present study investigated the relationship between grain yield of rainfed and irrigated wheat (Triticum aestivum L.) with climatic variables including minimum and maximum temperature and rainfall in different counties of Tehran, Alborz, Golestan, Kermanshah, and Fars provinces. The drought index (AI) was also calculated to quantify the drought phenomenon. Also, the relationship between drought index and grain yield of rainfed and irrigated wheat was calculated in annual and seasonal dimensions. The results showed that Gorgan (2306.8 kg ha−1) and Larestan (559.3 kg ha−1) have a higher and lower potential for rainfed wheat production compared with other studied areas, respectively. Also, based on the results of the drought index, the climate of Lamerd, Larestan, and Ravansar counties has become drier in the period 1995–2015 and the climate of Karaj has become wetter. The results showed that winter compared with other seasons had the highest drought trend during the studied years. The relationship between rainfed wheat yield and climatic variables in seasonal scale showed that temperature and rainfall in winter and spring had a greater effect on rainfed wheat grain yield in the study areas than other seasons. The relationship between AI and rainfed wheat grain yield showed that areas such as Gorgan, Karaj, and Larestan with increasing drought index value, rainfed wheat grain yield increased. Unlike rainfed wheat, Marvdasht county had the highest potential for irrigated wheat production compared with other areas with the averages of 4481 kg ha−1, while Lamerd had yield of less than 2.7 tons ha−1. The correlation between grain yield and climate variables on seasonal scale showed that the role of temperature was greater than rainfall in irrigated wheat agro-ecosystems. Also, significant correlations between yield and temperature were more in winter and spring seasons than in summer and fall seasons. © 2023, The Author(s), under exclusive licence to Accademia Nazionale dei Lincei."
"Context: In traditional software systems, Requirements Engineering (RE) activities are well-established and researched. However, building Artificial Intelligence (AI) based software with limited or no insight into the system's inner workings poses significant new challenges to RE. Existing literature has focused on using AI to manage RE activities, with limited research on RE for AI (RE4AI). Objective: This paper investigates current approaches for specifying requirements for AI systems, identifies available frameworks, methodologies, tools, and techniques used to model requirements, and finds existing challenges and limitations. Method: We performed a systematic mapping study to find papers on current RE4AI approaches. We identified 43 primary studies and analyzed the existing methodologies, models, tools, and techniques used to specify and model requirements in real-world scenarios. Results: We found several challenges and limitations of existing RE4AI practices. The findings highlighted that current RE applications were not adequately adaptable for building AI systems and emphasized the need to provide new techniques and tools to support RE4AI. Conclusion: Our results showed that most of the empirical studies on RE4AI focused on autonomous, self-driving vehicles and managing data requirements, and areas such as ethics, trust, and explainability need further research. © 2023 Elsevier B.V."
"Annually, one-third of the food produced globally is lost or wasted. A considerable portion of global food waste comprises dry foods that are rejected due to their unattractive appearance. One effective technique to solve this problem is by developing dryers that consistently produce dry foods that are visually appealing and have a long shelf life. The beating heart of such dryers is a computer vision (CV) system that monitors the visual attributes of the food, in real time, during the drying process. Unfortunately, there are currently no real-time CV systems for monitoring the visual attributes of food during fluidized bed drying. This setback is linked to figure-ground separation challenges encountered while segmenting real-time images of the food. Sadly, when current CV systems are used to monitor visual attributes of food during fluidized bed drying, these CV systems fail miserably because they are not designed to account for three major dryer-dependent determinants—the layout, the state and pattern of motion, and the behavior of food materials within the image captured during fluidized bed drying. To solve this lingering problem, this paper reviewed various computer vision systems based on the three determinants. This study revealed that input images for the different CV systems can be categorized as being either static-type images or chaotic-type images. The CV systems were grouped into “Static-input offline CV systems,” “Static-input online CV systems,” and “Chaotic-input online CV systems.” Building on the insight gained while reviewing the three classes of CV systems, two novel AI-driven solutions for monitoring visual attributes of food, in real time, during fluidized bed drying were proposed. The first solution was a “two-pass” deep learning system that predicts visual attributes from segmented results. While the second solution was a “single-pass” deep learning system that by-passes the segmentation step, thus saving computational cost. When such AI-driven solutions are merged with a control system and then integrated with fluidized bed dryers, this union could open the gateway to intelligent drying, where dryers consistently produce high-quality dry foods. By extension, consistency in product quality could reduce global food losses and waste significantly. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Background: Whereas Artificial Intelligence (AI) based tools have recently been introduced in the field of gastroenterology, application in inflammatory bowel disease (IBD) is in its infancies. We established AI-based algorithms to distinguish IBD from infectious and ischemic colitis using endoscopic images and clinical data. Methods: First, we trained and tested a Convolutional Neural Network (CNN) using 1796 real-world images from 494 patients, presenting with three diseases (IBD [n = 212], ischemic colitis [n = 157], and infectious colitis [n = 125]). Moreover, we evaluated a Gradient Boosted Decision Trees (GBDT) algorithm using five clinical parameters as well as a hybrid approach (CNN + GBDT). Patients and images were randomly split into two completely independent datasets. The proposed approaches were benchmarked against each other and three expert endoscopists on the test set. Results: For the image-based CNN, the GBDT algorithm and the hybrid approach global accuracies were.709,.792, and.766, respectively. Positive predictive values were.602,.702, and.657. Global areas under the receiver operating characteristics (ROC) and precision recall (PR) curves were.727/.585,.888/.823, and.838/.733, respectively. Global accuracy did not differ between CNN and endoscopists (.721), but the clinical parameter-based GBDT algorithm outperformed CNN and expert image classification. Conclusions: Decision support systems exclusively based on endoscopic image analysis for the differential diagnosis of colitis, representing a complex clinical challenge, seem not yet to be ready for primetime and more diverse image datasets may be necessary to improve performance in future development. The clinical value of the proposed clinical parameters algorithm should be evaluated in prospective cohorts. © 2023 The Authors. European Journal of Clinical Investigation published by John Wiley & Sons Ltd on behalf of Stichting European Society for Clinical Investigation Journal Foundation."
"As indoor space is the primary place for pedestrian activities, obtaining intelligent monitoring of indoor pedestrians is crucial for intelligent video surveillance. Previous studies have verified the effectiveness of spatiotemporal constraints in multitarget multicamera tracking (MTMCT). Pedestrians are generally subjected to fine spatiotemporal constraints within buildings, based on which the indoor geographic information system (GIS) technology can obtain automatic spatiotemporal modeling. Combined with artificial intelligence (AI) technology, we established a research framework of 'GIS+AI+IMPMCT. ' Specifically, we proposed indoor multipedestrian multicamera tracking (IMPMCT) based on fine spatiotemporal constraints. First, we used GIS to map the indoor monitoring images of buildings and automatically model the fine spatiotemporal relationship among the semantics of the entrance of the surveillance zone. Subsequently, we used the machine learning model of pedestrian localization and tracking to obtain local trajectories of pedestrians and combined them with map information to extract entrance semantics of trajectories. Finally, we used the local trajectory semantics and surveillance entrance semantic constraints to obtain a fine spatiotemporal constraint weight matrix between trajectories and fused pedestrian's apparent features to obtain trajectory matching results. To verify our method, we established an IMPMCT data set containing fine indoor spatiotemporal information. Our method obtained an IDF1 of 0.805, which is better than those of other methods. Furthermore, the tracking results obtained by the proposed method contained both image space and geospatial trajectories.  © 2014 IEEE."
"Resistive random-access memory (RRAM) based non-volatile computing-in-memory (nvCIM) has been regarded as a promising solution to enable efficient data-intensive artificial intelligence (AI) applications on resource-limited edge systems. However, existing weighted-current summation-based nvCIM suffers from device non-idealities and significant time, storage, and energy overheads when processing high-precision analog signals. To address these issues, we propose a 3T2R digital nvCIM macro for a fully hardware-implemented binary convolutional neural network (HBCNN), focusing on accelerating edge AI applications at low weight precision. By quantizing the voltage-division results of RRAMs through inverters, the 3T2R macro provides a stable rail-to-rail output without analog-to-digital converters or sensing amplifiers. Moreover, both batch normalization and sign activation are integrated on-chip. The hybrid simulation results show that the proposed 3T2R digital macro achieves an 86.2% (95.6%) accuracy on the CIFAR-10 (MNIST) dataset, corresponding to a 4.7% (1.9%) accuracy loss compared to the software baselines, which also feature a peak energy efficiency of 51.3 TOPS/W and a minimum latency of 8 ns, realizing an energy-efficient, low-latency, and robust AI processor.  © 2004-2012 IEEE."
"The digital health industry is experiencing fast-paced research which can provide digital care programs and technologies to enhance the competence of healthcare delivery. Orthopedic literature also confirms the applicability of artificial intelligence (AI) and machine learning (ML) models to medical diagnosis and clinical decision-making. However, implant monitoring after primary surgery often happens with a wellness visit or when a patient complains about it. Neglecting implant design and other technical errors in this scenario, unmonitored circumstances, and lack of post-surgery monitoring may ultimately lead to the implant system’s failure and leave us with the only option of high-risk revision surgery. Preventive maintenance seems to be a good choice to identify the onset of an irreversible prosthesis failure. Considering all these aspects for hip implant monitoring, this paper explores existing studies linking ML models and intelligent systems for hip implant diagnosis. This paper explores the feasibility of an alternative continuous monitoring technique for post-surgery implant monitoring backed by an in vitro ML case study. Tribocorrosion and acoustic emission (AE) data are considered based on their efficacy in determining irreversible alteration of implant material to prevent total failures. This study also facilitates the relevance of developing an artificially intelligent implant monitoring methodology that can function with daily patient activities and how it can influence the digital orthopedic diagnosis. Graphical Abstract: AI-based non-invasive hip implant monitoring system enabling point-of-care testing [Figure not available: see fulltext.] © 2023, International Federation for Medical and Biological Engineering."
"The acceptability of artificially intelligent interactive voice response (AI-IVR) systems in cardiovascular research settings is unclear. As a result, we evaluated peoples’ attitudes regarding the Amazon Echo Show 8 device when used for electronic data capture in cardiovascular clinics. Participants were recruited following the Voice-Based Screening for SARS-CoV-2 Exposure in Cardiovascular clinics study. Overall, 215 people enrolled and underwent screening (mean age 46.1; 55% females) in the VOICE-COVID study and 58 people consented to participate in a post-screening survey. Following thematic analysis, four key themes affecting AI-IVR acceptability were identified. These were difficulties with communication (44.8%), limitations with available interaction modalities (41.4%), barriers with the development of therapeutic relationships (25.9%), and concerns with universality and accessibility (8.6%). While there are potential concerns with the use of AI-IVR technologies, these systems appeared to be well accepted in cardiovascular clinics. Increased development of these technologies could significantly improve healthcare access and efficiency. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Differentiation of ductal carcinoma in situ (DCIS, a precancerous lesion of the breast) from fibroadenoma (FA) using ultrasonography is significant for the early prevention of malignant breast tumors. Radiomics-based artificial intelligence (AI) can provide additional diagnostic information but usually requires extensive labeling efforts by clinicians with specialized knowledge. This study aims to investigate the feasibility of differentially diagnosing DCIS and FA using ultrasound radiomics-based AI techniques and further explore a novel approach that can reduce labeling efforts without sacrificing diagnostic performance. We included 461 DCIS and 651 FA patients, of whom 139 DCIS and 181 FA patients constituted a prospective test cohort. First, various feature engineering-based machine learning (FEML) and deep learning (DL) approaches were developed. Then, we designed a difference-based self-supervised (DSS) learning approach that only required FA samples to participate in training. The DSS approach consists of three steps: (1) pretraining a Bootstrap Your Own Latent (BYOL) model using FA images, (2) reconstructing images using the encoder and decoder of the pretrained model, and (3) distinguishing DCIS from FA based on the differences between the original and reconstructed images. The experimental results showed that the trained FEML and DL models achieved the highest AUC of 0.7935 (95% confidence interval, 0.7900–0.7969) on the prospective test cohort, indicating that the developed models are effective for assisting in differentiating DCIS from FA based on ultrasound images. Furthermore, the DSS model achieved an AUC of 0.8172 (95% confidence interval, 0.8124–0.8219), indicating that our model outperforms the conventional radiomics-based AI models and is more competitive. Graphical Abstract: [Figure not available: see fulltext.] © 2023, International Association of Scientists in the Interdisciplinary Areas."
"Artificial neural networks (ANN) are artificial intelligence (AI) techniques used in the automated recognition and classification of pathological changes from clinical images in areas such as ophthalmology, dermatology, and oral medicine. The combination of enterprise imaging and AI is gaining notoriety for its potential benefits in healthcare areas such as cardiology, dermatology, ophthalmology, pathology, physiatry, radiation oncology, radiology, and endoscopic. The present study aimed to analyze, through a systematic literature review, the application of performance of ANN and deep learning in the recognition and automated classification of lesions from clinical images, when comparing to the human performance. The PRISMA 2020 approach (Preferred Reporting Items for Systematic Reviews and Meta-analyses) was used by searching four databases of studies that reference the use of IA to define the diagnosis of lesions in ophthalmology, dermatology, and oral medicine areas. A quantitative and qualitative analyses of the articles that met the inclusion criteria were performed. The search yielded the inclusion of 60 studies. It was found that the interest in the topic has increased, especially in the last 3 years. We observed that the performance of IA models is promising, with high accuracy, sensitivity, and specificity, most of them had outcomes equivalent to human comparators. The reproducibility of the performance of models in real-life practice has been reported as a critical point. Study designs and results have been progressively improved. IA resources have the potential to contribute to several areas of health. In the coming years, it is likely to be incorporated into everyday life, contributing to the precision and reducing the time required by the diagnostic process. © 2023, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine."
"Nowadays, machine learning (ML) and deep learning (DL) methods have become fundamental building blocks for a wide range of AI applications. The popularity of these methods also makes them widely exposed to malicious attacks, which may cause severe security concerns. To understand the security properties of the ML/DL methods, researchers have recently started to turn their focus to adversarial attack algorithms that could successfully corrupt the model or clean data owned by the victim with imperceptible perturbations. In this paper, we study the Label Flipping Attack (LFA) problem, where the attacker expects to corrupt an ML/DL model's performance by flipping a small fraction of the labels in the training data. Prior art along this direction adopts combinatorial optimization problems, leading to limited scalability toward deep learning models. To this end, we propose a novel minimax problem which provides an efficient reformulation of the sample selection process in LFA. In the new optimization problem, the sample selection operation could be implemented with a single thresholding parameter. This leads to a novel training algorithm called Sample Thresholding. Since the objective function is differentiable and the model complexity does not depend on the sample size, we can apply Sample Thresholding to attack deep learning models. Moreover, since the victim's behavior is not predictable in a poisonous attack setting, we have to employ surrogate models to simulate the true model employed by the victim model. Seeing the problem, we provide a theoretical analysis of such a surrogate paradigm. Specifically, we show that the performance gap between the true model employed by the victim and the surrogate model is small under mild conditions. On top of this paradigm, we extend Sample Thresholding to the crowdsourced ranking task, where labels collected from the annotators are vulnerable to adversarial attacks. Finally, experimental analyses on three real-world datasets speak to the efficacy of our method.  © 1979-2012 IEEE."
"Purpose: The purpose of this study was to ascertain how real options investment perspective could be applied towards monetization of customer futures through the deployment of machine learning (ML) and artificial intelligence (AI)-based persuasive technologies. Design/methodology/approach: The authors embarked on a theoretical treatise as advocated by scholars (Cornelissen, 2019; Barney, 2018; Cornelissen, 2017; Smithey Fulmer, 2012; Bacharach, 1989; Whetten, 1989; Weick,1989). Towards this end, theoretical argumentative logic was incrementally used to build an integrated perspective on the deployment of learning and AI-based persuasive technologies. This was carried out with strategic real options investment perspective to secure customer futures on m-commerce apps and e-commerce sites. Findings: M-commerce apps and e-commerce sites have been deploying ML and AI-based tools (referred to as persuasive technologies), to nudge customers for increased and quicker purchase. The primary objective was to increase engagement time of customers (at an individual level), grow the number of customers (at market level) and increase firm revenue (at an organizational level). The deployment of any persuasive technology entailed increased investment (cash outflow) but was also expected to increase the level of revenue and margin (cash inflow). Given the dynamics of market and the emergent nature of persuasive technologies, ascertaining favourable cash flow was challenging. Real options strategy provided a robust theoretical perspective to time the persuasive technology-related investment in stages. This helped managers to be on time with loading customer purchase with increased temporal immediacy. A real options investment space involving six spaces has also been developed in this conceptual work. These were Never Invest, Immediately Investment, Present-day Investment Possibility, Possibly Invest Later, Invest Probably Later and Possibly Never Invest. Research limitations/implications: The foundations of this study domain encompassed work done by an eclectic mix of scholars like from technology management (Siggelkow and Terwiesch, 2019a; Porter and Heppelmann, 2014), real options (Trigeorgis and Reuer, 2017; Luehrman, 1998a, 1998b), marketing intelligence and planning (Appel et al., 2020; Thaichon et al., 2019; Thaichon et al., 2020; Ye et al., 2019) and strategy from a demand positioning school of thought (Adner and Zemsky, 2006). Practical implications: The findings would help managers to comprehend what level of investments need to be done in a staggered manner. The phased way of investing towards the deployment of ML and AI-based persuasive technologies would enable better monetization of customer futures. This would aid marketing managers for increased customer engagement at the individual level, fast monetization of customer futures and increased number of customers and consumption on m-commerce apps and e-commerce sites. Originality/value: This was one of the first studies to apply real options investment perspective towards the deployment of ML and AI-based persuasive technologies for monetizing customer futures. © 2022, Emerald Publishing Limited."
"In the present research, we discuss the temperature evolution of ensemble containing sets of two even–even nuclei which are well separated and are obtained in the neutron-less spontaneous fission of Cf252. We study various sets of two nuclei produced in fission at different temperatures in terms of their statistical and thermodynamical properties. The semiclassical trace formula for spherically symmetric harmonic oscillator potential with spin-orbit interactions has been deployed, which explains the quantum details of the energy level spectra in terms of the corresponding classical periodic orbits. The temperature dependence of charge yields in spontaneous fission of Cf252 is studied in the range T= 0.5 –5 MeV. Transition of fission fragment yields from asymmetric to symmetric components is observed with the increase in temperature. The ensemble of fission fragments is also studied in terms of its Gibbs free energy, internal energy, entropy, excitation energy and heat capacity. The level density parameter as function of fragment mass numbers Ai(i= 1 , 2) is also investigated. © 2022, Indian Association for the Cultivation of Science."
"In this age of the fourth industrial revolution 4.0, the digital world has a plethora of data, including the internet of things, mobile, cybersecurity, social media, forecasts, health data, and so on. The expertise of machine learning and artificial intelligence (AI) is required to soundly evaluate the data and develop related smart and automated applications, These fields use a variety of machine learning techniques including supervised, unsupervised, and reinforcement learning. The objective of the study is to present the role of artificial neural networks and machine learning in utilizing spatial information. Machine learning and AI play an increasingly important role in disaster risk reduction from hazard mapping and forecasting severe occurrences to real-time event detection, situational awareness, and decision assistance. Some of the applications employed in the study to analyze the various ANN domains included weather forecasting, medical diagnosis, aerospace, facial recognition, stock market, social media, signature verification, forensics, robotics, electronics hardware, defense, and seismic data gathering. Machine learning determines the many prediction models for problems involving classification, regression, and clustering using known variables and locations from the training dataset, spatial data that is based on tabular data creates different observations that are geographically related to one another for unknown factors and places. The study presents that the Recurrent neural network and convolutional neural network are the best method in spatial information processing, healthcare, and weather forecasting with greater than 90% accuracy. © 2022, The Author(s), under exclusive licence to Korea Spatial Information Society."
"Programming online judges (POJs) have been increasingly used in CS1 classes, as they allow students to practice and get quick feedback. For instructors, it is a useful tool for creating assignments and exams. However, selecting problems in POJs is time consuming. First, problems are generally not organized based on topics covered in the CS1 syllabus. Second, assessing whether problems require similar effort to be completed and map onto the same topic is a subjective and expert-dependent task. The difficulty increases if the instructor must create variations of these assessments, e.g., to avoid plagiarism. Thus, here, we research how to support CS1 instructors in the task of selecting problems, to compose one-size-fits-all or personalized assignments/exams. Our solution is to propose a novel intelligent recommender system, based on a fine-grained data-driven analysis of the students' effort on solving problems in the integrated development environment of a POJ system, and automatic detection of topics for CS1 problems, based on problem descriptions. Data collected from 2714 students are processed to support, via our artificial intelligence (AI) method recommendations, the instructors' decision-making process. We evaluated our method against the state of the art in a simple blind experiment with CS1 instructors ($N =$ 35). Results show that our recommendations are 88% accurate, surpassing our baseline ($p<$ 0.05). Finally, our work paves the way for novel POJ smart learning environments, wherein instructors define learning tasks (assignments/exams) supported by AI.  © 2008-2011 IEEE."
"The fashion industry is at the brink of radical transformation. The emergence of Artificial Intelligence (AI) in fashion applications creates many opportunities for this industry and make fashion a better space for everyone. Interesting to this matter, we proposed a virtual try-on interface to stimulate consumers purchase intentions and facilitate their online buying decision process. Thus, we present, in this paper, our flexible person generation system for virtual try-on that aiming to treat the task of human appearance transfer across images while preserving texture details and structural coherence of the generated outfit. This challenging task has drawn increasing attention and made huge development of intelligent fashion applications. However, it requires different challenges, especially in the case of a wide divergences between the source and target images. To solve this problem, we proposed a flexible person generation framework called Dress-up to treat the 2D virtual try-on task. Dress-up is an end-to-end generation pipeline with three modules based on the task of image-to-image translation aiming to sequentially interchange garments between images, and produce dressing effects not achievable by existing works. The core idea of our solution is to explicitly encode the body pose and the target clothes by a pre-processing module based on the semantic segmentation process. Then, a conditional adversarial network is implemented to generate target segmentation feeding respectively, to the alignment and translation networks to generate the final output results. The novelty of this work lies in realizing the appearance transfer across images with high quality by reconstructing garments on a person in different orders and looks from simlpy semantic maps and 2D images without using 3D modeling. Our system can produce dressing effects and provide significant results over the state-of-the-art methods on the widely used DeepFashion dataset. Extensive evaluations show that Dress-up outperforms other recent methods in terms of output quality, and handles a wide range of editing functions for which there is no direct supervision. Different types of results were computed to verify the performance of our proposed framework and show that the robustness and effectiveness are high by utilizing our method. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"This paper presents a proposal of specific curriculum in Artificial Intelligence (AI) for high school students, which has been organized as a two-year subject. The curriculum was designed based on two premises. The first one is that, although the proposal is targeted to scientific programmes, the involved students and teachers do not have any previous knowledge about AI. Accordingly, the teaching units have been designed with the aim of supporting teachers in a new discipline for them and, in addition, providing an introductory level to students. The main didactical objective is to establish the fundamentals of AI from a practical perspective, learning technical concepts by using them to solve specific problems. The approach that has been followed in the teaching units is focused on developing embedded intelligence solutions, that is, programming real-world devices which interact with real environments. To this end, and to address a second fundamental premise of low investment capability at schools, it has been decided to use Smartphones as the central technological element to implement such embedded intelligence at classes. This curriculum has been developed within the Erasmus + project entitled ""AI + : Developing an Artificial Intelligence Curriculum adapted to European High School"". The project was carried out by a team of AI experts and high school teachers who created the teaching units, and a group of students that tested them for three years, providing feedback to make the curriculum feasible for its introduction in schools in the short-term. The main results obtained from its implementation within the project scope are presented and discussed here, with the aim of contributing to the AIEd community progress by means of a practical pilot experience. Although the curriculum has been designed and tested at European level, it has been created with a general perspective of AI education, so it can be applied worldwide. © 2022, The Author(s)."
"With the continuous development of Industry 4.0 technology, the embedded devices in Industrial Internet of Things (IIoT) are showing explosive growth, and large-scale cyber attacks or related security incidents continue to sound the alarm bell of information security. IIoT has strict requirements on computing performance and energy consumption, which poses severe challenges to cryptographic algorithms, especially public key cryptographic algorithms with high computational complexity. Embedded graphic processing unit (GPU) devices, always as edge computing nodes or AI accelerators, are widely deployed in IIoT applications. In this article, we propose an embedded GPU-based Four Q (EG-Four Q) elliptic curve public key cryptographic acceleration scheme. As far as we know, EG-Four Q is the first work to completely implement Four Q on the GPU platforms, including finite field operations, point arithmetic, and scalar multiplication. Relying only on 36-W power consumption, our scalar multiplication performance reaches 1717 kops/s with the latency of 2.38 ms. In terms of the energy-efficiency ratio, EG-Four Q has significant advantages over other platforms such as advanced RISC machines (ARM) CPU, Intel CPU, field programmable gate array (FPGA), and desktop GPUs. The throughput of EG-Four Q is 1.75 times that of the fastest elliptic curve cryptography implementation based on the same platform and even exceeds the performance of Intel top server CPU E5-2699v3 (18-core). Based on the embedded GPU Xavier, EG-Four Q can act as a cryptographic edge computing module or even a cloud cryptographic accelerator, providing more efficient elliptic curve cryptographic services for IIoT.  © 2005-2012 IEEE."
[No abstract available]
"Pattern discovery in multidimensional data sets has been the subject of research for decades. There exists a wide spectrum of clustering algorithms that can be used for this purpose. However, their practical applications share a common post-clustering phase, which concerns expert-based interpretation and analysis of the obtained results. We argue that this can be the bottleneck in the process, especially in cases where domain knowledge exists prior to clustering. Such a situation requires not only a proper analysis of automatically discovered clusters but also conformance checking with existing knowledge. In this work, we present Knowledge Augmented Clustering (KnAC). Its main goal is to confront expert-based labelling with automated clustering for the sake of updating and refining the former. Our solution is not restricted to any existing clustering algorithm. Instead, KnAC can serve as an augmentation of an arbitrary clustering algorithm, making the approach robust and a model-agnostic improvement of any state-of-the-art clustering method. We demonstrate the feasibility of our method on artificially, reproducible examples and in a real life use case scenario. In both cases, we achieved better results than classic clustering algorithms without augmentation. © 2022, The Author(s)."
"Aspergillus flavus and Aspergillus parasiticus are among the few aspergilli that produce aflatoxins and cause aflatoxin-related toxicity. To protect against these dangers, it is crucial to produce plant-based solutions that not only protect the balance of the ecosystem but also the health of living beings using natural methods. In this study, the effects of different doses of A. aestivus, B. vulgaris, and M. alba leaf extracts on the development and reproduction of A. flavus and A. parasiticus fungi were investigated and an experimental study was conducted. The inhibition rates of A. aestivus, B. vulgaris, and M. alba extracts were calculated by applying them to petri dishes containing A. flavus and A. parasiticus fungi at concentrations of 1, 5, 10, 15, 20, 25, and 30 mg/ml. As a result of the research, the inhibition rates of A. aestivus, B. vulgaris, and M. alba extracts were found to be between 3.13–89.94%, 2.63–76.3%, and 2.29–73.9%, respectively. In terms of the lethal dose, the plant with the highest amount of development-inhibiting active ingredient per unit was A. aestivus with a concentration of 22 mg/ml and an inhibition rate of 50.8%. B. vulgaris and M. alba also showed similar effects with inhibition rates of 50.33% and 50.14% at concentrations of 26 mg/ml, respectively. An artificial intelligence model has been developed to estimate the antifungal effect on doses that have not been measured. In the development of the model, training was performed using the decision tree regressor (DT), extra trees regressor (ET), random forest regressor (RF), gradient boosting regressor (GBR), light gradient boosting machine (LIGHTGBM), K-nearest neighbors regressor (K-NN), AdaBoost regressor (ADA), ridge regression (RIDGE), least angle regression (LAR), and Bayesian ridge (BR) algorithms. The successful algorithm was evaluated according to the performance criteria of mean square error (MSE), mean absolute error (MAE), root mean square error (RMSE), and coefficient of determination R2, and the K-NN model with an R2 value of 0.96 was found to be the most successful. The K-NN model was used to estimate the inhibition percentage values of the unmeasured leaf extract doses with the least error in the 1–30 mg/ml dose range. With this expert model, the development of organic fungicides with varying dose and leaf types can be performed much faster. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
"Light quality is important for microalgal biomass and bio-component accumulation. In this study, 8 kinds of combined monochromatic red (R) and blue light (B) were employed to grow Phaeodactylum tricornutum using white light (W) as a control. The results indicated that P. tricornutum had the highest specific growth rate under white light, reaching 0.151 (day−1), and the highest biomass (dry weight), reaching 302.77 mg L−1. The red light and 2R5B were the best for P. tricornutum producing carotenoids and protein, respectively. Carbohydrate was not significantly affected by light quality. The lipid content and lipid production under 5R2B were the highest, reaching 27.85% (per dry weight) and 75.56 mg/L (per culture), respectively. The best production of palmitic acid (C16:0), palmitoleic acid (C16:1 (n7)), and eicosapentaenoic acid (EPA, C20:5 (n3)) was observed at 4R3B, red light, and 1R6B respectively. The highest proportions per total fatty acids of C16:0, C16:1 (n7), and EPA were determined as 26.57%, 60.59%, and 13.87%, which were 15.71%, 11.37%, and 29.49% higher than those under white light, respectively. Compared with white light, cells that were grown under blue light, red light, and 1R6B revealed the improved lipid nutrition quality, reduced AI and TI values, increased HI (h/H) values, and increased n3:n6 ratios (under 1R6B only). Combined monochromatic light (except 1R6B) produced biodiesel with higher CN values, lower DU and IV values, and higher oxidation stability, but slightly reduced fluidity at low temperatures. The manipulated spectrum or light quality is a promising strategy to regulate the product property of P. tricornutum cultures. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
"In sports data analysis and visualization, understanding collective tactical behavior has become an integral part. Interactive and automatic data analysis is instrumental in making use of growing amounts of compound information. In professional team sports, gathering and analyzing sportsperson monitoring data are common practice, intending to evaluate fatigue and succeeding adaptation responses, analyze performance potential, and reduce injury and illness risk. Data visualization technology born in the era of big data analytics provides a good foundation for further developing fitness tools based on artificial intelligence (AI). Hence, this study proposed a video-based effective visualization framework (VEVF) based on artificial intelligence and big data analytics. This study uses the machine learning method to categorize the sports video by extracting both the videos' temporal and spatial features. Our system is based on convolutional neural networks united with temporal pooling layers. The experimental outcomes demonstrate that the recommended VEVF model enhances the accuracy ratio of 98.7%, recall ratio of 94.5%, F1-score ratio of 97.9%, the precision ratio of 96.7%, the error rate of 29.1%, the performance ratio of 95.2%, an efficiency ratio of 96.1% compared to other existing models. © 2021, The Author(s)."
"Predictably, the upcoming six generation (6G) networks demand ultra-massive interconnectivity comprising densely congested sustainable small-to-tiny networks. The conventional radio access network (RAN) will be redesigned to provide the necessary intelligence in all areas to meet required network flexibility, full coverage, and massive access. In this respect, this paper focuses on intelligent massive RAN (mRAN) architecture and key technologies fulfilling the requirements. Particularly, we investigate potential artificial intelligence algorithms for network and resource management issues in 6G mRAN. Furthermore, we summarize the research issues in edge technologies and physical layer intelligence on 6G network architecture. © 2022 The Author(s)"
"Random numbers are essential to most computer applications. Still, producing high-quality random sequences is a big challenge. Inspired by the success of artificial neural networks and reinforcement learning, we propose a novel and effective end-to-end learning system to generate pseudorandom sequences that operates under the upside-down reinforcement learning framework. It is based on manipulating the generalized information entropy metric to derive commands that instantaneously guide the agent toward the optimal random behavior. Using a wide range of evaluation tests, the proposed approach is compared against three state-of-the-art accredited pseudorandom number generators (PRNGs). The experimental results agree with our theoretical study and show that the proposed framework is a promising candidate for a wide range of applications.  © 2020 IEEE."
"Natural language processing (NLP) offers many opportunities in Nuclear Cardiology. These opportunities include applications in converting nuclear cardiology imaging reports to digital searchable information that may be used as Big Data for machine learning and registries. Another major NLP application is, with the support of AI, in automatically translating MPI image features directly into nuclear cardiology reports. This review describes the symbiotic relationship between AI and NLP in that NLP is being used to facilitate AI applications and, AI techniques are being used to facilitate NLP. This article reviews the fundamentals of NLP and describes various conventional and AI techniques that have been applied in imaging. Key nuclear cardiology applications are reviewed such as conversion of MPI free-text reports to digital documents as well as direct conversion of MPI images into structured medical reports. © 2022, The Author(s) under exclusive licence to American Society of Nuclear Cardiology."
"Nowadays Artificial Intelligence (AI) has become a fundamental component of healthcare applications, both clinical and remote, but the best performing AI systems are often too complex to be self-explaining. Explainable AI (XAI) techniques are defined to unveil the reasoning behind the system’s predictions and decisions, and they become even more critical when dealing with sensitive and personal health data. It is worth noting that XAI has not gathered the same attention across different research areas and data types, especially in healthcare. In particular, many clinical and remote health applications are based on tabular and time series data, respectively, and XAI is not commonly analysed on these data types, while computer vision and Natural Language Processing (NLP) are the reference applications. To provide an overview of XAI methods that are most suitable for tabular and time series data in the healthcare domain, this paper provides a review of the literature in the last 5 years, illustrating the type of generated explanations and the efforts provided to evaluate their relevance and quality. Specifically, we identify clinical validation, consistency assessment, objective and standardised quality evaluation, and human-centered quality assessment as key features to ensure effective explanations for the end users. Finally, we highlight the main research challenges in the field as well as the limitations of existing XAI methods. © 2022, The Author(s)."
"Purpose: To investigate the value of the combined diagnosis of multiparametric MRI-based deep learning models to differentiate triple-negative breast cancer (TNBC) from fibroadenoma magnetic resonance Breast Imaging-Reporting and Data System category 4 (BI-RADS 4) lesions and to evaluate whether the combined diagnosis of these models could improve the diagnostic performance of radiologists. Methods: A total of 319 female patients with 319 pathologically confirmed BI-RADS 4 lesions were randomly divided into training, validation, and testing sets in this retrospective study. The three models were established based on contrast-enhanced T1-weighted imaging, diffusion-weighted imaging, and T2-weighted imaging using the training and validation sets. The artificial intelligence (AI) combination score was calculated according to the results of three models. The diagnostic performances of four radiologists with and without AI assistance were compared with the AI combination score on the testing set. The area under the curve (AUC), sensitivity, specificity, accuracy, and weighted kappa value were calculated to assess the performance. Results: The AI combination score yielded an excellent performance (AUC = 0.944) on the testing set. With AI assistance, the AUC for the diagnosis of junior radiologist 1 (JR1) increased from 0.833 to 0.885, and that for JR2 increased from 0.823 to 0.876. The AUCs of senior radiologist 1 (SR1) and SR2 slightly increased from 0.901 and 0.950 to 0.925 and 0.975 after AI assistance, respectively. Conclusion: Combined diagnosis of multiparametric MRI-based deep learning models to differentiate TNBC from fibroadenoma magnetic resonance BI-RADS 4 lesions can achieve comparable performance to that of SRs and improve the diagnostic performance of JRs. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"With the breakthrough of AlphaGo, human-computer gaming AI has ushered in a big explosion, attracting more and more researchers all over the world. As a recognized standard for testing artificial intelligence, various human-computer gaming AI systems (AIs) have been developed, such as Libratus, OpenAI Five, and AlphaStar, which beat professional human players. The rapid development of human-computer gaming AIs indicates a big step for decision-making intelligence, and it seems that current techniques can handle very complex human-computer games. So, one natural question arises: What are the possible challenges of current techniques in human-computer gaming and what are the future trends? To answer the above question, in this paper, we survey recent successful game AIs, covering board game AIs, card game AIs, first-person shooting game AIs, and real-time strategy game AIs. Through this survey, we 1) compare the main difficulties among different kinds of games and the corresponding techniques utilized for achieving professional human-level AIs; 2) summarize the mainstream frameworks and techniques that can be properly relied on for developing AIs for complex human-computer games; 3) raise the challenges or drawbacks of current techniques in the successful AIs; and 4) try to point out future trends in human-computer gaming AIs. Finally, we hope that this brief review can provide an introduction for beginners and inspire insight for researchers in the field of AI in human-computer gaming. © 2023, The Author(s)."
"Rationale and Objectives: We sought to determine the perceived impact of artificial intelligence (AI) and other emerging technologies (ET) on various specialties by medical students in both 2017 and 2021 and how this might affect their residency selections. Materials and Methods: We conducted a brief, anonymous survey of all medical students at a single institution in 2017 and 2021. Survey questions evaluated (1) incentives motivating residency selection and career path, (2) degree of interest in each specialty, (3) perceived effect that ET will have on job prospects for each specialty, and (4) those specialties that students would not consider because of concerns regarding ET. Results: A total of 72% (384/532) and 54% (321/598) of medical students participated in the survey in 2017 and 2021, respectively, and results were largely stable. Students perceived ET would reduce job prospects for pathology, diagnostic radiology, and anesthesiology, and enhance prospects for all other specialties (p < 0.01) except dermatology. For both surveys, 23% of students would NOT consider diagnostic radiology because ET would make it obsolete, higher than all other specialties (p < 0.01). Regarding the one student class that was surveyed twice, 50% felt ET would reduce job prospects for radiology in 2017, increasing to 71% in 2021 (p < 0.01), and similar percentages—20% in 2017 and 23% in 2021—said they explicitly would not consider radiology because of concerns levied by ET. Conclusions: Current perceptions of ET likely affect residency selection for a large proportion of medical students and may impact the future of various specialties, particularly diagnostic radiology. © 2022 The Association of University Radiologists"
"Recent advances in drone and artificial intelligence (AI) technologies have enabled many innovative applications, such as package delivery, reconnaissance, and search and rescue, to name a few. In this article, we propose AI wings, an artificial intelligence of things (AIoT) drone system for commanding multiple unmanned aerial vehicles and deploying AI models. We integrated ArduPilot with the Android mobile platform, which equips DIY drones with AI computing power and 4G/5G connectivity. Embedded control software is developed to cooperate with the AI Wings cloud. Users can easily convert ArduPilot drones into AIoT drones using Android phones, and connect to a cloud server to create their own Internet of Drones. Our cloud server is also integrated with the drone simulation software AirSim for simulating drone missions in virtual reality (VR) worlds. The virtual simulation enables users to test software/hardware configurations as well as train AI models. Moreover, to ensure secure communication, we propose an authentication protocol based on elliptic-curve cryptography with pseudoidentities and time freshness check. In summary, AI Wings provides a cloud server for commanding drone fleets securely, software/hardware design for AIoT drones, and VR simulation for training and testing AI models. Users can install the AI models on the drones directly. To test the system, we built an experimental medical drone service, which delivers an automated external defibrillator to people with a sudden cardiac attack in the shortest time possible.  © 2007-2012 IEEE."
"IT systems monitoring is a crucial process for managing and orchestrating network resources, allowing network providers to rapidly detect and react to most impediment causing network degradation. However, the high growth in size and complexity of current operational networks (2022) demands new solutions to process huge amounts of data (including alarms) reliably and swiftly. Further, as the network becomes progressively more virtualized, the hosting of NFV on cloud environments adds a magnitude of possible bottlenecks outside the control of the service owners. In this paper, we propose two deep learning anomaly detection solutions that leverage service exposure and apply it to automate the detection of service degradation and root cause discovery in a cloudified mobile network that is orchestrated by ETSI OSM. A testbed is built to validate these AI models. The testbed collects monitoring data from the OSM monitoring module, which is then exposed to the external AI anomaly detection modules, tuned to identify the anomalies and the network services causing them. The deep learning solutions are tested using various artificially induced bottlenecks. The AI solutions are shown to correctly detect anomalies and identify the network components involved in the bottlenecks, with certain limitations in a particular type of bottlenecks. A discussion of the right monitoring tools to identify concrete bottlenecks is provided.  © 2004-2012 IEEE."
"American Indian (AI) mothers experience high rates of postpartum depression (PPD). We evaluated the factor structure of the Edinburgh Postnatal Depression Scale (EPDS) among AI mothers from a rural AI serving health system. We also investigated potential associations between EPDS scores and selected psychosocial factors (n = 315). Exploratory Factor Analysis (n = 157) showed that a one-factor structure best fits the data. A Confirmatory Factor Analysis was then conducted to examine the fit of the one-factor model (n = 158). Goodness-of-fit statistics showed overall poor model fit (RMSEA =.13) which may be suggestive of an indicator of depression among Natives not detected by the EPDS. Results of the multiple regression analysis were non-significant. The findings demonstrated that while the EPDS measured aspects of PPD, there may be additional aspects of depression specific to the AI women in our sample not captured by the EPDS. Limitations and directions for future research are discussed. © 2022, This is a U.S. Government work and not under copyright protection in the US; foreign copyright protection may apply."
"The growing computing power, easy acquisition of large-scale data, and constantly improved algorithms have led to a new wave of artificial intelligence (AI) applications, which change the ways we live, manufacture, and do business. Along with this development, a rising concern is the relationship between AI and human intelligence, namely, whether AI systems may one day overtake, manipulate, or replace humans. In this paper, we introduce a novel concept named hybrid human-Artificial intelligence (H-AI), which fuses human abilities and AI capabilities into a unified entity. It presents a challenging yet promising research direction that prompts secure and trusted AI innovations while keeping humans in the loop for effective control. We scientifically define the concept of H-AI and propose an evolution road map for the development of AI toward H-AI. We then examine the key underpinning techniques of H-AI, such as user profile modeling, cognitive computing, and human-in-The-loop machine learning. Afterward, we discuss H-Al's potential applications in the area of smart homes, intelligent medicine, smart transportation, and smart manufacturing. Finally, we conduct a critical analysis of current challenges and open gaps in H-AI, upon which we elaborate on future research issues and directions.  © 1996-2012 Tsinghua University Press."
"Purpose: Previous studies have shown a wide range of efficacy (29 to 71%) of a mandibular advancement device (MAD) in the treatment of obstructive sleep apnea (OSA). Currently, the ability to preselect suitable patients for MAD therapy based on individual characteristics related to upper airway collapsibility is limited. We investigated if the use of non-custom interim MAD during drug-induced sleep endoscopy (DISE) could be a valuable screening tool to predict MAD treatment outcome. Methods: In a single-center prospective study including a consecutive series of patients with OSA, we compared DISE outcomes with a MAD in situ with polysomnography results after 3 months of using the same MAD that was used during DISE. Results: Of 41 patients who completed the study, the median apnea–hypopnea index (AHI) was 16.0 events/h [IQR 7.4–23.4]. Respiratory outcomes on polysomnography, including apnea index (AI), total AHI, AHI in supine position, and oxygen desaturation index, all significantly improved after 3 months of MAD treatment. With complete improvement of the upper airway obstruction with the MAD in situ during DISE in supine position, patients were 6.3 times more likely to be a responder to MAD treatment compared to patients with a persisting complete obstruction, although not statistically significant (OR 6.3; 95%CI 0.9–42.7; p = 0.060). Conclusion: The potential predictive value with regard to MAD therapy outcomes of the use of an interim MAD during DISE would be an important finding, since the prediction of MAD therapy outcome is of great clinical and scientific interest. A study with a larger cohort should be performed to further investigate our findings. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
"Are wealthy neighborhoods visually more attractive than poorer neighborhoods? Past studies provided a positive answer to this question for characteristics such as green space and visible pollution. The condition of streets is one of the characteristics that can not only contribute to neighborhoods’ aesthetics, but can also affect residents’ health and mobility. In this study, we investigate whether street condition of wealthy neighborhoods is different from poorer neighborhoods. We resolved the difficulty of data collection using a dataset that utilized artificial intelligence and laser imaging techniques to collect the data of street condition from 98 zip codes in Los Angeles, CA, and later, we conducted correlations between the metrics of neighborhood affluence and their street condition. Our results showed no positive correlation between neighborhood affluence and the condition of streets. On the contrary, the results favored a negative correlation, indicating that poorer neighborhoods had better streets than wealthy neighborhoods. By discussing possible reasons for these results, we call for future research that would show the direction and the extent of correlation between neighborhood affluence and street condition for other cities and in larger scales. Additionally, we discuss how artificial intelligence and automatic data collection techniques enable us to gather data of street condition for urban planning and management. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
"Cities today are dynamic urban ecosystems with evolving physical, socio-cultural, and technological infrastructures. Many contestations arise from the effects of inequitable access and intersecting crises currently faced by cities, which may be amplified by the algorithmic and data-centric infrastructures being introduced in urban contexts. In this article, I argue for a critical lens into how inter-related urban technologies, big data and policies, constituted as Urban AI, offer both challenges and opportunities. I examine scenarios of contestations in urban mobility, defined broadly to include equitable access, movement, and liberty to engage with the socio-cultural, political, and urban fabric of cities. I anchor my arguments through a framework of rights, risks, and responsibilities for critically examining and configuring the roles, values and ethical implications for all stakeholders including human, AI and non-human entities within an urban ecosystem. As a way forward, I examine the European Commission’s proposed regulations on AI systems through an illustrative case study of an automated parking control system introduced by the City of Amsterdam. In moving beyond the city to broader urban ecosystems, I highlight the role of engaging Indigenous perspectives for designing and reconciling the implications of equitable and sustainable Urban AI ecosystems in the future. © 2022, The Author(s)."
[No abstract available]
"Grading assignments is inherently subjective and time-consuming; automatic scoring tools can greatly reduce teacher workload and shorten the time needed for providing feedback to learners. The purpose of this paper is to propose a novel method for automatically scoring student responses to picture-cued writing tasks. As a popular paradigm for language instruction and assessment, a picture-cued writing task typically requires students to describe a picture or pictures. Correspondingly, the automatic scoring methods must measure the link(s) between visual pictures and their textual descriptions. For this purpose, we first designed a picture-cued writing test and collected nearly 4 k responses from 279 K12 students. Based on these responses, we then developed an AI scoring model by incorporating the emerging cross-modal matching technology and some NLP algorithms. The performance of the model was evaluated carefully with six popular measures and was found to demonstrate accurate scoring results with a small mean absolute error of 0.479 and a high adjacent-agreement rate of 90.64%. We believe this method could reduce the subjective elements inherent in human grading and save teachers’ time from the mundane task of grading to other valuable endeavors such as designing teaching plans based on AI-generated diagnosis of student progress. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Traditional AI techniques for offline misuse network intrusion detection have performed well, assuming that the traffic from the datasets is sufficiently large for generalization, balanced, independently and identically distributed—exhibiting homogeneous behavior with little to no context change. However, the rapidly expanding IoT network is an ensemble of proliferating internet-connected devices catering to the growing need for handling highly distributed, heterogeneous, and time-critical workloads that conform to none of the above assumptions. Moreover, the evolving Botnet-based attack vectors exploit the non-standardized and poorly scrutinized architectural vulnerabilities of such devices—leading to compounding threat intensity, rapidly rendering the network defenseless. Furthermore, the memory, processor, and energy resource constraints of the IoT devices necessitate lightweight device-specific intrusion detection policies for effective and updated rule learning in real-time through the edge infrastructures. However, the existing methods proposed to solve such issues are either centralized, data and resource-intensive, context-unaware, or inefficient for online rule learning with smaller and imbalanced data samples. Thus, this paper addresses such issues through a context-aware expert system-based feature subset framework with minimal processing overhead and a decentralized on-device misuse detection scheme for IoT—called HetIoT-NIDS, capable of efficiently inferring over smaller data samples, tolerant to class imbalance, and deployable on the low-memory and low-power edge of IoT devices. Furthermore, HetIoT-NIDS facilitates threat localization within the deployed device, preventing threat progression and intensity compounding. The experiments and analyses of the propounded algorithms and the resulting training times and model sizes prove that the proposed approach is efficient and adaptable to online and offline misuse intrusion detection, especially on smaller data sample sizes. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"This paper utilizes artificial intelligence (AI) techniques to address the trajectory tracking problem of a continuous polymerization reactor with unknown dynamics and unknown asymmetric input dead-zone nonlinearities. A backstepping fractional sliding mode control (BFOSMC) has been proposed to force the average molecular weight (AMW) and the reactor temperature to track the desired trajectories. Due to the simplicity, easy implementation, and robustness of particle swarm optimization (PSO) technique, it is used to optimize the parameters of the controller. The unknown dynamics of the reactor are approximated with feed-forward neural networks (NN). The input dead-zones are estimated and compensated by adaptive offset terms in the control laws. A Lyapunov theory is employed to prove the stability of the NNBFOSMC. The simulation study verifies that our AI-based algorithm resulted in a better performance. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Artificial Intelligence (AI) is revolutionizing many industries and becoming increasingly ubiquitous in everyday life. To empower children growing up with AI to navigate society’s evolving sociotechnical context, we developed three middle school AI literacy curricula: Creative AI, Dancing with AI, and How to Train Your Robot. In this paper we discuss how we leveraged three design principles—active learning, embedded ethics, and low barriers to access – to effectively engage students in learning to create and critique AI artifacts. During the summer of 2020, we recruited and trained in-service, middle school teachers from across the United States to co-instruct online workshops with students from their schools. In the workshops, a combination of hands-on unplugged and programming activities facilitated students’ understanding of AI. As students explored technical concepts in tandem with ethical ones, they developed a critical lens to better grasp how AI systems work and how they impact society. We sought to meet the specified needs of students from a range of backgrounds by minimizing the prerequisite knowledge and technology resources students needed to participate. Finally, we conclude with lessons learned and design recommendations for future AI curricula, especially for K-12 in-person and virtual learning. © 2022, The Author(s)."
"Using computer vision through artificial intelligence (AI) is one of the main technological advances in dentistry. However, the existing literature on the practical application of AI for detecting cephalometric landmarks of orthodontic interest in digital images is heterogeneous, and there is no consensus regarding accuracy and precision. Thus, this review evaluated the use of artificial intelligence for detecting cephalometric landmarks in digital imaging examinations and compared it to manual annotation of landmarks. An electronic search was performed in nine databases to find studies that analyzed the detection of cephalometric landmarks in digital imaging examinations with AI and manual landmarking. Two reviewers selected the studies, extracted the data, and assessed the risk of bias using QUADAS-2. Random-effects meta-analyses determined the agreement and precision of AI compared to manual detection at a 95% confidence interval. The electronic search located 7410 studies, of which 40 were included. Only three studies presented a low risk of bias for all domains evaluated. The meta-analysis showed AI agreement rates of 79% (95% CI: 76–82%, I2 = 99%) and 90% (95% CI: 87–92%, I2 = 99%) for the thresholds of 2 and 3 mm, respectively, with a mean divergence of 2.05 (95% CI: 1.41–2.69, I2 = 10%) compared to manual landmarking. The menton cephalometric landmark showed the lowest divergence between both methods (SMD, 1.17; 95% CI, 0.82; 1.53; I2 = 0%). Based on very low certainty of evidence, the application of AI was promising for automatically detecting cephalometric landmarks, but further studies should focus on testing its strength and validity in different samples. © 2022, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine."
"The management of customer services by telephone encounters several problems: an uncontrollable flow of calls, complicated resource management, a very high cost of service, and more. Opportunities to improve the quality of service, save time and money triggered the widespread implementation of artificial intelligence (AI) based callbot. This article outlines the straightforward workflow developed to model the architecture of the callbot. Therefore, several algorithms were evaluated and compared based on real knowledge of a call center of an insurance society. The algorithms considered are: k-nearest neighbours (KNN), support vector machine (SVM), random forests (RF), logistic regression (LR), and Naïve Bayes (NB). The comparison criteria are: correct responses, response time, accuracy, Cohen’s kappa and F1 score using n-gram (1.1) and (2.2). The results obtained show that the SVM (accuracy=70.29%) presents the best results on all the comparison criteria. The comparison between the results of the human agents and the callbot shows an improvement in several levels: the cost savings are greater than 80% on all the tests carried out, the holding time decrease to 0 seconds, and the processing time (almost a third or more). The results obtained sufficiently meet the objectives of this project. © 2023, Institute of Advanced Engineering and Science. All rights reserved."
"Artificial Intelligence (AI) often outperforms human doctors in terms of decisional speed. For some diseases, the expected benefit of a fast but less accurate decision exceeds the benefit of a slow but more accurate one. In such cases, we argue, it is often justified to rely on a medical AI to maximise decision speed – even if the AI is less accurate than human doctors. © The Author(s) 2022."
"This report describes a teaching experience with undergraduates to approach, in a simple and practical way, artificial intelligence (AI) and machine learning (ML) – general-purpose technologies that are highly demanded in any industry today. The article shows how business undergraduates with no prior experience in coding can use AI and ML to solve business-related problems, particularly human resource management (HRM) problems. AI and ML are powerful technologies that can improve the analytical skills and employability of those who understand them. © The Author(s) 2022."
"In theory, building automation and management systems (BAMSs) can provide all the components and functionalities required for analyzing and operating buildings. However, in reality, these systems can only ensure the control of heating ventilation and air conditioning system systems. Therefore, many other tasks are left to the operator, e.g. evaluating buildings’ performance, detecting abnormal energy consumption, identifying the changes needed to improve efficiency, ensuring the security and privacy of end-users, etc. To that end, there has been a movement for developing artificial intelligence (AI) big data analytic tools as they offer various new and tailor-made solutions that are incredibly appropriate for practical buildings’ management. Typically, they can help the operator in (i) analyzing the tons of connected equipment data; and; (ii) making intelligent, efficient, and on-time decisions to improve the buildings’ performance. This paper presents a comprehensive systematic survey on using AI-big data analytics in BAMSs. It covers various AI-based tasks, e.g. load forecasting, water management, indoor environmental quality monitoring, occupancy detection, etc. The first part of this paper adopts a well-designed taxonomy to overview existing frameworks. A comprehensive review is conducted about different aspects, including the learning process, building environment, computing platforms, and application scenario. Moving on, a critical discussion is performed to identify current challenges. The second part aims at providing the reader with insights into the real-world application of AI-big data analytics. Thus, three case studies that demonstrate the use of AI-big data analytics in BAMSs are presented, focusing on energy anomaly detection in residential and office buildings and energy and performance optimization in sports facilities. Lastly, future directions and valuable recommendations are identified to improve the performance and reliability of BAMSs in intelligent buildings. © 2022, The Author(s)."
"Purpose: To evaluate the diagnostic performance of the transfer learning approach for grading diagnosis of ACL injury on a new modified dual precision positioning of thin-slice oblique sagittal FS-PDWI (DPP-TSO-Sag-FS-PDWI) sequence. And compare the prediction performances between artificial intelligence (AI) and radiologists. Materials and Methods: Patients with both DPP-TSO-Sag-FS-PDWI sequence and arthroscopic results were included. We performed a transfer learning approach using the pre-trained EfficientNet-B0 model, including whole image and regions of interest (ROI) image inputs, and reset its parameters to achieve an automatic hierarchical diagnosis of ACL. Results: A total of 235 patients (145 men and 90 women, 37.91 ± 14.77 years) with 665 images were analyzed. The consistencies of AI and arthroscopy (Kappa value > 0.94), radiologists and arthroscopy (Kappa value > 0.83, p = 0.000) were almost perfect. No statistical difference exists between the whole image and radiologists in the diagnosis of normal ACL (p = 0.063) and grade 3 injury (p = 1.000), while the whole image was better than radiologists in grade 1 (p = 0.012) and grade 2 injury (p = 0.003). Conclusion: The transfer learning approach exhibits its feasibility in the diagnosis of ACL injury based on the new modified MR DPP-TSO-Sag-FS-PDWI sequence, suggesting that it can help radiologists hierarchical diagnose ACL injuries, especially grade 2 injury. © 2023, The Author(s) under exclusive licence to Japan Radiological Society."
"To support the sustainability of future cities, residents’ living spaces need to be built and used efficiently, while supporting residents’ communal wellbeing. Nordic superblock is a new planning, housing, and living concept in which residents of a neighborhood—a combination of city blocks—share yards, common spaces and utilities. Sharing living spaces is an essential element of this approach. In this study, our goal was to study the ways in which intelligent technology solutions—such as proactive, data-driven Artificial Intelligence (AI) applications—could support and even motivate the use of common areas in superblocks. To this end, we conducted a two-phase qualitative study: in the first phase, potential superblock residents (N = 12) shared their perspectives of sharing of living spaces in general, and more specifically of how intelligent technologies could support sharing spaces. In the second phase, two workshops with experts (N = 7) were held to gather understanding of possibilities of intelligent technologies in meeting the residents’ expectations of space sharing. The results illustrate space sharing and communality as supportive factors for one another, enabled but also complicated by social interaction. Major possibilities for intelligent technologies to advance space sharing were seen in organizing the use of spaces and facilitating social interaction in the community. As an outcome, four roles incorporating several use purposes of intelligent technologies were found. The findings can inform the Human-Centered AI (HCAI) research and design improving sustainable living in future urban neighborhoods. © 2022, The Author(s)."
"Climate change and rapid urban development have intensified the impact of hurricanes, especially on the Southeastern Coasts of the United States. Localized and timely risk assessments can facilitate coastal communities’ preparedness and response to imminent hurricanes. Existing assessment methods focused on hurricane risks at large spatial scales, which were not specific or could not provide actionable knowledge for residents or property owners. Fragility functions and other widely utilized assessment methods cannot model the complex relationships between building features and hurricane risk levels effectively. Therefore, we develop and test a building-level hurricane risk assessment with deep feedforward neural network (DFNN) models. The input features of DFNN models cover the meta building characteristics, fine-grained meteorological, and hydrological environmental parameters. The assessment outcomes, that is, risk levels, include the probability and intensity of building/property damages induced by wind and surge hazards. We interpret the DFNN models with local interpretable model-agnostic explanations (LIME). We apply the DFNN models to a case building in Cameron County, Louisiana in response to a hypothetical imminent hurricane to illustrate how the building's risk levels can be timely assessed with the updating weather forecast. This research shows the potential of deep-learning models in integrating multi-sourced features and accurately predicting buildings’ risks of weather extremes for property owners and households. The AI-powered risk assessment model can help coastal populations form appropriate and updating perceptions of imminent hurricanes and inform actionable knowledge for proactive risk mitigation and long-term climate adaptation. © 2022 The Authors. Risk Analysis published by Wiley Periodicals LLC on behalf of Society for Risk Analysis."
"In reinforcement learning (RL) an agent interacts with the environment based on sequential decisions. This agent receives a reward from the environment according to decisions and tries to maximize the reward. RL is used in several domains, such as production, autonomous driving, business management, education, games, healthcare, natural language processing, robotics, and among others. RL methodologies require processing large volumes of data and computational power. To speed up these applications, field-programmable gate array (FPGA) are widely employed in the literature. This letter proposes an accelerator for the Markov decision process (MDP) implemented in the AI-Toolbox public library using high-level synthesis tools, using the tiger-antelope problem as use case. Our approach shows an acceleration greater than 7× compared to the original version.  © 2009-2012 IEEE."
"This paper seeks to bypass assumptions that researchers in critical algorithmic studies and urban studies find it difficult to study algorithmic systems due to their black-boxed nature. In addition, it seeks to work against the assumption that advocating for transparency in algorithms is, therefore, the key for achieving an enhanced understanding of the role of algorithmic technologies on modern life. Drawing on applied assemblage thinking via the concept of the urban assemblage, I demonstrate how the notion of urban assemblage can work as an alternative way to explore the distributed and potential dimensions of what has been termed as Urban AI phenomena. Rather than seeing Urban AI phenomena as black-boxed, unknown and opaque, the notion of urban assemblage locates such entities within the wider contests of the city: urban places, communities and politics, where human-algorithmic relationships gather and disperse. In addition, this approach focuses on the potentialities of Urban AI phenomena—how algorithmic systems can operate differently through different aspects of the city—which can be seen to manifest new forms of resistance, collective actions and democracy. I use a case study of an algorithmic system designed to facilitate digital democracy—vTaiwan—to exemplify how assemblage methodology foregrounds the role of cities as spaces and places for exploring the democratic possibilities of algorithmic systems. This paper concludes with discussion of how the assemblage methodology contributes to serve as a bridge between critical algorithm studies and recent studies of platform urbanism. © 2022, The Author(s)."
"Smart city discourses often invoke the Panopticon, a disciplinary architecture designed by Jeremy Bentham and popularly theorized by Michel Foucault, as a model for understanding the social impact of AI technologies. This framing focuses attention almost exclusively on the negative ramifications of Urban AI, correlating ubiquitous surveillance, centralization, and data consolidation with AI development, and positioning technologies themselves as the driving factor shaping privacy, sociality, equity, access, and autonomy in the city. This paper describes an alternative diagram for Urban AI—the Polyopticon: a distributed, polyvalent, multi-modal network of synthetic intelligences. It posits that fourth industrial revolution technologies change the political, social, and psychodynamic relationships of sentience and witness in the city, shifting the effects of watching and watched beyond the exclusive domain of top-down surveillance and discipline. The Polyopticon poses a more expansive and ambivalent spectrum of possibilities for Urban AI scenarios, one that undermines the totalizing, singular, and cerebral notion of intelligence that so often characterizes Urban AI and smart city critiques. © 2022, The Author(s)."
"The 'Furniture Assembly AI-Robot Challenge 2021' is a competition that evaluates the performance of the robot for an assigned furniture assembly task by combining both artificial intelligence (AI) and robot technology. To generate commands such that a robot can execute the assembly instructions, it is essential to develop an AI-based algorithm that can recognize and interpret the assembly process based on the given instructions. The assembly robot must be dexterous and able to safely execute assembly tasks without operator intervention.  © 1994-2011 IEEE."
"We consider the scenario of inference at the wireless edge, in which devices are connected to an edge server and ask the server to carry out remote classification, that is, classify data samples available at edge devices. This requires the edge devices to upload high-dimensional features of samples over resource-constrained wireless channels, which creates a communication bottleneck. The conventional feature pruning solution would require the device to have access to the inference model, which is not available in the current split inference scenario. To address this issue, we propose the progressive feature transmission (ProgressFTX) protocol, which minimizes the overhead by progressively transmitting features until a target confidence level is reached. A control policy is proposed to accelerate inference, comprising two key operations: importance-aware feature selection at the server and transmission-termination control. For the former, it is shown that selecting the most important features, characterized by the largest discriminant gains of the corresponding feature dimensions, achieves a sub-optimal performance. For the latter, the proposed policy is shown to exhibit a threshold structure. Specifically, the transmission is stopped when the incremental uncertainty reduction by further feature transmission is outweighed by its communication cost. The indices of the selected features and transmission decision are fed back to the device in each slot. The control policy is first derived for the tractable case of linear classification, and then extended to the more complex case of classification using a convolutional neural network. Both Gaussian and fading channels are considered. Experimental results are obtained for both a statistical data model and a real dataset. It is shown that ProgressFTX can substantially reduce the communication latency compared to conventional feature pruning and random feature transmission strategies. © 2002-2012 IEEE."
"5G's feature of ubiquity in network coverage is expected to be extended to the computing domain in 6G. To accomplish ubiquity in communication and computation, integration of satellite, aerial and terrestrial networks is foreseen. In particular, the rising amount of applications such as In-Flight Entertainment and Connectivity Services (IFECS) and Software Defined Networking-enabled satellites renders network management more challenging. Moreover, the stringent Quality of Service (QoS) demands of these applications require edge computing. Network performance can be boosted by considering components of the aerial network, as potential Multi-Access Edge Computing (MEC) nodes. Thus, to cater to the QoS-critical applications, we propose an Aerial-Aided Multi-Access Edge Computing (AA-MEC) architecture that provides a framework for optimal management of computing resources and internet-based services in the sky. Furthermore, we formulate optimization problems to minimize the network latency for the two use cases of providing IFECS to aircrafts and services for offloading AI/ML tasks from satellites. Due to the dynamicity of satellite and aerial networks, we propose a re-configurable optimization. For this transforming network, we continuously identify the optimal MEC node for each application and the optimal path to the destination MEC node. In summary, our results demonstrate, that using AA-MEC improves network latency performance by 10.43% compared to the traditional approach of using only terrestrial MEC nodes, for latency-critical applications such as online gaming. Furthermore, while comparing our proposed dynamic approach with a static one, we record a minimum of 6.7% decrease in flow latency for IFECS and 56.03% decrease for computation offloading.  © 1965-2011 IEEE."
"Principal Component Analysis (PCA) is one of the most widely used data analysis methods in machine learning and AI. This manuscript focuses on the mathematical foundation of classical PCA and its application to a small-sample-size scenario and a large dataset in a high-dimensional space scenario. In particular, we discuss a simple method that can be used to approximate PCA in the latter case. This method can also help approximate kernel PCA or kernel PCA (KPCA) for a large-scale dataset. We hope this manuscript will give readers a solid foundation on PCA, approximate PCA, and approximate KPCA. © 2022, The Author(s)."
"Online gaming is a multibillion dollar industry that entertains a large, global population. Empowering online games with AI has made a great success, however, ignores the explainability of black-box model makes AI less responsible and hinders its further development. In this article, we introduce and discuss the audience and the concept of XAI (eXplainable AI) in online games. We propose a GXAI workflow, which combines the strong expressiveness of multiview data sources and the clear transparency of multiview black-box models. We present four specific classifiers and explainers in the character portrait view, the behavior sequence view, the client image view, and the social graph view. Experiments conducted on real-world datasets for game cheating detection and player churn prediction show the accuracy of classification and the rationality of explanation. We also discover and present numerous interesting and valuable findings from the individual, local, and global explanations. We implement and deploy three practical applications, including evidence and reason generation, model debugging and testing, and model compression and comparison in NetEase Games and have received quite positive reviews from user studies. More future work is in progress since this is the first work that introduces XAI in online games. © 2018 IEEE."
"Based on the big data GIS system, this paper first analyzes the methods and research results of the visualization of soil and water conservation level data at home and abroad. And for the proposed visual protection plan for land and water resources, a relatively excellent data architecture and the latest persistence plan have been designed. Based on this point, this paper investigates the cause of rainfall erosion and fluctuation in the mountain environment of S city and studies and determines the important technical points of water and soil conservation data images. Based on the results of this kind of research, the article further analyzes the temporal and spatial characteristics of the data and, based on the RUSLE model, conducts a directional consideration of the distribution characteristics of environmental rainfall erosion in mountainous environments: that is, comprehensively examines the impact of rainfall erosion in mountainous environments from both natural and social factors. The main factor of the pattern. Using multi-linear regression equations, combined with environmental variables and bioclimatic variables, the spatial distribution of land in Q mountainous areas is predicted under the background of global change. Finally, the article conducted an in-depth study on the people flow planning of the Leshan Giant Buddha Scenic Spot, introduced the construction of the Leshan Giant Buddha Scenic Spot Project Network, and adopted comprehensive technical solutions such as ""portrait photography reconnaissance surveillance camera + online photography AI portrait recognition technology"" to build the number of passengers identify and plan the design of the system and elaborate on the system's functions, such as visitor statistics, hotspot analysis, travel route analysis, video tracking, and data display. The article applies the research results of rainfall erosion in mountain environment based on big data GIS to the research on the people flow planning of Leshan Giant Buddha, which promotes the rapid development of scenic spots. © 2022, The Author(s) under exclusive licence to Institute of Geophysics, Polish Academy of Sciences & Polish Academy of Sciences."
"Soft skills (such as communication and collaboration) are rarely addressed in programming courses, mostly because they are difficult to teach, assess, and grade. A quantitative, modular, AI-based approach for assessing and grading students' collaboration has been examined in this article. The pedagogical underpinning of the approach includes a pedagogical framework and a quantitative soft skill assessment rubric, which have been adapted and used in an extracurricular Java programming course. The objective was to identify pros and cons of using different AI methods within this approach when it comes to assessing and grading collaboration in group programming projects. More specifically, fuzzy rules and several machine learning methods (ML onward) have been examined to see which one would yield the best results regarding performance, interpretability/explainability of recommendations, and feasibility/practicality. The data used for training and testing span four academic years, and the results suggest that almost all of the examined AI methods, when used within the proposed AI-based approach, can provide adequate grading recommendations as long as teachers cover other aspects of the assessment not covered by the rubrics: code quality, plagiarism, and project completion. The fuzzy-rule-based method requires time and effort to be spent on (manual) creation and tuning of fuzzy rules and sets, whereas the examined ML methods require lesser initial investments but do need historical data for training. On the other hand, the fuzzy-rule-based method can provide the best explanations on how the assessment/grading was made - something that proved to be very important to teachers.  © 2008-2011 IEEE."
"Context: Games are a well-established scenario to test AI and multiagent systems (MAS) proposals due to their popularity and defiance. However, there is no big picture of the application of this technology to games, the evolution of the kind of problem tackled, or the game scenarios in which agents have been experimented. Objective: To perform a systematic mapping to characterize the state of the art in the field of MAS applied to virtual games and to identify trends, strengths, and gaps for further research. Method: A systematic mapping study has been conducted to find primary studies in the field. A search was performed on title, abstracts, and keywords, whilst classification, data extraction, and further analysis were performed according to specific criteria focused on MAS papers with experimentation and evidence in a game scenario. Results: 78 studies published between 1998 and 2021 were found. Studies have been classified according to the MAS problem faced and the agent reasoning strategy. We detect that machine learning is the most common AI technique for MAS in games, considering both reinforcement learning and evolutionary techniques. MAS are used in a variety of gaming genres, especially in real-time strategy (RTS), sports, and simulation. Conclusions: RTS and sports games are well suited for concrete MAS problems, such as multiagent planning and task allocation. Expanding evidence and experimentation on other aspects related to scalability and usability issues is discussed. Those MAS problems and experiments that remain slightly modeled on games or are not thoroughly studied yet have been also identified. © 2018 IEEE."
[No abstract available]
"The world we live in has been taken quite surprisingly by the outbreak of a novel virus namely SARS-CoV-2. COVID-19 i.e. the disease associated with the virus, has not only shaken the world economy due to enforced lockdown but has also saturated the public health care systems of even most advanced countries due to its exponential spread. The fight against COVID-19 pandemic will continue until majority of world’s population get vaccinated or herd immunity is achieved. Many researchers have exploited the Artificial intelligence (AI) knacks based IoT architecture for early detection and monitoring of potential COVID-19 cases to control the transmission of the virus. However, the main cause of the spread is that people infected with COVID-19 do not show any symptoms and are asymptomatic but can still transmit virus to the masses. Researcher have introduced contact tracing applications to automatically detect contacts that can be infected by the index case. However, these fully automated contact tracing apps have not been accepted due to issues like privacy and cross-app compatibility. In the current study, an IoT based COVID-19 detection and monitoring system with semi-automated and improved contact tracing capability namely COVICT has been presented with application of real-time data of symptoms collected from individuals and contact tracing. The deployment of COVICT, the prediction of infected persons can be made more effective and contaminated areas can be identified to mitigate the further propagation of the virus by imposing Smart Lockdown. The proposed IoT based architecture can be quite helpful for regulatory authorities for policy making to fight COVID-19. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"This paper presents a property-directed approach to verifying recurrent neural networks (RNNs). To this end, we learn a deterministic finite automaton as a surrogate model from a given RNN using active automata learning. This model may then be analyzed using model checking as a verification technique. The term property-directed reflects the idea that our procedure is guided and controlled by the given property rather than performing the two steps separately. We show that this not only allows us to discover small counterexamples fast, but also to generalize them by pumping toward faulty flows hinting at the underlying error in the RNN. We also show that our method can be efficiently used for adversarial robustness certification of RNNs. © 2022, The Author(s)."
[No abstract available]
"Facial symmetry plays an important role in facial attractiveness and is one of the major criteria used to determine attractiveness in humans. In craniomaxillofacial surgery, facial symmetry is one of the main considerations. The aim of this study was to determine anthropometric measurements quantitatively and investigate the relationship between facial symmetry and attractiveness in a local Malay population. The study included 30 photographed Malay individuals and 100 photograph assessors, all aged between 18 and 26 years. The assessors indicated their preferences regarding the more attractive face on original and manipulated (symmetrical face) photographs. None of the photographed subjects had a perfectly symmetrical face (asymmetry index (AI) of 0%); 33.3% of the photographed subjects had an AI in the range of 1.6–2.0%. The majority of assessors chose the manipulated symmetrical face as the most attractive (manipulated photograph selected in 91.2% of cases). As facial symmetry is considered a critical factor in attractiveness, it is beneficial to consider balance and symmetry prior to facial reconstruction. The AI values found in this study may be useful as guidance to determine the normal minimum balance of facial symmetry. No AI values indicating perfect symmetry were observed for the unedited facial anthropometric measurements. However, the projection of a perfectly symmetrical face does influence the perception of facial attractiveness. © 2022 International Association of Oral and Maxillofacial Surgeons"
"Artificial Intelligence (AI) technologies are now widely employed to overcome human-induced faults in a variety of systems used in our daily lives, thanks to the digital transformation.One example of such systems is online document tracking systems (DTS). The DTS’s reliability and preferability are enhanced by automatic document classification and understanding features. Although automatic document classification systems can assist humans in document understanding tasks, most of of them are not designed to function with Portable Document Format (PDF), which contains text, tables or figures. In this study, we investigate separate ways to efficiently classify student documents that are uploaded in PDF format and are required for university education. We propose three possible techniques for this issue. The first approach is based on Optical Character Recognition (OCR) and traditional machine learning methods. The second is purely on deep learning. The third one is based on fusion of deep learning methods based on entropy. The proposed techniques can classify twelve distinct types of digital documents. The validity of the proposed methods has been verified by student affairs department of Kocaeli University in Turkey. The system has not only increased the efficiency of online document uploading steps for students, but also reduced the human cost for tracking the documents. The highest F-score (94.45%) is obtained by the ensemble of EfficientNetB3 and ExtraTree. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Over the past few years, the awareness that the full potential of artificial intelligence (AI) could be attained only through the establishment of a trustworthy and human-centric framework has expanded, thereby prompting demand for regulatory frameworks as well as engendering a flourish of initiatives that set ethical codes and good governance principles for AI development. This study investigates whether the convergence of many of the proposed ethical frameworks around a narrow set of values and principles may be interpreted as a case of transnational norms emergence, a pre-condition for a more structured global regulatory framework or policy regime. Moreover, it explores how this emerging normative framework is reframed in its concrete implementation. Findings suggest that AI governance poses a complex dilemma: while its hybrid governance ecosystem entrusts developers and deployers, mainly from the private sector and technical communities, with the task of translating principles into workable tools, their institutional logics substantially narrow the scope and purposes of the ethical approach. © 2022"
"Algorithms based on deep network models are being used for many pattern recognition and decision-making tasks in robotics and AI. Training these models requires a large labeled dataset and considerable computational resources, which are not readily available in many domains. Also, it is difficult to explore the internal representations and reasoning mechanisms of these models. As a step towards addressing the underlying knowledge representation, reasoning, and learning challenges, the architecture described in this paper draws inspiration from research in cognitive systems. As a motivating example, we consider an assistive robot trying to reduce clutter in any given scene by reasoning about the occlusion of objects and stability of object configurations in an image of the scene. In this context, our architecture incrementally learns and revises a grounding of the spatial relations between objects and uses this grounding to extract spatial information from input images. Non-monotonic logical reasoning with this information and incomplete commonsense domain knowledge is used to make decisions about stability and occlusion. For images that cannot be processed by such reasoning, regions relevant to the tasks at hand are automatically identified and used to train deep network models to make the desired decisions. Image regions used to train the deep networks are also used to incrementally acquire previously unknown state constraints that are merged with the existing knowledge for subsequent reasoning. Experimental evaluation performed using simulated and real-world images indicates that in comparison with baselines based just on deep networks, our architecture improves reliability of decision making and reduces the effort involved in training data-driven deep network models. © 2022, The Author(s)."
"Introduction: Pneumonia is a microorganism infection that causes chronic inflammation of the human lung cells. Chest X-ray imaging is the most well-known screening approach used for detecting pneumonia in the early stages. While chest-Xray images are mostly blurry with low illumination, a strong feature extraction approach is required for promising identification performance. Objectives: A new hybrid explainable deep learning framework is proposed for accurate pneumonia disease identification using chest X-ray images. Methods: The proposed hybrid workflow is developed by fusing the capabilities of both ensemble convolutional networks and the Transformer Encoder mechanism. The ensemble learning backbone is used to extract strong features from the raw input X-ray images in two different scenarios: ensemble A (i.e., DenseNet201, VGG16, and GoogleNet) and ensemble B (i.e., DenseNet201, InceptionResNetV2, and Xception). Whereas, the Transformer Encoder is built based on the self-attention mechanism with multilayer perceptron (MLP) for accurate disease identification. The visual explainable saliency maps are derived to emphasize the crucial predicted regions on the input X-ray images. The end-to-end training process of the proposed deep learning models over all scenarios is performed for binary and multi-class classification scenarios. Results: The proposed hybrid deep learning model recorded 99.21% classification performance in terms of overall accuracy and F1-score for the binary classification task, while it achieved 98.19% accuracy and 97.29% F1-score for multi-classification task. For the ensemble binary identification scenario, ensemble A recorded 97.22% accuracy and 97.14% F1-score, while ensemble B achieved 96.44% for both accuracy and F1-score. For the ensemble multiclass identification scenario, ensemble A recorded 97.2% accuracy and 95.8% F1-score, while ensemble B recorded 96.4% accuracy and 94.9% F1-score. Conclusion: The proposed hybrid deep learning framework could provide promising and encouraging explainable identification performance comparing with the individual, ensemble models, or even the latest AI models in the literature. The code is available here: https://github.com/chiagoziemchima/Pneumonia_Identificaton. © 2023"
"The workload of some radiologists increased dramatically in the last several, which resulted in a potentially reduced quality of diagnosis. It was demonstrated that diagnostic accuracy of radiologists significantly reduces at the end of work shifts. The study aims to investigate how radiologists cover chest X-rays with their gaze in the presence of different chest abnormalities and high workload. We designed a randomized experiment to quantitatively assess how radiologists’ image reading patterns change with the radiological workload. Four radiologists read chest X-rays on a radiological workstation equipped with an eye-tracker. The lung fields on the X-rays were automatically segmented with U-Net neural network allowing to measure the lung coverage with radiologists’ gaze. The images were randomly split so that each image was shown at a different time to a different radiologist. Regression models were fit to the gaze data to calculate the treads in lung coverage for individual radiologists and chest abnormalities. For the study, a database of 400 chest X-rays with reference diagnoses was assembled. The average lung coverage with gaze ranged from 55 to 65% per radiologist. For every 100 X-rays read, the lung coverage reduced from 1.3 to 7.6% for the different radiologists. The coverage reduction trends were consistent for all abnormalities ranging from 3.4% per 100 X-rays for cardiomegaly to 4.1% per 100 X-rays for atelectasis. The more image radiologists read, the smaller part of the lung fields they cover with the gaze. This pattern is very stable for all abnormality types and is not affected by the exact order the abnormalities are viewed by radiologists. The proposed randomized experiment captured and quantified consistent changes in X-ray reading for different lung abnormalities that occur due to high workload. © 2023, The Author(s) under exclusive licence to Society for Imaging Informatics in Medicine."
"A new method for local and global explanation of the machine learning black-box model predictions by tabular data is proposed. It is implemented as a system called AFEX (Attention-like Feature EXplanation) and consisting of two main parts. The first part is a set of the one-feature neural subnetworks, which aim to get a specific representation for every feature in the form of a basis of shape functions. The subnetworks use shortcut connections with trainable parameters to improve the network training performance. The second part of AFEX produces shape functions of features as the weighted sum of the basis shape functions where weights are computed by using an attention-like mechanism. The most important advantage of AFEX is that it identifies pairwise interactions between features based on pairwise multiplications of shape functions corresponding to different features. A modification of AFEX with incorporating an additional surrogate model, which approximates the black-box model, is proposed. AFEX is trained end-to-end on a whole dataset only once such that it does not require to train neural networks again in the explanation stage. Numerical experiments with synthetic and real data illustrate AFEX. The corresponding code implementing the method is publicly available. © 2022, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
"Background: During the COVID-19 pandemic, deferral of inpatient elective surgical procedures served as a primary mechanism to increase surge inpatient capacity. Given the benefit of bariatric surgery on treating obesity and associated comorbidities, decreased access to bariatric surgery may have long-term public health consequences. Understanding the extent of the disruption of the COVID-19 pandemic to bariatric surgery will help health systems plan for appropriate access. Materials and methods: This is an observational cohort study using the PINC AI Healthcare Database from 1/1/2019–6/31/2021. A Poisson regression model with patient characteristics and hospital-fixed effects was used to assess the relative monthly within-hospital reduction in surgical encounters, variations by race and ethnicity, and shift from inpatient to outpatient procedures. A multivariate linear probability model was used to assess the change in 30-day readmissions from 2020 and 2021 compared to 2019. Results: Among 309 hospitals, there were 46,539 bariatric procedures conducted in 2019 with a 14.8% reduction in volume to 39,641 procedures in 2020. There were 22,642 bariatric procedures observed from January to June of 2021. The most pronounced decrease in volume occurred in April with an 89.7% relative reduction from 2019. Black and Hispanic patients were more likely to receive bariatric surgery after the height of the pandemic compared to white patients. A clinically significant shift from inpatient to outpatient bariatric surgical procedures was not observed. Relative to 2019, there were no significant differences in bariatric surgical readmission rates. Conclusion: During the pandemic there was a sizable decrease in bariatric surgical volume. There did not appear to be disparities in access to bariatric surgery for minority patients. We did not observe a meaningful shift toward outpatient bariatric surgical procedures. Post-pandemic, monitoring is needed to assess if hospitals have been able to meet the demand for bariatric surgical procedures. Graphical abstract: [Figure not available: see fulltext.] © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Introduction: A limitation to expanding laparoscopic simulation training programs is the scarcity of expert evaluators. In 2019, a new digital platform for remote and asynchronous laparoscopic simulation training was validated. Through this platform, 369 trainees have been trained in 14 institutions across Latin America, collecting 6729 videos of laparoscopic training exercises. The use of artificial intelligence (AI) has recently emerged in surgical simulation, showing usefulness in training assessment, virtual reality scenarios, and laparoscopic virtual reality simulation. An AI algorithm to assess basic laparoscopic simulation training exercises was developed. This study aimed to analyze the agreement between this AI algorithm and expert evaluators in assessing basic laparoscopic-simulated training exercises. Methods: The AI algorithm was trained using 400-bean drop (BD) and 480-peg transfer (PT) videos and tested using 64-BD and 43-PT randomly selected videos, not previously used to train the algorithm. The agreement between AI and expert evaluators from the digital platform (EE) was then analyzed. The exercises being assessed involve using laparoscopic graspers to move objects across an acrylic board without dropping any objects in a determined time (BD < 24 s, PT < 55 s). The AI algorithm can detect object movement, identify if objects have fallen, track grasper clamps location, and measure exercise time. Cohen’s Kappa test was used to evaluate the agreement between AI assessments and those performed by EE, using a pass/fail nomenclature based on the time to complete the exercise. Results: After the algorithm was trained, 79.69% and 93.02% agreement were observed in BD and PT, respectively. The Kappa coefficients test observed for BD and PT were 0.59 (moderate agreement) and 0.86 (almost perfect agreement), respectively. Conclusion: This first approach of AI use in basic laparoscopic skills simulated training assessment shows promising results, providing a preliminary framework to expand the use of AI to other basic laparoscopic skills exercises. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
[No abstract available]
"Deep learning models for identification and subsequent mitigation of tokamak plasma disruption have recently shown great promise for reliable predictions for machines other than the one on which it has been trained. The performance of such artificial intelligence (AI)/machine learning (ML) models strongly depends on the training data. Considering the sparse availability of universal high quality data underscores the requirement for synthetic data for the training of the AI/ML models. Synthetic data generation methods reported in the current literature have limitations in terms of quantity, diversity and preserving the temporal dynamics of the experimental seed data (SD). The article presents generative adversarial networks based procedure capable enough to generate unlimited device-independent temporal evolution of tokamak plasma current. The synthetic data improves with the employment of the classified SD while retaining the characteristics of the original data. The procedure offers a substantial volume of synthetic data with a very impressive diversity, thereby ensuring the requirements for successful AI/ML model training. © 2022 Wiley-VCH GmbH."
"Modern solutions based on Artificial Intelligence (AI) play an important role in the management of drone’s resources in Space-Air-Ground-Integrated-Network (SAGIN). AI can use information collected by drone sensors to develop routing protocols, optimize communication networks, improve energy efficiency, and predict user behavior. In this regard, the analysis of data loss in SAGIN with AI is relevant. This work is devoted to the calculation of packet losses in SAGIN, containing additional hardware of AI system. Based on the original model containing a Base Station (BS), a stratospheric Remotely Piloted Air System (RPAS) with an AI system, a low-orbit satellite, a low-altitude RPAS and a user of a terrestrial cellular network, data traffic was simulated using NetCracker Professional 4.1 software. The AI system was simulated by a cloud structure with the ability to change the delay and the probability of packet losses. Quantitative characteristics of traffic in SAGIN channels with such a model of the AI hardware system are obtained. The dependences of packets losses on the size of messages and the data transfer rate are calculated. The dependences of BS uplink Average Load and the packets travel time on the TS, as well as the dependences of the Bit Error Rate (BER) on the Average Load, are obtained. The results are valuable in terms of practical guidelines for choosing data transfer modes and the necessary hardware parameters for an AI system. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"In the last few years, the use of artificial intelligence (AI) and machine learning (ML) techniques have received considerable notice as trending technologies in the petroleum industry. The utilization of new tools and modern technologies creates huge volumes of structured and un-structured data. Organizing and processing of these information at faster pace for the performance assessment and forecasting for field development and management is continuously growing as an important field of investigation. Various difficulties which were faced in predicting the operative features by utilizing the conventional methods have directed the academia and industry toward investigations focusing on the applications of ML and data driven approaches in exploration and production operations to achieve more accurate predictions which improves decision-making processes. This research provides a review to examine the use cases and application of AI and ML techniques in petroleum industry for optimization of the upstream processes such as reservoir studies, drilling and production engineering. The challenges related to routine approaches for prognosis of operative parameters have been evaluated and the use cases of performance optimizations through employing data-driven approaches resulted in enhancement of decision-making workflows have been presented. Moreover, possible scenarios of the way that artificial intelligence will develop and influence the oil and gas industry and how it may change it in the future was discussed. © 2022 Chinese Petroleum Society"
"This paper explores the complex relationship between intellectual property (IP) and the transdisciplinary collaborative design (co-design) of new digital technologies for agriculture (AgTech). More specifically, it explores how prioritizing the capturing of IP as a central researcher responsibility can cause disruptions to research relationships and project outcomes. We argue that boundary-making processes associated with IP create a particular context through which responsibility can, and must, be located and cultivated by researchers working within transdisciplinary collaborations. We draw from interview data and situated IP practices from a transdisciplinary co-design project in Aotearoa New Zealand to illustrate how IP is a fluid boundary-requiring-and-producing object that impels researchers into its management, and produces tensions that need to be noticed and skillfully navigated within research relations. We propose located response-ability as a conceptual tool and practice to reposition IP within the relations that make up a transdisciplinary co-design project, as opposed to prioritizing IP by default without recognizing its possible impacts on collaborative relations and other project aims and accountabilities. This can support researchers practicing responsible innovation in making everyday decisions on how to protect potential IP without disrupting the collaborative relations that make the creation of potential IP possible, and the existence of protected IP relevant and beneficial to project collaborators and wider societal actors. This may help to ensure that societal benefits can be generated, and positive science–society relationships prioritized and preserved, in the design of new AgTech. © 2022, The Author(s)."
"Non-dimensional similarity groups and analytically solvable proximity equations can be used to estimate integral fluid film parameters of elastohydrodynamically lubricated (EHL) contacts. In this contribution, we demonstrate that machine learning (ML) and artificial intelligence (AI) approaches (support vector machines, Gaussian process regressions, and artificial neural networks) can predict relevant film parameters more efficiently and with higher accuracy and flexibility compared to sophisticated EHL simulations and analytically solvable proximity equations, respectively. For this purpose, we use data from EHL simulations based upon the full-system finite element (FE) solution and a Latin hypercube sampling. We verify that the original input data are required to train ML approaches to achieve coefficients of determination above 0.99. It is revealed that the architecture of artificial neural networks (neurons per layer and number of hidden layers) and activation functions influence the prediction accuracy. The impact of the number of training data is exemplified, and recommendations for a minimum database size are given. We ultimately demonstrate that artificial neural networks can predict the locally-resolved film thickness values over the contact domain 25-times faster than FE-based EHL simulations (R2 values above 0.999). We assume that this will boost the use of ML approaches to predict EHL parameters and traction losses in multibody system dynamics simulations. [Figure not available: see fulltext.]. © 2022, The author(s)."
"Background: Fibro-Scope is an artificial intelligence/neural network system to determine the fibrosis stage in nonalcoholic steatohepatitis (NASH) using 12 parameters of the patient: age, sex, height, weight, waist circumference (WC), platelet count, and the levels of aspartate and alanine aminotransferase, gamma-glutamyltransferase, cholesterol, triglycerides, and type IV collagen 7S. However, measurement of WC is unstable and often missing from patient databases. Herein, we created Fibro-Scope V1.0.1 that has the same detection power as its predecessor, without the need to consider WC. Methods: To build a new AI diagnostic system available for the global needs, data from 764 patients with NASH and bridging fibrosis (STELLAR-3) or compensated cirrhosis (STELLAR-4) that participated in two phase III trials were added to the Japanese data. Finally, the data of a total of 898 patients in the training and of 300 patients in the validation studies were analyzed, respectively. Results: The discrimination of F0–2 from F3,4 through Fibro-Scope V1.0.1 was characterized by a 99.8% sensitivity, a 99.6% specificity, a 99.8% positive predictive value, and a 99.6% negative predictive value in a training study with gray zone analysis; similar effectiveness was also revealed in the analysis without a gray zone. In the validation studies with and without gray zone analysis, high sensitivity and specificity were also identified. Fibro-Scope V1.0.1 exerted a diagnostic accuracy for F3,4 advanced fibrosis that was comparable to that of the original Fibro-Scope and delivered high (> 92%) sensitivity and specificity. Conclusion: Fibro-Scope V1.0.1 can accurately diagnose F3,4 fibrosis without the need of WC. © 2022, Asian Pacific Association for the Study of the Liver."
"Today, artificial intelligence (AI) can be found in nearly every aspect of travel and tourism, from personalization and recommendation systems to robotics to conversational systems to intelligent travel agents to forecasting and prediction tools and systems for language translation. As a result, this study examines the connection between large-scale big data analysis and smart tourism, resulting in a massive data platform for forecasting and providing feedback on smart tourist developments.This article aims to shed light on the importance of AI in the travel and tourism sector. Many cybersecurity firms are stepping to raise their efforts using artificial intelligence to attain this goal because effective information security is necessary for better detection.An intelligent all-area-advanced tourism cloud platform is demonstrated in this paper, as well as how to design the overall system framework, system structure, and database systems needed to integrate all of the province's tourism resources into a single information resource sharing system.Furthermore, this paper presents an intelligent decision-support system based on big data to reimagine tourism public administration and service. It moreover examines the ramifications of this decision-making mode and implementation procedures. This study covers the framework operation's elements, environment features, and promotion mode by creating a tourism public management and service framework based on big data. Big data-driven decision-support and management can overhaul tourism public management and service models. Cyber threat intelligence and applications, including those that defend systems, networks, programs, devices, and data from cyber threats, could significantly impact this tourism industry.The present tourism industry's problem-solving efficiency, quality, and services have increased. The simulation evaluation can promote the public tourism service in sustainable tourism development with an improved decision accuracy of 97.21%. © 2022, The Author(s), under exclusive licence to Springer-Verlag France SAS, part of Springer Nature."
"Death by suicide is the seventh leading death cause worldwide. The recent advancement in Artificial Intelligence (AI), specifically AI applications in image and voice processing, has created a promising opportunity to revolutionize suicide risk assessment. Subsequently, we have witnessed fast-growing literature of research that applies AI to extract audiovisual non-verbal cues for mental illness assessment. However, the majority of the recent works focus on depression, despite the evident difference between depression symptoms and suicidal behavior non-verbal cues. In this paper, we review the recent works that study suicide ideation and suicide behavior detection through audiovisual feature analysis, mainly suicidal voice/speech acoustic features analysis and suicidal visual cues. Automatic suicide assessment is a promising research direction that is still in the early stages. Accordingly, there is a lack of large datasets that can be used to train machine leaning and deep learning models proven to be effective in other, similar tasks. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
"Ultrasonic Welding is a popular welding procedure that uses high-frequency energy to heat joints. It is a complicated process involving a number of variable parameters that can each greatly modify the final weld product. A number of Artificial Intelligence (AI) technologies have thus been employed to regress and classify results such as weld parameters such as failure load, weld quality and joint strength on the basis of different parameters including power output, annealing temperature and vibration amplitude. Artificial neural network models are the most popular and adept at weld modeling on varying materials and composites. This paper reviews and compares the materials, feature extraction techniques and AI architectures and their performances on predicting a host of welding objectives.  © 2023 World Scientific Publishing Europe Ltd."
"With accelerating advances in artificial intelligence, it is clear that introducing K-12 students to AI is essential for preparation to interact with and potentially develop AI technologies. To succeed as the workers, creators, and innovators of the future, we argue students should encounter core concepts of AI as early as elementary school. However, building a curriculum that introduces AI content to K-12 students presents significant challenges, such as connecting to prior knowledge, developing curricula that are meaningful for students, and creating content that teachers feel confident to teach. To lay the groundwork for elementary AI education, we investigated the everyday experiences and ideas of students in grades 4 and 5 (ages 9 to 11) about AI to inform possible entry points for learning. This yielded themes around student conceptions, examples, and ethics of AI. For each theme, we juxtapose the student ideas with the teachers’ reflections on those ideas as frames of reference to consider in co-designing curricular approaches. © 2022, International Artificial Intelligence in Education Society."
"Objectives: Differentiation between COVID-19 and community-acquired pneumonia (CAP) in computed tomography (CT) is a task that can be performed by human radiologists and artificial intelligence (AI). The present study aims to (1) develop an AI algorithm for differentiating COVID-19 from CAP and (2) evaluate its performance. (3) Evaluate the benefit of using the AI result as assistance for radiological diagnosis and the impact on relevant parameters such as accuracy of the diagnosis, diagnostic time, and confidence. Methods: We included n = 1591 multicenter, multivendor chest CT scans and divided them into AI training and validation datasets to develop an AI algorithm (n = 991 CT scans; n = 462 COVID-19, and n = 529 CAP) from three centers in China. An independent Chinese and German test dataset of n = 600 CT scans from six centers (COVID-19 / CAP; n = 300 each) was used to test the performance of eight blinded radiologists and the AI algorithm. A subtest dataset (180 CT scans; n = 90 each) was used to evaluate the radiologists’ performance without and with AI assistance to quantify changes in diagnostic accuracy, reporting time, and diagnostic confidence. Results: The diagnostic accuracy of the AI algorithm in the Chinese-German test dataset was 76.5%. Without AI assistance, the eight radiologists’ diagnostic accuracy was 79.1% and increased with AI assistance to 81.5%, going along with significantly shorter decision times and higher confidence scores. Conclusion: This large multicenter study demonstrates that AI assistance in CT-based differentiation of COVID-19 and CAP increases radiological performance with higher accuracy and specificity, faster diagnostic time, and improved diagnostic confidence. Key Points: • AI can help radiologists to get higher diagnostic accuracy, make faster decisions, and improve diagnostic confidence. • The China-German multicenter study demonstrates the advantages of a human-machine interaction using AI in clinical radiology for diagnostic differentiation between COVID-19 and CAP in CT scans. © 2022, The Author(s), under exclusive licence to European Society of Radiology."
"COVID-19 is a novel virus that presents challenges due to a lack of consistent and in-depth research. The news of the COVID-19 spreads across the globe, resulting in a flood of posts on social media sites. Apart from health, social, and economic disturbances brought by the COVID-19 pandemic, another important consequence involves public mental health crises which is of greater concern. Data related to COVID-19 is a valuable asset for researchers in understanding people's feelings related to the pandemic. It is thus important to extract the early information evolving public sentiments on social platforms during the outbreak of COVID-19. The objective of this study is to look at people's perceptions of the COVID-19 pandemic who interact with each other and share tweets on the Twitter platform. COVIDSenti, a large-scale benchmark dataset comprising 90,000 COVID-19 tweets collected from February to March 2020, during the initial phases of the outbreak served as the foundation for our experiments. A pre-trained bidirectional encoder representations from transformers (BERT) model is fine-tuned and embeddings generated are combined with two long short-term memory networks to propose the residual encoder transformation network model. The proposed model is used for multiclass text classification on a large dataset labeled as positive, negative, and neutral. The experimental outcomes validate that: (1) the proposed model is the best performing model, with 98% accuracy and 96% F1-score; (2) It also outperforms conventional machine learning algorithms and different variants of BERT, and (3) the approach achieves better results as compared to state-of-the-art on different benchmark datasets. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Automatic detection of lung diseases using AI-based tools became very much necessary to handle the huge number of cases occurring across the globe and support the doctors. This paper proposed a novel deep learning architecture named LWSNet (Light Weight Stacking Network) to separate Covid-19, cold pneumonia, and normal chest x-ray images. This framework is based on single, double, triple, and quadruple stack mechanisms to address the above-mentioned tri-class problem. In this framework, a truncated version of standard deep learning models and a lightweight CNN model was considered to conviniently deploy in resource-constraint devices. An evaluation was conducted on three publicly available datasets alongwith their combination. We received 97.28%, 96.50%, 97.41%, and 98.54% highest classification accuracies using quadruple stack. On further investigation, we found, using LWSNet, the average accuracy got improved from individual model to quadruple model by 2.31%, 2.55%, 2.88%, and 2.26% on four respective datasets. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Explainable artificial intelligence (XAI) focuses on transparent AI models and decisions, which are easy to understand, analyze, and augment by a nontechnical audience. Fuzzy logic systems (FLS)-based XAI provides an explainable framework while also modeling uncertainties in real-world environments. However, most real-life processes are not characterized by high uncertainty alone; they are also inherently time dependent, i.e., the processes are time variant. In this work, we present a novel temporal type-2 FLS-based approach for time-dependent XAI (TXAI) systems, which can account for the likelihood of a sample occurrence in the time domain by its the frequency. In the proposed temporal type-2 fuzzy sets (TT2FSs), a 4-D time-dependent membership function integrates the universe of discourse, its membership, and its frequency of occurrence across time. The TXAI system manifested better classification prowess in cross-validation tests, with a mean recall of 95.40% than a standard XAI system (based on nontemporal general type-2 fuzzy sets) that had a mean recall of 87.04%. TXAI also performed significantly better than most nonexplainable AI systems, with between 3.95% and 19.04% improvement gain in mean recall. In addition, TXAI can also outline the most likely time-dependent trajectories using the frequency and time dimensions embedded in the TXAI model; viz. given a rule at a determined time interval, what will be the next most likely rule at a subsequent time interval. In this regard, the proposed TXAI system can have profound implications for delineating the evolution of real-life time-dependent processes, such as behavioral or biological processes.  © 2020 IEEE."
"Due the quick spread of coronavirus disease 2019 (COVID-19), identification of that disease, prediction of mortality rate and recovery rate are considered as one of the critical challenges in the whole world. The occurrence of COVID-19 dissemination beyond the world is analyzed in this research and an artificial-intelligence (AI) based deep learning algorithm is suggested to detect positive cases of COVID19 patients, mortality rate and recovery rate using real-world datasets. Initially, the unwanted data like prepositions, links, hashtags etc., are removed using some pre-processing techniques. After that, term frequency inverse-term frequency (TF-IDF) andBag of Words (BoW) techniques are utilized to extract the features from pre-processed dataset. Then, Mayfly Optimization (MO) algorithm is performed to pick the relevant features from the set of features. Finally, two deep learning procedures, ResNet model and GoogleNet model, are hybridized to achieve the prediction process. Our system examines two different kinds of publicly available text datasets to identify COVID-19 disease as well as to predict mortality rate and recovery rate using those datasets. There are four different datasets are taken to analyse the performance, in which the proposed method achieves 97.56% accuracy which is 1.40% greater than Linear Regression (LR) and Multinomial Naive Bayesian (MNB), 3.39% higher than Random Forest (RF) and Stochastic gradient boosting (SGB) as well as 5.32% higher than Decision tree (DT) and Bagging techniques if first dataset. When compared to existing machine learning models, the simulation result indicates that a proposed hybrid deep learning method is valuable in corona virus identification and future mortality forecast study. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Cognitive Autonomous Networks (CAN) promise to advance Self Organizing Networks (SON) by applying artificial intelligence to significantly raise the degree of automation in mobile networks. In CAN, Cognitive Functions (CFs) learn the optimal configuration parameter values to optimize specific network metrics, with the execution coordinated via a controller. In open, multi-vendor systems however, the CF's learning ability may raise a new risk: a manipulative CF (MCF) may learn not only its objective, but also to manipulate the coordination system in pursuit of that objective. In this paper we propose and evaluate our proposed functionality, called CoDeRa, that neutralizes manipulative CF behavior. However, although CoDeRa is effective against MCFs, it is inadequate to resolving error propagation in CF coordination, caused by corrupted network data, for which we have proposed an alternate simpler and cost efficient network management architecture. Our evaluation shows that the proposed design is robust against the observed concerns, and in context of ongoing worldwide standardization efforts, we summarize the relevance and implications of our proposed architecture.  © 2004-2012 IEEE."
"Objective: To evaluate the prevalence of the ‘posterior crescent sign’ in symptomatic patients referred for MRI/MR arthrogram of the hip and identify any correlation with imaging features of joint pathology. Materials and methods: Retrospective imaging assessment of a cohort of 1462 hips, from 1380 included MR examinations (82 bilateral) retrieved from a search of all examinations in patients 16–50 years old from June 2018 to June 2021, with median age 45.8 years (range 17.8–50.0) and 936 hips (64%) in women. Radiographic and MR findings related to hip dysplasia, femoroacetabular impingement and osteoarthritis were assessed. Results: Fifty-one hips (3.5%) were positive for the posterior crescent sign, median age of 45.8 years (range 17.8–50.0) and 29 (58%) in women. Radiographic findings included the following: mean lateral centre edge angle (LCEA) 22.2° (± 7.8°) with LCEA < 20° in 15 (31%) and LCEA 20–25° in 17 (35%) and mean acetabular index (AI) of 13.1° (± 5.8°) with AI > 13° in 22 (45%). MR findings included the following: mean anterior acetabular sector angle (AASA) 54.3° (± 9.8°), mean posterior acetabular sector angle (PASA) 92.7° (± 7.0°), labral tear at 3–4 o’clock in 20 (39%), high-grade acetabular chondral loss in 42 (83%) and ligamentum teres abnormality in 20 (39%). Conclusion: The posterior crescent sign occurs in 3.5% of symptomatic young and middle-aged adults on MR. It is associated with overt and borderline hip dysplasia and other findings of hip instability. It is also associated with osteoarthritis in some cases and should be interpreted with caution in these patients. © 2022, The Author(s), under exclusive licence to International Skeletal Society (ISS)."
"The use of machine learning (ML) allows us to automate and scale the decision-making processes. The key to this automation is the development of ML models that generalize training data toward unseen data. Such models can become extremely versatile and powerful, which makes democratization of artificial intelligence (AI) possible, that is, providing ML to non-ML experts such as software engineers or domain experts. Typically, automated ML (AutoML) is being referred to as a key step toward it. However, from our perspective, we believe that democratization of the verification process of ML systems is a larger and even more crucial challenge to achieve the democratization of AI. Currently, the process of ensuring that an ML model works as intended is unstructured. It is largely based on experience and domain knowledge that cannot be automated. The current approaches such as cross-validation or explainable AI are not enough to overcome the real challenges and are discussed extensively in this article. Arguing toward structured verification approaches, we discuss a set of guidelines to verify models, code, and data in each step of the ML lifecycle. These guidelines can help to reliably measure and select an optimal solution, besides minimizing the risk of bugs and undesired behavior in edge-cases."
"Consequential historical decisions that shaped transportation systems and their influence on society have many valuable lessons. The decisions we learn from and choose to make going forward will play a key role in shaping the mobility landscape of the future. This is especially pertinent as artificial intelligence (AI) becomes more prevalent in the form of autonomous vehicles (AVs). Throughout urban history, there have been cyclical transport oppressions of previous-generation transportation methods to make way for novel transport methods. These cyclical oppressions can be identified in the baroque and modernist periods, and a third oppression may occur in the contemporary period. To explore the idea of a third oppression, we focus on the bicycle and outline the history of cycling to understand how historical mode oppression unfolded. We then present several social and political factors that contributed to the oppression of cycling and share recommendations for how to avoid future oppressions including political, social, and design actions for researchers and policymakers to take. This paper argues that priorities for AI-enabled mobility and cyclist needs be advanced in proportion to the extent that they contribute to societal goals of urban containment, public realm, and proximal cities. Additionally, future mobility evolutions should prioritise mobility justice and mode choice over inducing a singular transportation method. © 2022, The Author(s)."
"There is a growing concern about typically opaque decision-making with high-performance machine learning algorithms. Providing an explanation of the reasoning process in domain-specific terms can be crucial for adoption in risk-sensitive domains such as healthcare. We argue that machine learning algorithms should be interpretable by design and that the language in which these interpretations are expressed should be domain- and task-dependent. Consequently, we base our model's prediction on a family of user-defined and task-specific binary functions of the data, each having a clear interpretation to the end-user. We then minimize the expected number of queries needed for accurate prediction on any given input. As the solution is generally intractable, following prior work, we choose the queries sequentially based on information gain. However, in contrast to previous work, we need not assume the queries are conditionally independent. Instead, we leverage a stochastic generative model (VAE) and an MCMC algorithm (Unadjusted Langevin) to select the most informative query about the input based on previous query-answers. This enables the online determination of a query chain of whatever depth is required to resolve prediction ambiguities. Finally, experiments on vision and NLP tasks demonstrate the efficacy of our approach and its superiority over post-hoc explanations.  © 1979-2012 IEEE."
"Objective: To compare the performances of artificial intelligence (AI) to those of radiologists in wrist fracture detection on radiographs. Methods: This retrospective study included 637 patients (1917 radiographs) with wrist trauma between January 2017 and December 2019. The AI software used was a deep neuronal network algorithm. Ground truth was established by three senior musculoskeletal radiologists who compared the initial radiology reports (IRR) made by non-specialized radiologists, the results of AI, and the combination of AI and IRR (IR+AI) Results: A total of 318 fractures were reported by the senior radiologists in 247 patients. Sensitivity of AI (83%; 95% CI: 78–87%) was significantly greater than that of IRR (76%; 95% CI: 70–81%) (p < 0.001). Specificities were similar for AI (96%; 95% CI: 93–97%) and for IRR (96%; 95% CI: 94–98%) (p = 0.80). The combination of AI+IRR had a significantly greater sensitivity (88%; 95% CI: 84–92%) compared to AI and IRR (p < 0.001) and a lower specificity (92%; 95% CI: 89–95%) (p < 0.001). The sensitivity for scaphoid fracture detection was acceptable for AI (84%) and IRR (80%) but poor for the detection of other carpal bones fracture (41% for AI and 26% for IRR). Conclusions: Performance of AI in wrist fracture detection on radiographs is better than that of non-specialized radiologists. The combination of AI and radiologist’s analysis yields best performances. Key Points: • Artificial intelligence has better performances for wrist fracture detection compared to non-expert radiologists in daily practice. • Performance of artificial intelligence greatly differs depending on the anatomical area. • Sensitivity of artificial intelligence for the detection of carpal bones fractures is 56%. © 2022, The Author(s), under exclusive licence to European Society of Radiology."
"We developed and tested a novel template matching approach for signal quality assessment on electrocardiogram (ECG) data. A computational method was developed that uses a sinusoidal approximation to the QRS complex to generate a correlation value at every point of an ECG. The strength of this correlation can be numerically adapted into a ‘score’ for each segment of an ECG, which can be used to stratify signal quality. The algorithm was tested on lead II ECGs of intensive care unit (ICU) patients admitted to the Mount Sinai Hospital (MSH) from January to July 2020 and on records from the MIT BIH arrhythmia database. The algorithm was found to be 98.9% specific and 99% sensitive on test data from the MSH ICU patients. The routine performs in linear O(n) time and occupies O(1) heap space in runtime. This approach can be used to lower the burden of pre-processing in ECG signal analysis. Given its runtime (O(n)) and memory (O(1)) complexity, there are potential applications for signal quality stratification and arrhythmia detection in wearable devices or smartphones. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
"Background: We present an automatic method for coronary artery calcium (CAC) quantification and cardiovascular risk categorization in CT attenuation correction (CTAC) scans acquired at rest and stress during cardiac PET/CT. The method segments CAC according to visual assessment rather than the commonly used CT-number threshold. Methods: The method decomposes an image containing CAC into a synthetic image without CAC and an image showing only CAC. Extensive evaluation was performed in a set of 98 patients, each having rest and stress CTAC scans and a dedicated calcium scoring CT (CSCT). Standard manual calcium scoring in CSCT provided the reference standard. Results: The interscan reproducibility of CAC quantification computed as average absolute relative differences between CTAC and CSCT scan pairs was 75% and 85% at rest and stress using the automatic method compared to 121% and 114% using clinical calcium scoring. Agreement between automatic risk assessment in CTAC and clinical risk categorization in CSCT resulted in linearly weighted kappa of 0.65 compared to 0.40 between CTAC and CSCT using clinically used calcium scoring. Conclusion: The increased interscan reproducibility achieved by our method may allow routine cardiovascular risk assessment in CTAC, potentially relieving the need for dedicated CSCT. © 2022, The Author(s)."
"This article provides an in-depth look at how K-12 students should be introduced to Machine Learning and the knowledge and skills they will develop as a result. We begin with an overview of the AI4K12 Initiative, which is developing national guidelines for teaching AI in K-12, and briefly discuss each of the “Five Big Ideas in AI” that serve as the organizing framework for the guidelines. We then discuss the general format and structure of the guidelines and grade band progression charts and provide a theoretical framework that highlights the developmental appropriateness of the knowledge and skills we want to impart to students and the learning experiences we expect them to engage in. Development of the guidelines is informed by best practices from Learning Sciences and CS Education research, and by the need for alignment with CSTA’s K-12 Computer Science Standards, Common Core standards, and Next Generation Science Standards (NGSS). The remainder of the article provides an in-depth exploration of the AI4K12 Big Idea 3 (Learning) grade band progression chart to unpack the concepts we expect students to master at each grade band. We present examples to illustrate the progressions from two perspectives: horizontal (across grade bands) and vertical (across concepts for a given grade band). Finally, we discuss how these guidelines can be used to create learning experiences that make connections across the Five Big Ideas, and free online tools that facilitate these experiences. © 2022, International Artificial Intelligence in Education Society."
"Artificial intelligence (AI) is embedded in a wide variety of Smart City applications and infrastructures, often without the citizens being aware of the nature of their “intelligence”. AI can affect citizens’ lives concretely, and thus, there may be uncertainty, concerns, or even fears related to AI. To build acceptable futures of Smart Cities with AI-enabled functionalities, the Human-Centered AI (HCAI) approach offers a relevant framework for understanding citizen perceptions. However, only a few studies have focused on clarifying the citizen perceptions of AI in the context of smart city research. To address this gap, we conducted a two-phased study. In the pre-study, we explored citizen perceptions and experiences of AI with a short survey (N = 91). Second, scenario-based interviews (N = 7) were utilized to gain in-depth insights of citizen perceptions of AI in the Smart City context. Five central themes were recognized: (1) I don’t like them monitoring me, (2) I want maximum gain for minimum effort, (3) I don’t want AI to mimic people, (4) I’ll avoid using AI if I consider the risk too high, and (5) I don’t need to be concerned about AI. These offer an idea of human-centered requirements worth considering while designing AI applications for future Smart Cities. © 2022, The Author(s)."
"Purpose: To validate the potential application of THEIA™ as clinical decision making assistant in a national screening program. Methods: A total of 900 patients were recruited from either an urban large eye hospital, or a semi-rural optometrist led screening provider, as they were attending their appointment as part of New Zealand Diabetic Eye Screening Programme. The de-identified images were independently graded by three senior specialists, and final results were aggregated using New Zealand grading scheme, which was then converted to referable/non-referable and Healthy/mild/more than mild/sight threatening categories. Results: THEIA™ managed to grade all images obtained during the study. Comparing the adjudicated images from the specialist grading team, “ground truth”, with the grading by the AI platform in detecting “sight threatening” disease, at the patient level THEIA™ achieved 100% imageability, 100% [98.49–100.00%] sensitivity and [97.02–99.16%] specificity, and negative predictive value of 100%. In other words, THEIA™ did not miss any patients with “more than mild” or “sight threatening” disease. The level of agreement between the clinicians and the aggregated results was (k value: 0.9881, 0.9557, and 0.9175), and the level of agreement between THEIA™ and the aggregated labels was (k value: 0.9515). Conclusion: This multi-centre prospective trial showed that THEIA™ did not miss referable disease when screening for diabetic retinopathy and maculopathy. It also had a very high level of granularity in reporting the disease level. As THEIA™ has been tested on a variety of cameras, operating in a range of clinics (rural/urban, ophthalmologist-led\optometrist-led), we believe that it will be a suitable addition to a public diabetic screening program. © 2022, The Author(s)."
"The sudden arrival of COVID-19 called for new technologies to manage the healthcare system and to reduce the burden of patients in the hospitals. Artificial intelligence (AI) which involved using computers to model intelligent behavior became an important choice. Various AI applications helped a lot in the management of healthcare and delivering quick medical consultations and various services to a wide variety of patients. These new technological developments had significant roles in detecting the COVID-19 cases, monitoring them, and forecasting for the future. Artificial intelligence is applied to mimic the functional system of human intelligence. AI techniques and applications are also applied in proper examinations, prediction, analyzing, and tracking of the whereabouts of patients and the projected results. It also played a significant role in recognizing and proposing the generation of vaccines to prevent COVID-19. This study is therefore an attempt to understand the major role and use of AI in healthcare institutions by providing urgent decision-making techniques that greatly helped to manage and control the spread of the COVID-19 disease. © 2023, Institute of Advanced Engineering and Science. All rights reserved."
"Solving Math Word Problems (MWPs) automatically is a challenging task for AI-tutoring in online education. Most of the existing State-Of-The-Art (SOTA) neural models for solving MWPs use Goal-driven Tree-structured Solver (GTS) as their decoders. However, owing to the defects of the tree-structured recurrent neural networks, GTS can not obtain the information of all generated nodes in each decoding time step. Therefore, the performance for long math expressions is not satisfactory enough. To address such limitations, we propose a Goal Selection and Feedback (GSF) decoding module. In each time step of GSF, we firstly feed the latest result back to all goal vectors through goal feedback operation, and then the goal selection operation based on attention mechanism is designed for generate the new goal vector. Not only can the decoder collect the historical information from all generated nodes through goal selection operation, but also these generated nodes are always updated timely by goal feedback operation. In addition, a Multilayer Fusion Network (MFN) is proposed to provide a better representation of each hidden state during decoding. Combining the ELECTRA language model with our novel decoder, experiments on the Math23k, Ape-clean, and MAWPS datasets show that our model outperforms the SOTA baselines, especially on the MWPs of complex samples with long math expressions. The ablation study and case study further verify that our model can better solve the samples with long expressions, and the proposed components are indeed able to help enhance the performance of the model. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"In the present study, multilayer perceptron (MLP) neural network and support vector regression (SVR) models were developed to assess the suitability of groundwater for drinking purposes in the northern Khartoum area, Sudan. The groundwater quality was evaluated by predicting the groundwater quality index (GWQI). GWQI is a statistical model that uses sub-indices and accumulation functions to reduce the dimensionality of groundwater quality data. In the first stage, GWQI was calculated using 11 physiochemical parameters collected from 20 groundwater wells. These parameters include pH, EC, TDS, TH, Cl−, SO4−2, NO3−, Ca+2, Mg+2, Na+, and HCO3−. The primary investigation confirmed that all parameters except for EC and NO3− are beyond the standard limits of the World Health Organization (WHO). The measured GWQI ranged from 21 to 396. As a result, groundwater samples were classified into three classes. The majority of the samples, roughly 75%, projected into the excellent water category; 20% were considered good water and 5% were classified as unsuitable. GWQI models are powerful tools in groundwater quality assessment; however, the computation is lengthy, time-consuming, and often associated with calculation errors. To overcome these limitations, this study applied artificial intelligence (AI) techniques to develop a reliable model for the prediction of GWQI by employing MLP neural network and SVR models. In this stage, the input data were the detected physiochemical parameters, and the output was the computed GWQI. The dataset was divided into two groups with a ratio of 80% to 20% for models training and validation. The predicted (AI) and actual (calculated GWQI) models were compared using four statistical criteria, namely, mean square error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination (R2). Based on the obtained values of the performance measures, the results revealed the robustness and efficiency of MLP and SVR models in modeling GWQI. Consequently, groundwater quality in the north Khartoum area is evaluated as suitable for human consumption except for BH 18, where highly mineralized water is observed. The developed approach is advantageous in groundwater quality evaluation and is recommended to be incorporated in groundwater quality modeling. © 2022, The Author(s)."
"I write a note on the recent Letter by Xu et al. (Opt Lett 47:509, 2022) where an adaptive liquid lens using a novel transparent electrically responsive fluid, dibutyl adipate (DBA), was demonstrated. The DBA liquid lens with a hemispherical plano-convex shape was able to change its curvature according to different input voltages. In this lens, when the DC voltage is removed, the shape of the DBA liquid is restored. This DBA liquid lens has all the features like high transmittance, thermal stability, simple structure, and imaging capability. In this note, it is highlighted that this DBA liquid is not only a promising candidate for fabricating novel adaptive liquid lenses, but it is also very useful for future artificial intelligence (AI)-based photonic devices. © 2022, The Author(s), under exclusive licence to The Optical Society of India."
"Cognitive radio (CR), non-orthogonal multiple access (NOMA), and full-duplex (FD) communications have been considered key technologies for providing spectrum utilization improvement and higher energy efficiency on the Internet of Things (IoT) networks and next-generation communication. However, security concerns are still an issue to be addressed because confidential information is exposed in wireless systems. To solve this problem, we design a novel artificial intelligence (AI)-based framework for maximizing the secrecy energy efficiency (SEE) in FD cooperative relay underlay CR-NOMA systems that are exposed to multiple eavesdroppers. First, we formulate the non-convex SEE optimization problem as bi-level optimization, subject to constraints that satisfy the quality-of-service requirements of secondary users. In particular, the outer problem is solved with ensemble learning (EL) to select the optimal relay. Regarding the inner problem, we propose a quantum particle swarm optimization (QPSO)-based technique to optimize power allocation. In addition, for comparison purposes, we describe a cooperative relay CR network with orthogonal multiple access (OMA), rate-splitting multiple access (RSMA), and half-duplex technologies. Moreover, we evaluate comparative schemes based on machine learning algorithms and swarm intelligence baseline schemes. Furthermore, the proposed EL-aided QPSO-based framework achieves performance close to the optimal solutions, with a meaningful reduction in computation complexity. © 2017 IEEE."
"External audit is undergoing rapid changes where more and more routine tasks are automated with analytics and artificial intelligence (AI) instruments. The paper addresses a research problem of mapping data analytics to audit tasks and develops a framework aligning audit phases and AI and using data analytics in teaching audit with AI. The paper contributes to the literature on using data analytics with AI in knowledge specific areas and particularly critical for emerging audit analytics, which is data analytics in external financial audit application. The paper employs the process model methodology (Wynn and Clarkson, Research in Engineering Design 29:161–202, 2018) and the hybrid approach of curriculum development (Dzuranin et al., Journal of Accounting Education 43:24–39, 2018). The framework is extended further by inclusion of knowledge areas and skills recommendations for each identified stage. This inclusion is linked to the peak accounting body guidelines to ensure compliance with course certification and future job prospects. The developed framework is implemented using audit management platform MindBridge AI. The developed teaching and learning materials show implementation of the framework on the practical level. The developed framework was evaluated in a focus group with accounting academics and industry professionals. Its implementation was evaluated in a series of workshops and a survey with participants. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Automated systems based on artificial intelligence (AI) increasingly support decisions with ethical implications where decision makers need to trust these systems. However, insights regarding trust in automated systems predominantly stem from contexts where the main driver of trust is that systems produce accurate outputs (e.g., alarm systems for monitoring tasks). It remains unclear whether what we know about trust in automated systems translates to application contexts where ethical considerations (e.g., fairness) are crucial in trust development. In personnel selection, as a sample context where ethical considerations are important, we investigate trust processes in light of a trust violation relating to unfair bias and a trust repair intervention. Specifically, participants evaluated preselection outcomes (i.e., sets of preselected applicants) by either a human or an automated system across twelve selection tasks. We additionally varied information regarding imperfection of the human and automated system. In task rounds five through eight, the preselected applicants were predominantly male, thus constituting a trust violation due to potential unfair bias. Before task round nine, participants received an excuse for the biased preselection (i.e., a trust repair intervention). The results of the online study showed that participants have initially less trust in automated systems. Furthermore, the trust violation and the trust repair intervention had weaker effects for the automated system. Those effects were partly stronger when highlighting system imperfection. We conclude that insights from classical areas of automation only partially translate to the many emerging application contexts of such systems where ethical considerations are central to trust processes. © 2022, The Author(s)."
"Background: Valve-sparing aortic root replacement with the David procedure is an alternative to the Bentall procedure in patients with aortic root aneurysm. The aim of this study was to describe our long-term experience with this technique and the predictive factors of late failure. Methods: Between January 1998 and August 2019, 300 consecutive patients underwent a David procedure. Clinical and echocardiographic early- and long-term outcomes were analyzed. Median follow-up was 7.0 years (range, 4.1-11.5), with 98.3% complete. Results: Early mortality was 1%. No early valve-related reoperations occurred. There were 9 cardiac-related deaths and 22 reinterventions (19 valve-related). All patients survived reoperation. In 3 patients reintervention consisted of transcatheter aortic valve implantation. Overall survival rates were 95.3% (95% confidence interval [CI], 92.0-97.2), 91.1% (95% CI, 86.5-94.2), and 82.9% (95% CI, 75.3-88.4) at 5, 10, and 15 years, respectively. Freedom from postoperative aortic insufficiency (AI) grade ≥ 2 was 84.8% (95% CI, 79.9-88.6) and 74.3% (95% CI, 67.4-79.9) at 5 and 10 years, respectively. Freedom from reintervention for aortic valve disease was 97.1% (95% CI, 94.2-98.5), 92.9% (95% CI, 88.2-95.7), and 92.5% (95% CI, 87.1-95.7) at 5, 10, and 15 years, respectively. Preoperative AI ≥ 2 (hazard ratio, 1.782; 95% CI, 1.352-2.350) and a ventriculoaortic junction ≥ 29 mm (hazard ratio, 3.379; 95% CI, 1.726-6.616) were predictive factors for postoperative AI ≥ 2 in a multivariate analysis (P <.001). Conclusions: Preoperative AI ≥ 2 and a ventriculoaortic junction ≥ 29 mm were identified as risk factors for late postoperative AI ≥ 2. © 2023 The Society of Thoracic Surgeons"
"Objective: To determine if the artificial intelligence-based Thyroid Imaging, Reporting and Data System (AI TIRADS) would perform better than the American College of Radiology (ACR) TIRADS in monitoring malignant thyroid nodules not recommended for biopsy using follow-up thresholds. Methods: A total of 3499 thyroid nodules with surgical histopathology and ultrasound features were retrospectively reviewed and categorized using ACR TIRADS and AI TIRADS. The recommendations for biopsy and follow-up divided nodules into three groups 1) fine needle aspiration (FNA), 2) follow-up ultrasound, and 3) no further evaluation. Results: Of the total 1608 malignant nodules in this study, 974 malignant nodules would not be biopsied in ACR TIRADS compared with 967 in AI TIRADS. While 60.0% (584/974) of these non-biopsied malignancies could be followed-up by ultrasound in ACR TIRADS and 62.8% (607/967) in AI TIRADS. For the malignancies of no further evaluation, 97.4% (380/390) were sized <10 mm in ACR TIRADS and 93.3% (336/360) in AI TIRADS. Compared with ACR TIRADS, AI TIRADS had lower unnecessary FNA rate and missing cancer rate (41.0% vs 47.8% and 22.8% vs 27.5%, P <.05, respectively) while having higher specificity and AUC as well as lower sensitivity (65.0% vs 57.9%, 0.895 vs 0.881, and 96.1% vs 97.8%, all P <.05). Conclusions: Using the follow-up thresholds, more than half of the malignancies not being biopsied were monitored by ultrasound in both ACR TIRADS and AI TIRADS, and AI TIRADS had lower missing cancer rate. More than 90% of malignancies recommended for no further evaluation were <10 mm in diameter. © 2022 American Institute of Ultrasound in Medicine."
"Given the popular presupposition of human reasoning as the standard for learning and decision making, there have been significant efforts and a growing trend in research to replicate these innate human abilities in artificial systems. As such, topics including Game Theory, Theory of Mind, and Machine Learning, among others, integrate concepts that are assumed components of human reasoning. These serve as techniques to replicate and understand the behaviors of humans. In addition, next-generation autonomous and adaptive systems will largely include AI agents and humans working together as teams. To make this possible, autonomous agents will require the ability to embed practical models of human behavior, allowing them not only to replicate human models as a technique to ""learn""but also to understand the actions of users and anticipate their behavior, so as to truly operate in symbiosis with them. The main objective of this article is to provide a succinct yet systematic review of important approaches in two areas dealing with quantitative models of human behaviors. Specifically, we focus on (i) techniques that learn a model or policy of behavior through exploration and feedback, such as Reinforcement Learning, and (ii) directly model mechanisms of human reasoning, such as beliefs and bias, without necessarily learning via trial and error.  © 2023 Copyright held by the owner/author(s)."
"Tadeai (Awa-ai, Persicaria tinctoria) is an indigo plant cultivated in large quantities in Tokushima prefecture, Japan. The leaves of Tadeai are used for dyeing (Ai-zome), but there is no use for the other parts, i.e. flowers, stems, and roots, most of which are discarded. In this study, we search for new ways to use Tadeai including parts other than the leaves and examined the antioxidant activity of water extract from Tadeai. To obtain a water extract having a high antioxidant activity efficiently, a pressurized microwave-assisted hydrothermal treatment followed by water extraction was used and examined. The amount of phenolic compounds contained in water extract from Tadeai and its antioxidant activity were evaluated by the Folin–Ciocalteu method and the DPPH radical scavenging activity test, respectively. The effective condition was a processing temperature of 200 °C for a processing time of 2 min, and the highest amount of phenolic compounds contained in water extract was obtained when using a root as a sample. Furthermore, the differences among Tadeai varieties were also examined. The best result of the antioxidant activities was from the root of the “Kojoko Akahana”. Its amount of phenolic compounds and EC50 value were 44.1 mg-catechin equiv./g-dry vegetable sample and 0.018 g/L, respectively. In this study, it was shown that the antioxidant activity of water extract from the parts other than the leaves is high, so it is expected that Tadeai waste will be effectively utilized as a raw material for producing antioxidants by using the pressurized microwave-assisted hydrothermal treatment followed by water extraction in the future. Graphical abstract: [Figure not available: see fulltext.] © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Malaria is a member of a small group of diseases that can be highly severe. The presence of parasites in the environment might lead to sickness in the intestines. To visually identify the parasitemia, is a challenging task. The proposed solution categorizes and analyzes malaria impaired red blood cells. The parasite Plasmodium falciparum is the source of erythrocyte contamination. The system process the data in three phases/stages, the first phase preprocess the data by correcting the difference in luminance, the second phase performs segmentation of image pixels to detect erythrocytes. In the final phase a two stage classification model is used for identification of infected red blood cells. The system utilizes 3000 labeled images as the dataset for the purpose of training and 500 images for testing. The proposed system identifies infectious red blood cells with a Recall of 93% and 98.7% of Specificity. The model could demonstrate a recall of 77.9% and Specificity of 90.9% in identification of the infectious stage. © 2023 Little Lion Scientific."
"Background and Objective: Bronchopulmonary dysplasia (BPD) remains a major cause of morbidity and mortality in very preterm infants though early non-invasive ventilation and surfactant treatment and other neonatal therapies have improved the outcome. Therefore, it is necessary to find effective supplemental methods for prediction of BPD. Better understanding of the etiology and molecular mechanisms involved in the pathogenesis of BPD is necessary for development of new effective early treatments. It is generally accepted that BPD is a multifactorial disease often associated with intrauterine infections and placental perfusion disorders. Methods: Recently a new method to predict BPD at birth using artificial intelligence (AI) has been developed. This new method improves the likelihood of developing effective early treatments for BPD. The method combines information on early surfactant treatment, birth weight and gestational age (GA) with analysis of the mid-infrared spectrum of the molecules in gastric aspirate which are produced in the newborn’s lungs. The described methods for early treatment of BPD in this review among others covers inositol, retinol, super oxide dismutase, Clara cell 10 protein, corticosteroids, azithromycin, macrolide and stem cell therapy besides general treatments as nasal continuous positive airway pressure (NCPAP), surfactant, caffeine, oxygen saturation targeting and nutrition. The literature search was performed systemically online via the databases PubMed and Medline between January 1, 2011 and December 31, 2022. Key Content and Findings: A method to predict BPD immediately after birth is now available allowing possibility to develop effective early treatments of BPD. Conclusions: The present review focuses on early prediction and the existing pharmacologic interventions highlighting the potential to improve the outcome of infants with BPD. © Pediatric Medicine. All rights reserved."
"Background: The diagnostic and prediction criteria of residual hip dysplasia (RHD) remains controversial. There were no studies that focused on the risk factors of RHD after closed reduction (CR) in children with developmental dislocation of the hips (DDH) over 12 months of age. In this study, we assessed the percentage of RHD in DDH patients aged 12 to 18 months vs. that in DDH patients aged over 18 months after CR and determine the predictors of RHD. Meanwhile, we tested the reliability of our RHD criteria compared with Harcke standard. Methods: Patients over 12 months of age who underwent successful CR from October 2011 to November 2017 and followed up for at least 2 years were enrolled. Gender, affected side, age at CR and follow-up time were recorded. Acetabular index (AI), horizontal acetabular width (AWh), center-to-edge angle (CEA), and femoral head coverage (FHC) were measured. The cases were divided into two groups according to whether older than 18 months. RHD was determined according to our criteria. Results: A total of 82 patients (107 hips) were included, including 69 females (84.1%), 13 males (15.9%), 25 patients (30.5%) with bilateral DDH, 33 patients (40.2%) with left side, 24 patients (29.3%) with right side, 40 patients (49 hips) with age 12-18 months, and 42 patients (58 hips) with age >18 months. At a mean follow-up of 47.8 [24-92] months, the percentage of RHD was higher in patients >18 months of age (58.6%) than patients 12-18 months of age (40.8%), but the difference was not statistically significant. Binary logistic regression analysis showed that pre-AI, pre-AWh, and improvement in AI and AWh (P=0.025, 0.016, 0.001, 0.003, respectively) had significant difference. The sensitivity and specialty of our RHD criteria were 81.82% and 82.69%, respectively. Conclusions: For patients with DDH over 18 months, CR is still a choice. We documented four predictors of RHD, suggesting that we should focus on the developmental potential of an individual's acetabulum. Our RHD criteria may be one of the reliable and useful tools in clinical practice to help determine whether to perform continuous observation or surgery, but further research is needed due to limited sample size and follow-up time. © Translational Pediatrics. All rights reserved."
"Many industrialised countries have benefited from the advent of twenty-first century technologies, especially automation, that have fundamentally changed manufacturing and industrial production processes. The next step in the evolution of automation is the development of artificial intelligence (AI), i.e. intelligence which is demonstrated by machines and systems, which cannot only perform tasks but also work synergistically with humans and nature. Intelligent systems that can see, analyse situations and respond sensitively to real-time cues, from human gestures and facial expressions to pedestrians crossing a busy street, will reshape transportation, precision agriculture, biodiversity conservation, environmental modelling, public health, construction and manufacturing, as well as initiatives designed to promote prosperity on Earth. This paper explores the connections between AI systems and sustainable development (SD) research. By means of a literature review, world survey, and case studies, ways in which AI can support research on SD and, inter alia, contribute to a more sustainable and equitable world, are identified. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
"The ongoing boom of applications for artificial intelligence (AI) is based on algorithms that were inspired by neuroscience discoveries in the 1960s. This is a timely book to introduce the new discoveries and ideas in neuroscience, for the next wave of more powerful AI. AI researchers are all interested in the human brain, which is more capable and energy-efficient, but do not have good reading materials from the rather separate subfields of neuroscience, all with plenty of jargons. Based on hundreds of publications from top journals, the book fills in the gap between existing computational hardware/algorithms and emerging knowledge from neuroscience. © 2023 Jenny Stanford Publishing Pte. Ltd. All rights reserved."
"This paper proposes an Advanced Actor-Critic algorithm, which is improved based on the conventional Actor-Critic algorithm, to train the agent to play the complex strategy game StarCraft II. A series of advanced features have been incorporated, including the distributional advantage estimation, information entropy-based uncertainty estimation, self-confidence-based exploration, and normal constraint-based update strategy. A case study including seven StarCraft II mini-games is investigated to identify the effectiveness of the proposed approach, where the famous A3C algorithm is adopted as the comparative baseline. The results verify the superiority of the improved algorithm in accuracy and training efficacy, in complex environment with high-dimensional and hybrid state and action space. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature."
"In order to reduce the production cost of lithium-ion batteries and the damage to the environment, cobalt-free cathode materials have become the focus of the lithium-ion battery industry. High-voltage spinel LiNi0.5Mn1.5O4 (LNMO) as an excellent cathode material has broad application prospects and huge possibility of being put into commercial production, due to its high voltage platform (∼4.7 V vs. Li), high theoretical energy density (∼650 W h kg−1), low environmental impact and low cost. However, the cycle performance and high temperature stability of this cathode are terrible, hindering its large-scale production and use. Recently, numerous approaches have been devoted to improving the cycle stability of high-voltage LNMO cathodes, and some good results have been achieved, which can help us to deepen our understanding of the degradation mechanism of LNMO. In order to highlight this accomplishment and further solve the pending issues of LNMO, it is necessary to summarize the achievements obtained from these studies. In this review, we first introduce the research background, advantages and disadvantages of LNMO cathode materials; secondly, the basic principles of LNMO such as the crystal structure and discharge mechanism were analyzed; then, we discuss the recent advances in boosting the electrochemical performance of LNMO, including structural design, crystal plane regulation, doping, surface coating, etc. Finally, the further perspectives and research directions of LNMO are discussed, such as machine learning and AI-assisted virtual experiments, which provide new insight for the development of LNMO cathode materials. © 2023 The Royal Society of Chemistry."
"Purpose: This paper aims to examine the value relevance (VR) of accounting information (AI) presented by Egyptian listed non-financial companies. Further, the study investigates the influence of institutional ownership on the value relevance of AI in a developing market, namely, the Egyptian market. Design/methodology/approach: The study uses data from 2014 to 2017 with a total of 248 observations and analyses the data using regression analysis. Data are collected from the nonfinancial companies listed on the Egyptian Stock Exchange. Findings: The authors found that the AI reported by the Egyptian listed non-financial companies is value relevant. Regarding the influence of institutional ownership, it is found to significantly impact the VR of AI reported by the sample companies. This model investigated the effect of corporate size and financial leverage as controlling variables and found that they have an insignificant influence on the VR of AI. Originality/value: The current study findings enrich the literature by enhancing the understanding regarding institutional owners’ impact on corporate value. Further, bringing evidence from an emerging market can have implications for accounting researchers interested in addressing other emerging markets with similar contextual and institutional environments. © 2021, Emerald Publishing Limited."
"Solar energy is an infinite source of clean energy and is a growing alternative for fossil fuels nowadays. Electricity generated by solar PV panels is inexhaustible and does not pollute, thus contributing to sustainable development. A stand-alone photovoltaic (SAPV) system is one such designed setup aimed at this prospect. The need for employing a Data Logger (DL) is essential in developing countries where rural or remote areas are not connected to the power grid, since DL helps greatly in rural electrification. The proposed methodology aims to implement a cheap and energy-efficient Arduino-based DL to increase productivity. A DL is an electronic device which is used in the initial stages of solar power generation to help minimize costs through intelligent automation and energy management. DL aids in remote monitoring and controlling data, i.e., Electrical PV parameters automatically over time. This PV DL records various electrical parameters and calculates the energy yield. Such environmental conditions are then measured, documented, analyzed and validated. A user-friendly hardware prototype of a cost-efficient DL is implemented and data is recorded at specific time intervals throughout the day. A mathematical AI model is applied to the data for prediction of missing or null values in the dataset, and the results are analyzed to show the percentage error of predicted data using this model. © 2023 Scrivener Publishing LLC."
[No abstract available]
[No abstract available]
"POET-Inspired Neuroevolutionary System for KreativitY (PINSKY) is a system for open-ended learning through neuroevolution in game-based domains. It builds on the Paired Open-Ended Trailblazer (POET) system, which originally explored learning and environment generation for bipedal walkers, and adapts it to games in the General Video Game AI (GVGAI) system. Previous work showed that by coevolving levels and neural network policies, levels could be found for which successful policies could not be created via optimization alone. Studied in the realm of artificial life as a potentially open-ended alternative to gradient-based fitness, minimal criteria (MC)-based selection helps foster diversity in evolutionary populations. The main question addressed by this article is how the open-ended learning actually works, focusing in particular on the role of transfer of policies from one evolutionary branch ('species') to another. We analyze the dynamics of the system through creating phylogenetic trees, analyzing evolutionary trajectories of policies, and temporally breaking down transfers according to species type. Furthermore, we analyze the impact of the minimal criterion on generated level diversity and interspecies transfer. The most insightful finding is that interspecies transfer, while rare, is crucial to the system's success. © 2018 IEEE."
[No abstract available]
"In this contribution a point cloud classification in an urban context has been presented. The aim of the work is to test a semiautomatic classification approach and to verify its usefulness in the scan-to BIM process, and to validate how much it is straightforward for the definition of different point cloud LODs. The work methodology is structured in three phases. The first concerns data acquisition and processing through geomatic instruments and methodologies that guarantee a complete and expeditious survey such as ground-based MMS and UAV for aerial photogrammetry. The second phase concerns the testing of an online software that performs point cloud classifications through AI algorithms. The system allows either to use standard classifiers that are already available, or to create a customizable catalogue of the different classes that one wants to attribute to the urban scene. Following the automatic classification process, where all objects have been identified, manual corrections can be made to improve the classification of objects into specific classes. The third step is object detection and extraction. Here, the relationship between automatic classification, point cloud density, object identification and the various degrees of LOD definition was explored. The higher the LOD, the greater the number of objects that can be identified, particularly those elements related to street furniture and urban facilities. Once these objects have been classified, it is then possible to extract them in interoperable format. This allows such data to be managed and shared through BIM platform. © Author(s) 2023."
"Gadgets organizations are progressively depending on ""savvy fabricating"" to conquer the difficulties of intricacy, customization, authorization, globalization, and buyer assumptions for close wonderful quality in an industry that requests new items at an uncommon speed. Smart manufacturing is a procedure for expanding modern productivity by fusing PC controls, recreation, enormous information, and different types of robotization. This is for the most part because of expanded mechanization, progressed change, the connecting of automated and true circumstances (as empowered by the Internet of Things or IIoT), and arising current and assembling advances. The fourth mechanical insurgency and the various innovative empowering dealers of that upheaval are glaringly connected to industry 4.0. Furthermore nowadays, the term 'alert' is almost inextricably linked with IoT (which in our mechanical business sectors and savvy plants is known as Industrial IoT), advanced investigation, man-made brainpower and AI, massive data, and all of the innovations of the purported third stage, as well as all of the advances that are standard for fourth modern revolution technology known as Industry 4.0. As the technology's potential applications expand, businesses are beginning to embrace 3D printing to create new commercial models and prospects. This chapter describes the use of 3D printing technology in smart manufacturing for product development. Honeycomb structure is well defined through 3D printing technology is one of the attractive focus area of this chapter. © 2023 Scrivener Publishing LLC."
"Chemical exchange saturation transfer (CEST) MRI has positioned itself as a promising contrast mechanism, capable of providing molecular information at sufficient resolution and amplified sensitivity. However, it has not yet become a routinely employed clinical technique, due to a variety of confounding factors affecting its contrast-weighted image interpretation and the inherently long scan time. CEST MR fingerprinting (MRF) is a novel approach for addressing these challenges, allowing simultaneous quantitation of several proton exchange parameters using rapid acquisition schemes. Recently, a number of deep-learning algorithms have been developed to further boost the performance and speed of CEST and semi-solid macromolecule magnetization transfer (MT) MRF. This review article describes the fundamental theory behind semisolid MT/CEST-MRF and its main applications. It then details supervised and unsupervised learning approaches for MRF image reconstruction and describes artificial intelligence (AI)-based pipelines for protocol optimization. Finally, practical considerations are discussed, and future perspectives are given, accompanied by basic demonstration code and data. © 2022 The Authors. NMR in Biomedicine published by John Wiley & Sons Ltd."
"Context: Business processes have long relied on two types of resources to perform work: humans and information systems (IS). Despite the high performance of IS, still, much of the repetitive work has been done by humans for reasons including implementation cost and technical debt, among others. Now, robots are joining offices as a possible third workforce. Robotic Process Automation (RPA) is already a reality in many organizations, performing much of the work previously dependent on manual labor. Problem: While RPA offers innovative ways to automate work, it is also limited in automating complex activities. To propose solutions, we must understand these limitations by evaluating how organizations are automating their processes with RPA and trying to overcome these problems. Solution: In this study, we seek answers about how organizations are dealing with the limitations of RPA and what technologies are being used to create smarter automation. IS Theory: Work systems theory. Method: Questionnaire survey with professionals regarding the application of RPA in their organizations and a descriptive evaluation of the current state of RPA adoption in the evaluated organizations in Latin America. Summary of Results: While some practitioners have not perceived limitations, others reported obstacles to process unstructured data and automate complex decisions, as well as preparing bots to learn and adapt. AI techniques and business automation technologies have been experimented with as ways to surpass some limitations. Contributions and Impact to IS: This paper provides an overview of the current state of RPA adoption in Latin American organizations and how it is evolving to create smarter, more sustainable automation. © 2023 Copyright held by the owner/author(s)."
"Research Summary: We apply a resource-based view to investigate how the adoption of Artificial Intelligence (AI) affects competitive capabilities and performance. Following prior work on using chess as a controlled setting for studying competitive interactions, we compare the same players’ capabilities and performance across conventional, centaur, and engine chess tournaments. Our analysis shows that AI adoption triggers interrelated substitution and complementation dynamics, which make humans’ traditional competitive capabilities obsolete, while creating new sources of persistent heterogeneity when humans interact with chess engines. These novel human-machine capabilities are unrelated, or even negatively related, to traditional capabilities. We contribute an integrated view of substitution and complementation, which identifies AI as the driver of these dynamics and explains how they jointly shift the sources of competitive advantage. Managerial Summary: AI-based technologies increasingly substitute and complement humans in managerial tasks such as decision making. We investigate how such change affects the sources of competitive advantage. AI-based engines’ adoption in chess allows us to investigate competitive capabilities and performance in human, AI, and hybrid settings. We find that neither humans nor AI in isolation explain performance differences in the AI and hybrid settings. Instead, a new decision-making resource emerges at the human-AI intersection, which drives performance but is unrelated or even negatively related to humans’ original capability. Our results document how AI adoption changes the sources of competitive advantage and, in turn, requires managers to develop new capabilities to stay relevant in an AI-based competitive landscape. © 2022 The Authors. Strategic Management Journal published by John Wiley & Sons Ltd."
"Breast cancer is the second leading cause of death in females. As such, women have high incidence and mortality rates of breast cancer. The incidence rate has been on the rise over time. The earlier breast cancer is caught, the better it shows prognosis and the lower the mortality rate is. For this reason, many researchers and medical doctors have heeded a lot of attention to the CAD systems to detect and classify breast cancer. They have proposed a myriad of methods and techniques. Among them, the CAD system based on artificial intelligence (AI) can process plenty of information fast, and its performance is evaluated to be high. As an AI algorithm, YOLO has excellent detection performance and can detect objects effectively in real time. In this paper, we proposed an anomaly detection model of mammography using a YOLOv4-based histogram. In terms of breast cancer diagnosis, mammography features a fast diagnosis time and an inexpensive cost. For this reason, it is often applied to breast cancer diagnosis. Mammography, however, generates an image only with brightness values, so that a mammogram image has a lot of noise and image edges are dim. To enhance these image edges, we create a difference through histogram and brightness range control and threshold-based region removal methods and expand the single channel of mammogram images using the generated images. Through the expansion, the image edges are enhanced and converted into a single channel again and are learned through YOLO. For performance evaluation, the method proposed in this study is compared with ResNet18, ResNet50, GoogleNet, and VGG16. According to an experiment, the proposed method had the highest accuracy, or 95.74%, followed by GoogleNet (89.9%), VGG16 (88.93%), ResNet50 (87.77%), and ResNet18 (87.67%) in order. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
"Algorithmic decision-making and big data systems are increasingly being used to provide innovative and essential services in the public sector. Such public services that utilize AI entail many related risks and responsibilities for citizens and public sector providers. Furthermore, the distinct regulatory demands and responsibilities of public sector services require crucial consideration of inclusiveness and civic empowerment. In this empirical study, we examine practitioners' attitudes, practices and challenges of implementing inclusive AI services in the public sector that can empower greater civic agency. We conducted in-depth interviews with ten practitioners responsible for managing, developing or designing AI-enabled public services across three big public organizations in Finland in domains relating to the municipality, taxes, and social insurance. The results show that the discussion on inclusion and civic empowerment is just in its beginning in the public sector. Practitioners perceive the concept of inclusion as devising accessible public services for all members of society. Civic empowerment was understood as 1) institutional transparency, 2) civic participation in shaping the services, and 3) easing the use of the services for their users. The research suggests two distinct socio-cultural constructs emerging among practitioners that may influence (or hinder) how civic empowerment is manifested in such services: expert cultures and risk-averse cultures. The contributions of the study are twofold. First, we describe the practitioners' perspectives on empowerment and inclusion in regard to public sector AI. Further, we recognize how expert and risk-averse cultures among practitioners explain their actions and restraints in devising public sector AI services.  © 2023 Owner/Author."
"Recent research works have shown the robustness towards the recommendation system for athletics using an AI automated system that enhances the longevity of the system. With this model, the automated recommendation helps to improve the quality of athletes during the process of training or other processes. Moreover, domain experts only can understand the rationale of the recommender system where the analyzed data is stored in the cloud system. This research proposes a machine learning–based solution for an athletic dataset that automatically predicts the state of the individual with features like age, gender, calories, temperature, pressure, heart rate, pulse rate, sugar level, respiratory conditions, and state of the body. This research concentrates on modeling a framework for implementing the machine learning approaches with an optimization problem. Here, a novel extreme multi-gradient evolutionary computation (EMGEC) with improved grey wolf optimization (IGWO) is proposed to achieve exploration and exploitation during the selection of features. The dataset collected from the athletes during the marathon (running) is collected from online resources and the feature subsets are extracted from the dataset. The features of these data are analyzed and encoded before placing it over the cloud environment. The performance of the proposed machine learning approach is compared with other approaches and provides better prediction accuracy, precision, recall, and F-measure respectively. The accuracy of the anticipated model is 83.13%, precision is 91.1%, and recall is 91.3% which is substantially higher than of other approaches. The proposed model shows a better trade-off in contrast to prevailing approaches like SVM, RF, k-NN, and logistic regression. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
"When Deep Neural Networks (DNNs) are used in safety-critical systems, engineers should determine the safety risks associated with failures (i.e., erroneous outputs) observed during testing. For DNNs processing images, engineers visually inspect all failure-inducing images to determine common characteristics among them. Such characteristics correspond to hazard-triggering events (e.g., low illumination) that are essential inputs for safety analysis. Though informative, such activity is expensive and error prone.To support such safety analysis practices, we propose Simulator-based Explanations for DNN failurEs (SEDE), a technique that generates readable descriptions for commonalities in failure-inducing, real-world images and improves the DNN through effective retraining. SEDE leverages the availability of simulators, which are commonly used for cyber-physical systems. It relies on genetic algorithms to drive simulators toward the generation of images that are similar to failure-inducing, real-world images in the test set; it then employs rule learning algorithms to derive expressions that capture commonalities in terms of simulator parameter values. The derived expressions are then used to generate additional images to retrain and improve the DNN.With DNNs performing in-car sensing tasks, SEDE successfully characterized hazard-triggering events leading to a DNN accuracy drop. Also, SEDE enabled retraining leading to significant improvements in DNN accuracy, up to 18 percentage points. © 2023 Association for Computing Machinery."
"This article explores the value of simulation for autonomous-vehicle research and development. There is ample research that details the effectiveness of simulation for training humans to fly and drive. Unfortunately, the same is not true for simulations used to train and test artificial intelligence (AI) that enables autonomous vehicles to fly and drive without humans. Research has shown that simulation ""fidelity""is the most influential factor affecting training yield, but psychological fidelity is a widely accepted definition that does not apply to AI because it describes how well simulations engage various cognitive functions of human operators. Therefore, this investigation reviewed the literature that was published between January 2010 and May 2022 on the topic of simulation fidelity to understand how researchers are defining and measuring simulation fidelity as applied to training AI. The results reported herein illustrate that researchers are generally using agreed-upon terms such as physical fidelity, but there is an emerging definition of functional fidelity that is being adopted to replace the concept of psychological fidelity for training AI instead of humans. © 2023 SAE International."
"This article argues that it is far from trivial to convert social science concepts into accurate categories on which algorithms work best. The literature raises this concern in a general way; for example, Deeks notes that legal concepts, such as proportionality, cannot be easily converted into code noting that ‘The meaning and application of these concepts is hotly debated, even among lawyers who share common vocabularies and experiences’ (Deeks in Va Law Rev 104, pp. 1529–1593, 2018). The example discussed here is recidivism prediction, where the factors that are of interest are difficult to capture adequately through questionnaires because survey responses do not necessarily indicate whether the behaviour that is of interest is present. There is room for improvement in how questions are phrased, in the selection of variables, and by encouraging practitioners to consider whether a particular variable is the sort of thing that can be measured by questionnaires at all. © 2022, The Author(s)."
"As clean water can be considered among the essentials of human life, there is always a requirement to seek its foremost and high quality. Water primarily becomes polluted due to organic as well as inorganic pollutants, including nutrients, heavy metals, and constant contamination with organic materials. Predicting the quality of water accurately is essential for its better management along with controlling pollution. With stricter laws regarding water treatment to remove organic and biologic materials along with different pollutants, looking for novel technologic procedures will be necessary for improved control of the treatment processes by water utilities. Linear regression-based models with relative simplicity considering water prediction have been typically used as available statistical models. Nevertheless, in a majority of real problems, particularly those associated with modeling of water quality, non-linear patterns will be observed, requiring non-linear models to address them. Thus, artificial intelligence (AI) can be a good candidate in modeling and optimizing the elimination of pollutants from water in empirical settings with the ability to generate ideal operational variables, due to its recent considerable advancements. Management and operation of water treatment procedures are supported technically by these technologies, leading to higher efficiency compared to sole dependence on human operations. Thus, establishing predictive models for water quality and subsequently, more efficient management of water resources would be critically important, serving as a strong tool. A systematic review methodology has been employed in the present work to investigate the previous studies over the time interval of 2010–2020, while analyzing and synthesizing the literature, particularly regarding AI application in water treatment. A total number of 92 articles had addressed the topic under study using AI. Based on the conclusions, the application of AI can obviously facilitate operations, process automation, and management of water resources in significantly volatile contexts. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature."
"Aiming at the problems of poor model performance and low training efficiency in training streaming data of AI models that provide intelligent services, a high-performance federated continual learning algorithm for heterogeneous streaming data (FCL-HSD) was proposed in the distributed terminal system with privacy data. In order to solve the problem of the current model forgetting old data, a model with dynamically extensible structure was introduced in the local training stage, and an extension audit mechanism was designed to ensure the capability of the AI model to recognize old data at the cost of small storage overhead. Considering the heterogeneity of terminal data, a customized global model strategy based on data distribution similarity was designed at the central server side, and an aggregation-by-block manner was implemented for different modules of the model. The feasibility and effectiveness of the proposed algorithm were verified under various data increment scenarios with different data sets. Experimental results show that, compared with existing works, the proposed algorithm can effectively improve the model performance to classify old data on the premise of ensuring the capability to classify new data. © 2023 Editorial Board of Journal on Communications. All rights reserved."
"Objectives: Applications of artificial intelligence (AI) have the potential to improve aspects of healthcare. However, studies have shown that healthcare AI algorithms also have the potential to perpetuate existing inequities in healthcare, performing less effectively for marginalised populations. Studies on public attitudes towards AI outside of the healthcare field have tended to show higher levels of support for AI among socioeconomically advantaged groups that are less likely to be sufferers of algorithmic harms. We aimed to examine the sociodemographic predictors of support for scenarios related to healthcare AI. Methods: The Australian Values and Attitudes toward AI survey was conducted in March 2020 to assess Australians' attitudes towards AI in healthcare. An innovative weighting methodology involved weighting a non-probability web-based panel against results from a shorter omnibus survey distributed to a representative sample of Australians. We used multinomial logistic regression to examine the relationship between support for AI and a suite of sociodemographic variables in various healthcare scenarios. Results: Where support for AI was predicted by measures of socioeconomic advantage such as education, household income and Socio-Economic Indexes for Areas index, the same variables were not predictors of support for the healthcare AI scenarios presented. Variables associated with support for healthcare AI included being male, having computer science or programming experience and being aged between 18 and 34 years. Other Australian studies suggest that these groups may have a higher level of perceived familiarity with AI. Conclusion: Our findings suggest that while support for AI in general is predicted by indicators of social advantage, these same indicators do not predict support for healthcare AI. © 2023 BMJ Publishing Group. All rights reserved."
"Context: Task-oriented conversational systems demand a high volume of data to understand human language. One of the major challenges of Natural Language Processing (NLP) is the lack of structured annotated data to improve and refine language models, therefore, institutions often generate or mine their own data and have to annotate it themselves. Problem: The annotation process is time-consuming and costly process that usually results in errors due to human fatigue and often acts as the blocking phase for many smaller teams developing AI. Companies frequently report scarcity and poor data quality when developing these systems. Solution: This paper presents Assis, a modular, adaptable tool for semi-automatic annotation (manual and AI annotation). The tool automates and organizes the intentions and entities in task-oriented conversations. Our proposal combines components that facilitate the visual assimilation of the annotation process. Assis can be embedded with continuously refined language models based on previously annotated sentences. IS theory: Assis was developed with the idea of Design Theory in mind, using its base of knowledge to evaluate the existing and proposed tools to its goal of facilitating annotation. Method: Empirical results from user experience in real-life case studies and satisfaction with both the annotation results as well as the user experience, in comparison to the same study groups conducting the annotation without tools or in another software, using a feedback form after use. Results: During one of the case studies, the tool was used to annotate more than 800 messages, with user feedback relating a high satisfaction with the reduction of the required time. Contributions and Impact in the IS area: The tool innovates with its deployless architecture, modularity and adaptability, while introducing two new concepts for text annotation: dialogue topics and entity propagation. © 2023 Copyright held by the owner/author(s)."
"The wear of cutting tools, cutting force determination, surface roughness variations and other machining responses are of keen interest to latest researchers. The variations of these machining responses results in change in dimensional accuracy and productivity upto great extent. In addition, an excessive increase in wear leads to catastrophic consequences, exceeding the tool breakage. Therefore, this article discusses the online trend of modern approaches in tool condition monitoring while different machining operations. For this purpose, the effective use of new sensors and artificial intelligence (AI) is considered and followed during this holistic review work. The sensor systems used for monitoring tool wear are dynamometers, accelerometers, acoustic emission sensors, current and power sensors, image sensors, other sensors. These systems allow to solve the problem of automation and modeling of technological parameters of the main types of cutting, such as turning, milling, drilling and grinding. The modern artificial intelligence methods are considered, such as: Neural networks, Image recognition, Fuzzy logic, Adaptive neuro-fuzzy inference systems, Bayesian Networks, Support vector machine, Ensembles, Decision and regression trees, k-nearest neighbors, Artificial Neural Network, Markov model, Singular Spectrum Analysis, Genetic algorithms. Discussions also includes the main advantages, disadvantages and prospects of using various AI methods for tool wear monitoring. Moreover, the problems and future directions of the main processing methods using AI models are also highlighted. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Epidemiologic studies detected an inverse relationship between HDL (high-density lipoprotein) cholesterol (HDL-C) levels and atherosclerotic cardiovascular disease (ASCVD), identifying HDL-C as a major risk factor for ASCVD and suggesting atheroprotective functions of HDL. However, the role of HDL-C as a mediator of risk for ASCVD has been called into question by the failure of HDL-C-raising drugs to reduce cardiovascular events in clinical trials. Progress in understanding the heterogeneous nature of HDL particles in terms of their protein, lipid, and small RNA composition has contributed to the realization that HDL-C levels do not necessarily reflect HDL function. The most examined atheroprotective function of HDL is reverse cholesterol transport, whereby HDL removes cholesterol from plaque macrophage foam cells and delivers it to the liver for processing and excretion into bile. Indeed, in several studies, HDL has shown inverse associations between HDL cholesterol efflux capacity and ASCVD in humans. Inflammation plays a key role in the pathogenesis of atherosclerosis and vulnerable plaque formation, and a fundamental function of HDL is suppression of inflammatory signaling in macrophages and other cells. Oxidation is also a critical process to ASCVD in promoting atherogenic oxidative modifications of LDL (low-density lipoprotein) and cellular inflammation. HDL and its proteins including apoAI (apolipoprotein AI) and PON1 (paraoxonase 1) prevent cellular oxidative stress and LDL modifications. Importantly, HDL in humans with ASCVD is oxidatively modified rendering HDL dysfunctional and proinflammatory. Modification of HDL with reactive carbonyl species, such as malondialdehyde and isolevuglandins, dramatically impairs the antiatherogenic functions of HDL. Importantly, treatment of murine models of atherosclerosis with scavengers of reactive dicarbonyls improves HDL function and reduces systemic inflammation, atherosclerosis development, and features of plaque instability. Here, we discuss the HDL antiatherogenic functions in relation to oxidative modifications and the potential of reactive dicarbonyl scavengers as a therapeutic approach for ASCVD. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"Reinforcement learning (RL) algorithms have performed well in playing challenging board and video games. More and more studies focus on improving the generalization ability of RL algorithms. The General Video Game AI (GVGAI) Learning Competition aims to develop agents capable of learning to play different game levels that were unseen during training. This article summarizes the five years' GVGAI Learning Competition editions. At each edition, three new games were designed. The training and test levels were designed separately in the first three editions. Since 2020, three test levels of each game were generated by perturbing or combining two training levels. Then, we present a novel RL technique with dual-observation for general video game playing, assuming that it is more likely to observe similar local information in different levels rather than global information. Instead of directly inputting a single, raw pixel-based screenshot of the current game screen, our proposed general technique takes the encoded, transformed global, and local observations (LOs) of the game screen as two simultaneous inputs, aiming at learning local information for playing new levels. Our proposed technique is implemented with three state-of-the-art RL algorithms and tested on the game set of the 2020 GVGAI Learning Competition. Ablation studies show the outstanding performance of using encoded, transformed global, and LOs as input. © 2018 IEEE."
"In order to allow users to incorrectly identify images by manipulating them using deep neural networks, this paper analyses the shortcomings of deep learning for image classification. It also develops a game that uses this technique. In the game, players can select one of their preferred product categories, causing the model to classify other product categories incorrectly as the one they selected. The goal of this game is to demonstrate to players the limitations of AI. We evaluate these programs based on their overall effectiveness, user satisfaction, and achievement of their objectives. The results show that this program is a successful method for arousing curiosity and stimulating thought. They can learn to appreciate the limitations of AI and the need to prioritize AI security in their daily activities. © 2023 - IOS Press. All rights reserved."
"We live in an algorithmic society. Algorithms have become the main mediator through which power is enacted in our society. This book brings together three academic fields - Public Administration, Criminal Justice and Urban Governance - into a single conceptual framework, and offers a broad cultural-political analysis, addressing critical and ethical issues of algorithms. Governments are increasingly turning towards algorithms to predict criminality, deliver public services, allocate resources, and calculate recidivism rates. Mind-boggling amounts of data regarding our daily actions are analysed to make decisions that manage, control, and nudge our behaviour in everyday life. The contributions in this book offer a broad analysis of the mechanisms and social implications of algorithmic governance. Reporting from the cutting edge of scientific research, the result is illuminating and useful for understanding the relations between algorithms and power.Topics covered include: Algorithmic governmentality Transparency and accountability Fairness in criminal justice and predictive policing Principles of good digital administration Artificial Intelligence (AI) in the smart city This book is essential reading for students and scholars of Sociology, Criminology, Public Administration, Political Sciences, and Cultural Theory interested in the integration of algorithms into the governance of society. © 2023 selection and editorial matter, Michael Filimowicz. All rights reserved."
[No abstract available]
"American Indian (AI) adolescents experience disproportionate alcohol-related consequences. The present study evaluated the psychometric properties and application of the American Drug and Alcohol Survey (ADAS™) alcohol-related consequence scale for AI adolescents through a secondary analysis of a large population-based sample of adolescents living on or near AI reservations. We found support for the ADAS alcohol-related consequence scale as a one-factor model, invariant discretely across race, sex assigned at birth, and age, and with good internal consistency. Evidence for construct validity was found through significant positive correlations between frequency of past 12 months of drinking, frequency of past 12 months of intoxication, and lifetime alcohol-related consequences. AI adolescents were significantly more likely to report more alcohol-related consequences than their non-Hispanic White peers. Race significantly interacted with frequency of drinking in predicting alcohol-related consequences such that these associations were stronger for AI adolescents. However, race did not significantly interact with frequency of intoxication in predicting alcohol-related consequences. Results from this study demonstrate the utility of the ADAS alcohol-related consequence scale for use across demographic groups with little risk of measurement bias. © The Author(s) 2022."
"Artificial intelligence (AI) transits from merely adopted technology to fueling everyday decision-making systems from medication to navigation. With this combination of AI in decision-making systems (ADMS), the present study explores how text-based users' data from social media helps organize the users' perspectives of ADMS? To investigate our research questions, we used a framework consisting of three phases, exploratory, confirmatory, and validatory. We applied hierarchy clustering and topic modeling in the exploratory study, hypothesis building, and empirical analysis during the confirmatory study and support vector machine (SVM) in the validatory study. Our findings suggest that users are primarily concerned about the risk involved in using ADMS. Factors like accountability, self-efficacy, knowledge of ADMS individuals' attitudes towards ADMS impact the perception of ADMS among individuals. This study's theoretical and practical implications have great scope as ADMS is still in its elementary stage. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Cardiovascular diseases are the leading cause of death globally and contribute significantly to the cost of healthcare. Artificial intelligence (AI) is poised to reshape cardiology. Using supervised and unsupervised learning, the two main branches of AI, several applications have been developed in recent years to improve risk prediction, allow large-scale analysis of medical data, and phenotype patients for personalized medicine. In this review, we examine the key advances in AI in cardiology and its limitations regarding bias in the data, standardization in reporting, data access, and model trust and accountability in cases of error. Finally, we discuss implementation methods to unleash AI’s potential in making healthcare more accurate and efficient. Graphical Abstract: Several steps need to be followed and challenges overcome in order to successfully integrate AI in clinical practice and ensure its longevity [Figure not available: see fulltext.]. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
Objective：To discuss the therapeutic effect of Saposhnikoviae Radix on rheumatoid arthritis and its effect on the nuclear factor-κB（NF-κB）signaling pathway，and to provide the basis for the treatment of rheumatoid arthritis. Methods：Fifty rats were randomly divided into control group，model group，dexamethasone group，low dose of Saposhnikoviae Radix group，and high dose of Saposhnikoviae Radix group，and there were 10 rats in each group；except for control group，the rats in the other groups were given intradermal injection of type Ⅱ collagen at the root of the tail and left posterior foot to prepare the rheumatoid arthritis models；after the models were prepared successfully，the rats in dexamethasone group were given dexamethasone（2 mg·kg－1），the rats in low and high doses of Saposhnikoviae Radix groups were given Saposhnikoviae Radix wild product （ 0. 45 and 0. 90 g·kg－1），and equal volume of normal saline was given to the rats in control group and model group. The arthritis index（AI）and foot swelling degrees of the rats in various groups were recorded，the organ coefficients of the rats in various groups were calculated；enzyme-linked immunosorben assay（ELISA）method was used to detect the levels of tumor necrosis factor-α（TNF-α），interleukin-1β（IL-1β），and interleukin-6（IL-6）in serum of the rats in various groups，and the expression levels of NF- κB，NF- κB inhibitor α（IkB- α），IκB- α kinase β（IKK- β），cyclooxidase-2（COX-2），and TNF-α proteins in synovial tissue of the rats in various groups were detected by Western blotting method. Results：Compared with control group，the AI score and swelling degree of foot of the rats in model group were significantly decreased （P<0. 01），while the thymus index was significantly increased（P<0. 01）；compared with model group，the AI score and swelling degree of foot of the rats in dexamethasone group and low and high doses of Saposhnikoviae Radix groups were significantly decreased（P<0. 05 or P<0. 01），and the thymus indexes were significantly decreased（P< 0. 01）. The ELISA results showed that compared with control group，the levels of TNF-α，IL-1β，and IL-6 in serum of the rats in model group were increased（P<0. 01）；compared with model group，the levels of TNF- α， IL-1β，and IL-6 in serum of the rats in dexamethasone group， low and high doses of Saposhnikoviae Radix groups were significantly decreased（P<0. 01）. The HE staing results showed that the synovial tissue of the rats in control group was normal，and the synovial tissue of the rats in model group showed hyperplasia accompanied with extensive infiltration of inflammatory cells；compared with model group，the degrees of proliferation of synovial tissue and infiltration of inflammatory cells of the rats in dexamethasone group，low and high doses of Saposhnikoviae Radix groups were decreased. The Western blotting results showed that compared with control group，the expression levels of NF-κB，IKK-β，COX-2，and TNF-α proteins in synovial tissue of the rats in model group were increased（P<0. 01），the expression level of IκB-α protein was decreased（P<0. 01）；compared with model group，the expression levels of NF- κB，IKK- β，COX-2，and TNF- α proteins in synovial tissue of the rats in dexamethasone group，low and high doses of Saposhnikoviae Radix groups were decreased（P<0. 01），the expression level of IκB- α protein was increased（P<0. 01）. Conclusion：Saposhnikoviae Radix wild product can improve the joint injury by reducing the levels of proinflammatory cytokines and inhibiting the excessive activation of NF-κB signaling pathway in synovial tissue，thus playing a therapeutic role in the rheumatoid arthritis. © 2023 Authors. All rights reserved.
"There is robust evidence for the clinical efficacy of telemedicine in heart failure (HF) patients to reduce mortality and morbidity. For the first time, the Federal Joint Committee (G-BA) has approved telemedicine for HF patients as a digital method of care for a well-defined heart failure population. Patients with HF and a reduced left ventricular ejection fraction (LVEF) < 40 % are now eligible for telemonitoring in a real-world settings of out-patient care in Germany. The implementation of telemedicine in the German health care system is a complex process including the introduction of telemedical technologies, educational programs for the patients as well as the implementation of standard operating procedures (SOPs) for the staff of telemedical centers. The ongoing research in telemedicine in HF patients is focusing on three issues: a) research to extend the suitable HF-population for telemonitoring; b) research on new telemedical sensor technologies, e.g. a new pulmonary pressure measurement system (Cordella) and a system for wireless measurement of left atrial pressure (V-LAP); c) the introduction of methods of artifical intelligence (AI), e.g. the AI-based speech analysis using a smart phone to characterize the pulmonary fluid status. © 2023 Georg Thieme Verlag. All rights reserved."
"Event-based neural networks are currently being explored as efficient solutions for performing AI tasks at the extreme edge. To fully exploit their potential, event-based neural networks coupled to adequate preprocessing must be investigated. Within this context, we demonstrate a 4-b-weight spiking neural network (SNN) for radar gesture recognition, achieving a state-of-the-art 93% accuracy within only four processing time steps while using only one convolutional layer and two fully connected layers. This solution consumes very little energy and area if implemented in event-based hardware, which makes it suited for embedded extreme-edge applications. In addition, we demonstrate the importance of signal preprocessing for achieving this high recognition accuracy in SNNs compared to deep neural networks (DNNs) with the same network topology and training strategy. We show that efficient preprocessing prior to the neural network is drastically more important for SNNs compared to DNNs. We also demonstrate, for the first time, that the preprocessing parameters can affect SNNs and DNNs in antagonistic ways, prohibiting the generalization of conclusions drawn from DNN design to SNNs. We demonstrate our findings by comparing the gesture recognition accuracy achieved with our SNN to a DNN with the same architecture and similar training. Unlike previously proposed neural networks for radar processing, this work enables ultralow-power radar-based gesture recognition for extreme-edge devices.  © 2012 IEEE."
"Spalt-like transcription factor 4 (SALL4) is an oncofetal protein that has been identified to drive cancer progression in hepatocellular carcinoma (HCC) and hematological malignancies. Furthermore, a high SALL4 expression level is correlated to poor prognosis in these cancers. However, SALL4 lacks well-structured small-molecule binding pockets, making it difficult to design targeted inhibitors. SALL4-induced expression of oxidative phosphorylation (OXPHOS) genes may serve as a therapeutically targetable vulnerability in HCC through OXPHOS inhibition. Because OXPHOS functions through a set of genes with intertumoral heterogeneous expression, identifying therapeutic sensitivity to OXPHOS inhibitors may not rely on a single clear biomarker. Here, we developed a workflow that utilized molecular beacons, nucleic-acid-based, activatable sensors with high specificity to the target mRNA, delivered by nanodiamonds, to establish an artificial intelligence (AI)-assisted platform for rapid evaluation of patient-specific drug sensitivity. Specifically, when the HCC cells were treated with the nanodiamond-medicated OXPHOS biosensor, high sensitivity and specificity of the sensor allowed for improved identification of OXPHOS expression in cells. Assisted by a trained convolutional neural network, drug sensitivity of cells toward an OXPHOS inhibitor, IACS-010759, could be accurately predicted. AI-assisted OXPHOS drug sensitivity assessment could be accomplished within 1 day, enabling rapid and efficient clinical decision support for HCC treatment. The work proposed here serves as a foundation for the patient-based subtype-specific therapeutic research platform and is well suited for precision medicine. © 2023 American Chemical Society. All rights reserved."
"Purpose: Various consumer-facing artificial intelligence (AI) applications are used to interact with consumers at all purchase stages, and related research has sharply increased. This study aims to synthesize the literature related to consumer–AI interaction using the customer journey framework, identify the factors affecting AI's effectiveness in interactive marketing and offer an agenda for future research. Design/methodology/approach: This study undertakes a framework-based systematic review of 239 articles on AI in marketing from the consumer perspective published in peer-reviewed journals from 2007 to 2021. Findings: This review identifies the roles of AI touch points and factors affecting the acceptance and effectiveness of consumer–AI interaction in each stage of the customer journey. Originality/value: This study is the first to review the existing literature using a customer journey framework to identify the factors that influence customer interactions with AI touch points at each purchase stage and pave the way for future research. © 2022, Emerald Publishing Limited."
"Artificial intelligence (AI) is rapidly transforming the healthcare and medical and dental education sectors. With advancements in AI technology and its integration into routine tasks, the field of healthcare and education is rapidly evolving. This article aims to provide an in-depth analysis of the impact of AI in these sectors and to discuss the advantages and disadvantages of its integration. The article will begin by examining the use of AI in healthcare, including its impact on patient care, diagnosis and treatment, and the benefits it brings to medical professionals and patients alike. The article will then delve into the use of AI in medical and exploring its impact on student learning and teaching practices, and the benefits and challenges it presents for educators and students. Additionally, this article will also cover the impact of AI on the publishing of scientific articles in journals. With the increasing volume of submissions and the need for more efficient management, AI is being utilised to streamline the peer-review process and improve the quality of peer-review. The article will also delve into the possibility of AI enabling new forms of publication and supporting reproducibility, helping to improve the overall quality of scientific publications. Furthermore, the authors of this article have written it using AI, making it a landmark paper that showcases the true technological power of AI in the field of writing. © 2023, The Author(s)."
"Intelligent writing assistants use artificial intelligence to support the partial automation of the writing process. Existing research has investigated the interaction between humans and automated systems and has identified the maintenance of situation awareness (SA) as a key challenge for humans. Especially in the context of intelligent writing assistants, humans have to maintain SA as they are held responsible for the written text. Eye tracking is the key technology that enables the non-invasive detection of situation awareness based on eye movements. Building on existing research on human-robot/AI collaboration and their interplay with SA theory, we propose the augmentation of human interaction with intelligent writing assistants through the use of eye tracking technology. On this basis, writing assistants can be adapted to users' cognitive states such as SA. We argue that for the successful implementation of intelligent writing assistants in the real world, eye-based analysis of SA and augmentation are key.  © 2023 Owner/Author."
[No abstract available]
"Approximately one billion individuals suffer from mental health disorders, such as depression, bipolar disorder, schizophrenia, and anxiety. Mental health professionals use various assessment tools to detect and diagnose these disorders. However, these tools are complex, contain an excessive number of questions, and require a significant amount of time to administer, leading to low participation and completion rates. Additionally, the results obtained from these tools must be analyzed and interpreted manually by mental health professionals, which may yield inaccurate diagnoses. To this extent, this research utilizes advanced analytics and artificial intelligence to develop a decision support system (DSS) that can efficiently detect and diagnose various mental disorders. As part of the DSS development process, the Network Pattern Recognition (NEPAR) algorithm is first utilized to build the assessment tool and identify the questions that participants need to answer. Then, various machine learning models are trained using participants’ answers to these questions and other historical data as inputs to predict the existence and the type of their mental disorder. The results show that the proposed DSS can automatically diagnose mental disorders using only 28 questions without any human input, to an accuracy level of 89%. Furthermore, the proposed mental disorder diagnostic tool has significantly fewer questions than its counterparts; hence, it provides higher participation and completion rates. Therefore, mental health professionals can use this proposed DSS and its accompanying assessment tool for improved clinical decision-making and diagnostic accuracy. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Introduction Cardiovascular disease (CVD) prevention relies on timely identification of and intervention for individuals at risk. Risk assessment models such as the Framingham Risk Score (FRS) have been shown to over-estimate or under-estimate risk in certain groups, such as socioeconomically disadvantaged populations. Artificial intelligence (AI) and machine learning (ML) could be used to address such equity gaps to improve risk assessment; however, critical appraisal is warranted before ML-informed clinical decision-making is implemented. Methods and analysis This study will employ an equity-lens to identify sources of bias (ie, race/ethnicity, gender and social stratum) in ML algorithms designed to improve CVD risk assessment relative to the FRS. A comprehensive literature search will be completed using MEDLINE, Embase and IEEE to answer the research question: do AI algorithms that are designed for the estimation of CVD risk and that compare performance with the FRS address the sources of bias inherent in the FRS? No study date filters will be imposed on the search, but English language filters will be applied. Studies describing a specific algorithm or ML approach that provided a risk assessment output for coronary artery disease, heart failure, cardiac arrhythmias (ie, atrial fibrillation), stroke or a global CVD risk score, and that compared performance with the FRS are eligible for inclusion. Papers describing algorithms for the diagnosis rather than the prevention of CVD will be excluded. A structured narrative review analysis of included studies will be completed. Ethics and dissemination Ethics approval was not required. Ethics exemption was formally received from the General Research Ethics Board at Queen's University. The completed systematic review will be submitted to a peer-reviewed journal and parts of the work will be presented at relevant conferences.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ."
"Gas therapy based on nitric oxide (NO) has emerged as a potential therapeutic approach for cancer, and in conjunction with multi-mode combination therapy, offers new possibilities for achieving significant hyperadditive effects. In this study, an integrated AI-MPDA@BSA nanocomposite for diagnosis and treatment was constructed for PDA based photoacoustic imaging (PAI) and cascade NO release. Natural NO donor L-arginine (L-Arg) and photosensitizer (PS) IR780 were loaded into mesoporous polydopamine (MPDA). Bovine serum albumin (BSA) was conjugated to the MPDA to increase the dispersibility and biocompatibility of the nanoparticles, as well as to serve as a gatekeeper controlling IR780 release from the MPDA pores. The AI-MPDA@BSA produced singlet oxygen (1O2) and converted it into NO through a chain reaction based on L-Arg, enabling a combination of photodynamic therapy and gas therapy. Moreover, due to the photothermal properties of MPDA, the AI-MPDA@BSA performed good photothermal conversion, which allowed photoacoustic imaging. As expected, both in vitro and in vivo studies have confirmed that the AI-MPDA@BSA nanoplatform has a significant inhibitory effect on cancer cells and tumors, and no apparent systemic toxicity or side effects were detected during the treatment period. © 2023"
"Machine-learning-based systems are now part of a wide array of real-world applications seamlessly embedded in the social realm. In the wake of this realization, strict legal regulations for these systems are currently being developed, addressing some of the risks they may pose. This is the coming of age of the concepts of interpretability and explainability in machine-learning-based data analysis, which can no longer be seen just as an academic research problem. In this paper, we discuss explainable and interpretable machine learning as post hoc and ante-hoc strategies to address regulatory restrictions and highlight several aspects related to them, including their evaluation and assessment and the legal boundaries of application. © 2023 Elsevier B.V."
[No abstract available]
"The Toulmin model has been proved useful in law and argumentation theory. This model describes the basic process in justifying a claim, which comprises six elements, i.e., claim (C), data (D), warrant (W), backing (B), qualifier (Q), and rebuttal (R). Specifically, in justifying a claim, one must put forward ‘data’ and a ‘warrant’, whereas the latter is authorized by ‘backing’. The force of the ‘claim’ being justified is represented by the ‘qualifier’, and the condition under which the claim cannot be justified is represented as the ‘rebuttal’. To further improve the model, (Goodnight, Informal Logic 15:41–52, 1993) points out that the selection of a backing needs justification, which he calls legitimation justification. However, how such justification is constituted has not yet been clarified. To identify legitimation justification, we separate it into two parts. One justifies a backing’s eligibility (legitimation justification1; LJ1); the other justifies its superiority over other eligible backings (legitimation justification2; LJ2). In this paper, we focus on LJ1 and apply it to the legal justification (of judgements) in hard cases for illustration purposes. We submit that LJ1 refers to the justification of the legal interpretation of a norm by its backing, which can be further separated into several orderable subjustifications. Taking the subjustification of a norm’s existence as an example, we show how it would be influenced by different positions in the philosophy of law. Taking the position of the theory of natural law, such subjustification is presented and evaluated. This paper aims not only to inform ongoing theoretical efforts to apply the Toulmin model in the legal field, but it also seeks to clarify the process in the justification of legal judgments in hard cases. It also offers background information for the possible construction of related AI systems. In our future work, LJ2 and other subjustifications of LJ1 will be discussed. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
"Cognitive psychology is a science of human knowledge, which means that people perceive, acquire, memorize, think, and comprehend intellectual capabilities. The psychological strategy involves controlling every action and status of the human body. The problematic states of psychological facts include mental disorders like depression, stress, anxiety, and inferiority complex, leading to memory loss. The emerged technique of cognitive psychological managing framework using artificial intelligence (CPMF-AI) is introduced. The proposed framework is extended to forecast the psychological standards of the human brain for practical well-being. There are four methods to monitor memory power, stress, and other human mental disorders. They are distant neural systems (DNS), convolutional psychology tracking systems (CPTS), intelligent neural systems (INS), and memory-building strategies (MBS). Besides language aspects, physical aspects play a vital part in human-robot interaction (HRI) and make the difference compared to the more limited HRI communication. These methodologies are integrated into four case studies to detect neural passage systems for monitoring mental issues. The simulation analysis helps enhance the framework's accuracy and minimize the error rate. Thus, the proposed system of cognitive technology is comparatively better than the existing methods.  © 2023 World Scientific Publishing Company."
"Cooperative behavior represents a situation in which individuals sometimes act in a way that produces a gain to another at a cost to themselves. This may be explained by a history of repeated interactions with others in which such behavior has resulted in reciprocal cooperation from others. Sometimes, even with reciprocal cooperation, gains and costs are unbalanced between partners. In this case, there is evidence that people may present an aversion to both disadvantageous and advantageous distributions of gains. In other words, they may act in such a way as to ensure an equal outcome among all group members. Aversion to inequity that benefits oneself (advantageous inequity (AI) aversion) may be more dependent on social and cultural cues than aversion to inequity that benefits others (disadvantageous inequity (DI) aversion). Using both between-subjects (Experiment 1) and within-subjects (Experiment 2) manipulations, the influence of recent experience with AI on participants’ willingness to produce DI was explored within the context of a two-player card game. In initial game phases, the percentage of trials in which the participant experienced AI was manipulated. In subsequent game phases, participants had the opportunity to produce DI to themselves. The results from both experiments suggest that aversion to DI is reduced by recent experience with AI. This procedure allows social influences on DI to be tested, which may be important for providing a psychological explanation of cultural differences in aversion to DI. © 2022, The Psychonomic Society, Inc."
"Qualitative Research has gained greater scientific recognition in recent years, given the improvement of denser methodologies, more backed by knowledge from different areas of expertise. One of the factors that con-tributed to improving the quality of qualitative studies was incorporating a set of tools, most of them digital. Although these tools can support and reduce subjectivity, they must be aligned with the theoretical-conceptual-methodological frameworks of the research, which are the axes that give it coherence and cohesion. The researcher’s skills are fundamental to guarantee the integrity and ethics of the research construction process, from project formulation to disseminating results. In this context, in the case of qualitative data analysis, tools based on Artificial Intelligence (AI) can help researchers identify patterns and trends through a large vol-ume of data, generate visualisations and syntheses and even offer sug-gestions for other questions or research areas. On the other hand, this dimension implies that the researcher develops digital and multimodal literacies. This essay is expected to discuss the challenges to overcome in developing and using accurate tools. © 2023, Edicoes Universitarias Lusofonas. All rights reserved."
"To our knowledge, there are no extensive validation studies of existing ambiguity intolerance (AI) scales in the home countries of most of the participants in this study. Thus, we aimed to construct a scale for measuring overall AI that would be brief but also encompassing a wide range of behaviors that could be described as AI. Given that empirical research does not, overall, lend solid support to the AI construct multidimensionality, the scale was conceived as unidimensional. The study was conducted on 4 samples (5437 participants in total). Three samples were from the Serbian population and the fourth consisted of 4 separate subsamples that made up an international validation study of the SAIS-7 in four different languages (Serbian, Bulgarian, English, and Greek). The results of reliability, dimensionality, measurement invariance, convergent, external and incremental validity assessment suggest adequacy of the SAIS-7. The SAIS-7 is a solid brief measure of overall AI, and extensive analysis of different aspects of its functioning leads to a conclusion that this scale represents an economical measure with good content coverage that is recommendable for use. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"In many parts of the world, heart disease is the leading cause of mortality diagnosis is critical Towards Efficient Medical Care and prevention of heart attacks and other cardiac events. Deep learning algorithms have shown promise in accurately predicting heart disease based on medical data, including electrocardiograms (ECGs) and other health metrics. With this abstract, Specifically, we advocate for deep learning algorithm in accordance with CNNs for Deep Learning effective heart disease prediction. The proposed method uses a combination of ECG signals, demographic data, and clinical measurements Identifying risk factors for cardiovascular disease in patients. The proposed CNN-based model includes several layers, such as convolutional ones, pooling ones, and fully connected ones. The model takes input in the form of ECG signals, along with demographic data and clinical measurements, and uses convolutional layers to get features out of raw data. To lessen the effect of this, pooling layers are dimensionality of the extracted features, while layers that are already completely linked to estimate the risk of cardiovascular disease based on the extracted features. Training and evaluating the suggested model, We consulted a broad pool of ECG signals together with patient clinical data, both with and without heart disease. Training and test sets were created from the dataset testing arrays, and the prototype was trained using backpropagation and stochastic gradient descent. The model was evaluated using standard quantitative indicators such the F1 score, recall rate, and accuracy rate. The outcomes of experiments demonstrate the suggested CNN-based model achieves high accuracy in predicting heart disease, with an overall accuracy of over 90%. The model also outperforms several alternatives to classical techniques for heart disease prediction, including the more conventional forms of AI algorithms different forms of deep learning models. In conclusion, the proposed deep learning algorithm based on CNNs shows great potential for effective heart disease prediction. The model can be integrated into healthcare systems to provide accurate and timely diagnosis and treatment for patients with heart disease. Further research can be done to optimize the model's performance and test its effectiveness on different patient populations. © 2023 L. Kumar et al."
"Approximately 70% of American Indian/Alaska Native (AI/AN) individuals reside in urban areas. Urban Indian Health Organizations (UIHOs) provide culturally engaged primary care for AI/AN patients and members of other racial and ethnic groups who have experienced disparities in diabetes and hypertension care, and are commonly affected by social and economic barriers to care. We assessed whether disparities were present between the racial and ethnic groups served by the largest UIHO in the USA. We developed retrospective cohorts of patients with hypertension or diabetes receiving primary care from this UIHO, measuring differences between AI/AN, Spanish-preferring Latinx, English-preferring Latinx, Black, and White patients in mean systolic blood pressure (SBP) and mean hemoglobin A1c (A1c) as primary outcomes. To assess processes of care, we also compared visit intensity, missed visits, and medication treatment intensity in regression models adjusted for sociodemographic and clinical characteristics. For hypertension (n = 2148), adjusted mean SBP ranged from 135.8 mm Hg among Whites to 141.3 mm Hg among Blacks (p = 0.06). For diabetes (n = 1211), adjusted A1c ranged from 7.7% among English-preferring Latinx to 8.7% among Blacks (p = 0.38). Care processes for both hypertension and diabetes varied across groups. No group consistently received lower-quality care. This UIHO provided care of comparable quality for hypertension and diabetes among urban-dwelling AI/ANs and members of other racial, ethnic, and language preference groups. Systematic assessments of care quality in UIHOs may help demonstrate the importance of their role in providing care and improve the quality of care. © 2022, W. Montague Cobb-NMA Health Institute."
"Tactile perception is the basis of human motion. Achieving artificial tactility is one of the challenges in the fields of smart robotics and artificial intelligence (AI), because touch emulation relies on high-performance pressure sensor arrays, signal reading, information processing, and feedback control. In this paper, we report an integrated intelligent tactile system (IITS) that is integrated with a humanoid robot to achieve human-like artificial tactile perception. The IITS is a closed-loop system that includes a multi-channel tactile sensing e-skin, a data acquisition and information processing chip, and a feedback control. With customized preset values of threshold pressures, the IITS-integrated robot can flexibly grasp various objects. The IITS has potential applications in the design of prosthetic hands, space manipulators, deep-sea exploration robots, and human-robot interactions. © 2023 Science China Press"
"Artificial intelligence (AI) and the use of machine learning (ML) and deep learning (DL) technologies are becoming increasingly popular in companies. These technologies enable companies to leverage big quantities of data to improve system performance and accelerate business development. However, despite the appeal of ML/DL, there is a lack of systematic and structured methods and processes to help data scientists and other company roles and functions to develop, deploy and evolve models. In this paper, based on multi-case study research in six companies, we explore practices and challenges practitioners experience in developing ML/DL models as part of large software-intensive embedded systems. Based on our empirical findings, we derive a conceptual framework in which we identify three high-level activities that companies perform in parallel with the development, deployment and evolution of models. Within this framework, we outline activities, iterations and triggers that optimize model design as well as roles and company functions. In this way, we provide practitioners with a blueprint for effectively integrating ML/DL model development into the business to achieve better results than other (algorithmic) approaches. In addition, we show how this framework helps companies solve the challenges we have identified and discuss checkpoints for terminating the business case. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley & Sons Ltd."
"The deployment and maintenance of large smart infrastructures used for powering data-driven decision making, regardless of retrofitted or newly deployed infrastructures, still lack automation and mostly rely on extensive manual effort. In this article, we focus on the two main challenges in the lifecycle of smart infrastructures: deployment and operation, each of which is rather generic and applies to all infrastructures. We discuss the existing technologies designed to help improve and automate deployment and operation for smart infrastructures in general and use the smart grid as a guiding example to ground some examples across the article. Next, we identify and discuss opportunities where the broad field of artificial intelligence (AI) can help further improve and automate the lifecycle of smart infrastructures to eventually improve their reliability and drive down their deployment and operation costs. Finally, based on the usage of AI for web and social networks as well as our previous experience in AI for networks and cyber-physical systems, we provide decision guidelines for the adoption of AI.  © 2007-2011 IEEE."
"Emotional AI is an emerging technology used to make probabilistic predictions about the emotional states of people using data sources, such as facial (micro)-movements, body language, vocal tone or the choice of words. The performance of such systems is heavily debated and so are the underlying scientific methods that serve as the basis for many such technologies. In this article I will engage with this new technology, and with the debates and literature that surround it. Working at the intersection of criminology, policing, surveillance and the study of emotional AI this paper explores and offers a framework of understanding the various issues that these technologies present particularly to liberal democracies. I argue that these technologies should not be deployed within public spaces because there is only a very weak evidence-base as to their effectiveness in a policing and security context, and even more importantly represent a major intrusion to people’s private lives and also represent a worrying extension of policing power because of the possibility that intentions and attitudes may be inferred. Further to this, the danger in the use of such invasive surveillance for the purpose of policing and crime prevention in urban spaces is that it potentially leads to a highly regulated and control-oriented society. I argue that emotion recognition has severe impacts on the right to the city by not only undertaking surveillance of existing situations but also making inferences and probabilistic predictions about future events as well as emotions and intentions. © 2022, The Author(s)."
"Purpose: The application of artificial intelligence (AI) in the customer market has completely changed customer behaviors. This study aims to investigate the customers' co-creation experiences with AI in the digital age. Design/methodology/approach: An online survey was used to collect data from 699 customers who had used AI-enabled banking services. Hypotheses were validated using partial least squares modeling. Findings: The findings indicate that the customer response capabilities (e.g. perceived response expertise and perceived response speed) serve as the intermediate processes between the AI service quality and the overall co-creation experience with AI. Moreover, AI function-customer ability fit negatively moderates the direct relationship between the AI service quality and the overall co-creation experience with AI. Originality/value: This study improves the current understanding of co-creation by investigating the human–machine co-creation (e.g. customer–AI co-creation) instead of human–human co-creation. © 2023, Emerald Publishing Limited."
"The use of machine learning in AI solutions can speed up the delivery of results, allowing these tools to achieve even more impressive feats. The present study presented a developed system that can be applied and used in many fields, such as a smart attendance system in a classroom setting and an intruder detection system. The two fused algorithms-Haar Cascade and Local Binary Pattern Histogram are used to train and utilize a system to recognize a registered face. Amid health crises, multiple face detection and face mask detection systems were also taken into consideration in the developed system. © 2023 ACM."
"Background: A common symptom of obsessive-compulsive disorder is the persistent avoidance of cues incorrectly associated with negative outcomes. This maladaptation becomes increasingly evident as subjects fail to respond to extinction-based treatments such as exposure-with-response prevention therapy. While previous studies have highlighted the role of the insular-orbital cortex in fine-tuning avoidance-based decisions, little is known about the projections from this area that might modulate compulsive-like avoidance. Methods: Here, we used anatomical tract-tracing, single-unit recording, and optogenetics to characterize the projections from the insular-orbital cortex. To model exposure-with-response prevention and persistent avoidance in rats, we used the platform-mediated avoidance task followed by extinction-with-response prevention training. Results: Using tract-tracing and unit recording, we found that projections from the agranular insular/lateral orbital (AI/LO) cortex to the prefrontal cortex predominantly target the rostral portion of the prelimbic (rPL) cortex and excite rPL neurons. Photoinhibiting this projection induced persistent avoidance after extinction-with-response prevention training, an effect that was still present 1 week later. Consistent with this, photoexcitation of this projection prevented persistent avoidance in overtrained rats. This projection to rPL appears to be key for AI/LO's effects, considering that there was no effect of photoinhibiting AI/LO projections to the ventral striatum or basolateral amygdala. Conclusions: Our findings suggest that projections from the AI/LO to the rPL decreases the likelihood of avoidance behavior following extinction. In humans, this connectivity may share some homology of projections from lateral prefrontal cortices (i.e., ventrolateral prefrontal cortex, orbitofrontal cortex, and insula) to other prefrontal areas and the anterior cingulate cortex, suggesting that reduced activity in these pathways may contribute to obsessive-compulsive disorder. © 2022 Society of Biological Psychiatry"
"Exploration of novel strategies for early pregnancy diagnosis is pivotal in enhancing the reproductive potential and monetary gains from dairy herds. In buffalo, the trophectoderm cells of the elongating conceptus secrete interferon-tau that stimulates the transcription of various genes in peripheral blood mononuclear cells (PBMC) during the peri-implantation period. We explored the differential expression of classical (ISG15) and novel (LGALS3BP and CD9) early pregnancy markers in PBMC of buffaloes during various stages of pregnancy. Natural heat was detected in buffaloes by assessing the vaginal fluid, and artificial insemination (AI) was done. Whole blood was collected from the jugular vein in EDTA-containing vacutainers for PBMC isolation before AI (0-day) and 20, 25 and 40 d post-AI. On day 40, transrectal ultrasonography examination was performed to confirm pregnancy. The inseminated non-pregnant animals served as control. Total RNA was extracted using the TRIzol method. The temporal abundance of ISG15, LGALS3BP and CD9 genes in PBMC was compared between pregnant and non-pregnant groups (n = 9 per group) using real time-qPCR. Results showed transcripts of ISG15 and LGALS3BP were more abundant at 20 d in the pregnant group compared to the 0 d and 20 d values of the non-pregnant group. However, due to variability in expression, threshold (Ct) cycle of RT-qPCR alone could not distinguish pregnant and non-pregnant animals. In conclusion, ISG15 and LGALS3BP transcripts abundance in PBMCs are potential candidate biomarkers for early prediction of buffalo pregnancy 20-days post-AI, but further work is required to allow the development of a reliable new methodology. Copyright © The Author(s), 2023. Published by Cambridge University Press on behalf of Hannah Dairy Research Foundation."
"Given that artificial intelligence (AI) has been predicted to eventually take on human tasks demanding logical thinking, it makes sense that we should examine psychological responses of humans when their performance is inferior to AI. Research has demonstrated that after people fail a task, whether they reorient their behavior towards success depends on what they attribute the failure to. This study investigated the causal attributions people made in a competition task requiring such thinking. We also recorded whether they wanted to re-challenge the games after they were defeated by AI. Experiments 1 (N = 74) and 2 (N = 788) recruited Japanese participants, while Experiment 3 (N = 500) comprised American participants. There were two conditions: in the first, participants competed against an AI opponent and in the other, they believed they were competing against a human. The results of the three experiments showed that participants attributed the loss to their own and their opponent’s abilities more than any other factor, irrespective of the opponent type. The number of participants choosing to re-challenge the game did not differ significantly between the AI and human conditions in Experiments 1 and 3, although the number was lower in the AI condition than in the human condition in Experiment 2. Besides providing fresh insight on how people make causal attributions when competing against AI, our findings also predict how people will respond after their jobs are replaced by AI. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"Responsible Artificial Intelligence (AI) has recently gained a lot of attention, especially in the last few years. Scholars have conducted systematic literature reviews to gain more knowledge about responsible AI. However, no study has collected and evaluated the most significant barriers to responsible AI. We filled this gap in the literature by identifying eleven barriers and categorized them, using the Technology-Organization-Environment framework, into three categories. We collected data from seven experts and used the analytical hierarchy process to evaluate the importance of the barriers. The results indicated that technology, as a category, is the most important. The findings also recommended that data quality is the most vital among all eleven barriers. We offered eleven propositions as a theoretical contribution for future researchers in terms of conceptual development. We discussed the implications of the findings for research and practice. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"A sustainable and reliable machining process is the main goal of seeking machine digitization. Artificial Intelligence (AI), and Cyber-Physical System (CPS) combined with Artificial Intelligence are used for process control. This has become more essential in the case of machining of high-cost aerospace materials and critical product specifications. In this paper, a novel self-healing mechanism was developed to recover a CNC machine from producing parts that do not conform, to surface roughness’s specifications. The machine settings are reconfigured autonomously and online to recover from the effect of tool wear and to keep the surface roughness within the design specifications. The proposed self-healing mechanism is based on a pattern recognition algorithm called Logical Analysis of Data (LAD). This algorithm generates patterns that characterize the out-of-specification state, and provides a corrective setting within the recovery patterns of the within-specification state by using various distance approaches. The developed self-healing mechanism is composed of three modules: CPS model of the CNC machine (module 1), classification into, out of, or within-specification states (module 2), and a self-healing controller (module 3) that is activated if the state of out-of-specification is found by module 2. The three modules are software. The current hardware system of the machine is not altered. The proposed self-healing is applicable and integrable to CNC machines with a wide range of machining parameters of feed rate from 20 mm/min to 750 mm/min and spindle speed from 15,000 RPM to 35,000 RPM. To validate the developed mechanism, a deep learning artificial model was developed on physical data to emulate the CNC milling machine in a CPS simulation environment, and test runs were executed. The proposed self-healing mechanism was evaluated under several simulation runs that covered the ranges of CNC machine settings. The measure of performance of the proposed mechanism is the out-of-specification clearing time. The test runs show that the proposed self-healing mechanism was able to clear the out-of- specification state and to recover the within-specification state in less than five seconds, with the best distance metric approach. The results of the time response for each test run are reported. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"The increasing use of artificial intelligence (AI) by public actors has led to a push for more transparency. Previous research has conceptualized AI transparency as knowledge that empowers citizens and experts to make informed choices about the use and governance of AI. Conversely, in this paper, we critically examine if transparency-as-knowledge is an appropriate concept for a public realm where private interests intersect with democratic concerns. We conduct a practice-based design research study in which we prototype and evaluate a transparent smart electric vehicle charge point, and investigate experts’ and citizens’ understanding of AI transparency. We find that citizens experience transparency as burdensome; experts hope transparency ensures acceptance, while citizens are mostly indifferent to AI; and with absent means of control, citizens question transparency’s relevance. The tensions we identify suggest transparency cannot be reduced to a product feature, but should be seen as a mediator of debate between experts and citizens. © 2022, The Author(s)."
"Businesses in high-risk environments have been reluctant to adopt modern machine learning approaches due to their complex and uninterpretable nature. Most current solutions provide local, instance-level explanations, but this is insufficient for understanding the model as a whole. In this work, we show that strategy clusters (i.e., groups of data instances that are treated distinctly by the model) can be used to understand the global behavior of a complex ML model. To support effective exploration and understanding of these clusters, we introduce StrategyAtlas, a system designed to analyze and explain model strategies. Furthermore, it supports multiple ways to utilize these strategies for simplifying and improving the reference model. In collaboration with a large insurance company, we present a use case in automatic insurance acceptance, and show how professional data scientists were enabled to understand a complex model and improve the production model based on these insights.  © 1995-2012 IEEE."
"Eye tracking measures can provide means to understand the underlying development of human working memory. In this study, we propose to develop machine learning algorithms to find an objective relationship between human eye movements via oculomotor plant and their working memory capacity, which determines subjective cognitive load. Here we evaluate oculomotor plant features extracted from saccadic eye movements, traditional positional gaze metrics, and advanced eye metrics such as ambient/focal coefficient , gaze transition entropy, low/high index of pupillary activity (LHIPA), and real-time index of pupillary activity (RIPA). This paper outlines the proposed approach of evaluating eye movements for obtaining an objective measure of the working memory capacity and a study to investigate how working memory capacity is affected when reading AI-generated fake news.  © 2023 Owner/Author."
[No abstract available]
"With the increasing use of algorithms in high-stakes areas such as criminal justice and health has come a significant concern about the fairness of prediction-based decision procedures. In this article I argue that a prominent class of mathematically incompatible performance parity criteria can all be understood as applications of John Broome’s account of fairness as the proportional satisfaction of claims. On this interpretation these criteria do not disagree on what it means for an algorithm to be fair. Rather they express different understandings of what grounds a claim to a good being allocated by an algorithmic decision procedure. I then argue that an important implication of the Broomean interpretation is that it strengthens the case for outcome-based criteria. Finally, I consider how a version of the levelling-down objection to performance parity criteria arises within the Broomean account. © 2022, The Author(s), under exclusive licence to Springer Nature B.V."
"Purpose: The primary purpose of this paper is to examine how generative Artificial Intelligence (AI) such as ChatGPT may serve as a new context for management theories and concepts. Design/methodology/approach: The paper presents the analyses of selected management theories on decision-making, knowledge management, customer service, human resource management and administrative tasks and explains what may change after generative AI adoption. Findings: The paper indicates that some management theories and concepts need to be studied in the generative AI environment that may influence managerial work at the strategic, functional and administrative levels. Research limitations/implications: This paper is an opinion piece article and does not refer to empirical data. It formulates some conclusions to further empirical research studies. Originality/value: The paper analyzes selected management theories in a new technological setting. The paper also provides information about the functions of generative AI that are useful in understanding and overcoming how new technology may change organizations and management. © 2023, Pawel Korzynski, Grzegorz Mazurek, Andreas Altmann, Joanna Ejdys, Ruta Kazlauskaite, Joanna Paliszkiewicz, Krzysztof Wach and Ewa Ziemba."
"Purpose: Due to the popularity of mobile devices and the development of artificial intelligence (AI), AI-powered mobile fitness applications (MFAs) have entered people's daily lives. However, the extant literature lacks empirical investigations that explore users' continuance usage intentions regarding AI-powered MFAs. To fill this research gap, this paper employs goal-setting theory to establish a research model for exploring how AI-enabled features (i.e. intelligence and anthropomorphism) affect users' perceptions of goal difficulties and goal specificities, which in turn affect their MFA continuance usage intentions. Design/methodology/approach: This paper uses a survey method to analyze the research model, and a total of 223 responses are collected. The partial least squares (PLS) technique is utilized for data analysis. Findings: The results show that intelligence and anthropomorphism affect the continuance usage intention of MFA users through their goal difficulty and specificity. Both intelligence and anthropomorphism positively influence goal specificity, whereas they negatively affect goal difficulty. In addition, goal specificity increases users' MFA continuance usage intention, whereas goal difficulty decreases users' continuance usage intention. The findings of this study provide theoretical contributions for AI technology adoption research and offer practical strategies for firms to retain MFA users. Originality/value: Based on goal-setting theory, this study reveals that as two primary AI features of contemporary mobile fitness apps, intelligence and anthropomorphism, can increase comprehension of users' perceptions regarding goal difficulty and specificity in the context of users' continuance usage intentions toward AI-powered MFAs. © 2023, Emerald Publishing Limited."
"This study aims to investigate the role of artificial intelligence (AI) driven facial recognition to enhance a value proposition by influencing different areas of services in the travel and tourism industry. We adopted semi-structured interviews to derive insights from 26 respondents. Thematic analysis reveals the development of four main themes (personalization, data-driven service offering, security and safety, and seamless payments). Further, we mapped the impact of AI- driven facial recognition to enhance value and experience for corporate guests. Findings indicate that AI-based facial recognition can facilitate the travel and tourism industry in understanding travelers’ needs, optimization of service offers, and value-based services, whereas data-driven services can be realized in the form of customized trip planning, email, and calendar integration, and quick bill summarization. This contributes to strengthening the tourism literature through the lens of organizational information processing theory. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"The present research aims to identify significant contributors, recent dynamics, domains and advocates for future study directions in the arena of integration of Artificial Intelligence (AI) with Human Resource Management (HRM), in the context of various functions and practices in organizations. The paper adopted a methodology comprising of bibliometrics, network and content analysis (CA), on a sample of 344 documents extracted from the Scopus database, to identify extant research on this theme. Along with the bibliometric analysis, systematic literature review was done to propose an Artificial Intelligence and Human Resource Management Integration (AIHRMI) framework. Five clusters were recognized, and CA was conducted on the documents placed in the group of articles. It was found that vital research concentration in this arena is primarily about AI embeddedness in various HRM functions such as recruitment, selection, onboarding, training and learning, performance analysis, talent acquisition, as well as management and retention. The study proposes an AIHRMI framework developed from various studies considered in the current research. This model can provide guidance and future directions for several organizations in expansion of use of AI in HRM. © 2021, The Author(s), under exclusive licence to Springer Nature Switzerland AG."
[No abstract available]
"First, the benefits of AI-based training for business development are presented. Then, the theoretical literature on various stages from traditional training to AI-based training is analyzed. Finally, we analyze AI applications in the training process and managerial response. This study presents a framework for the application of artificial intelligence (AI) technology to training and managerial challenges. AI tools can be applied to the training process, including knowledge management (KM), needs analysis, training organization, and results feedback. AI-based training transforms organizations into knowledge organizations that can meet the demands of personalized training and improve learning quality. AI tools bring about a shift in the training phases of knowledge base creation, needs surveys, the organization of training, and feedback on results. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature."
"In 2021, the Inspur AI Research Institute introduced the AI Megatron Model Yuan-1.0, a massive Chinese language AI model containing 245.7 billion parameters. This model surpassed OpenAI's GPT-3, making it the world's largest Chinese NLP model. Although the model was pre-trained using Nvidia's Megatron framework with model parallelism, data parallelism, and pipelining optimizations, there is still room for improvement in terms of training time, cost, and convergence. To achieve better performance, this paper investigates the impacts of batch size and learning rate on model training time and accuracy to balance model performance. We replaced the pipelining optimization with the more efficient DeepSpeed framework, and combined DeepSpeed's ZeRO-based data parallelism with Nvidia's Megatron-LM model parallelism to achieve higher performance on Nvidia GPU clusters with high-bandwidth interconnects. Additionally, we used a curriculum learning-based method and four types of sparse attention as a new optimization approaches. The results showed that the training time was reduced by 20% and the throughput increased by 20% compared to the 47 billion parameters Yuan-1.0 model. Approximately, the optimized model achieved performance improvement in downstream tasks with the same training data. © 2023 ACM."
"Social media platforms have enabled users to share their thoughts, ideas, and opinions on different subject matters and meanwhile generate lots of information which can be adopted to understand people’s emotion towards certain products. This information can be effectively applied for Aspect Category Detection (ACD). Similarly, people’s emotions and recommendation-based Artificial Intelligence (AI)-powered systems are in trend to assist vendors and other customers to improve their standards. These systems have applications in all sorts of business available on multiple platforms. However, the current conventional approaches fail in providing promising results. Thus, in this paper, we propose novel convolutional attention-based bidirectional modified LSTM by combining the techniques of the next word, next sequence, and pattern prediction with ACD. The proposed approach extracts significant features from public reviews to detect entity and attribute pair, which are treated as a sequence or pattern from a given opinion. Next, we trained our word vectors with the proposed model to strengthen the ACD process. Empirically, we compare the approach with the state-of-the-art ACD models that use SemEval-2015, SemEval-2016, and SentiHood datasets. Results show that the proposed approach effectively achieves 78.96% F1-Score on SemEval-2015, 79.10% F1-Score on SemEval-2016, and 79.03% F1-Score on SentiHood which is higher than the existing approaches. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature."
"The new area of education has progressively become where virtual reality interacts and integrates. The influence of credit management in the learning process is improved by digital twin technology. The next study paradigms of learning space will be the reconstruction of smart learning spaces based on digital twins. Hence, this paper, Digital Twin Approach (DTA), has been proposed to reconstruct the learning area and encourage the education revolution and a new learning community. In real-time manual interaction, the simulated twin will imitate diverse scenarios and create situations through AI decision-making and automatic twin implementation. The use of digital twin technology offers a new technology integration and development path to smart-university management in the investigation of the construction of smart university facilities. Thus, the experimental results show that the DTA technology can motivate students to study and boost learning when effectively utilized.  © 2023 World Scientific Publishing Company."
"Significant changes in marine environment characteristics led to complicated alterations in acoustic underwater channels in times, frequency, and space. UWA communication systems that fulfill UWA business criteria in different marine conditions have not yet been established. In all weather situations, it is significantly more difficult to develop large-scale, reliable, and durable UWA networking applications. In recent years, the fast growth of artificial intelligence (AI) and big data has been inspired for new technologies and it is a breakthrough of traditional UWA communication technology by the development of the Internet of underwater things. This paper summarized the advancement of various Modulation techniques, methods, and AI technology used for UWA communications globally. The UWA channel characteristics and the major approach are also discussed that applying in AI technology for UWA communication. The use of AI for UWA communications in physical and network layers is summarized. Finally, it summarizes the potential junction between AI and UWA research in communication and its future outlook.  © 2023 Author(s)."
"Summary: ChatGPT is an open artificial intelligence chat box that could revolutionize academia and augment research writing. This study had an open conversation with ChatGPT and invited the platform to evaluate this article through series of five questions on base of thumb arthritis to test if its contributions and contents merely add artificial unusable input or help us augment the quality of the article. The information ChatGPT-3 provided was accurate, albeit surface-level, and lacks analytical ability to dissect for important limitations about base of thumb arthritis, which would not be conducive to potentiating creative ideas and solutions in plastic surgery. ChatGPT failed to provide relevant references and even ""created"" references instead of indicating its inability to perform the task. This highlights that as an AI-generator for medical publishing text, ChatGPT-3 should be used cautiously. © 2023 Lippincott Williams and Wilkins. All rights reserved."
"The world receives more than 200 thousand people in a day and it is expected that the total world population will reach 9.6 billion by the year 2050. This will result in extra food demand, which can only be met from enhanced crop yield. Therefore, modernization of the agricultural sector becomes the need of the hour. There are many constraints that are responsible for the low production of crops, which can be overcome by using drone technology in the agriculture sector. This paper presents an analysis of drone technologies and their modifications with time in the agriculture sector in the last decade. The application of drones in the area of crop monitoring, and pesticide spraying for Precision Agriculture (PA) has been covered. The work done related to drone structure, multiple sensor development, innovation in spot area spraying has been presented. Moreover, the use of Artificial Intelligent (AI) and deep learning for the remote monitoring of crops has been discussed. © 2022 China Agricultural University"
"Purpose: The purpose of this study is to explore and use artificial intelligence (AI) techniques for identifying the relevant attributes necessary to file a suspicious activity report (SAR) using historical customer transactions. This method is known as predictive modeling, a statistical approach which uses machine learning algorithm to predict outcomes by using historical data. The models are applied to a modified data set designed to mimic transactions of retail banking within the USA. Design/methodology/approach: Machine learning classifiers, as a subset of AI, are trained using transactions that meet or exceed the minimum threshold amount that could generate an alert and report a SAR to the government authorities. The predictive models are developed to use customer transactional data to predict the probability that a transaction is reportable. Findings: The performance of the machine learning classifiers is determined in terms of accuracy, misclassification, true positive rate, false positive rate and false negative rate. The decision tree model provided insight in terms of the attributes relevant for SAR filing based on the rule-based criteria of the algorithm. Originality/value: This research is part of emerging studies in the field of compliance where AI/machine learning technology is used for transaction monitoring to identify relevant attributes for suspicious activity reporting. The research methodology may be replicated by other researchers, Bank Secrecy Act/anti-money laundering (BSA/AML) officers and model validation analysts for BSA/AML compliance models. © 2022, Emerald Publishing Limited."
"Introduction: Pembrolizumab, an immune-checkpoint inhibitor, is approved for first-line treatment of metastatic NSCLC in patients with tumours expressing programmed death-ligand 1 (PD-L1) with tumour proportion score (TPS) of ≥50%. We aimed to clarify some uncertainties regarding use of immunotherapy in patients with previous autoimmune (AI) disorders and assess real-world outcomes following treatment completion. Methods: We performed a retrospective case record review of 82 patients with tumours expressing PD-L1 at TPS ≥ 50% and receiving first-line Pembrolizumab. Survival was estimated using the Kaplan Meier method. Results: After 36.93 months (IQR: 34.37–40.20) median follow-up, median OS was 13.6 months (95% CI 8.9–19.3). There were 10 patients (12%) with AI co-morbidities and there was a trend toward improved median OS for this group versus those without AI comorbidity, 42 months (14.87-NR) versus 10.7 months (7.3–17.8), p = 0.073. Sixteen patients (20%) with nonprogressive disease at 2 years had significantly better median OS compared to those who did not complete 2 years of treatment, NR (42- NR) and 8.7 (5.8–14.1), p < 0.001. Immune related adverse events (irAE) of any grade occurred in 90% of the AI cohort compared with 70.8% of patients without AI comorbidity. Low grade adrenal insufficiency was the only irAE which occurred at a significantly higher rate in the AI group, p = 0.02. Conclusion: Patients with previous AI diseases tolerate treatment well, and there is a non-significant trend for improved outcomes in this group. Patients who complete the course of pembrolizumab have significantly better survival outcomes than those who do not. © The Author(s) 2022."
"The rapid expansion of artificial intelligence (AI) necessitates promoting AI education at the K-12 level. However, educating young learners to become AI literate citizens poses several challenges. The components of AI literacy are ill-defined and it is unclear to what extent middle school students can engage in learning about AI as a sociotechnical system with socio-political implications. In this paper we posit that students must learn three core domains of AI: technical concepts and processes, ethical and societal implications, and career futures in the AI era. This paper describes the design and implementation of the Developing AI Literacy (DAILy) workshop that aimed to integrate middle school students’ learning of the three domains. We found that after the workshop, most students developed a general understanding of AI concepts and processes (e.g., supervised learning and logic systems). More importantly, they were able to identify bias, describe ways to mitigate bias in machine learning, and start to consider how AI may impact their future lives and careers. At exit, nearly half of the students explained AI as not just a technical subject, but one that has personal, career, and societal implications. Overall, this finding suggests that the approach of incorporating ethics and career futures into AI education is age appropriate and effective for developing AI literacy among middle school students. This study contributes to the field of AI Education by presenting a model of integrating ethics into the teaching of AI that is appropriate for middle school students. © 2022, International Artificial Intelligence in Education Society."
"Background Improving palliative care (PC) is demanding due to the increase in people with PC needs over the next few years. An early identification of PC needs is fundamental in the care approach: it provides effective patient-centred care and could improve outcomes such as patient quality of life, reduction of the overall length of hospitalisation, survival rate prolongation, the satisfaction of both the patients and caregivers and cost-effectiveness. Methods We reviewed literature with the objective of identifying and discussing the most important ethical challenges related to the implementation of AI-based data processing services in PC and advance care planning. Results AI-based mortality predictions can signal the need for patients to obtain access to personalised communication or palliative care consultation, but they should not be used as a unique parameter to activate early PC and initiate an ACP. A number of factors must be included in the ethical decision-making process related to initiation of ACP conversations, among which are autonomy and quality of life, the risk of worsening healthcare status, the commitment by caregivers, the patients' psychosocial and spiritual distress and their wishes to initiate EOL discussions Conclusions Despite the integration of artificial intelligence (AI)-based services into routine healthcare practice could have a positive effect of promoting early activation of ACP by means of a timely identification of PC needs, from an ethical point of view, the provision of these automated techniques raises a number of critical issues that deserve further exploration.  © 2023 BMJ Publishing Group. All rights reserved."
"INTRODUCTION: In December of 2019, the infection which caused the pandemic started in the Hubei territory of Wuhan, China. They were identified as SARS-CoV-2, a highly infectious, easily transmissible virus that has caused an increasing number of deaths worldwide. Covid can be perceived with a testing strategy known as RT-PCR. As of now, this technique is broadly utilized for identifying the infection. OBJECTIVES: The imaging modalities are utilized for various degrees of seriousness from asymptomatic to basic cases. Side effects of an individual contaminated with COVID-19 incorporate gentle hack, fever, chest torment, weakness, and so forth An individual with an extreme fundamentalailment requires basic consideration. Imaging has assumed a larger part during the flare-up, with CT being a betteroption than invert transcriptase-polymerase chain response testing. METHODS: With artificial intelligence and robotics, a variety of devices and solutions have been introduced to improve contactless service for humans. The presentationof AI technology may be a distinct advantage for the contactless treatment of patients. Information technology and AI could solve the testing and tracking system without any human interaction. RESULTS: CT imaging methods permit radiologists and doctors to distinguish inner structures and see their shape, size, thickness, and surface, which could help in the early discovery of asymptomatic cases. CONCLUSION: This detailed information data can be utilized to decide whether there's a clinical issue, provide the extent and accurate area of the matter, and uncover other significant details which will assist the doctor with deciding the best treatment. © 2023 R. Kishore Kanna et al."
"Knowledge graph embedding models have gained significant attention in AI research. The aim of knowledge graph embedding is to embed the graphs into a vector space in which the structure of the graph is preserved. Recent works have shown that the inclusion of background knowledge, such as logical rules, can improve the performance of embeddings in downstream machine learning tasks. However, so far, most existing models do not allow the inclusion of rules. We address the challenge of including rules and present a new neural based embedding model (LogicENN). We prove that LogicENN can learn every ground truth of encoded rules in a knowledge graph. To the best of our knowledge, this has not been proved so far for the neural based family of embedding models. Moreover, we derive formulae for the inclusion of various rules, including (anti-)symmetric, inverse, irreflexive and transitive, implication, composition, equivalence and negation. Our formulation allows to avoid grounding for implication and equivalence relations. Our experiments show that LogicENN outperforms the existing models in link prediction.  © 1979-2012 IEEE."
[No abstract available]
"In this paper, we introduce the Shareish web platform to foster mutual aid following principles of gift economy and generalized exchange. Its design is grounded in prior work (in CT, CSCW, and solidarity HCI) and it aims at leveraging community assets through donation, free loan, requests of goods and services, and free event announcements. Authenticated users can visualize localized items on a map or through lists, search with filters, add new content with rich textual and visual descriptions, discuss about specific content with others users, and get notifications when new content is created in their neighborhood. In addition, we evaluate AI technologies to ease content creation. The platform can be easily replicated and improved by grassroots movements or researchers seeking autonomy as its source code is made freely available and its installation relies on modern deployment strategies. A demonstration server is available (https://shareish.org/, see Section Online Resources).  © 2023 Owner/Author."
"In this review, we explore the historical development and future prospects of artificial intelligence (AI) and deep learning in astronomy. We trace the evolution of connectionism in astronomy through its three waves, from the early use of multilayer perceptrons, to the rise of convolutional and recurrent neural networks, and finally to the current era of unsupervised and generative deep learning methods. With the exponential growth of astronomical data, deep learning techniques offer an unprecedented opportunity to uncover valuable insights and tackle previously intractable problems. As we enter the anticipated fourth wave of astronomical connectionism, we argue for the adoption of GPT-like foundation models fine-tuned for astronomical applications. Such models could harness the wealth of high-quality, multimodal astronomical data to serve state-of-the-art downstream tasks. To keep pace with advancements driven by Big Tech, we propose a collaborative, open-source approach within the astronomy community to develop and maintain these foundation models, fostering a symbiotic relationship between AI and astronomy that capitalizes on the unique strengths of both fields. © 2023 The Authors."
"Recently, virus diseases, such as SARS-CoV, MERS-CoV, and COVID-19, continue to emerge and pose a severe public health problem. These diseases threaten the lives of many people and cause serious social and economic losses. Recent developments in information technology (IT) and connectivity have led to the emergence of Internet of Things (IoT) and Artificial Intelligence (AI) applications in many industries. These industries, where IoT and AI together are making significant impacts, are the healthcare and the diagnosis department. In addition, by actively communicating with smart devices and various biometric sensors, it is expanding its application fields to telemedicine, healthcare, and disease prevention. Even though existing IoT and AI technologies can enhance disease detection, monitoring, and quarantine, their impact is very limited because they are not integrated or applied rapidly to the emergence of a sudden epidemic. Especially in the situation where infectious diseases are rapidly spreading, the conventional methods fail to prevent large-scale infections and block global spreads through prediction, resulting in great loss of lives. Therefore, in this paper, various sources of infection information with local limitations are collected through virus disease information collector, and AI analysis and severity matching are performed through AI broker. Finally, through the Integrated Disease Control Center, risk alerts are issued, proliferation block letters are sent, and post-response services are provided quickly. Suppose we further develop the proposed integrated virus disease control model. In that case, it will be possible to proactively detect and warn of risk factors in response to infectious diseases that are rapidly spreading worldwide and strengthen measures to prevent spreading of infection in no time. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature."
"Objective This study investigated changes in kidney histology over time in patients with lupus nephritis (LN) undergoing immunosuppressive treatment. Methods Patients with proliferative±membranous LN were studied. After a diagnostic kidney biopsy (Bx1), patients had protocol biopsy 2 (Bx2) at 9 (6-15) months and protocol biopsy 3 (Bx3) at 42 (28-67) months. Kidney histological activity and chronicity indices (AI, CI) were measured. Results AI declined in a biphasic fashion, falling rapidly between Bx1 and Bx2 and then more slowly between Bx2 and Bx3. Patients were divided into those who achieved histological remission, defined as an AI=0 at Bx3 (group 1), and those with persistent histological activity (AI >0) at Bx3 (group 2). The early decline in AI was 1.6 times greater (95% CI 1.30, 1.91) in group 1 than group 2 (p=0.01). Between Bx2 and Bx3, the AI decline was 2.19-fold greater (95% CI 2.09, 2.29) in group 1 versus group 2 (p=7.34×10 -5). Individual histological components of the AI resolved at different rates. Inflammatory lesions like glomerular crescents, karyorrhexis and necrosis mostly resolved by Bx2, whereas endocapillary hypercellularity, subendothelial hyaline deposits and interstitial inflammation resolved slowly, accounting for residual histological activity at biopsy 3 in group 2. In contrast, CI increased rapidly, by 0.15 units/month between Bx1 and Bx2, then plateaued. There were no differences in the rate of accumulation of chronic damage between group 1 and group 2. The increase in CI was significantly related to the severity of glomerular crescents (p=0.044), subendothelial hyaline deposits (p=0.002) and interstitial inflammation (p=0.015) at Bx1. Conclusions LN histological activity takes months to years to resolve, providing a rationale for the need of long-term, well-tolerated maintenance immunosuppression. Despite responding, LN kidneys accrue chronic damage early during treatment. This finding provides an explanation for the association of chronic progressive kidney disease with recurrent episodes of LN.  © BMJ Publishing Group Limited 2023. No "
"Highly sophisticated capabilities of artificial intelligence (AI) have skyrocketed its popularity across many industry sectors globally. The public sector is one of these. Many cities around the world are trying to position themselves as leaders of urban innovation through the development and deployment of AI systems. Likewise, increasing numbers of local government agencies are attempting to utilise AI technologies in their operations to deliver policy and generate efficiencies in highly uncertain and complex urban environments. While the popularity of AI is on the rise in urban policy circles, there is limited understanding and lack of empirical studies on the city manager perceptions concerning urban AI systems. Bridging this gap is the rationale of this study. The methodological approach adopted in this study is twofold. First, the study collects data through semi-structured interviews with city managers from Australia and the US. Then, the study analyses the data using the summative content analysis technique with two data analysis software. The analysis identifies the following themes and generates insights into local government services: AI adoption areas, cautionary areas, challenges, effects, impacts, knowledge basis, plans, preparedness, roadblocks, technologies, deployment timeframes, and usefulness. The study findings inform city managers in their efforts to deploy AI in their local government operations, and offer directions for prospective research. © 2022, The Author(s)."
"This book focuses on the intelligent technologies that are transforming creative practices and industries. The future of creative work will be more complicated than ""the robots will take our jobs."" The workplace is becoming increasingly hybridized, with human and computational labor complementing each other. Some economic roles for the former will no doubt fade over time, while new roles are created to produce artificial intelligence (AI)-related technologies and implementations for productivity. New tools for the generation and personalization of content across platforms will be as ubiquitous as the automation of repetitive tasks in content creation workflows. Cultural conceptions of what it means to be a creative worker will necessarily change as a result of these transformations in human-machine labor. The volume covers the possibilities of humans and robots developing collegial relationships, creative cybernetics as machines and artists become co-creators of art, the reconcentration of corporate power as AI transforms the music industry, the rhetoric of algorithm-driven cultural production in streaming media and how artisans provide a model of counter-hegemony to automation processes. Scholars and students from many backgrounds, as well as policy makers, journalists and the general reading public, will find a multidisciplinary approach to questions posed by creative labor and industry research from communication, philosophy, robotics, media, music and the creative arts, informatics, information science, and computer science and engineering. © 2023 selection and editorial matter, Michael Filimowicz. All rights reserved."
"Tuberculosis (TB) is one of the highest causes of death in Indonesia. The main reason is lack of the health facilities. Computer-aided diagnosis (CAD) is a tool for early treatment and screening of many diseases, including TB. This paper proposed a design of a CAD system in Indonesia specifically for TB. The design gives the analysis of self-assessment concepts, use-case diagrams, and black-box diagrams. The black box utilizes chest x-ray (CXR) data for the medical image processing (MIP) method, and artificial intelligence (AI) for classification and visualization of the TB. This CAD design of self-assessment of TB has a capability to help the health practitioners read and interpret the diagnosis result more easily. © 2023 EDP Sciences. All rights reserved."
"This paper seeks to analyze the regulatory framework of energy economics in the Philippines. It looks at the historical legal background of these regulations, seeking to understand how different factors contribute to the performance of the energy industry in the Philippines today. The first part provides the recent state of electricity in the Philippines, in contrast to other countries in comparatively the same circumstantial environments but with vastly different payment rates for electricity. The second part delves deeper and discusses its history, tracing the creation of landmark private industries, community cooperatives, and regulatory bodies throughout the century, up to the present day and how the current energy regulatory framework operates. It seeks to isolate the extent of influence on this particular variable that regulatory frameworks exert. Lastly, the final part dissects the Philippine regulatory framework, applying standards and concepts for efficiency in regulation and identifying areas for improvement. It concludes by providing five key recommendations: adopting data-driven regulation; integrating AI analytics; promoting a principle-based regulatory system; improving incentives against corruption and patronage politics, and; increasing consumer education. These steps can provide the backbone of reform that the Philippine regulatory system needs in order to push forward an efficient, human rights-responsive social energy framework for its citizens. © 2023, Econjournals. All rights reserved."
"Feature importance is often used to explain clinical prediction models. In this work, we examine three challenges using experiments with electronic health record data: computational feasibility, choosing between methods, and interpretation of the resulting explanation. This work aims to create awareness of the disagreement between feature importance methods and underscores the need for guidance to practitioners how to deal with these discrepancies. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"There is now a plethora of internet of things (IoT) devices on the market that can connect to the internet and the desired environment to produce sufficient and reliable data that is required by the government administration for a variety of purposes. Additionally, the potential benefits of incorporating artificial intelligence (AI) and machine learning into governance are numerous. Governments can use AI and machine learning to enforce the law, detect fraud, and monitor urban areas by identifying problems before they occur. The government can also use AI to easily automate processes and replace mundane and repetitive tasks. AI, IoT, and Blockchain Breakthroughs in E-Governance defines and emphasizes various AI algorithms as well as new internet of things and blockchain breakthroughs in the field of e-governance. Covering key topics such as machine learning, government, and artificial intelligence, this premier reference source is ideal for government officials, policymakers, researchers, academicians, practitioners, scholars, instructors, and students. © 2023, IGI Global. All rights reserved."
"More than 40% of the adult population suffers from functional gastrointestinal disorders, now considered disorders of the 'gut-brain axis' (GBA) interactions, a very complex bidirectional neural, endocrine, immune, and humoral communication system modulated by the microbiota. To help discover, understand, and manage GBA disorders, the OnePlanet research center is developing digital twins focused on the GBA, combining novel sensors with artificial intelligence algorithms, providing descriptive, diagnostic, predictive or prescriptive feed-back. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"The field of artificial intelligence (AI) has gained increasing importance in recent years due to its potential to sustain growth and prosperity in a disruptive way. However, the role of special hardware for AI is still underdeveloped, and dedicated AI-capable hardware is crucial for effective and efficient processing. Moreover, hardware aspects are often neglected in university teaching, which emphasizes theoretical foundations and algorithmic implementations. As a result, there is a need for courses that focus on AI hardware development and its diverse applications. In response to this need, the BB-KI Chips consortium aims to develop a series of hardware-oriented courses with real-world AI applications. This consortium includes the Technical University of Munich (TUM) and the University of Potsdam (UP), which both offer a wide range of courses that focus on AI basics, AI algorithmic development, general computer architectures, chip design, and as well applications of AI. In the BB-KI-CHIPS project, these different capacities are planned to be tightly integrated into a unified curriculum covering knowledge from chip design over AI algorithms and techniques to applications.  ©  Author(s) 2023. CC BY 4.0 License."
"In the present era of the Fourth Industrial Revolution, i.e., 4IR or Industry 4.0, the digital world has witnessed enormous technological advancement. Artificial Intelligence (AI), Machine learning (ML) such as supervised learning (SL), unsupervised learning (USL), semi-supervised learning, and reinforcement learning (RL) are the most exciting recent technologies in today's world. Learning algorithms such as web search engines like Google or Yahoo, E-Mail filtering, Social Media, Airlines, E-hailing ride/price matching are the most valuable features of ML in our daily lives. The e-hailing, which is a process of ordering a cab or taxi online through a digital app, relies on machine learning to perform ride fare calculations. This paper briefly explains how e-hailing providers build dynamic pricing using reinforcement learning and associated problems in the existing pricing algorithm. Often dynamic pricing terms are unfair/discriminated pricing, and it remains controversial amongst consumers. The article discusses how supervised learning can help address pricing issues linked to the dynamic pricing algorithm's reinforcement learnings. The expected outcome of this paper is to help play a reference in creating a practical solution that can resolve pricing issues that float around dynamic pricing algorithms. © 2023 Author(s)."
"Throughout the history of the Communist Party of China (CPC), it has always attached great importance to theoretical innovation based on practice. It has always equipped itself with the achievements of theoretical innovation, educated its people, led its way forward and pooled its strength of struggles. More than seventy years ago, Popular Philosophy- a popular book written by the famous Marxist philosopher Ai Siqi, led generations of intellectuals with aspirations to take the right path of life and inluenced several generations of Chinese readers. © 2023 Peter Lang Group AG. All rights reserved."
"For artificial intelligence (AI) based systems to become clinically relevant, they must perform well. Machine Learning (ML) based AI systems require a large amount of labelled training data to achieve this level. In cases of a shortage of such large amounts, Generative Adversarial Networks (GAN) are a standard tool for synthesising artificial training images that can be used to augment the data set. We investigated the quality of synthetic wound images regarding two aspects: (i) improvement of wound-type classification by a Convolutional Neural Network (CNN) and (ii) how realistic such images look to clinical experts (n = 217). Concerning (i), results show a slight classification improvement. However, the connection between classification performance and the size of the artificial data set is still unclear. Regarding (ii), although the GAN could produce highly realistic images, the clinical experts took them for real in only 31% of the cases. It can be concluded that image quality may play a more significant role than data size in improving the CNN-based classification result. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Purpose: The introduction of artificial intelligence (AI) technology has had a substantial influence on the retail industry. However, AI adoption entails considerable responsibilities and risks for senior managers. In this study, the authors developed an evaluation and selection mechanism for successful AI technology adoption in the retail industry. The multifaceted measurement and identification of critical factors (CFs) can enable retailers to adopt AI technology effectively and maintain a sustainable competitive advantage. Design/methodology/approach: The evaluation and adoption of organisational AI technology involve multifaceted decision-making for management. Therefore, the authors used the analytic network process to develop an AI evaluation framework for calculating the weight and importance of each consideration. An expert questionnaire survey was distributed to senior retail managers and 17 valid responses were obtained. Finally, the Vlse Kriterijumska Optimizacija Kompromisno Resenje (VIKOR) method was used to identify CFs for AI adoption. Findings: The results revealed five CFs for AI adoption in the retail industry. The findings indicated that after AI adoption, top retail management is most concerned with factors pertaining to business performance and minor concerned about the internal system's functional efficiency. Retailers pay more attention to technology and organisation context, which are matters under the retailers' control, than to external uncontrollable environmental factors. Originality/value: The authors developed an evaluation framework and identified CFs for AI technology adoption in the retail industry. In terms of practical application, the results of this study can help AI service providers understand the CFs of retailers when adopting AI. Moreover, retailers can use the proposed multifaceted evaluation framework to guide their adoption of AI technology. © 2023, Emerald Publishing Limited."
"Purpose: The study applied the stimulus–organism–response (S–O–R) framework to investigate the influence of flow elements (e.g. perceived control, concentration and cognitive enjoyment) on artificial intelligence (AI)-enabled e-tail services in evoking awe experience in online fashion apparel context. Design/methodology/approach: Data of 739 active users of online fashion retail shoppers were collected using Amazon Mechanical Turk (MTurk). Partial least square-structural equation modeling was used for analysis. Findings: This study suggested the relevance of AI-enabled services in evoking flow and stimulating the customers' awe experience in online fashion shopping. Practical implications: The use of AI could help online fashion retailers to improve the experiential elements by using stimuli that evoke feelings of vastness, novelty and mysticism. Originality/value: The study offers insights about the relevance and applicability of AI in enhancing the flow elements and awe experience on online fashion apparel shopping in an emerging economy. © 2023, Emerald Publishing Limited."
"Patients can control, share, and manage their health records with family, friends, and healthcare professionals utilizing electronic health records (EHRs), which use an open channel, or the Internet. When a lot of data is available, DL methods show promise in these health applications. A distributed blockchain-based IoT system would benefit greatly from these ideas. This research proposes novel technique in Healthcare Data Based Feature Selection and Classification Using Blockchain and Machine Learning Architectures. The network has been secured using centralized blockchain sensor network. Here the input sensor-based healthcare data has been collected and processed for noise removal and smoothening. Then the processed data feature has been selected using Greedy Mixed Forward Colony Optimization feature selection. The suggested framework's superiority is supported by security research and experimental findings using the IoT-Botnet and ToN-IoT datasets. Proposed technique attained acc of 95%, precision of 85%, recall of 76%, F1 score of 63%, sec rate of 95%, DTRate of 85%. © 2023, IGI Global. All rights reserved."
"World Health Organization's (WHO) emergency learning platform OpenWHO provided by Hasso Plattner Institut (HPI) delivered online learning in real-time and in multiple languages during the COVID-19 pandemic. The challenge was to move from manual transcription and translation to automated to increase the speed and quantity of materials and languages available. TransPipe tool was introduced to facilitate this task. We describe the TransPipe development, analyze its functioning and report key results achieved. TransPipe successfully connects existing services and provides a suitable workflow to create and maintain video subtitles in different languages. By the end of 2022, the tool transcribed nearly 4,700 minutes of video content and translated 1,050,700 characters of video subtitles. Automated transcription and translation have enormous potential as a public health learning tool, allowing the near-simultaneous availability of video subtitles on OpenWHO in many languages, thus improving the usability of the learning materials in multiple languages for wider audiences. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Intelligent manufacturing is used to build up adaptable and versatile assembling activities locally or internationally by utilizing the incorporated ICT and AI that can consolidate progressed processing capacity by assembling hardware. Industry 4.0 development integrates the industries as self-adaptive, self-learning, and flexible as per new conditions from SM/IM. Intelligent manufacturing is quite similar to a conventional manufacturing system in many ways. The similarities are based on timely accession, dispensation, and use of actual data from machines as well as processes in manufacturing. The information gathered from the manufacturing data can be analyzed through the IoT-based system with effective information [10] sharing which helps in improving the standard of the products. The standards of the products can be improved on basis of their quality, efficiency, reliability, and recyclability as well. System and product sustainability achieved through data management, data mining is nothing but the contribution of an intelligent manufacturing system to the factories for future growth and stability. The architecture of a cloud-based manufacturing system [22, 25-27] is equipped with smart components. For the ease of users, the data is transferred to the cloud through a specifically designed architecture called SOA. © 2023 Scrivener Publishing LLC."
"Unmanned aerial vehicles (known as drones) and artificial intelligence are attracting academic and industrial attention. Unmanned aerial vehicles (UAVs) have increased the ability to manage and supervise from remote locations. Unmanned aerial vehicles are employed in a variety of purposes and are becoming extremely popular. Recent advancements in unmanned drones and artificial intelligence have created a new opportunity for independent flying and maintenance. Artificial intelligence and deep learning are already driving the advancement of unmanned aerial vehicles and ensuring their independent future. Computer vision has made significant advances in image feature extraction and classification, as well as object recognition, making this a very appealing research subject when used on unmanned aerial vehicles. Because UAVs learn via techniques and apply them for decision making purposes, artificial intelligence is also vital and beneficial, but it can also be hazardous and severe. Artificial intelligence has decreased the number of obstacles faced by unmanned aerial vehicles while also boosting their capabilities and allowing them to enter new markets. The combination of Unmanned Drones and Artificial intelligence has resulted in quick and accurate results. The integration of UAVs and artificial intelligence has aided in real-time surveillance, data collection and analysis, and forecasting in computer/wireless network infrastructure, smart cities, the armed services, agricultural production, and mining. This study obtained and consolidated studies on the usage of UAVs, along with artificial intelligence and related algorithms, in various domains and countries. The goal was to illustrate overall scope and significance of artificial intelligence models in improving UAV performance, problem solving, and a variety of application domains. © 2023 Scrivener Publishing LLC. All rights reserved."
"Data is the integration of various AI (AI) algorithms for important features, the online details are scattered everywhere and controlled separately who do not believe in each other, and the use of information in complex Cyberspace. Too fast and hard to authorize or verify. Because of this, it is very difficult to enable data sharing on cyberspace of big data important, and as a matter-of-fact powerful AI. In this paper, we propose Secretary Network, I construction that will keep data safe, computer programming, and sharing within the Internet environment, aimed at a real cyber safe real estate data and thus improve AI with multiple data sources, with to combine three key elements: Block proprietary data sharing guarantee, which allows for reliable data sharing within a large area to make real big data; Creating a reliable cyberspace, based on AI a secure computer platform is used for mass production smart safety rules. A reliable way to exchange the purchase price Security, they provide the way participants can see economic rewards when releasing their data or service, which encourages the sharing of information and thus achieves better AI performance. Moreover, we talk standard usage of Sec Net and its another possible method, as well as analysis its effectiveness from the aspect of network security and economic revenue. © 2023 Author(s)."
"Understanding the aspects of progression for atherosclerotic cardiovascular disease and treatment is key to building reliable clinical decision-support systems. To promote system trust, one step is to make the machine learning models (used by the decision support systems) explainable for clinicians, developers, and researchers. Recently, working with longitudinal clinical trajectories using Graph Neural Networks (GNNs) has attracted attention among machine learning researchers. Although GNNs are seen as black-box methods, promising explainable AI (XAI) methods for GNNs have lately been proposed. In this paper, which describes initial project stages, we aim at utilizing GNNs for modeling, predicting, and exploring the model explainability of the low-density lipoprotein cholesterol level in long-term atherosclerotic cardiovascular disease progression and treatment. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Street crashes are the most widely recognized types of mishaps and passing around the world, and the critical explanations behind these mishaps are as a rule tanked tiredness and crazy conduct of the driver. As indicated by the World Health Association, street traffic wounds have ascended to 1.25 billion around the world, which makes driver tiredness recognition a significant expected region to deflect various rest incited street mishaps. This task proposes a plan to recognize sleepiness utilizing AI calculations, subsequently disturbing the driver progressively to forestall a impact. The model uses the calculation, alongside the Open CV library to screen the constant video of the driver and to identify the eyes of the driver. The framework utilizes the Eye Aspect Ratio (EAR) idea to decide whether the eyes are open or shut. We additionally feed an informational index record comprising of the facial highlight's information focuses to train the AI calculation. The model investigates each casing of the video, which assists with perceiving the condition of the driver. Moreover, a Raspberry Pi single-board PC, joined with a camera module and a caution framework, works with the undertaking to copy a minimal sleepiness discovery framework appropriate for various cars. Tired driving is one of the huge purposes behind passing occurring in road disasters. In this paper, a module for Advanced Driver Help System (ADAS) is presented to downsize the number of mishaps because of driver exhaustion and thusly increment the transportation wellbeing. The transporters who drive for reliable broadened periods (especially around evening time), transport drivers of critical distance courses, or overnight vehicles are more defenseless to this issue. Driver laziness is an overcast terrible dream to voyagers in every country. Every year, a tremendous number of wounds and passing occur in light of exhaustion-related road setbacks. Consequently, the acknowledgment of the driver's exhaustion and its sign is a working space of investigation in view of its tremendous rational relevance. © 2023 Author(s)."
"Artificial Intelligence (AI) techniques are of great potential to fundamentally change antibiotic discovery industries. Efficient and effective molecular featurization is key to all highly accurate learning models for antibiotic discovery. In this paper, we propose a fingerprint-enhanced graph attention network (FinGAT) model by the combination of sequence-based 2D fingerprints and structure-based graph representation. In our feature learning process, sequence information is transformed into a fingerprint vector, and structural information is encoded through a GAT module into another vector. These two vectors are concatenated and input into a multilayer perceptron (MLP) for antibiotic activity classification. Our model is extensively tested and compared with existing models. It has been found that our FinGAT can outperform various state-of-the-art GNN models in antibiotic discovery. © 2023 American Chemical Society. All rights reserved."
"Around the world, the concept of a smart city is becoming an increasingly important research area. The amount of data collected and the number of mounted sensors, the surveillance equipment, and other gadgets that must be installed in a smart city are so large that using a mobile platform to monitor and keep replacing them can save effort and materials. Currently, people are experiencing rapid evolution in every field, owing to continuing developments in communication technology and other advances such as fully independent autonomous drones. These developments have already led to an improvement in people's lives, such as in wellbeing or quality of life, in the energy sector, in the shipping industry, in the supervision and surveillance guidelines for large households and commercial projects. As such, this chapter discusses the assimilation of the most widely used communication technology trend, the Internet of things (IoT), with drones or unmanned aerial vehicles (UAVs). The uses of IoT in UAVs to examine numerous buildings in smart cities are evaluated and the use of such IoT-enabled self-governing aerial vehicles is reinforced for guaranteeing smart city applications and safety measures. The chapter also describes the important drawbacks and limitations of conventional technologies for the same reason, such as optimization methods in trajectory tracking, portable artificial intelligence (AI) and machine vision methodologies, collaboration in IoT correspondence, and network infrastructure scalability. Here, we suggest an open-source smart IoT technology that would enable the deployment of a variety of sensible or smart city services, including smart homes, smart unmanned drones, smart energy grids, smart transportation networks, and other prospective services that could be developed in response to market demands. We hope that readers will find the discussions on multiple open research issues relevant and helpful. © 2023 Scrivener Publishing LLC. All rights reserved."
[No abstract available]
"The proceedings contain 12 papers. The topics discussed include: nurturing talents of geomatics surveyors to meet the challenges of land supply, works projects and smart city of Hong Kong; challenge based learning geoscience: student-oriented teaching for digital mapping; exploration and application of ‘1+2+3’ teaching mode using virtual reality environments for geomatics undergraduate students; environment monitoring along China-Europe railway express with remote sensing and artificial intelligent technology: a regional collaboration project between China and Serbia; virtual reality traverse: a novel way to teach traverse surveying; the design of scalable Web GIS microservice framework for undergraduate education; geospatial approach for petrol pumps valuation with urban prediction modelling by cellular automata in creeds of metropolitan expanse; and integrating AI hardware in academic teaching: experiences and scope from Brandenburg and Bavaria."
"Electric Vehicles (EV) generally provide environmentally beneficial mobility, however they are still in their early stages of development. At peak hours, there are insufficient EV charging stations and grid instability due to the delayed charging time. Normally, this issue is solved by switching PEV charging to the grid's off-peak hours. This update not only reduces the number of times the battery needs to be charged, but it also raises questions about the charging price per hour. To moderate this problem, this paper proposes an Artificial Intelligent based SOC (State Of Charging) control in grid connected PV based electric vehicles using SEPIC converter. At the electric vehicle charging station with battery backup, a high gain, quick charging DC-DC SEPIC converter is used for grid integrated solar PV. The performance of SEPIC converter has been analyzed using MATLAB/Simulink tool. © 2023 Author(s)."
"Some public areas like beaches are crowded during the weekends and are tedious to manage with a limited number of authorities. People in these places are prone to danger. Aside from cases of drowning, there have also been reports of suicide on beaches. Other mischievous or dangerous activities (e.g., theft) happen in such places. Most of these incidents happen when the crowd is bigger and those in charge of surveillance will find it challenging to monitor on the ground. The proposed solution is to build a deep learning-based model for the detection of drowning in the sea or any swimmers taken into sudden high waves. We will be using thermal images from the drone and extracting the features of humans, the model is trained to detect those large sets of images. So, once a person moves away from the safety swimming line on the beach, the system will alert the persons near the shoreline and the coast guards. We will also use the posture estimation model to detect miscellaneous activities and take pictures and alert the officials to take action. This drone will be self-automated based on its set boundary and it will map with GPS integration. With this, we can avoid untoward activities from happening with smart surveillance using drones. © 2023 Scrivener Publishing LLC. All rights reserved."
"To keep up with rapid human population growth, food demand should increase. Quickly spreading illnesses can fundamentally affect plant yields, in any event, slaughtering whole harvests. As a consequence, detecting and preventing disease at an early stage is critical. Traditional approaches depend on lab studies and human experience, both of which are typically costly and in short in supply in most developing countries. As smartphones have become more popular, even in remote regions, researchers have turned to automated image processing to identify plant disease. This paper compares deep learning to conventional machine learning algorithms and explores the most recent developments in this field. AI, crop infections, and deep learning are a portion of the terms that ring a bell when considering this exploration. © 2023 Author(s)."
"The interest in the application of AI in medicine has intensely increased over the past decade with most of the changes in the past five years. Most recently, the application of deep learning algorithms in prediction and classification of cardiovascular diseases (CVD) using computed tomography (CT) images showed promising results. The notable and exciting advancement in this area of study is, however, associated with different challenges related to the findability (F), accessibility(A), interoperability(I), reusability(R) of both data and source code. The aim of this work is to identify reoccurring missing FAIR-related features and to assess the level of FAIRness of data and models used to predict/diagnose cardiovascular diseases from CT images. We evaluated the FAIRness of data and models in published studies using the RDA (Research Data Alliance) FAIR Data maturity model and FAIRshake toolkit. The finding showed that although AI is anticipated to bring ground breaking solutions for complex medical problems, the findability, accessibility, interoperability and reusability of data/metadata/code is still a prominent challenge. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Although there has been no shortage of technological innovation in recent decades, a solution to sociodemographic disparities in the forensic setting has remained elusive. Artificial intelligence (AI) is a uniquely powerful emerging technology that is likely to either exacerbate or mitigate existing disparities and biases. This column argues that the implementation of AI in forensic settings is inevitable, and that practitioners and researchers should focus on developing AI systems that reduce bias and advance sociodemographic equity rather than attempt to impede the use of this novel technology.  © 2023 Wolters Kluwer Health, Inc. All rights reserved."
"Remote monitoring is becoming increasingly popular among healthcare professionals and patients for diagnosing and treating heart disease. Several smart devices connected to smartphones have been developed and validated in recent years, but their clinical use is still limited. Significant advances in the field of artificial intelligence (AI) are also revolutionizing several fields, yet the impact that these innovations could have on routine clinical practice is still unknown. We review the evidence and uses of the main smart devices currently available as well as the latest applications of AI in the field of cardiology, with the aim to ultimately evaluate the potential of this technology to transform modern clinical practice. © 2023 Editions Medecine et Hygiene. All rights reserved."
"With its seeming competence to mimic human responses, ChatGPT, an emerging AI-powered chatbot, has spurred great interest. This study aims to explore the role of ChatGPT in synthesizing medication literature and compare it with a hybrid summarization system. We tested ten medications' effectiveness with reference to their definitions and descriptions extracted from DrugBank. ChatGPT could generate coherent summaries that are not backed by evidence. In contrast, our approach can provide a highly structured and concise synthesis of related evidence, but the resulting summary is not as fluent and convincing as ChatGPT. Therefore, we recommend integrating both techniques to achieve the best performance. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Objective. Artificial intelligence (AI) methods have gained popularity in medical imaging research. The size and scope of the training image datasets needed for successful AI model deployment does not always have the desired scale. In this paper, we introduce a medical image synthesis framework aimed at addressing the challenge of limited training datasets for AI models. Approach. The proposed 2D image synthesis framework is based on a diffusion model using a Swin-transformer-based network. This model consists of a forward Gaussian noise process and a reverse process using the transformer-based diffusion model for denoising. Training data includes four image datasets: chest x-rays, heart MRI, pelvic CT, and abdomen CT. We evaluated the authenticity, quality, and diversity of the synthetic images using visual Turing assessments conducted by three medical physicists, and four quantitative evaluations: the Inception score (IS), Fréchet Inception Distance score (FID), feature similarity and diversity score (DS, indicating diversity similarity) between the synthetic and true images. To leverage the framework value for training AI models, we conducted COVID-19 classification tasks using real images, synthetic images, and mixtures of both images. Main results. Visual Turing assessments showed an average accuracy of 0.64 (accuracy converging to 50 % indicates a better realistic visual appearance of the synthetic images), sensitivity of 0.79, and specificity of 0.50. Average quantitative accuracy obtained from all datasets were IS = 2.28, FID = 37.27, FDS = 0.20, and DS = 0.86. For the COVID-19 classification task, the baseline network obtained an accuracy of 0.88 using a pure real dataset, 0.89 using a pure synthetic dataset, and 0.93 using a dataset mixed of real and synthetic data. Significance. A image synthesis framework was demonstrated for medical image synthesis, which can generate high-quality medical images of different imaging modalities with the purpose of supplementing existing training sets for AI model deployment. This method has potential applications in many data-driven medical imaging research. © 2023 The Author(s). Published on behalf of Institute of Physics and Engineering in Medicine by IOP Publishing Ltd."
"Farmers of the twentieth century are adopting the use of leading technical facilities in the field of agriculture to sustain the competitiveness of the global market economy. With the help of modern technology, they are trying to reduce production costs and improve crop yield with better product quality. A new era in which many traditional agricultural practices are replaced by cutting-edge technologies like Precision Farming (PF), which entails applying the agronomic variables in the right place, at the right time, has been ushered in by technological advancements made in monitoring, supervision, management, and control systems. One of the methods that will enable quick and non-destructive analysis of agricultural data will turn out to be an IoT integrated smart agriculture drone. This includes taking pictures to analyze crop behavior, finding out how much water the soil can contain, managing irrigation systems, etc. Numerous engineering disciplines, including aerodynamics, electronics, computer programming, and economics are integrated in the design, development, and implementation of drone-based agricultural systems. In considering the above thought, a problem taken into consideration is for a grape field of approximately 10 acres located near Nashik (28 36'N, 77 12'E) in the state of Maharashtra to analyze the performance of IoT enabled agriculture drones for PF. Main objective is to spray the insecticide with an agriculture drone only on the detection of the green zone of the crop or canopy for effective spraying and hence to reduce the wastage and cost of spraying. Drone image sensor will capture the field images, atmospheric parameters like, temperature, humidity in the field and analyze it on the cloud platform for analyzing the crop health and related parameters. An agriculture drone of 16 L capacity is proposed for spraying 1 acre of land. It could take 7-10 minutes of flight time of the drone to spray the insecticide which can lead to reducing the labor cost by 65%, spraying time by 85% and amount of insecticide by 50%. Drone camera and spraying mechanism is to be synchronized to achieve the objective through IoT platform. For effective spraying from the bottom and the top of the leaf, synchronized UGV and UAV spraying is proposed. Further, time series analysis of the photographic images and video footage captured by the drone camera can be done for the prediction, modeling of crop yields and auditing them with computer vision capabilities of UAV, and developing sufficient datasets. These data sets can be used for machine learning algorithms for AI or DL and for future research related to crop disease forecasting, canopy cover, stress management, and be able to guide farmers for taking corrective actions in advance and evolve best practices for overall improvement in the yield. © 2023 Scrivener Publishing LLC. All rights reserved."
"Graph data, which often includes a richness of relational information, are used in a vast variety of instructional puzzles these days. Modelling physics systems, detecting fake news on social media, gaining an understanding of molecular fingerprints, predicting protein interfaces, and categorising illnesses all need graph input models. Reasoning on extracted structures, such as phrase dependency trees and picture scene graphs, is essential research that is necessary for other domains, such as learning from non-structural data such as texts and photos. These types of structures include phrase dependency trees and image scene graphs. Graph reasoning models are used for this kind of investigation. GNNs have the ability to express the dependence of a graph via the use of message forwarding between graph nodes. Graph convolutional networks (GCN), graph attention networks (GAT), and graph recurrent networks (GRN) have all shown improved performance in response to a range of deep learning challenges over the course of the last few years. © 2023, IGI Global."
"Attendance can be defined as a way of means oral action which can be used to determine if a person is present or not in a particular place. For instance, we can consider a class room of students and a teacher as a real time example. A teacher may take attendance at the start or end of a session. But the manual way of taking attendance may sometimes lead to many issues such as missing a student's number, students answering to their number multiple times and so on. As days become more digital, Machine Learning continues to provide us with a lot of opportunities to improve methods used for computer vision applications. Hence, we believe using AI for Face - affirmation in the programmed participation frameworks will help provide better working. In this undertaking, we analyse the face assertion and face region calculations, to provide the PC structures, the constraint of finding and seeing human faces rapidly and absolutely in pictures or accounts with the sole target which is that the frameworks can be utilized in giving interest. © 2023 Author(s)."
"Long COVID, frequently associated to neuropsychiatric manifestations, impacts the ability of patients to return to work and often requires adjustments of the previous workstation. Due to the duration of the symptoms and the professional consequences, disability insurance (DI) procedures may be necessary. Because the persistent symptoms of Long COVID are often subjective and unspecific, the medical report to the DI should include a detailed description of the functional impact of these manifestations. © 2023 Editions Medecine et Hygiene. All rights reserved."
"Purpose: This study aims to explore and clarify the role of national traditional festival tourism in cultivating national identity (NI) and confirm its construction model. Design/methodology/approach: Based on social identity theory and complexity theory, a complex nurturing framework for visitors’ NI is developed. The paper with 479 samples used fuzzy-set qualitative comparative analysis to analyse NI from the holistic perspective of “cultural inheritance” (festival authenticity [FA], historical re-enactment [HR] and cultural experience [CE]) and “inherited innovation” (event design innovation [EDI], cultural innovation [CUI], aesthetic innovation [AI] and creative innovation [CRI]). Findings: The findings indicated three driving modes of forming NI: cultural inheritance-led, inherited innovation-led and the dual coordination of cultural inheritance and inherited innovation. FA, HR, CE, AI and CRI are core incentives, whereas event design and CUI are AI. Practical implications: The findings provide directions for strengthening visitors’ national emotion, which has significant value for the development of traditional festival tourism. Originality/value: The study offers a new perspective for the cultivation of NI in the tourism context and provides theoretical guidance for the coordinated development of cultural inheritance and inherited innovation in national traditional festival tourism destinations. © 2022, Emerald Publishing Limited."
"In Sweden, the term information-driven care has recently been put forward by healthcare organizations and researchers as a means for taking a comprehensive approach to the introduction of Artificial Intelligence (AI) in healthcare. The aim of this study is to systematically generate a consensus definition of the term information-driven care. To this end, we are conducting a Delphi study utilizing literature and experts' opinions. The definition is needed to enable knowledge exchange on information-driven care and operationalize its introduction into healthcare practice. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Machining is a subtractive manufacturing method that removes chip from a work item. Machining can be broadly divided into two kinds depending on the cutting tool and energy sources: (i) conventional machining and (ii) non-conventional machining. Turning and milling are two common conventional machining processes whereas electrical discharge machining (EDM), ultrasonic machining (USM), laser beam machining (LBM), etc. are non-conventional machining processes. Improving productivity necessitates the selection of process parameters, cutting tools, and machines with attention. However, over the previous few decades, such parameters have been chosen via a standard approach. The efficiency of the machining process can be improved if the industry adopts intelligent machining techniques that can offer self-optimization and adaptation to unforeseen situations. Machine learning (ML) algorithms have been used to diagnose and forecast the health of machine tools, optimize process parameters, and anticipate the quality of the end products manufactured by both conventional and non-conventional machining processes, all of which contribute to increased productivity. This chapter explores several ML processes and how they are used in various machining processes. © 2023 Scrivener Publishing LLC."
[No abstract available]
[No abstract available]
"Morphological measurements of nanoparticles in electron microscopy images are tedious, laborious, and often succumb to human errors. Deep learning methods in artificial intelligence (AI) paved the way for automated image understanding. This work proposes a deep neural network (DNN) for the automated segmentation of a Au spiky nanoparticle (SNP) in electron microscopic images, and the network is trained with a spike-focused loss function. The segmented images are used for the growth measurement of the Au SNP. The auxiliary loss function captures the spikes of the nanoparticle, which prioritizes the detection of spikes in the border regions. The growth of the particles measured by the proposed DNN is as good as the measurement in manually segmented images of the particles. The proposed DNN composition with the training methodology meticulously segments the particle and consequently provides accurate morphological analysis. Furthermore, the proposed network is tested on an embedded system for integration with the microscope hardware for real-time morphological analysis.  © 2023 The Authors. Published by American Chemical Society."
"Nowadays, industry 4.0 is very popular technology for industrial automation & rapid development of electrical and electronics technology, information technology, or advanced manufacturing technology. In this research concluded the challenges of cyber security and AI based industry 4.0, and discusses the key emerging technologies like internet of things, cloud computing, cyber security, artificial intelligence, etc., for making an industry 4.0 revolution. In this research the analysis of major issues and potential solution to key emerging technologies of new world is presented. This research gives the all-industrial revolution comparison analysis to growth of manufacturing industries and its benefits of different revolution. In this research paper give the analysis of why industry 4. 0 is best for the advanced manufacturing for growth of industries. This chapter provides a holistic view of cyber security and artificial intelligence in the industry 4.0 landscape, identifying and analyzing its fundamental concept, challenges, and ongoing future trends. © 2023, IGI Global. All rights reserved."
"Advance Face Recognition Attendance System is takes understudies or agents support by Face Recognition. Has a spot with this endeavor each and every understudies and laborers can save their huge Time. This is one of the AI's Concept. This Face Recognition Attendance system will be use on Open CV Library. This Face Recognition Attendance structure at first compasses each understudy's face then it will normally put interest. There are a couple of different methods of stepping support, the most notable methods of stamping or call the understudies. We have proposed to execute this structure through this colossal application are joined. The current execution fuses facial distinctive proof that is proficient and the possible results of delegate interest in view of the facial endorsement. This system would now have the option to be used in a space wherein interest expects a critical part. It has involved python is to perform face affirmation to convey the nuances name the understudy. Who go to class and to communicate the investment to the subject. The data is presently to save in the memory. The human face is one of the ordinary qualities that can especially recognize an individual. Right when an individual recognized, its cooperation will be cut down normally saving essential information into an overwhelm sheet. Around the day's end, the overwhelm sheet containing interest information with respect to all individuals. © 2023 Author(s)."
[No abstract available]
"While electric power artificial intelligence technology continues to make breakthroughs, it also faces many challenges in practical applications. Based on the hypothetical analysis and application paradigm research of electric power artificial intelligence, this article explores the innovative application mode based on self-learning AI. First, aiming at the bottlenecks of credibility ethics, data distribution, and evolutionary migration that existed in the research of electric power artificial intelligence, three mechanisms of data knowledge fusion, parallel interaction, and model evolution are proposed and elaborated. Furthermore, based on the biological-brain-cognitive principle, a consciousness guided self-learning technology suitable for the application of artificial intelligence in the power field is proposed. The self-learning applications such as model construction, data organization, and training optimization are carried out by constructing the machine consciousness guidance algorithm to solve the problems of training and optimization of electric power artificial intelligence model under the conditions of complex rules, low data value, and scene generalization. Finally, application exploration is carried out in the field of equipment operation and maintenance. By guiding the understanding and analysis of knowledge, data and tasks through the consciousness mechanism, an intelligent application for the generation of end-to-end algorithms for complex operation and maintenance tasks can be built. ©2023 Chin.Soc.for Elec.Eng."
[No abstract available]
"The WHO Early AI-Supported Response with Social Listening (EARS) platform was developed to help inform infodemic response during the COVID-19 pandemic. There was continual monitoring and evaluation of the platform and feedback from end-users was sought on a continual basis. Iterations were made to the platform in response to user needs, including the introduction of new languages and countries, and additional features to better enable more fine-grained and rapid analysis and reporting. The platform demonstrates how a scalable, adaptable system can be iterated upon to continue to support those working in emergency preparedness and response. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
[No abstract available]
"Introduction Stroke is a time-critical condition and one of the leading causes of mortality and disability worldwide. To decrease mortality and improve patient outcome by improving access to optimal treatment, there is an emerging need to improve the accuracy of the methods used to identify and characterise stroke in prehospital settings and emergency departments (EDs). This might be accomplished by developing computerised decision support systems (CDSSs) that are based on artificial intelligence (AI) and potential new data sources such as vital signs, biomarkers and image and video analysis. This scoping review aims to summarise literature on existing methods for early characterisation of stroke by using AI. Methods and analysis The review will be performed with respect to the Arksey and O'Malley's model. Peer-reviewed articles about AI-based CDSSs for the characterisation of stroke or new potential data sources for stroke CDSSs, published between January 1995 and April 2023 and written in English, will be included. Studies reporting methods that depend on mobile CT scanning or with no focus on prehospital or ED care will be excluded. Screening will be done in two steps: title and abstract screening followed by full-text screening. Two reviewers will perform the screening process independently, and a third reviewer will be involved in case of disagreement. Final decision will be made based on majority vote. Results will be reported using a descriptive summary and thematic analysis. Ethics and dissemination The methodology used in the protocol is based on information publicly available and does not need ethical approval. The results from the review will be submitted for publication in a peer-reviewed journal. The findings will be shared at relevant national and international conferences and meetings in the field of digital health and neurology.  © 2023 BMJ Publishing Group. All rights reserved."
"Reinforcement learning has been giving new strategies to ad-lib in the fields of medication and the drug industry. It is probably the best strategy among all the AI strategies. Reinforcement learning has been compelling to discover strategies that could treat the immense number of deadly infections. In this paper, a detailed new methodology about what Reinforcement Leaning and what is drug revelation and what are various techniques utilized in Reinforcement learning, how Reinforcement learning could be utilized in different utilization of medication plan such as target identification, hit discovery, hit to lead, lead optimization. The consequence of the paper illustrates the summarized data of different approaches for the development and discovery of drugs. A clear illustration of various work done in the field of drug discovery, discuss different pros and cons of Reinforcement learning as well emphasize primary challenges need to handleand how to defeat them in the future. © 2023 American Institute of Physics Inc.. All rights reserved."
"Generally, drones are known as Unmanned Aerial Vehicle (UAV). Humans or robots can control the UAV remotely. Drone technology was introduced in twentieth century for military purposes and it came to common people hands by twenty-first century. Artificial Intelligence (AI) helps the machine to think artificially and it makes them to behave like humans. In this digital era, most of electronic devices are converted into smart devices with help of the AI technology. This chapter explores the role of AI in drone technologies. Drone technologies are used by the following organizations: military, industries, and logistics. AI applications of drones are discussed in this chapter with detailed uses cases. AI technology helps the drone to collect data and predict the outcome instantly. The smart drone greatly helps people to execute the hard work they are reluctant to do and to produce more efficient results. This chapter provides an overview of the issues and challenges of drone technologies using AI applications for researchers and industry persons. © 2023 Scrivener Publishing LLC. All rights reserved."
"The project targets constructing an AI model that will actually want to order the different hand motions utilized for fingerspelling in gesture based communication. In this client free model, classification AI algorithm are prepared utilizing a bunch of picture information and testing is done on a totally unique arrangement of information. For the picture dataset, there are some depth pictures are utilized, which gave preferable yields over a portion of the past literary works, neglected to the decreased preprocessing time. Different AI algorithm are applied on the datasets, including Convolutional Neural Network (CNN). An endeavor is made to expand the precision of the CNN model by pre-training it on the ImageNet dataset. Be that as it may, a little dataset was utilized for pre-preparing, which gave a precision of 18% during preparing. Communication through signing comprises of fingerspelling, what explains words character by character, and word level affiliation which includes hand signals that pass on the word meaning. Fingerspelling is a fundamental instrument in communication through signing, as it empowers the correspondence of names, addresses and different words that don't convey an importance in word level affiliation. Despite this, fingerspelling isn't broadly utilized as it is trying to comprehend and hard to utilize. Besides, there is no widespread gesture based communication and not many individuals know it, which makes it a lacking option for correspondence. © 2023 Author(s)."
"The COVID-19 issue has, albeit at varied rates, pushed digital transformation in industry and services across the majority of nations. Achieving the objectives on the path to excellence depends on the delivery of customized goods or services and changing consumer needs. Digital marketing can aid in expanding customer reach internationally and strengthening client relationships. While data-driven decision-making reduces risk, technology can help marketers deliver customers more of what they want and need. Artificial intelligence (AI), the internet of things (IoT), remote collaboration, cloud computing, blockchain, and data analytics are among the technologies that are rapidly transforming the way that digital marketers do business and develop strategies leading to global economic growth. This book chapter explores the potential of AI and IoT in digital marketing. It also provides information on the impact of new technologies on digital marketing and the top trends in this field. © 2023, IGI Global."
"Designing access control policies is often expensive and tedious due to the heterogeneous systems, services, and diverse user demands. Although ABAC policy and decision engine creation methods based on machine learning have been proposed, they cannot make good access decisions for applications and situations not envisioned by the decision-makers who provide training examples. It results in over-and under-permissiveness. In this paper, we propose a framework that refines pre-developed policies. It creates a decision engine that makes better decisions than those policies. Inspired by multiple criteria decision theory, our method uses the policy manager's qualitative intentions behind their judgments to guide access decisions so that more benefits are expected. In the evaluation, we prepare a coarse and relatively elaborate policy. We refine the coarse policy to obtain a decision engine that is compared for the similarity in access decisions with the elaborate policy using AUC as a measure. The results show that our method improves the coarse policy by a difference of 12-26% in AUC and outperforms the conventional machine learning methods by a difference of 3-11% in AUC. © 2023 ACM."
"We could not deny that artificial intelligence has had an impact on healthcare. However, it has certain issues. In recent years, civil law and medical ethics have both addressed this issue. The law of liability is necessary when artificial intelligence is applied in healthcare services because it raises the possibility of bad decisions and the issue of who has responsibility for them. The Ethical and Governance Recommendations for AI for Health Systems, released by the World Health Organization, intends to establish ethical guidelines on the deployment of artificial intelligence to address the potential ethical and legal implications of non-discrimination and accountability. Providers of artificial intelligence services must adhere to moral and legal principles that are consistent with international considerations and user protection laws. To build AI ethics, it is necessary to raise accountability and enhance legal and regulatory frameworks. © 2023 by IGI Global. All rights reserved."
"Shortawn foxtail (Alopecurus aequalis Sobol.) is an obligate wetland plant that is widely distributed throughout Europe, temperate Asia, and North America. In China, it is widespread in the middle and lower reaches of the Yangtze River as a noxious weed in winter cropping fields with a rice (Oryza sativa L.) rotation. The acetolactate synthase (ALS)-inhibiting herbicide mesosulfuron-methyl has been widely used to control annual grass and broadleaf weeds, including A. aequalis, in wheat (Triticum aestivum L.) fields, leading to the selection of herbicide-resistant weeds. In this study, an A. aequalis population, AHFT-4, that survived mesosulfuron-methyl at the field-recommended rate (9 g ai ha-1) was collected in Anhui Province. Single-dose testing confirmed that the suspected resistant AHFT-4 had evolved resistance to mesosulfuron-methyl. Target gene sequencing revealed a resistance mutation of Pro-197-Ala in ALS1 of the resistant plants, and a derived cleaved amplified polymorphic sequence marker was developed to specifically detect the mutation. A relative expression assay showed no significant difference in ALS expression between AHFT-4 and a susceptible population without or with mesosulfuron-methyl treatment. Whole-plant dose-response bioassays indicated that AHFT-4 had evolved broad-spectrum cross-resistance to ALS-inhibiting herbicides of all five chemical families tested, with GR50 resistance index (RI) values ranging from 21 to 206. However, it remained susceptible to the photosystem II inhibitor isoproturon. Pretreatment with the cytochrome P450 inhibitor malathion or the glutathione S-transferase inhibitor 4-chloro-7-nitrobenzoxadiazole had no significant effects on the resistance of AHFT-4 to mesosulfuron-methyl. To our knowledge, this study reports for the first time the ALS gene Pro-197-Ala substitution conferring broad-spectrum cross-resistance to ALS-inhibiting herbicides in A. aequalis.  © The Author(s), 2023. Published by Cambridge University Press on behalf of the Weed Science Society of America."
Early insights and opportunities of AI-powered pair-programming tools.  © 2023 ACM.
"Purpose: Innovation in fintech presents great opportunities and huge challenges for accounting practices around the world. This paper aims to examine the impact of Fintech on accounting practices including financial reporting, performance management, budgeting, auditing, risk and fraud management. Fintech is proxied by the adoption of AI and big data analysis in accounting practices. Design/methodology/approach: We chose African countries as our focus countries and surveyed chartered and qualified accountants in both Ghana and Nigeria. With 201 questionnaires qualified for our final analyses, we adopted the structural equation modelling to analyse the impact of Fintech on accounting practices. Findings: The empirical results show that the impact of AI and big data on accounting practices is positive and significant, indicating that fintech could potentially mitigate the agency problem in accounting practices and lead to better accounting practices. Interestingly, we find that, in general, the impact of AI is larger than that of big data. Originality/value: Our results provide significant insights to regulators, policymakers and managers about the future development of adopting fintech in the regulation and governance framework at both macro and micro levels for accounting practice. © 2023, Emerald Publishing Limited."
[No abstract available]
"The usage of artificial intelligence (AI) technologies that depend on massive volumes of data, which are frequently made available through IoT, is thus directly related to the creation of smart governments. In order to increase the effectiveness of governance and the standard of living for citizens, Internet - of -things enabled AI technologies can be used in several important areas of smart government. In order to provide fulfilled government functions, AI and its subdomain innovations have the opportunity to address a number of current organizational inadequacies. In this chapter the foundations, benefits, and challenges implementing these technologies in the public sector or government are discussed. Following that, the various AI, Ml technologies and IoT frameworks for smart governance are explored. Then the focus is on the applications and use cases of IoT, AI and machine learning for smart governance. © 2023, IGI Global. All rights reserved."
"E-government is the effective and efficient delivery of high-quality information and governmental services to citizens and/or other governmental and non-government entities. Both the service provider and the citizens using the services will gain from big data analysis. Market Research indicates that 75% of government agencies are using big data to enhance the standard of living for individuals. Big data analytics gives a perspective into the effectiveness of government programmes and policies. In order to gauge public sentiment and comprehend citizens ' thoughts and attitudes about government programmes, forecasting and prescriptive analysis suggests the optimal course of action. This chapter will provide a brief overview of key topics to help readers fully comprehend how big data is used across the government department. © 2023, IGI Global. All rights reserved."
This chapter will assist the stakeholders in their digital transformation journey for implementing a visibility solution. It will touch upon the broader need for Supply Chain Visibility in large organizations. A problem-solution approach will be followed. The problem of not having a single placeholder for visibility and tracking will be highlighted. A solution will be described in detail. A couple of solutions will be proposed. The process to implement a home-grown control tower solution will be explained. Another technology Driven SCV process with a focus on real-time data sharing will be proposed. This solution will be complemented with intelligent decision making using the latest technologies like IOT and AI. Use cases will be shared for both the approaches. A generic supply chain visibility framework with a high level of technical details will be shared. The Change Management process will be explained in detail and a process for efficient collaboration between various stakeholders will be shared. © 2023 by IGI Global. All rights reserved.
"Our last emerging trend article introduced Risks 1.0 (fairness and bias) and Risks 2.0 (addictive, dangerous, deadly, and insanely profitable). This article introduces Risks 3.0 (spyware and cyber weapons). Risks 3.0 are less profitable, but more destructive. We will summarize two recent books, Pegasus: How a Spy in Your Pocket Threatens the End of Privacy, Dignity, and Democracy and This is How They Tell Me the World Ends: The Cyberweapons Arms Race. The first book starts with a leak of 50,000 phone numbers, targeted by spyware named Pegasus. Pegasus uses a zero-click exploit to obtain root access to your phone, taking control of the microphone, camera, GPS, text messages, etc. The list of 50,000 numbers includes journalists, politicians, and academics, as well as their friends and family. Some of these people have been murdered. The second book describes the history of cyber weapons such as Stuxnet, which is described as crossing the Rubicon. In the short term, it sets back Iran's nuclear program for less than the cost of conventional weapons, but it did not take long for Iran to build the fourth-biggest cyber army in the world. As spyware continues to proliferate, we envision a future dystopia where everyone spies on everyone. Nothing will be safe from hacking: not your identity, or your secrets, or your passwords, or your bank accounts. When the endpoints (phones) have been compromised, technologies such as end-to-end encryption and multi-factor authentication offer a false sense of security; encryption and authentication are as pointless as closing the proverbial barn door after the fact. To address Risks 3.0, journalists are using the tools of their trade to raise awareness in the court of public opinion. We should do what we can to support them. This paper is a small step in that direction.  © The Author(s), 2023. Published by Cambridge University Press."
"The self-consistent phonon (SCP) method allows one to include anharmonic effects when treating a many-body quantum system at thermal equilibrium. The system is then described by an effective temperature-dependent harmonic Hamiltonian, which can be used to estimate its various dynamic and static properties. In this paper, we combine SCP with ab initio (AI) potential energy evaluation in which case the numerical bottleneck of AI-SCP is the evaluation of Gaussian averages of the AI potential energy and its derivatives. These averages are computed efficiently by the quasi-Monte Carlo method utilizing low-discrepancy sequences leading to a fast convergence with respect to the number, S, of the AI energy evaluations. Moreover, a further substantial (an-order-of-magnitude) improvement in efficiency is achieved once a numerically cheap approximation of the AI potential is available. This is based on using a perturbation theory-like (the two-grid) approach in which it is the average of the difference between the AI and the approximate potential that is computed. The corresponding codes and scripts are provided. © 2023 Author(s)."
"The purpose of the study was to develop a priori perspectives and practical experience of implementing digital marketing strategies of medical tourism subjects. At the same time, communication systems (channels, means, tools) were subject to analysis and digital systems and technologies for providing medical services (techniques, AI-based technologies, etc.) and facility management. The system of digital communications, the nodal channels of the medical product, which are informationally connected in the distribution system, are defined: B2B (medical and tourist companies), B2C (product implementation through other distribution channels), C2C (communication between consumers, in particular opinion leaders, people with successful medical results, etc.). The considered direct distribution channels (B2C) are key drivers of medical product strategy, which should become a priority in the development of state programs for the development of tourism and support for medicine and investment goals for the development and implementation of medical innovations. © 2023 by IGI Global. All rights reserved."
It handles the pipeline issue in the oil and gas industry and expects the issue events and their answer for change the insufficiency comparatively as restoring the beginner stage of pipeline disappointment information in the model. Applications involving AI in oil and gas draw in PCs to rapidly and conclusively take a gander at enormous extents of information. Therefore to perceive the kind of wickedness that happens and change them considerably more effectively in the business issue correction requires a lot of extent of time and exertion and this will reduce the expense and time sufficiency in the oil and gas creation ventures. © 2023 Author(s).
"Computational Linguistics (CL) associated with the Internet of Multimedia Things (IoMT)-enabled multimedia computing applications brings several research challenges, such as real-time speech understanding, deep fake video detection, emotion recognition, home automation, and so on. Due to the emergence of machine translation, CL solutions have increased tremendously for different natural language processing (NLP) applications. Nowadays, NLP-enabled IoMT is essential for its success. Sarcasm detection, a recently emerging artificial intelligence (AI) and NLP task, aims at discovering sarcastic, ironic, and metaphoric information implied in texts that are generated in the IoMT. It has drawn much attention from the AI and IoMT research community. The advance of sarcasm detection and NLP techniques will provide a cost-effective, intelligent way to work together with machine devices and high-level human-to-device interactions. However, existing sarcasm detection approaches neglect the hidden stance behind texts, thus insufficient to exploit the full potential of the task. Indeed, the stance, i.e., whether the author of a text is in favor of, against, or neutral toward the proposition or target talked in the text, largely determines the text's actual sarcasm orientation. To fill the gap, in this research, we propose a new task: stance-level sarcasm detection (SLSD), where the goal is to uncover the author's latent stance and based on it to identify the sarcasm polarity expressed in the text. We then propose an integral framework, which consists of Bidirectional Encoder Representations from Transformers (BERT) and a novel stance-centered graph attention networks (SCGAT). Specifically, BERT is used to capture the sentence representation, and SCGAT is designed to capture the stance information on specific target. Extensive experiments are conducted on a Chinese sarcasm sentiment dataset we created and the SemEval-2018 Task 3 English sarcasm dataset. The experimental results prove the effectiveness of the SCGAT framework over state-of-the-art baselines by a large margin.  © 2023 Association for Computing Machinery."
"This paper introduces a new diagnostic system for Autism Spectrum Disorder (ASD) using explainable artificial intelligence (AI). The goal is to develop a reliable and interpretable tool that helps healthcare professionals accurately identify individuals with ASD. The study follows a systematic methodology involving comprehensive data collection, feature engineering, and advanced machine learning algorithms, such as decision trees and support vector machines. By analyzing various patient data, including behavioral observations and medical history, the system identifies important features and patterns associated with ASD. The diagnostic system achieves promising results, with the decision tree model achieving an accuracy of 85% and the support vector machine model achieving 86%. These outcomes demonstrate the potential of the system to accurately identify ASD cases. The clinical relevance and practical implications of the diagnostic system are discussed, emphasizing its ability to enhance the accuracy and efficiency of ASD diagnoses. The paper also identifies limitations and proposes future enhancements, including expanding datasets to cover a wider age range and demographic factors, incorporating additional relevant features such as genetic markers and neuroimaging data, exploring alternative machine learning algorithms, and further advancing explainable AI techniques. Real-world validation and feedback from clinicians and caregivers are crucial for refining the system. Ultimately, this research aims to contribute to timely interventions and improved outcomes for individuals with ASD, providing valuable insights for clinicians, caregivers, and researchers in addressing the challenges of ASD diagnosis. © 2023, Ismail Saritas. All rights reserved."
"Due to popular successes (e.g., ChatGPT) Artificial Intelligence (AI) is on everyone's lips today. When advances in biotechnology are combined with advances in AI unprecedented new potential solutions become available. This can help with many global problems and contribute to important Sustainability Development Goals. Current examples include Food Security, Health and Well-being, Clean Water, Clean Energy, Responsible Consumption and Production, Climate Action, Life below Water, or protect, restore and promote sustainable use of terrestrial ecosystems, sustainably manage forests, combat desertification, and halt and reverse land degradation and halt biodiversity loss. AI is ubiquitous in the life sciences today. Topics include a wide range from machine learning and Big Data analytics, knowledge discovery and data mining, biomedical ontologies, knowledge-based reasoning, natural language processing, decision support and reasoning under uncertainty, temporal and spatial representation and inference, and methodological aspects of explainable AI (XAI) with applications of biotechnology. In this pre-Editorial paper, we provide an overview of open research issues and challenges for each of the topics addressed in this special issue. Potential authors can directly use this as a guideline for developing their paper. © 2023 The Authors"
"Preterm birth (PTB) is defined as delivery occurring before 37 weeks of gestation. In this paper, Artificial Intelligence (AI)-based predictive models are adapted to accurately estimate the probability of PTB. In doing so, pregnant women' objective results and variables extracted from the screening procedure in combination with demographics, medical history, social history, and other medical data are used. A dataset consisting of 375 pregnant women is used and a number of alternative Machine Learning (ML) algorithms are applied to predict PTB. The ensemble voting model produced the best results across all performance metrics with an area under the curve (ROC-AUC) of approximately 0.84 and a precision-recall curve (PR-AUC) of approximately 0.73. An attempt to provide clinicians with an explanation of the prediction is performed to increase trustworthiness. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Artificial intelligence-based (AI-based) network slicing bandwidth allocation enables the 5G/6G service providers to create multiple virtual networks atop a shared physical infrastructure while fulfilling varying end-user demands. Some researchers argue that AI-enable network may run the danger of having private information compromised. We still need a backup rule-based methodology to allocate bandwidth resource to each slice, if the AI-based method suddenly encounters security issues. To design such a rule-based methodology, this study attempts to answer two questions: (1) Is the network slicing bandwidth allocation problem the nondeterministic polynomial-time completeness (NP-completeness)? (2) Is there a heuristic methodology without any training process, which has equivalent performance compared to the AI-based methodology? This study first proves the classical network slicing bandwidth allocation problems is NP-completeness. This shows that the designed heuristic method is inescapably suboptimal to the network slicing bandwidth allocation problem. Secondly, this study proposes the Adaptive Hungarian Algorithm (AHA), which outperforms previous AI-empowered method and does not need any training process. The experiments demonstrate that AHA reached 93%–97% of the maximal system throughput by brute-and-force algorithm, compared to other methodologies only having at most 93% of the maximal system throughput. This also indicates that AHA is capable to solve the network slicing bandwidth allocation problem, if the telecommunication operators do not have sufficient sample complexity to train an AI model. © 2023 John Wiley & Sons Ltd."
"Automated machine learning has the ability to improve the efficiency of time series classification due to the ability to combine multiple feature extraction methods and classification models. In the paper, we propose a flexible AutoML approach that combines multiple feature generation strategies (spectral, wavelet, topological, quantile) and classifiers as parts of the modeling pipeline. It allows obtaining a more robust and lightweight solution for the time series classification problem. Generation of the pipeline is based on an evolutionary algorithm. Comparison with approaches with the highest results was conducted on the UEA/UCR archive for modeling quality analysis. The proposed approach allows, on the one hand, to solve the task of time series classification automatically and could be used as part of an industrial data processing pipeline. On the other hand, it could be used as a tool for modeling and studying natural process properties described as time series. © 2023 Elsevier B.V."
"IT solutions have been transforming the world of work, albeit with varying intensity, for decades. They affect, among other things, the organisation of work, work structures, employment patterns and working time. Solutions such as artificial intelligence (AI), business intelligence (BI), robotic process automation (RPA), workflow, process mining, etc. are now a daily reality in the corporate sector, be it physical or mental work. However, the spread of digital solutions across organisations, spaces and sectors is uneven. Can the impact of this uneven spread be observed in employment, especially in atypical forms of employment? In our questionnaire research, we seek to establish how employees with tertiary education in two pairs of sectors (Hungarian and Romanian non-profit sectors and Hungarian non-profit and for-profit sectors) in two countries perceive some demographic characteristics of teleworking. In our research, we analysed the responses of Hungarian and Romanian working-age citizens with tertiary education who had been working in telework for at least one year at the time of completing the questionnaire. In addition to the deterioration of work relationships and the lack of personal contacts already shown by many studies, it is observed that the work-life balance is upset for single people and those without children, while the number of children does not influence opinions on teleworking. The differences observed and presented in the study are typically not gender-specific but largely based on globalised perspectives. © 2023 Katalin Lipták, Erika Horváthné Csolák & Zoltán Musinszki, and the Centre of Sociological Research, Poland."
"Over the last decade, the explosion of 'Big Data' and its fusion with AI has led many to believe that the development and integration of AI systems in healthcare will usher in a transformative revolution that democratises access to high quality healthcare and collectively improve patient outcomes. However, the nature of market forces in the evolving data economy, has started to show evidence that the opposite is more likely to be true. This paper argues that there is a poorly understood 'Inverse Data Law' that will exacerbate the widening health divide between affluent and marginalised communities because: (1) data used to train AI systems favour individuals that are already engaged with healthcare, who have the lowest burden of disease, but the highest purchasing power; and (2) data used to drive market decisions around investment in AI health technology favours tools that increase the commodification of healthcare through over-testing, over-diagnosis, and the acute and episodic management of disease, over tools that support the patient to prevent disease. This dangerous combination is more likely to cripple efforts towards preventative medicine, as data collection and utilisation tends to be inversely proportional to the needs of the patients served - the inverse data law. The paper concludes by introducing important methodological considerations in the design and evaluation of AI systems to promote systems improvement for marginalised users. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"In the wake of the industrial revolution, the supply chain has been subjected to a linear evolution process over the course of the past four decades. However, companies today are operating in a new world, which presents new challenges. The digital revolution, which is characterized by innovations such social, news, events, and weather (SNEW) data, demand sensing, artificial intelligence (AI), machine learning (ML), amongst other things. Autonomous IBP involves a closed-loop continuous process, in which various internal and external stakeholders come together in a formal structured process to create an integrated company game plan. It has the ability to ingest real-time signals (from the digital edge via SNEW data and competitor promotions, as well as from physical assets like smartphones, sensors, radars, and satellites), provide predictive visibility to warn or sense a disruption before it happens, and then leverage prescriptive analytics to manage the unpredictable through risk mitigation plans. © 2023 by IGI Global. All rights reserved."
"The advent of technology backed by digitalization is bringing innovative and new ways in imparting higher education. The high speed of the internet readily available in the higher education institutions is further supporting such activities. Especially post-COVID-19, parents who used to keep an eye on children towards usage of smartphones began to provide such devices themselves to children for educational purposes. The AI-supported technology facilitated the process by creating artificial environment to understand the real-world. It provides a shield to adversities of the real situations. With the further advancement in the emerging field of metaverse, learners can view themselves as one of the participants on screen in the form of an avatar. Since the students are not aware of the challenges and limitations of such emerging areas, the responsibility of educationists is on the rise. The researchers have tried to review the scope of higher education resting on metaverse as one of the major aspects in the near future. © 2023, IGI Global. All rights reserved."
"The definition of the metaverse, a virtual space where people can interact with each other and computer-generated environments, is gaining traction. With the help of artificial intelligence (AI) and virtual reality (VR) technologies, the metaverse is being transformed into a highly immersive and interactive experience. This chapter explores the potential of the metaverse as a new form of tourism. By examining current trends in the development of AI and VR technologies, the authors explore the possibilities of creating highly realistic and personalized travel experiences that can be accessed from anywhere in the world. They also discuss the potential and benefits of the metaverse as a new tourism industry and the concerns that must be addressed in its development. Ultimately, this chapter argues that the metaverse has the potential to revolutionize the tourism industry by offering innovative and immersive experiences that will attract a new generation of travellers. © 2023, IGI Global. All rights reserved."
"The ethical implications and regulatory requirements of AI applications and decision support systems are generally the subjects of interdisciplinary research. Case studies are a suitable means to prepare AI applications and clinical decision support systems for research. This paper proposes an approach that describes a procedure model and a categorization of the contents of cases for socio-technical systems. The developed methodology was applied to three cases and serve the researchers in the DESIREE research project as a basis for qualitative research and for ethical, social, and regulatory analyses. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"As the use of artificial intelligence within healthcare is on the rise, an increased attention has been directed towards ethical considerations. Defining fairness in machine learning is a well explored topic with an extensive literature. However, such definitions often rely on the existence of metrics on the input data and well-defined outcome measurements, while regulatory definitions use general terminology. This work aims to study fairness within AI, particularly bringing regulation and theoretical knowledge closer. The study is done via a regulatory sandbox implemented on a healthcare case, specifically ECG classification. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"UAVs or drones have found widespread use in both military and civilian purposes in recent years, and anti-drone systems have been created to reduce risks using AI and GPS systems. Drones were once intended for military use and deployed in high-risk areas; but, as technology advances, drones are becoming more inexpensive. Due to the growing demand for their use in the military, drones are expected to play an increasingly significant role in future military operations on terrorism and illegal migration. Drones are currently used for land mine detection because landmines still go undetected, increasing the death rate and wreaking havoc on the environment. They can be replaced for a low price, which makes them acceptable for risky and politically sensitive operations. The goal of this project is to use a surveillance drone built to detect landmines to detect mines. A quad copter with a mine detector is placed to the prototype. This uses two separate detecting modalities, one of which is an induction metal detector. These are widely utilized to assist with the entire operation. The GPS was used to track the mine's location, and the location was recognized and communicated using the GSM module and Aurdino. A camera is also mounted to the quad copter to monitor suspected human presence at borders, with the data being sent with the operator on a regular basis. It can be improved further to strike targets with cruise missiles, allowing the UAV to manage (Ground Control Station) the borders and troop paths. © 2023 Scrivener Publishing LLC. All rights reserved."
"Adequate privacy protection is crucial for implementing modern AI algorithms in medicine. With Fully Homomorphic Encryption (FHE), a party without access to the secret key can perform calculations and advanced analytics on encrypted data without taking part of either the input data or the results. FHE can therefore work as an enabler for situations where computations are carried out by parties that are denied plain text access to sensitive data. It is a scenario often found with digital services that process personal health-related data or medical data originating from a healthcare provider, for example, when the service is delivered by a third-party service provider located in the cloud. There are practical challenges to be aware of when working with FHE. The current work aims to improve accessibility and reduce barriers to entry by providing code examples and recommendations to aid developers working with health data in developing FHE-based applications. HEIDA is available on the GitHub repository: https://github.com/rickardbrannvall/HEIDA. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"AI technology has made several significant advances, particularly in the areas of medical diagnosis, self-driving vehicles, face recognition, and many others. AI indicates massive advantages for community development, commercial pro-gress, in addition to safety improvement and human luxury. Nonetheless, the low level of explainability, data biases, data privacy, data security, and ethical concerns of AI-based technologies pose significant threats to developers, users, society, and mankind. One critical issue in AI research is recognising the ethical issues connected with AI. AI ethics is still very much in formative development, despite the fact that the term ""machine ethics""was coined in 2006 and till today AI ethics is still focusing on the need to investigate the ethics of AI and also how to develop ethical AI. The ethics of AI 1 looks at the ethical values, principles, instructions, norms, and processes associated with AI. The ethics of AI investigates ethical ideals, principles, instructions, rules, and processes related to AI technology. We may therefore develop AI with an ethical mindset by using appropriate AI ethics. Here, the researchers have examined many of most important research in the subject of AI ethics. In addition, the researchers have also identified the major concerns in this research and proposed strategies to overcome them. Over the course of this research, it is believed that the subject of AI ethics will be further explored by researchers in order to find cost-effective and feasible solutions. © 2023 Author(s)."
"The study on the impacts of climate variability on agricultural phenologydelves into the exploration of climate variability's influence on agricultural phenology through the synergistic utilization of geoinformatics, satellite agrometeorology, and AI techniques. Geoinformatics serves the purpose of identifying vulnerable locations, while satellite agrometeorology furnishes indispensable weather data crucial for crop production. By employing AI techniques to analyze extensive datasets, valuable patterns in crop phenology can be discerned, leading to significant insights into crop reactions to climate change. The integration of these methodologies enables researchers to develop a comprehensive comprehension of how climate variability impacts crop phenology, thereby facilitating the formulation of adaptation plans by policymakers and farmers. Ultimately, this research contributes to the promotion of sustainable farming practices and the enhancement of food security amidst the challenges posed by climate change. © 2023, Ismail Saritas. All rights reserved."
"Banks cannot afford to be complacent in their operations. Due to the dramatic changes brought about by improvements in computer technology (IT) as well as competitive intensity from FinTech businesses, they must re-evaluate their competing advantages. A major point of emphasis in this essay is that banks must not abandon relationship banking that encourages direct interaction with bank clients. Orienting relationship banking on the long term simplifies incentives while also supporting bank clients ' long-term requirements and objectives. However, because to the availability of IT-driven economy of scale as well as rivalry by FinTech start-ups and IT corporations, banks may be tempted to enter the transaction banking business. In this context, the paper evaluates the importance of distance, AI, and behavioural inclinations in the decision-making process. The suggestions for banking stability are analyzed in detail. The authors believe that relationship banking has the potential to reduce its disadvantages, but it should conform to the newer facts to flourish. © 2023, IGI Global. All rights reserved."
"In recent years, ""Data Fabric"" has become new analytic buzzword in Data Management agility where it has become a high priority in booming industries where they have an environment that is more complicated, scattered, and diversified. Data analytics experts began exploring beyond conventional data management techniques and shifted toward contemporary solutions like AI-enabled data integration in order to decrease human errors and total costs. Data fabric is a weave where it stretches spanning a wide area that connects numerous sites, various data source kinds, and accessing techniques. As it progresses through the various stages of the data fabric, the data collected from the source can be handled, processed, and stored. For a wide range of applications, the data can also be accessible by or shared with both internal and external apps. Data fabric applications' main objectives are to optimize supply chains for end-to-end products, complywith data rules, and enhance consumer engagement through more sophisticated mobile apps and interactions. Always Companies can gain a competitive edge with data, but to meet customer demands, they must supply data rapidly. Most of the enterprises implemented cloud migration and IoT, with increased cost-effective data storage and processing. Because of this data is no longer tied to local centers, and most of the data are located in different places and it is very difficult to manage [1]. A Data Fabric is a strategic solution to the enterprise to incur storage operations and leverages the best version of cloud migration. This architecture can support centrally managed, public and private clouds, IoT and other devices. This reduces management tasks through automation, accelerates the development and deployment process, and protects assets without interruption. It enables changes to be made quickly, resolving problems, managing risk, reducing IT operations and complying with regulations. In this chapter, the best open source data fabric tools that meet the enterprise requirements are listed and highlighted with its benefits and challenges. The greatest data fabric tools are profiled in one location, which makes it easy for researchers to choose the tool throughout their search. Data categorization and discovery, data quality and profiling, data lineage and governance, and data exploration and integration are the four main functions offered by the data fabric technologies. These data collaboration platforms combine data integration with business applications. Atlan, Cinchy, Data.world, Denodo, IBM, K2 View are few open source tools that are used by enterprise to manage their data and its integration. There are wide ranges of Open Source Data fabric tools that are quick to list its benefits. Instead of using proprietary systems, the majority of firms are interested in open source solutions due to lower costs. The capacity to modify and offer creative solutions on the code to satisfy business objectives is another crucial advantage of working with open source proponents. However, in this chapter we discuss about the Key features, benefits and technical challenges of different open source data integration tools in detail. The primary challenges in the utilizing open source data tool in enterprise is they lack in community support. In general The IT departments of many businesses rely on vendor support to enhance their internal capabilities [2]. Having open source tools, make the enterprise to face and resolve the issues by their own, which is very hard. When developing a data management environment, technology teams frequently underestimate the amount of time and expertise required to properly employ open source software. Most of the organizations they frequently underestimate the amount of work necessary to integrate open source with other subsystems and, as a result, incorrectly evaluate the total cost of ownership of employing open source systems. Most businesses meet few significant obstacles when working on open source pilot projects, but they may run into problems when attempting to manage and maintain those deployments on a wide scale. © 2023 Walter de Gruyter GmbH, Berlin/Boston. All rights reserved."
[No abstract available]
"Recent news suggests that the advent of AI-generated coding tools signal the end of humans programming. This news should not, however, suggest that students not learn how to program but instead that instructors rethink how they teach programming. Math education has already addressed the challenge of teaching fluency when there is technology for basic tasks by having students use multiple representations, different approaches, and explanations of others' work to emphasize problem-solving, critical thinking, and communication while still teaching basic skills. These approaches can also be applied to computer science education, especially in an introductory course, and with the same benefits.  © 2023 ACM."
"Our vision is to achieve societally responsible secure and trustworthy cyberspace that puts algorithmic and technological checks and balances on the indiscriminate sharing and analysis of data. We achieve this vision in a holistic manner by framing research directions with four major considerations: (i) Expanding knowledge and understanding of security and privacy perceptions and expectations in vulnerable groups, which significantly contribute to their unwillingness to share data, and use that knowledge to drive research in (a) mitigating missing/imbalanced data problems, (b) understanding and modeling security and privacy risks of data sharing, and (c) modeling utility of data sharing. (ii) Developing a risk-adaptive, policy model capable of capturing and articulating security and privacy expectations of users that are relevant in a particular context and develops associated technology to ensure provenance and accountability. (iii) Developing robust AI/ML algorithms that are transparent and explainable with respect to fairness and bias to reduce/eliminate discrimination, misuse, privacy violations, or other cyber-crimes. (iv) Developing models and techniques for a nuanced, contextually adaptive, and graded privacy paradigm that allows trade-offs between privacy and utility. Towards this, in this paper we present the SAFE-PASS framework to provide Stewardship, Advocacy, Fairness and Empowerment in Privacy, Accountability, Security, and Safety for Vulnerable Groups. © 2023 Owner/Author."
"An overall review of the studies on the carmina of the African provinces is presented, which in recent years have experienced growing attention from philologists (less from the epigraphists). It turned out that a number of texts lacked a correct epigraphic edition based on direct access to and autopsy of the documents, because many editors had concentrated on traditional philological analysis, without placing the finds in the archaeological sites of origin and historical contexts. The excavations in progress and the discoveries made at Pheradi Maius, Sidi Mohamed Lazrag at Simitthus, Uchi Maius, Thignica, Ammaedara, Thysdrus, Henchir Messala at the ancient Mascula in Algeria and the revision of the famous mosaic of Biserta at Bardo propose some novelties. With such an overview, it is now clear how far we still have to travel to get to the critical edition of all the carmina and to bring out the numerous unpublished inscriptions, in an area of the Roman Empire where literary training was particularly taken care of: in Roman Africa, epigraphic poetry sometimes even reflect a late archaic revival. © 2023 Walter de Gruyter GmbH, Berlin/Boston."
"Artificial intelligence (AI) is often presented as a technology that changes healthcare and is useful in clinical work in disease prediction, diagnosis, treatment effectiveness, and precision health. This study aimed to explore healthcare leaders' perceptions of the usefulness of AI applications in clinical work. The study was based on qualitative content analysis. Individual interviews were conducted with 26 healthcare leaders. The usefulness of AI applications in clinical care was described in terms of expected benefits for 1) patients as supporting individualized self-management and person-centered information support tools 2) healthcare professionals in terms of providing decision-support in diagnostics, risk assessments, treatment recommendations, warning systems, and as a new colleague supporting the clinical work, and 3) organizations as providing patient safety and decision-support in prioritizing healthcare resources in organizing healthcare. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Breast cancer in women is a type of disease that is the main cause of death in women according to world breast cancer data. Therefore, early detection of breasts is needed significantly to improve life. If a woman has been identified, then rehabilitation and treatment on an incentive basis are needed to reduce the worse. This study used a dataset collected by the University of Wisconsin Hospitals, Madison (https://atapdata.ai/). This research conducted experiments using several data mining classification strategies to predict breast cancer using machine learning algorithms. The Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Naive Bayes, Random Forest, Decision Tree, Deep Learning (H2O), and Neural Network are used to classify algorithms. From these algorithms'classification, we compare accuracy, best classification, and compare algorithm performance with curve ROC (RapidMiner Studio Core) to see which performance algorithm has the best quality for classification. From the analysis results, the deep learning algorithm with Tanh and Exprectifier activation function has a good accuracy of 93.14%, and the best classification with 89.62%. In addition, deep learning has found the best quality from the ROC curve results on the dataset used in this research. © 2023 EDP Sciences. All rights reserved."
"Artificial intelligence (AI) and regenerative medicine are becoming inextricably linked. As a result, the longterm outlook for the advancement of AI, regenerative medicine, and stem cells in the healthcare industry is incredibly bright. AI is being used in healthcare to discover new drugs and to provide personalized medicine based on big data. It can improve medical diagnosis and treatment plans. Regenerative medicine has the potential to repair damaged tissues and organs through stem cell-based regenerative medicine. Stem cell research is also undergoing significant development as a major component of the approach to regenerative medicine. In the future, it is expected that it will contribute to more personalized and more effective treatments. Integrating AI can help model and simulate cellular behavior, analyze, and process images for cell tracking and tissue engineering, and analyze large amounts of genomic and proteomic data. Together, these fields have the potential to transform healthcare and improve patient outcomes. © 2023 by IGI Global. All rights reserved."
"Next-Generation Network has a major impact on the telecommunication sector as it provides convergence of network and services. 5G networks are faster than the previous generation network and will meet the increasing demand of new consumers and business needs efficiently. NGN has become an emerging technology since it is a scalable, flexible, and more reliable network. In a very short span of time the wireless communication network has transited from 1G to 6G. As 5G networks have become more widely available, they will be a major driver of Artificial Intelligence (AI), Internet of Things (IoT), driverless vehicles, blockchain, augmented reality, and a potential breakthrough. 5G has reached unimaginable heights because of fast link, cost-effective, low latency, efficient data transmission, broader connectivity, and coverage. The aim of this article is to conduct a review of the literature and investigation of 5G requirements, network architecture, and its contributions towards its society. We also discuss the evolution and development of wireless technology through various generations, emphasizing the significance of 5G transformative networks, examining the challenges, and exploring the 5G applications in various sectors such as Healthcare, Entertainment, manufacturing industries, Automobile, Intelligent automation, and industry digitization. It also elaborates on shifting towards 6G and its system architecture. The sixth-generation (6G) wireless network will offer terrestrial, maritime networks, and airborne. Additionally, it will be extremely fast, and be able to take care of large number of devices with very low latency requirements. 6G networks would be much more heterogeneous than their predecessors, allowing for technologies such as virtual and augmented reality (VR/AR), ubiquitous instant messaging, widespread intelligence, and the internet of abilities that have yet to be seen in the mobile world. 7G should have no problem covering bandwidth or data capacity. Space roaming is at the core of the seventh generation of mobile wireless networks. It is becoming more and more necessary to reach an entirely wireless world where information can be accessed at any time and from any location, with lower costs, better quality, and access at any place. 7G will have no issues with data capacity coverage or handoff when it completes all of its phases. Users will be subject to only one demand during that period, namely, the cost of mobile phone calls and related services. © 2023 Scrivener Publishing LLC. All rights reserved."
"The evolution of clinical decision support (CDS) tools has been improved by usage of new technologies, yet there is an increased need to develop user-friendly, evidence-based, and expert-curated CDS solutions. In this paper, we show with a use-case how interdisciplinary expertise can be combined to develop CDS tool for hospital readmission prediction of heart failure patients. We also discuss how to make the tool integrated in clinical workflow by understanding end-user needs and have clinicians-in-the-loop during the different development stages. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Artificial intelligence (AI) is predicted to improve health care, increase efficiency and save time and recourses, especially in the context of emergency care where many critical decisions are made. Research shows the urgent need to develop principles and guidance to ensure ethical AI use in healthcare. This study aimed to explore healthcare professionals' perceptions of the ethical aspects of implementing an AI application to predict the mortality risk of patients in emergency departments. The analysis used an abductive qualitative content analysis based on the principles of medical ethics (autonomy, beneficence, non-maleficence, and justice), the principle of explicability, and the new principle of professional governance, that emerged from the analysis. In the analysis, two conflicts and/or considerations emerged tied to each ethical principle elucidating healthcare professionals' perceptions of the ethical aspects of implementing the AI application in emergency departments. The results were related to aspects of sharing information from the AI application, resources versus demands, providing equal care, using AI as a support system, trustworthiness to AI, AI-based knowledge, professional knowledge versus AI-based information, and conflict of interests in the healthcare system. © 2023 European Federation for Medical Informatics (EFMI) and IOS Press."
"Purpose: With the advent of ChatGPT, a sophisticated generative artificial intelligence (AI) tool, maintaining academic integrity in all educational settings has recently become a challenge for educators. This paper discusses a method and necessary strategies to confront this challenge. Design/methodology/approach: In this study, a language model was defined to achieve high accuracy in distinguishing ChatGPT-generated essays from human written essays with a particular focus on “not falsely” classifying genuinely human-written essays as AI-generated (Negative). Findings: Via support vector machine (SVM) algorithm 100% accuracy was recorded for identifying human generated essays. The author discussed the key use of Recall and F2 score for measuring classification performance and the importance of eliminating False Negatives and making sure that no actual human generated essays are incorrectly classified as AI generated. The results of the proposed model's classification algorithms were compared to those of AI-generated text detection software developed by OpenAI, GPTZero and Copyleaks. Practical implications: AI-generated essays submitted by students can be detected by teachers and educational designers using the proposed language model and machine learning (ML) classifier at a high accuracy. Human (student)-generated essays can and must be correctly identified with 100% accuracy even if the overall classification accuracy performance is slightly reduced. Originality/value: This is the first and only study that used an n-gram bag-of-words (BOWs) discrepancy language model as input for a classifier to make such prediction and compared the classification results of other AI-generated text detection software in an empirical way. © 2023, Emerald Publishing Limited."
"Objectives Artificial intelligence (AI) is increasingly tested and integrated into breast cancer screening. Still, there are unresolved issues regarding its possible ethical, social and legal impacts. Furthermore, the perspectives of different actors are lacking. This study investigates the views of breast radiologists on AI-supported mammography screening, with a focus on attitudes, perceived benefits and risks, accountability of AI use, and potential impact on the profession. Methods We conducted an online survey of Swedish breast radiologists. As early adopter of breast cancer screening, and digital technologies, Sweden is a particularly interesting case to study. The survey had different themes, including: attitudes and responsibilities pertaining to AI, and AI's impact on the profession. Responses were analysed using descriptive statistics and correlation analyses. Free texts and comments were analysed using an inductive approach. Results Overall, respondents (47/105, response rate 44.8%) were highly experienced in breast imaging and had a mixed knowledge of AI. A majority (n=38, 80.8%) were positive/somewhat positive towards integrating AI in mammography screening. Still, many considered there to be potential risks to a high/somewhat high degree (n=16, 34.1%) or were uncertain (n=16, 34.0%). Several important uncertainties were identified, such as defining liable actor(s) when AI is integrated into medical decision-making. Conclusions Swedish breast radiologists are largely positive towards integrating AI in mammography screening, but there are significant uncertainties that need to be addressed, especially regarding risks and responsibilities. The results stress the importance of understanding actor-specific and context-specific challenges to responsible implementation of AI in healthcare.  © Author(s) (or their employer(s)) 2023. Re-use permitted under CC BY. Published by BMJ."
"In India shrimp culture is very important agriculture allied sector, this field providing lot of employment to various people in different stages of its process starting from hatcheries to selling final shrimp to customers. Shrimp forming requires huge investments and majority of the times formers turning in losses due to various factors like virus attacks, climatic conditions, dynamic price drops etc. are major problem with various kinds of diseases attacks on shrimp. To solve these problems traditionally formers using various kinds of antibiotics in the pond to kill viruses and making the shrimp sustain from various dynamic climatic conditions but they are failing to handle the situation properly. Apart from that excess use of chemicals may damage health of shrimp intern damage humans who are consuming it. To have good environment inside the pond PH, Salinity, Ammonia and DO need to be controlled regularly so that shrimp in the pond will grow healthy there by former can able to produce high yields. To solve these problems, we are introducing technological solutions such as Internet of things. In this proposed chapter we are explaining how to deploy various sensors such as PH, Salinity, Ammonia and DO on Raspberry-PI with a GSM component to build wireless communication. Then corresponding properties from the water are periodically sensed and that information is sent to the former mobile number. DO (dissolved oxygen) levels are very crucial for the survival of shrimp, so to increase oxygen formers install fans inside the pond. Our model automatically turn fans on when DO levels are fallen down to certain threshold. In this system formers without visiting the pond can control it and knows about data time to time. Even former need not required visiting labs for water testing, this system will help former to maintain proper conditions inside the pond then that will help to achieve high yielding with low investment. © 2023 American Institute of Physics Inc.. All rights reserved."
"Machine-learning systems in chemistry need accurate and accessible training data. Until they get it, they won’t achieve their potential. [Figure not available: see fulltext.] © 2023, Springer Nature Limited."
"As the industrial players began starting to face the reality of the fourth industrial revolution, telecommunication industry is one of those significant business sectors required to adapt and remain to survive in such challenging situations, especially by the threats of their competitors. Artificial Intelligence (AI) plays an essential role in today's telecommunication industry. It can make predictions about decisions, examine big data points to offer solutions, and interact with customers in real-time. Many telco firms and organizations use Customer Relationship Management (CRM) to achieve effectiveness in a company's productivity and efficiency in the operational activities. This assessment revealed that an AI-driven CRM system could give a better solution for the telecommunication industry to manage relationships with externals, boost marketing and sales programs, prospect consumer data information, and record operational cases, all in one centralized location. © 2023 EDP Sciences. All rights reserved."
"Optimum human resource management and increased efficiency through automation are two of our main objectives need to achieve where the menial task of attendance taking will be done by a rover moving from class-to-class as per a fixed hourly schedule. The exact mechanism by which the rover operates is accomplished using two main concepts, viz., Voice Recognition and Computer Vision. The rover would initially use computer vision to navigate to and enter the classroom. It would then run facial recognition using the on-board camera. For the sake of redundancy and as a fallback, the rover will also initiate roll calls if necessary using its on-board devices like screen/speaker and a microphone. The need for marking students attendance in classes is of unquestionable importance. The lecturer has to take out time and perform a roll-call for this purpose, and the peon has to proceed from class-to-class with the register to collect the day attendance. This results in inefficient utilization of time by the lecturers and peons. This Automation frees them up to take care of other things, while at the same time ensuring that the attendance data is accurate. © 2023 American Institute of Physics Inc.. All rights reserved."